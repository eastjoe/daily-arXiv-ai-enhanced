<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 10]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Large Language Models are Highly Aligned with Human Ratings of Emotional Stimuli](https://arxiv.org/abs/2508.14214)
*Mattson Ogg,Chace Ashcraft,Ritwik Bose,Raphael Norman-Tenazas,Michael Wolmetz*

Main category: cs.AI

TL;DR: LLM对情绪的理解与人类相似，但存在差异，尤其在唤醒度方面。


<details>
  <summary>Details</summary>
Motivation: 了解LLM如何评价情绪化刺激，以指导LLM在日常生活中的应用。

Method: 对多个流行LLM和人类参与者进行情绪评价任务，比较其对文字和图像情绪内容的评价结果。

Result: GPT-4o在多种模式、刺激和大多数评价量表上与人类参与者的反应非常相似(许多情况下r=0.9或更高)，但唤醒度评价的吻合度较低，快乐度评价的吻合度最高。LLM在五类情绪框架下的评价结果比在二维框架下的评价结果更一致。LLM评价结果比人类评价更同质。

Conclusion: 大型语言模型(LLM)对情绪刺激的解读与人类相似，但在唤醒度评价上差异较大，五类情绪框架(快乐、愤怒、悲伤、恐惧、厌恶)比二维框架(唤醒度和效价)更贴合LLM的评价结果，且LLM评价结果比人类评价更同质。

Abstract: Emotions exert an immense influence over human behavior and cognition in both
commonplace and high-stress tasks. Discussions of whether or how to integrate
large language models (LLMs) into everyday life (e.g., acting as proxies for,
or interacting with, human agents), should be informed by an understanding of
how these tools evaluate emotionally loaded stimuli or situations. A model's
alignment with human behavior in these cases can inform the effectiveness of
LLMs for certain roles or interactions. To help build this understanding, we
elicited ratings from multiple popular LLMs for datasets of words and images
that were previously rated for their emotional content by humans. We found that
when performing the same rating tasks, GPT-4o responded very similarly to human
participants across modalities, stimuli and most rating scales (r = 0.9 or
higher in many cases). However, arousal ratings were less well aligned between
human and LLM raters, while happiness ratings were most highly aligned. Overall
LLMs aligned better within a five-category (happiness, anger, sadness, fear,
disgust) emotion framework than within a two-dimensional (arousal and valence)
organization. Finally, LLM ratings were substantially more homogenous than
human ratings. Together these results begin to describe how LLM agents
interpret emotional stimuli and highlight similarities and differences among
biological and artificial intelligence in key behavioral domains.

</details>


### [2] [Explaining Hitori Puzzles: Neurosymbolic Proof Staging for Sequential Decisions](https://arxiv.org/abs/2508.14294)
*Maria Leonor Pacheco,Fabio Somenzi,Dananjay Srinivas,Ashutosh Trivedi*

Main category: cs.AI

TL;DR: 神经符号方法结合SAT求解器和LLM解释数独解决方案，实验有效。


<details>
  <summary>Details</summary>
Motivation: 解释复杂的决策序列，数独游戏规则包含局部约束和连通性约束，前者可以用短的推理证明有效地解释，后者更适合用视觉解释。

Method: 结合SAT求解器和LLM，对数独难题的解决方案进行解释。

Result: 实现了一个辅助人类解决数独难题的工具，并提供了实验结果证明其有效性。

Conclusion: 提出了一种结合决策程序和大型语言模型 (LLM) 的神经符号方法来解释复杂的决策序列，并通过解释数独难题的解决方案来证明该方法。

Abstract: We propose a neurosymbolic approach to the explanation of complex sequences
of decisions that combines the strengths of decision procedures and Large
Language Models (LLMs). We demonstrate this approach by producing explanations
for the solutions of Hitori puzzles. The rules of Hitori include local
constraints that are effectively explained by short resolution proofs. However,
they also include a connectivity constraint that is more suitable for visual
explanations. Hence, Hitori provides an excellent testing ground for a flexible
combination of SAT solvers and LLMs. We have implemented a tool that assists
humans in solving Hitori puzzles, and we present experimental evidence of its
effectiveness.

</details>


### [3] [Automated Optimization Modeling through Expert-Guided Large Language Model Reasoning](https://arxiv.org/abs/2508.14410)
*Beinuo Yang,Qishen Zhou,Junyi Li,Xingchen Su,Simon Hu*

Main category: cs.AI

TL;DR: 新框架ORThought通过思维链推理自动化优化建模，并在复杂问题上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的优化建模方法存在标注错误率高、评估范围窄和计算效率低等问题。

Method: 提出了一种新的优化建模框架ORThought，该框架利用基于思维链的推理来自动化优化建模过程，并增强了现有数据集，引入了新的物流领域优化建模基准LogiOR。

Result: ORThought框架在复杂优化问题上取得了显著的优于现有方法的成果。

Conclusion: ORThought框架在复杂优化问题上优于现有方法，并对方法的成功因素和失败模式进行了系统分析，为未来基于LLM的优化建模研究提供了宝贵见解。

Abstract: Optimization Modeling (OM) is essential for solving complex decision-making
problems. However, the process remains time-consuming and error-prone, heavily
relying on domain experts. While Large Language Models (LLMs) show promise in
addressing these challenges through their natural language understanding and
reasoning capabilities, current approaches face three critical limitations:
high benchmark labeling error rates reaching up to 42\%, narrow evaluation
scope that only considers optimal values, and computational inefficiency due to
heavy reliance on multi-agent systems or model fine-tuning. In this work, we
first enhance existing datasets through systematic error correction and more
comprehensive annotation. Additionally, we introduce LogiOR, a new optimization
modeling benchmark from the logistics domain, containing more complex problems
with standardized annotations. Furthermore, we present ORThought, a novel
framework that leverages expert-level optimization modeling principles through
chain-of-thought reasoning to automate the OM process. Through extensive
empirical evaluation, we demonstrate that ORThought outperforms existing
approaches, including multi-agent frameworks, with particularly significant
advantages on complex optimization problems. Finally, we provide a systematic
analysis of our method, identifying critical success factors and failure modes,
providing valuable insights for future research on LLM-based optimization
modeling.

</details>


### [4] [The Agent Behavior: Model, Governance and Challenges in the AI Digital Age](https://arxiv.org/abs/2508.14415)
*Qiang Zhang,Pei Yan,Yijia Xu,Chuanpo Fu,Yong Fang,Yang Liu*

Main category: cs.AI

TL;DR: 通过分析人机行为差异，提出新模型和范式，以应对AI代理带来的安全和信任挑战。


<details>
  <summary>Details</summary>
Motivation: 解决AI代理在网络环境中与人类行为日益相似带来的信任、责任、安全和伦理挑战。

Method: 提出"网络行为生命周期"模型，分析人机行为差异，并验证模型有效性。

Result: 提出了"网络行为生命周期"模型和"人机行为差异(HABD)"模型，并通过实际案例验证了其有效性，为安全可靠的人机协作提供了理论基础和技术路线图。

Conclusion: 提出"网络行为生命周期"模型和"代理对代理(A4A)"范式，并通过"人机行为差异(HABD)"模型分析人机行为差异，以解决AI代理行为带来的信任、责任和安全挑战。

Abstract: Advancements in AI have led to agents in networked environments increasingly
mirroring human behavior, thereby blurring the boundary between artificial and
human actors in specific contexts. This shift brings about significant
challenges in trust, responsibility, ethics, security and etc. The difficulty
in supervising of agent behaviors may lead to issues such as data contamination
and unclear accountability. To address these challenges, this paper proposes
the "Network Behavior Lifecycle" model, which divides network behavior into 6
stages and systematically analyzes the behavioral differences between humans
and agents at each stage. Based on these insights, the paper further introduces
the "Agent for Agent (A4A)" paradigm and the "Human-Agent Behavioral Disparity
(HABD)" model, which examine the fundamental distinctions between human and
agent behaviors across 5 dimensions: decision mechanism, execution efficiency,
intention-behavior consistency, behavioral inertia, and irrational patterns.
The effectiveness of the model is verified through real-world cases such as red
team penetration and blue team defense. Finally, the paper discusses future
research directions in dynamic cognitive governance architecture, behavioral
disparity quantification, and meta-governance protocol stacks, aiming to
provide a theoretical foundation and technical roadmap for secure and
trustworthy human-agent collaboration.

</details>


### [5] [Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs](https://arxiv.org/abs/2508.14564)
*Luca Annese,Sabrina Patania,Silvia Serino,Tom Foulsham,Silvia Rossi,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.AI

TL;DR: 结构化示例不足以实现强大的视角转换，需要更高级的技术。


<details>
  <summary>Details</summary>
Motivation: 改进基于LLM的自主智能体的视角转换能力，尤其是在涉及主动感知、协作推理和视角转换的任务中。

Method: 使用Fast Downward规划器生成的转换后的解决方案图导出结构化示例，并将其转换为“思想-行动”示例，以改进基于LLM的智能体在ReAct框架内的性能。

Result: L型示例略微减少了澄清请求和整体行动步骤，但在需要对遮挡空间进行心理化或权衡认知行动成本的场景中，智能体仍然难以应对。

Conclusion: 结构化示例不足以实现强大的视角转换，需要显式信念跟踪、成本建模和更丰富的环境来支持基于LLM的智能体的社会化协作。

Abstract: Recent advances in large language models (LLMs) and reasoning frameworks have
opened new possibilities for improving the perspective -taking capabilities of
autonomous agents. However, tasks that involve active perception, collaborative
reasoning, and perspective taking (understanding what another agent can see or
knows) pose persistent challenges for current LLM-based systems. This study
investigates the potential of structured examples derived from transformed
solution graphs generated by the Fast Downward planner to improve the
performance of LLM-based agents within a ReAct framework. We propose a
structured solution-processing pipeline that generates three distinct
categories of examples: optimal goal paths (G-type), informative node paths
(E-type), and step-by-step optimal decision sequences contrasting alternative
actions (L-type). These solutions are further converted into ``thought-action''
examples by prompting an LLM to explicitly articulate the reasoning behind each
decision. While L-type examples slightly reduce clarification requests and
overall action steps, they do not yield consistent improvements. Agents are
successful in tasks requiring basic attentional filtering but struggle in
scenarios that required mentalising about occluded spaces or weighing the costs
of epistemic actions. These findings suggest that structured examples alone are
insufficient for robust perspective-taking, underscoring the need for explicit
belief tracking, cost modelling, and richer environments to enable socially
grounded collaboration in LLM-based agents.

</details>


### [6] [LeanGeo: Formalizing Competitional Geometry problems in Lean](https://arxiv.org/abs/2508.14644)
*Chendong Song,Zihan Wang,Frederick Pu,Haiming Wang,Xiaohan Lin,Junqi Liu,Jia Li,Zhengying Liu*

Main category: cs.AI

TL;DR: LeanGeo: 一个用于形式化和求解几何问题的Lean 4系统，及其基准LeanGeo-Bench，评估了大型语言模型的几何推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有几何求解系统难以整合，且几何证明依赖直观图表难以验证。

Method: 在Lean 4定理证明器中构建统一的形式系统LeanGeo，并创建基准LeanGeo-Bench。

Result: 评估了SOTA大型语言模型在LeanGeo-Bench上的能力和局限性，证明了自动几何推理仍需进一步发展。

Conclusion: 介绍了LeanGeo，一个用于在Lean 4定理证明器中形式化和求解竞赛级几何问题的统一形式系统，并提出了LeanGeo-Bench，一个包含IMO等来源问题的形式化几何基准。

Abstract: Geometry problems are a crucial testbed for AI reasoning capabilities. Most
existing geometry solving systems cannot express problems within a unified
framework, thus are difficult to integrate with other mathematical fields.
Besides, since most geometric proofs rely on intuitive diagrams, verifying
geometry problems is particularly challenging. To address these gaps, we
introduce LeanGeo, a unified formal system for formalizing and solving
competition-level geometry problems within the Lean 4 theorem prover. LeanGeo
features a comprehensive library of high-level geometric theorems with Lean's
foundational logic, enabling rigorous proof verification and seamless
integration with Mathlib. We also present LeanGeo-Bench, a formal geometry
benchmark in LeanGeo, comprising problems from the International Mathematical
Olympiad (IMO) and other advanced sources. Our evaluation demonstrates the
capabilities and limitations of state-of-the-art Large Language Models on this
benchmark, highlighting the need for further advancements in automated
geometric reasoning. We open source the theorem library and the benchmark of
LeanGeo at https://github.com/project-numina/LeanGeo/tree/master.

</details>


### [7] [Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent Framework with LLM and Knowledge Graph Integration](https://arxiv.org/abs/2508.14654)
*Peilin Ji,Xiao Xue,Simeng Wang,Wenhao Yan*

Main category: cs.AI

TL;DR: 针对极端城市降雨事件的应急调度，提出一种新的分层多智能体框架H-J，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在统一框架内协调感知、全局优化和多智能体协调，尤其在应对极端城市降雨事件的应急调度系统中面临挑战。

Method: 提出了一种分层多智能体框架H-J，该框架集成了知识引导提示、熵约束生成和反馈驱动优化。

Result: 实验结果表明，H-J框架在三种代表性条件下（极端降雨、间歇性阵雨和日常小雨）均优于基线方法。

Conclusion: H-J框架在交通顺畅度、任务成功率和系统鲁棒性方面优于基于规则和强化学习的基线方法，证明了基于知识约束的LLM方法在增强城市洪水响应能力方面的潜力。

Abstract: In recent years, the increasing frequency of extreme urban rainfall events
has posed significant challenges to emergency scheduling systems. Urban
flooding often leads to severe traffic congestion and service disruptions,
threatening public safety and mobility. However, effective decision making
remains hindered by three key challenges: (1) managing trade-offs among
competing goals (e.g., traffic flow, task completion, and risk mitigation)
requires dynamic, context-aware strategies; (2) rapidly evolving environmental
conditions render static rules inadequate; and (3) LLM-generated strategies
frequently suffer from semantic instability and execution inconsistency.
Existing methods fail to align perception, global optimization, and multi-agent
coordination within a unified framework. To tackle these challenges, we
introduce H-J, a hierarchical multi-agent framework that integrates
knowledge-guided prompting, entropy-constrained generation, and feedback-driven
optimization. The framework establishes a closed-loop pipeline spanning from
multi-source perception to strategic execution and continuous refinement. We
evaluate H-J on real-world urban topology and rainfall data under three
representative conditions: extreme rainfall, intermittent bursts, and daily
light rain. Experiments show that H-J outperforms rule-based and
reinforcement-learning baselines in traffic smoothness, task success rate, and
system robustness. These findings highlight the promise of uncertainty-aware,
knowledge-constrained LLM-based approaches for enhancing resilience in urban
flood response.

</details>


### [8] [MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers](https://arxiv.org/abs/2508.14704)
*Ziyang Luo,Zhiqi Shen,Wenzhuo Yang,Zirui Zhao,Prathyusha Jwalapuram,Amrita Saha,Doyen Sahoo,Silvio Savarese,Caiming Xiong,Junnan Li*

Main category: cs.AI

TL;DR: 新的基准测试 MCP-Universe 用于评估 LLM 与真实世界 MCP 服务器交互的性能，结果表明即使是顶尖模型也存在显著的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试过于简单，无法反映真实应用场景的复杂性。

Method: 提出一个名为 MCP-Universe 的综合基准，包含 6 个核心领域，涵盖 11 个不同的 MCP 服务器，并使用基于执行的评估器进行评估。

Result: 即使是 SOTA 模型如 GPT-5、Grok-4 和 Claude-4.0-Sonnet 也在 MCP-Universe 基准测试中表现出显著的性能限制，长上下文和未知工具是主要的挑战。

Conclusion: 现有基准测试过于简单，无法捕捉真实应用挑战，MCP-Universe 旨在通过与真实世界 MCP 服务器交互，在现实且困难的任务中评估大型语言模型 (LLM)。即使是 SOTA 模型也表现出明显的性能限制，并且存在长上下文和未知工具的挑战。

Abstract: The Model Context Protocol has emerged as a transformative standard for
connecting large language models to external data sources and tools, rapidly
gaining adoption across major AI providers and development platforms. However,
existing benchmarks are overly simplistic and fail to capture real application
challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To
address this critical gap, we introduce MCP-Universe, the first comprehensive
benchmark specifically designed to evaluate LLMs in realistic and hard tasks
through interaction with real-world MCP servers. Our benchmark encompasses 6
core domains spanning 11 different MCP servers: Location Navigation, Repository
Management, Financial Analysis, 3D Design, Browser Automation, and Web
Searching. To ensure rigorous evaluation, we implement execution-based
evaluators, including format evaluators for agent format compliance, static
evaluators for time-invariant content matching, and dynamic evaluators that
automatically retrieve real-time ground truth for temporally sensitive tasks.
Through extensive evaluation of leading LLMs, we find that even SOTA models
such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit
significant performance limitations. In addition, our benchmark poses a
significant long-context challenge for LLM agents, as the number of input
tokens increases rapidly with the number of interaction steps. Moreover, it
introduces an unknown-tools challenge, as LLM agents often lack familiarity
with the precise usage of the MCP servers. Notably, enterprise-level agents
like Cursor cannot achieve better performance than standard ReAct frameworks.
Beyond evaluation, we open-source our extensible evaluation framework with UI
support, enabling researchers and practitioners to seamlessly integrate new
agents and MCP servers while fostering innovation in the rapidly evolving MCP
ecosystem.

</details>


### [9] [Data-Driven Probabilistic Evaluation of Logic Properties with PAC-Confidence on Mealy Machines](https://arxiv.org/abs/2508.14710)
*Swantje Plambeck,Ali Salamati,Eyke Huellermeier,Goerschwin Fey*

Main category: cs.AI

TL;DR: 数据驱动方法，PAC学习范式，Mealy机，安全概率，主动学习，案例验证


<details>
  <summary>Details</summary>
Motivation: 针对CPS建模困难的问题，提出了一种数据驱动的方法来确定系统的安全概率，该方法能够在有限的时间范围内提供具有置信度的概率估计。

Method: 提出了一种基于PAC学习范式的数据驱动方法，该方法结合了离散逻辑和概率可达性分析，并采用主动学习策略，通过引导式采样新的学习数据来提高学习效率。

Result: 验证了该方法在自动车道保持系统案例中的有效性，并证明了该方法能够有效地估计CPS的安全概率。

Conclusion: 提出了一种基于PAC学习范式的数据驱动方法，用于确定Mealy机表示的CPS在有限时间范围内的安全概率，并通过自动车道保持系统案例进行了验证。

Abstract: Cyber-Physical Systems (CPS) are complex systems that require powerful models
for tasks like verification, diagnosis, or debugging. Often, suitable models
are not available and manual extraction is difficult. Data-driven approaches
then provide a solution to, e.g., diagnosis tasks and verification problems
based on data collected from the system. In this paper, we consider CPS with a
discrete abstraction in the form of a Mealy machine. We propose a data-driven
approach to determine the safety probability of the system on a finite horizon
of n time steps. The approach is based on the Probably Approximately Correct
(PAC) learning paradigm. Thus, we elaborate a connection between discrete logic
and probabilistic reachability analysis of systems, especially providing an
additional confidence on the determined probability. The learning process
follows an active learning paradigm, where new learning data is sampled in a
guided way after an initial learning set is collected. We validate the approach
with a case study on an automated lane-keeping system.

</details>


### [10] [Privileged Self-Access Matters for Introspection in AI](https://arxiv.org/abs/2508.14802)
*Siyuan Song,Harvey Lederman,Jennifer Hu,Kyle Mahowald*

Main category: cs.AI

TL;DR: AI内省定义应更严格，实验表明LLM的内省能力有限。


<details>
  <summary>Details</summary>
Motivation: 探索AI模型内省能力的定义和验证方法。

Method: 实验，使用LLM推理其内部温度参数。

Result: LLM表现出轻量级内省，但不符合更严格的内省定义。

Conclusion: 大型语言模型（LLM）似乎具有轻量级内省能力，但在更严格的定义下则不然。

Abstract: Whether AI models can introspect is an increasingly important practical
question. But there is no consensus on how introspection is to be defined.
Beginning from a recently proposed ''lightweight'' definition, we argue instead
for a thicker one. According to our proposal, introspection in AI is any
process which yields information about internal states through a process more
reliable than one with equal or lower computational cost available to a third
party. Using experiments where LLMs reason about their internal temperature
parameters, we show they can appear to have lightweight introspection while
failing to meaningfully introspect per our proposed definition.

</details>
