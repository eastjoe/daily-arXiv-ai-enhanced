<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness](https://arxiv.org/abs/2509.13332)
*Pratik Jayarao,Himanshu Gupta,Neeraj Varshney,Chaitanya Dwivedi*

Main category: cs.AI

TL;DR: 小型Qwen模型评估中，'思考型'LLM在准确性、效率和鲁棒性方面均优于'非思考型'LLM，即使后者使用了各种增强策略。


<details>
  <summary>Details</summary>
Motivation: 研究LLM作为评判者的可靠性、效率和鲁棒性，比较“思考型”和“非思考型”LLM的性能。

Method: 在RewardBench任务上比较不同大小的Qwen模型（0.6B、1.7B和4B参数）的准确性和计算效率，并评估非思考型模型的增强策略（上下文学习、规则引导、参考评估和n-best聚合）。

Result: “思考型”模型准确率高约10个百分点，计算开销低于2倍；增强策略效果有限，代价高昂（>8倍）。“思考型”模型在各种偏差条件下也更一致（平均高6%）。多语言实验结果也支持这一结论。

Conclusion: “思考型”LLM在LLM作为评判者的范式中具有显著优势，体现在准确性、效率和鲁棒性方面。

Abstract: As Large Language Models (LLMs) are increasingly adopted as automated judges
in benchmarking and reward modeling, ensuring their reliability, efficiency,
and robustness has become critical. In this work, we present a systematic
comparison of "thinking" and "non-thinking" LLMs in the LLM-as-a-judge paradigm
using open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B
parameters). We evaluate both accuracy and computational efficiency (FLOPs) on
RewardBench tasks, and further examine augmentation strategies for non-thinking
models, including in-context learning, rubric-guided judging, reference-based
evaluation, and n-best aggregation. Our results show that despite these
enhancements, non-thinking models generally fall short of their thinking
counterparts. Our results show that thinking models achieve approximately 10%
points higher accuracy with little overhead (under 2x), in contrast to
augmentation strategies like few-shot learning, which deliver modest gains at a
higher cost (>8x). Bias and robustness analyses further demonstrate that
thinking models maintain significantly greater consistency under a variety of
bias conditions such as positional, bandwagon, identity, diversity, and random
biases (6% higher on average). We further extend our experiments to the
multilingual setting and our results confirm that explicit reasoning extends
its benefits beyond English. Overall, our work results in several important
findings that provide systematic evidence that explicit reasoning offers clear
advantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency
but also in robustness.

</details>


### [2] [Evaluation Awareness Scales Predictably in Open-Weights Large Language Models](https://arxiv.org/abs/2509.13333)
*Maheep Chaudhary,Ian Su,Nikhil Hooda,Nishith Shankar,Julia Tan,Kevin Zhu,Ashwinee Panda,Ryan Lagasse,Vasu Sharma*

Main category: cs.AI

TL;DR: 大型语言模型(LLM) 会根据评估和部署环境的不同而改变行为，这种现象被称为“评估感知”，会影响AI安全评估。研究发现评估感知能力会随着模型参数规模的增大而增强，呈现幂律关系。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型的安全性，LLM的评估感知能力会影响评估结果。

Method: 使用线性探测方法研究了不同规模的15个LLM模型的评估感知能力。

Result: 评估感知能力随着模型规模的增大而增强，呈现幂律关系。

Conclusion: 该研究结果可以用于预测未来更大模型的欺骗行为，并指导设计更有效的AI安全评估策略。

Abstract: Large language models (LLMs) can internally distinguish between evaluation
and deployment contexts, a behaviour known as \emph{evaluation awareness}. This
undermines AI safety evaluations, as models may conceal dangerous capabilities
during testing. Prior work demonstrated this in a single $70$B model, but the
scaling relationship across model sizes remains unknown. We investigate
evaluation awareness across $15$ models scaling from $0.27$B to $70$B
parameters from four families using linear probing on steering vector
activations. Our results reveal a clear power-law scaling: evaluation awareness
increases predictably with model size. This scaling law enables forecasting
deceptive behavior in future larger models and guides the design of scale-aware
evaluation strategies for AI safety. A link to the implementation of this paper
can be found at
https://anonymous.4open.science/r/evaluation-awareness-scaling-laws/README.md.

</details>


### [3] [FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness](https://arxiv.org/abs/2509.13334)
*Anand Swaroop,Akshat Nallani,Saksham Uboweja,Adiliia Uzdenova,Michael Nguyen,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma,Maheep Chaudhary*

Main category: cs.AI

TL;DR: FRIT方法通过干预训练提高大型语言模型的推理可靠性，在GSM8K数据集上将Mistral模型的忠实推理提高了3.4个百分点，准确率提高了7.6个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有的链式思维推理方法容易产生不可靠的输出，FRIT旨在提高推理的因果一致性。

Method: 干预训练：通过系统性地破坏模型生成的链式思维步骤，创建忠实/不忠实的推理路径对，然后使用直接偏好优化方法训练模型偏好因果一致的推理路径。

Result: 在Qwen3-8B和Mistral-7B-v0.1模型上，FRIT提高了模型的忠实推理和准确率。

Conclusion: FRIT提供了一种可扩展的、无需监督的方法，用于训练大型语言模型生成更可靠和可解释的推理。

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving
large language model performance on complex tasks, but recent work shows that
reasoning steps often fail to causally influence the final answer, creating
brittle and untrustworthy outputs. Prior approaches focus primarily on
measuring faithfulness, while methods for systematically improving it remain
limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a
scalable alignment method that trains models to produce causally consistent
reasoning by learning from systematically corrupted examples. FRIT generates
synthetic training data by intervening on individual reasoning steps in
model-generated CoTs, creating faithful/unfaithful pairs that highlight when
reasoning breaks down. We then apply Direct Preference Optimization to teach
models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B
and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases
faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while
improving accuracy by $7.6$ percentage points. Our approach provides the first
scalable, supervision-free method for training language models to produce more
reliable and interpretable reasoning, addressing a critical gap between
reasoning performance and trustworthiness. We release our code at
\href{https://github.com/Anut-py/frit}.

</details>


### [4] [Position: AI Safety Must Embrace an Antifragile Perspective](https://arxiv.org/abs/2509.13339)
*Ming Jin,Hyunin Lee*

Main category: cs.AI

TL;DR: 该论文主张AI安全研究应采取韧性方法，以应对罕见事件和环境变化。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全测试方法静态且不足以应对长期安全挑战。

Method: 分析静态测试的局限性，并探讨韧性方法的潜力。

Result: 提出了一种新的AI安全评估方法，强调长期可靠性和对未来不确定性的适应能力。

Conclusion: 呼吁重新校准AI安全评估方法，促进韧性AI安全社区的建立。

Abstract: This position paper contends that modern AI research must adopt an
antifragile perspective on safety -- one in which the system's capacity to
guarantee long-term AI safety such as handling rare or out-of-distribution
(OOD) events expands over time. Conventional static benchmarks and single-shot
robustness tests overlook the reality that environments evolve and that models,
if left unchallenged, can drift into maladaptation (e.g., reward hacking,
over-optimization, or atrophy of broader capabilities). We argue that an
antifragile approach -- Rather than striving to rapidly reduce current
uncertainties, the emphasis is on leveraging those uncertainties to better
prepare for potentially greater, more unpredictable uncertainties in the future
-- is pivotal for the long-term reliability of open-ended ML systems. In this
position paper, we first identify key limitations of static testing, including
scenario diversity, reward hacking, and over-alignment. We then explore the
potential of antifragile solutions to manage rare events. Crucially, we
advocate for a fundamental recalibration of the methods used to measure,
benchmark, and continually improve AI safety over the long term, complementing
existing robustness approaches by providing ethical and practical guidelines
towards fostering an antifragile AI safety community.

</details>


### [5] [Imagined Autocurricula](https://arxiv.org/abs/2509.13341)
*Ahmet H. Güzel,Matthew Thomas Jackson,Jarek Luca Liesen,Tim Rocktäschel,Jakob Nicolaus Foerster,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.AI

TL;DR: 利用世界模型生成环境训练强化学习智能体，并提出一种名为IMAC的新方法，通过无监督环境设计自动生成训练课程，实现在少量数据上训练后，智能体能够很好地泛化到新的环境中。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习训练方法需要大量数据或精确的模拟环境，这在现实世界中常常难以获得。世界模型可以利用离线被动收集的数据生成多种多样的训练环境。

Method: 提出一种名为IMAC(Imagined Autocurricula)的新方法，利用无监督环境设计(UED)自动生成训练课程，在世界模型生成的想象环境中训练强化学习智能体。

Result: 在一系列具有挑战性的、程序生成的实验环境中，该方法取得了良好的迁移性能，即使只在较窄的数据集训练的世界模型中进行训练，也能够很好地泛化到新的环境中。

Conclusion: 该研究为利用大规模基础世界模型训练通用的、强大的智能体开辟了道路。

Abstract: Training agents to act in embodied environments typically requires vast
training data or access to accurate simulation, neither of which exists for
many cases in the real world. Instead, world models are emerging as an
alternative leveraging offline, passively collected data, they make it possible
to generate diverse worlds for training agents in simulation. In this work, we
harness world models to generate imagined environments to train robust agents
capable of generalizing to novel task variations. One of the challenges in
doing this is ensuring the agent trains on useful generated data. We thus
propose a novel approach, IMAC (Imagined Autocurricula), leveraging
Unsupervised Environment Design (UED), which induces an automatic curriculum
over generated worlds. In a series of challenging, procedurally generated
environments, we show it is possible to achieve strong transfer performance on
held-out environments, having trained only inside a world model learned from a
narrower dataset. We believe this opens the path to utilizing larger-scale,
foundation world models for generally capable agents.

</details>


### [6] [OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft](https://arxiv.org/abs/2509.13347)
*Zihao Wang,Muyao Li,Kaichen He,Xiangyu Wang,Zhancun Mu,Anji Liu,Yitao Liang*

Main category: cs.AI

TL;DR: 本文比较了用于视觉-语言-动作 (VLA) 模型的不同动作空间，发现最佳选择依赖于具体任务，并提出了一种新的链式动作 (CoA) 框架，该框架将高级规划和低级控制统一在一个模型中，从而提高了泛化能力和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型的动作空间选择问题尚未解决，限制了通用智能体的构建。

Method: 提出了一种新的链式动作 (CoA) 框架，该框架将抽象动作视为中间推理步骤，指导最终可执行动作的生成，并在多种动作空间上训练了一个多功能智能体。

Result: CoA框架提高了模型的稳健性和泛化能力，并在Minecraft环境中取得了最先进的结果。

Conclusion: CoA框架为构建更通用的VLA模型提供了一种新的方法，并通过开源的OpenHA套件促进了可重复性研究。

Abstract: The choice of action spaces is a critical yet unresolved challenge in
developing capable, end-to-end trainable agents. This paper first presents a
large-scale, systematic comparison of prominent abstracted action spaces and
tokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the
open-ended Minecraft. Our analysis reveals that no single action space is
universally optimal; instead, the most effective abstraction is highly
task-dependent, creating a dilemma for building generalist agents. To resolve
this, we introduce Chain of Action (CoA), a novel framework that unifies
high-level planning and low-level control within a single, monolithic VLA
model. CoA treats an abstracted action not as a command for a separate policy,
but as an intermediate reasoning step--akin to a chain of thought--that guides
the generation of the final, executable action. Furthermore, we demonstrate
that an All-in-One agent trained on a diverse mixture of action spaces using
the CoA paradigm learns a more robust and generalizable policy. This unified
agent achieves a new state-of-the-art, improving the overall task success rate
over strong, specialized baselines. To foster reproducible research, we release
the OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive
benchmark of over 800 distinct tasks, curated datasets, source code, and all
pretrained model checkpoints at https://github.com/CraftJarvis/OpenHA

</details>


### [7] [Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning](https://arxiv.org/abs/2509.13351)
*Pulkit Verma,Ngoc La,Anthony Favier,Swaroop Mishra,Julie A. Shah*

Main category: cs.AI

TL;DR: 该论文提出了一种新的指令微调框架PDDL-Instruct，通过逻辑链式思维推理增强LLM的符号规划能力，在标准基准测试中取得了高达94%的规划准确率，比基线模型提高了66%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在结构化符号规划方面能力有限，尤其是在需要形式化表示的领域。

Method: 提出了一种基于逻辑链式思维推理的指令微调框架PDDL-Instruct，指导模型进行精确的逻辑推理，从而自我纠正规划过程。

Result: 在多个规划领域上的实验结果表明，该方法显著提高了LLM的规划能力，规划准确率高达94%，比基线模型提高了66%。

Conclusion: 该工作弥合了LLM的通用推理能力和自动化规划所需逻辑精度之间的差距，为开发更好的AI规划系统提供了方向。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, yet their ability to perform structured symbolic planning
remains limited, particularly in domains requiring formal representations like
the Planning Domain Definition Language (PDDL). In this paper, we present a
novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'
symbolic planning capabilities through logical chain-of-thought reasoning. Our
approach focuses on teaching models to rigorously reason about action
applicability, state transitions, and plan validity using explicit logical
inference steps. By developing instruction prompts that guide models through
the precise logical reasoning required to determine when actions can be applied
in a given state, we enable LLMs to self-correct their planning processes
through structured reflection. The framework systematically builds verification
skills by decomposing the planning process into explicit reasoning chains about
precondition satisfaction, effect application, and invariant preservation.
Experimental results on multiple planning domains show that our
chain-of-thought reasoning based instruction-tuned models are significantly
better at planning, achieving planning accuracy of up to 94% on standard
benchmarks, representing a 66% absolute improvement over baseline models. This
work bridges the gap between the general reasoning capabilities of LLMs and the
logical precision required for automated planning, offering a promising
direction for developing better AI planning systems.

</details>


### [8] [Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning](https://arxiv.org/abs/2509.13352)
*Anis Koubaa,Khaled Gabr*

Main category: cs.AI

TL;DR: 该论文提出了一种名为Agentic UAVs的五层架构，利用LLM赋能无人机，提升其自主性和环境适应性，并在模拟搜救场景中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有无人机系统自主性有限，缺乏上下文感知和自主决策能力。

Method: 提出了一种五层架构(感知、推理、行动、集成、学习)，利用LLM、数据库查询和第三方系统交互增强无人机能力，并基于ROS2、Gazebo、YOLOv11和GPT-4构建原型系统。

Result: 模拟搜救场景实验表明，Agentic UAVs提高了目标检测置信度(0.79 vs 0.72)、人员检测率(91% vs 75%)和行动推荐率(92% vs 4.5%)。

Conclusion: 该框架通过较低的计算开销实现了无人机自主性和生态系统集成的新水平。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,
surveillance, and disaster response, yet most systems remain confined to SAE
Level 2--3 autonomy. Their reliance on rule-based control and narrow AI
restricts adaptability in dynamic, uncertain missions. Existing UAV frameworks
lack context-aware reasoning, autonomous decision-making, and ecosystem-level
integration; critically, none leverage Large Language Model (LLM) agents with
tool-calling for real-time knowledge access. This paper introduces the Agentic
UAVs framework, a five-layer architecture (Perception, Reasoning, Action,
Integration, Learning) that augments UAVs with LLM-driven reasoning, database
querying, and third-party system interaction. A ROS2 and Gazebo-based prototype
integrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3
deployment. In simulated search-and-rescue scenarios, agentic UAVs achieved
higher detection confidence (0.79 vs. 0.72), improved person detection rates
(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).
These results confirm that modest computational overhead enables qualitatively
new levels of autonomy and ecosystem integration.

</details>


### [9] [Semantic Fusion with Fuzzy-Membership Features for Controllable Language Modelling](https://arxiv.org/abs/2509.13357)
*Yongchao Huang,Hassan Raza*

Main category: cs.AI

TL;DR: 该论文提出了一种轻量级的语义融合方案，通过并行的模糊成员特征通道增强Transformer语言模型，从而改进文本生成效果。


<details>
  <summary>Details</summary>
Motivation: 改进Transformer语言模型的文本生成能力，特别是对极性和标点符号的控制能力。

Method: 使用模糊成员函数对每个token进行语义编码，生成句子级别的语义矩阵，并通过门控适配器将其融合到LM中。训练过程包括标准的下一个token预测、辅助损失和一个轻量级的统一器。

Result: 在合成语料库上，该方法提高了困惑度，并能够精确、用户可控地生成极性和标点符号，同时保持模型的简洁性。

Conclusion: 该方法是一种轻量级、可解释的条件自然语言生成方法，具有良好的应用前景。

Abstract: We propose semantic fusion, a lightweight scheme that augments a Transformer
language model (LM) with a parallel, fuzzy-membership feature channel that
encodes token-level semantics. Each token is represented by a vector of
interpretable features (e.g. part-of-speech cues, shallow roles, boundary
flags, sentiment polarity and strength) whose values are graded degrees from
differentiable membership functions (e.g. power kernels). These per-token
vectors form a sentence-level semantic matrix fused via a gated adapter into
the LM. Training uses standard next-token prediction, an auxiliary loss that
reconstructs the semantic features from hidden states, and a lightweight
uniformizer that regularizes adjective-class distributions. On a synthetic
two-clause corpus with held-out adjectives for out-of-distribution (OOD)
control, semantic fusion improves perplexity and enables precise,
user-controllable generation of polarity and punctuation while maintaining
model simplicity. This approach adds only small overhead, remains fully
compatible with tied input-output embeddings, and provides an interpretable
pathway for conditioned natural language generation.

</details>


### [10] [Asterisk Operator](https://arxiv.org/abs/2509.13364)
*Zixi Li*

Main category: cs.AI

TL;DR: 提出了一种新的统一抽象推理框架：星号算子，它基于邻接结构并行传播（ASPP），将结构化推理任务形式化为由隐式关系图引导的局部并行状态演化过程。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一种高效且收敛的计算范式来解决抽象推理问题。

Method: 星号算子结合了数学分析和实验验证，包括ARC2挑战和康威生命游戏。还提出了一种嵌入式星号蒸馏方法。

Result: 证明了星号算子在保持局部计算约束的同时实现了全局推理能力，并在ARC2验证集上取得了100%的准确率（仅使用6M参数）。

Conclusion: 星号算子是一种具有普适性、收敛性和优越性能的抽象推理计算范式，在神经符号推理方面取得了重大突破。

Abstract: We propose the \textbf{Asterisk Operator} ($\ast$-operator), a novel unified
framework for abstract reasoning based on Adjacency-Structured Parallel
Propagation (ASPP). The operator formalizes structured reasoning tasks as
local, parallel state evolution processes guided by implicit relational graphs.
We prove that the $\ast$-operator maintains local computational constraints
while achieving global reasoning capabilities, providing an efficient and
convergent computational paradigm for abstract reasoning problems. Through
rigorous mathematical analysis and comprehensive experiments on ARC2 challenges
and Conway's Game of Life, we demonstrate the operator's universality,
convergence properties, and superior performance. Our innovative
Embedding-Asterisk distillation method achieves 100\% accuracy on ARC2
validation with only 6M parameters, representing a significant breakthrough in
neural-symbolic reasoning.
  \textbf{Keywords:} Abstract Reasoning, Adjacency Structure, Parallel
Propagation, Asterisk Operator, Convergence, Universal Approximation

</details>


### [11] [$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation](https://arxiv.org/abs/2509.13368)
*Yuan Wei,Xiaohan Shan,Ran Miao,Jianmin Li*

Main category: cs.AI

TL;DR: 本文介绍了一种名为$Agent^2$的自动强化学习智能体设计框架，该框架利用大型语言模型自动生成高性能的强化学习智能体，并在多个基准测试中取得了优于人工设计的方案的结果。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习智能体开发需要大量的专业知识和时间，且失败率高。

Method: 提出了一种双智能体架构：生成器智能体负责分析任务并生成可执行的强化学习智能体；目标智能体是自动生成的强化学习智能体。该框架将强化学习开发分解为MDP建模和算法优化两个阶段。

Result: 在MuJoCo、MetaDrive、MPE和SMAC等基准测试中，$Agent^2$在所有任务上都始终优于人工设计的方案，性能提升高达55%。

Conclusion: 该框架实现了强化学习智能体设计的端到端自动化，标志着人工智能系统自动化领域的一个重大突破。

Abstract: Reinforcement learning agent development traditionally requires extensive
expertise and lengthy iterations, often resulting in high failure rates and
limited accessibility. This paper introduces $Agent^2$, a novel
agent-generates-agent framework that achieves fully automated RL agent design
through intelligent LLM-driven generation. The system autonomously transforms
natural language task descriptions and environment code into comprehensive,
high-performance reinforcement learning solutions without human intervention.
$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent
serves as an autonomous AI designer that analyzes tasks and generates
executable RL agents, while the Target Agent is the resulting automatically
generated RL agent. The framework decomposes RL development into two distinct
stages: MDP modeling and algorithmic optimization, enabling more targeted and
effective agent generation. Built on the Model Context Protocol, $Agent^2$
provides a unified framework that standardizes intelligent agent creation
across diverse environments and algorithms, while incorporating adaptive
training management and intelligent feedback analysis for continuous
improvement. Extensive experiments on a wide range of benchmarks, including
MuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently
outperforms manually designed solutions across all tasks, achieving up to 55%
performance improvement and substantial gains on average. By enabling truly
end-to-end, closed-loop automation, this work establishes a new paradigm in
which intelligent agents design and optimize other agents, marking a
fundamental breakthrough for automated AI systems.

</details>


### [12] [The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in VLMs](https://arxiv.org/abs/2509.13379)
*Asif Azad,Mohammad Sadat Hossain,MD Sadik Hossain Shanto,M Saifur Rahman,Md Rizwan Pervez*

Main category: cs.AI

TL;DR: 这篇论文对16个最先进的视觉语言模型(VLM)的6个多模态数据集上的不确定性量化进行了全面的基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型缺乏对不确定性量化的关注，该研究旨在评估VLM的不确定性量化能力。

Method: 在6个多模态数据集上，使用3个不同的评分函数评估了16个VLM(开源和闭源)的不确定性量化性能。

Result: 结果表明，更大的模型通常表现出更好的不确定性量化；更确定的模型具有更高的准确性；数学和推理任务的不确定性性能比其他领域差。

Conclusion: 这项工作为多模态系统中可靠的不确定性评估奠定了基础。

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in complex
visual understanding across scientific and reasoning tasks. While performance
benchmarking has advanced our understanding of these capabilities, the critical
dimension of uncertainty quantification has received insufficient attention.
Therefore, unlike prior conformal prediction studies that focused on limited
settings, we conduct a comprehensive uncertainty benchmarking study, evaluating
16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets
with 3 distinct scoring functions. Our findings demonstrate that larger models
consistently exhibit better uncertainty quantification; models that know more
also know better what they don't know. More certain models achieve higher
accuracy, while mathematical and reasoning tasks elicit poorer uncertainty
performance across all models compared to other domains. This work establishes
a foundation for reliable uncertainty evaluation in multimodal systems.

</details>


### [13] [From Next Token Prediction to (STRIPS) World Models -- Preliminary Results](https://arxiv.org/abs/2509.13389)
*Carlos Núñez-Molina,Vicenç Gómez,Hector Geffner*

Main category: cs.AI

TL;DR: 本文使用深度学习架构（Transformer）从动作轨迹中学习命题STRIPS世界模型。


<details>
  <summary>Details</summary>
Motivation: 学习仅从动作轨迹中学习命题STRIPS世界模型。

Method: 将任务转换为监督式下一个token预测问题，其中token是动作。Transformer架构可以忠实地表示命题STRIPS世界模型。

Result: 实验结果表明，模型可以仅从有效和无效的动作序列集中学习。

Conclusion: Transformer架构能够有效学习命题STRIPS世界模型

Abstract: We consider the problem of learning propositional STRIPS world models from
action traces alone, using a deep learning architecture (transformers) and
gradient descent. The task is cast as a supervised next token prediction
problem where the tokens are the actions, and an action $a$ may follow an
action sequence if the hidden effects of the previous actions do not make an
action precondition of $a$ false. We show that a suitable transformer
architecture can faithfully represent propositional STRIPS world models, and
that the models can be learned from sets of random valid (positive) and invalid
(negative) action sequences alone. A number of experiments are reported.

</details>


### [14] [SteeringControl: Holistic Evaluation of Alignment Steering in LLMs](https://arxiv.org/abs/2509.13450)
*Vincent Siu,Nicholas Crispino,David Park,Nathan W. Henry,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: SteeringControl基准评估了表征引导方法在偏差、有害生成和幻觉等核心一致性目标上的有效性及其对谄媚和常识道德等次要行为的影响，发现其性能依赖于引导方法、模型和目标行为的特定组合。


<details>
  <summary>Details</summary>
Motivation: 现有方法对表征引导的副作用研究不足，缺乏系统性理解。

Method: 构建了一个基于模块化引导框架的基准，收集了安全相关的初级和次级行为数据集，评估了五种流行的引导方法。

Result: 强引导性能依赖于引导方法、模型和目标行为的特定组合，不良组合可能导致严重的观念纠缠。

Conclusion: 发布了SteeringControl基准代码，为表征引导方法的评估提供了新的工具。

Abstract: We introduce SteeringControl, a benchmark for evaluating representation
steering methods across core alignment objectives--bias, harmful generation,
and hallucination--and their effects on secondary behaviors such as sycophancy
and commonsense morality. While prior alignment work often highlights
truthfulness or reasoning ability to demonstrate the side effects of
representation steering, we find there are many unexplored tradeoffs not yet
understood in a systematic way. We collect a dataset of safety-relevant primary
and secondary behaviors to evaluate steering effectiveness and behavioral
entanglement centered around five popular steering methods. To enable this, we
craft a modular steering framework based on unique components that serve as the
building blocks of many existing methods. Our results on Qwen-2.5-7B and
Llama-3.1-8B find that strong steering performance is dependent on the specific
combination of steering method, model, and targeted behavior, and that severe
concept entanglement can result from poor combinations of these three as well.
We release our code here:
https://github.com/wang-research-lab/SteeringControl.git.

</details>


### [15] [AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving](https://arxiv.org/abs/2509.13547)
*Harper Reed,Michael Sugimura,Angelo Zangari*

Main category: cs.AI

TL;DR: 赋予LLM代理人协作工具和自主性可显著提升其解决复杂编程问题的能力，尤其是在需要额外推理支架时。


<details>
  <summary>Details</summary>
Motivation: 研究赋予LLM代理人协作工具（类似人类）是否能提升其性能。

Method: 为Claude Code代理人配备基于MCP的社交媒体和日记工具，并在34个Aider Polyglot Python编程挑战中测试其性能。

Result: 协作工具显著提升了对最难问题的解决能力，成本降低15-40%，步骤减少12-27%，完成速度提升12-38%。不同模型自然采用了不同的协作策略，例如Sonnet 3.7广泛使用工具，Sonnet 4则选择性地使用日记进行语义搜索。代理人更倾向于写作而非阅读，表明结构化表达而非信息获取是提升的关键。

Conclusion: 人类启发的协作工具能系统性地提升AI代理人在能力边界上的性能，表明自适应协作界面可作为推理增强器，而非普遍效率提升手段。

Abstract: We investigate whether giving LLM agents the collaborative tools and autonomy
that humans naturally use for problem solving can improve their performance. We
equip Claude Code agents with MCP-based social media and journaling tools and
allow them to use these tools as they see fit. Across 34 Aider Polyglot Python
programming challenges, collaborative tools substantially improve performance
on the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and
12-38% faster completion than baseline agents. Effects on the full challenge
set are mixed, suggesting these tools act as performance enhancers when
additional reasoning scaffolding is most needed. Surprisingly, Different models
naturally adopted distinct collaborative strategies without explicit
instruction. Sonnet 3.7 engaged broadly across tools and benefited from
articulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,
leaning on journal-based semantic search when problems were genuinely
difficult. This mirrors how human developers adjust collaboration based on
expertise and task complexity. Behavioral analysis shows agents prefer writing
over reading by about 2-9x, indicating that structured articulation drives much
of the improvement rather than information access alone. Overall, AI agents can
systematically benefit from human-inspired collaboration tools at the edge of
their capabilities, pointing to adaptive collaborative interfaces as reasoning
enhancers rather than universal efficiency boosts.

</details>


### [16] [Gen AI in Proof-based Math Courses: A Pilot Study](https://arxiv.org/abs/2509.13570)
*Hannah Klawa,Shraddha Rajpal,Cigole Thomas*

Main category: cs.AI

TL;DR: 本文研究了生成式AI在三个本科数学课程中的使用和感知，发现学生对AI工具的使用和看法各不相同，对教学有启示意义。


<details>
  <summary>Details</summary>
Motivation: 当前AI检测工具不可靠，需要制定鼓励学生学习和批判性思维的政策。

Method: 通过调查问卷和学生访谈，分析学生如何使用AI工具，以及他们对AI工具有用性和局限性的看法。

Result: 学生对生成式AI工具的使用和看法各不相同，对其有用性和局限性有不同理解。

Conclusion: 需要进一步研究如何将生成式AI融入证明型数学教学。

Abstract: With the rapid rise of generative AI in higher education and the
unreliability of current AI detection tools, developing policies that encourage
student learning and critical thinking has become increasingly important. This
study examines student use and perceptions of generative AI across three
proof-based undergraduate mathematics courses: a first-semester abstract
algebra course, a topology course and a second-semester abstract algebra
course. In each case, course policy permitted some use of generative AI.
Drawing on survey responses and student interviews, we analyze how students
engaged with AI tools, their perceptions of generative AI's usefulness and
limitations, and what implications these perceptions hold for teaching
proof-based mathematics. We conclude by discussing future considerations for
integrating generative AI into proof-based mathematics instruction.

</details>


### [17] [Programmable Cognitive Bias in Social Agents](https://arxiv.org/abs/2509.13588)
*Xuan Liu,Haoyang Shang,Haojian Jin*

Main category: cs.AI

TL;DR: CoBRA工具包能更精确地控制大型语言模型中社会模拟中智能体的认知偏差。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以保证不同模型中智能体行为的一致性和细微差别。

Method: 提出CoBRA工具包，包含认知偏差指数和行为调节引擎两部分，前者量化智能体在经典社会科学实验中的反应，后者调整智能体行为以体现可控的认知偏差。

Result: 实验表明CoBRA能以模型无关的方式精确编程社会智能体的认知偏差。

Conclusion: CoBRA为在大型语言模型中进行社会模拟提供了更精确和一致的方法。

Abstract: This paper introduces CoBRA, a novel toolkit for systematically specifying
agent behavior in LLM-based social simulation. We found that conventional
approaches that specify agent behaviors through implicit natural language
descriptions cannot yield consistent behaviors across models, and the produced
agent behaviors do not capture the nuances of the descriptions. In contrast,
CoBRA presents a new approach to program agents' cognitive biases explicitly,
by grounding agents' expected behaviors using classic social science
experiments. CoBRA has two components: (1) Cognitive Bias Index that measures
the cognitive bias of a social agent, by quantifying the agent's reactions in a
set of validated classical social science experiments; (2) Behavioral
Regulation Engine that aligns the agent's behavior to demonstrate controlled
cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and
technical benchmarks. Our results suggest that CoBRA can precisely program the
cognitive bias demonstrated in a social agent in a model-agnostic manner.

</details>
