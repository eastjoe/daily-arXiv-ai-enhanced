<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Prescriptive Agents based on Rag for Automated Maintenance (PARAM)](https://arxiv.org/abs/2508.04714)
*Chitranshu Harbola,Anupam Purwar*

Main category: cs.AI

TL;DR: 基于LLM的预测性维护系统，结合轴承振动分析和多智能体生成，实现故障检测、分类、严重程度评估和结构化维护建议生成。


<details>
  <summary>Details</summary>
Motivation: 工业机械维护需要及时干预以防止灾难性故障并优化运营效率。

Method: 该系统将轴承振动数据序列化为自然语言，以便LLM处理；利用多智能体组件处理维护手册，进行语义搜索和网络搜索以获取全面的程序知识和最新的维护实践；最后，Gemini模型生成结构化的维护建议。

Result: 实验验证表明，该系统能够有效地进行异常检测并提供相关的维护指导，成功弥合了状态监测和可操作的维护计划之间的差距。

Conclusion: 该论文提出了一种基于大型语言模型 (LLM) 的预测性维护智能系统，该系统结合轴承振动频率分析和多智能体生成，能够对工业机械进行故障检测、故障类型分类和严重程度评估，并生成结构化的维护建议，包括立即行动、检查清单、纠正措施、零件需求和时间表规范。实验验证表明，该系统能够有效地进行异常检测并提供相关的维护指导。

Abstract: Industrial machinery maintenance requires timely intervention to prevent
catastrophic failures and optimize operational efficiency. This paper presents
an integrated Large Language Model (LLM)-based intelligent system for
prescriptive maintenance that extends beyond traditional anomaly detection to
provide actionable maintenance recommendations. Building upon our prior LAMP
framework for numerical data analysis, we develop a comprehensive solution that
combines bearing vibration frequency analysis with multi agentic generation for
intelligent maintenance planning. Our approach serializes bearing vibration
data (BPFO, BPFI, BSF, FTF frequencies) into natural language for LLM
processing, enabling few-shot anomaly detection with high accuracy. The system
classifies fault types (inner race, outer race, ball/roller, cage faults) and
assesses severity levels. A multi-agentic component processes maintenance
manuals using vector embeddings and semantic search, while also conducting web
searches to retrieve comprehensive procedural knowledge and access up-to-date
maintenance practices for more accurate and in-depth recommendations. The
Gemini model then generates structured maintenance recommendations includes
immediate actions, inspection checklists, corrective measures, parts
requirements, and timeline specifications. Experimental validation in bearing
vibration datasets demonstrates effective anomaly detection and contextually
relevant maintenance guidance. The system successfully bridges the gap between
condition monitoring and actionable maintenance planning, providing industrial
practitioners with intelligent decision support. This work advances the
application of LLMs in industrial maintenance, offering a scalable framework
for prescriptive maintenance across machinery components and industrial
sectors.

</details>


### [2] [GeoFlow: Agentic Workflow Automation for Geospatial Tasks](https://arxiv.org/abs/2508.04719)
*Amulya Bhattaram,Justin Chung,Stanley Chung,Ranit Gupta,Janani Ramamoorthy,Kartikeya Gullapalli,Diana Marculescu,Dimitrios Stamoulis*

Main category: cs.AI

TL;DR: GeoFlow 是一种自动生成地理空间任务主动工作流的新方法，提高效率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法侧重于推理分解，而忽略了 API 选择。GeoFlow 通过在运行时提供详细的工具调用目标来改进这一点。

Method: 该方法为每个代理提供详细的工具调用目标，以指导地理空间 API 的运行时调用。

Result: 与最先进的方法相比，GeoFlow 将主动成功率提高了 6.8%，并将令牌使用率降低了多达四倍。

Conclusion: GeoFlow 自动生成地理空间任务的主动工作流，提高了主动成功率并减少了令牌使用率。

Abstract: We present GeoFlow, a method that automatically generates agentic workflows
for geospatial tasks. Unlike prior work that focuses on reasoning decomposition
and leaves API selection implicit, our method provides each agent with detailed
tool-calling objectives to guide geospatial API invocation at runtime. GeoFlow
increases agentic success by 6.8% and reduces token usage by up to fourfold
across major LLM families compared to state-of-the-art approaches.

</details>


### [3] [Who is a Better Player: LLM against LLM](https://arxiv.org/abs/2508.04720)
*Yingjie Zhou,Jiezhang Cao,Farong Wen,Li Xu,Yanwei Jiang,Jun Jia,Ronghui Li,Xiaohong Liu,Yu Zhou,Xiongkuo Min,Jie Guo,Zicheng Zhang,Guangtao Zhai*

Main category: cs.AI

TL;DR: 论文提出用棋盘游戏评估大型语言模型，发现模型适应性强但稳定性不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于问答的基准方法依赖于数据，该论文旨在通过对抗性棋盘游戏竞争来弥补这一局限性，更全面地评估LLM的性能。

Method: 提出了一种基于对抗性棋盘游戏评估大型语言模型（LLM）性能的框架，使用Elo评分系统和性能循环图（PLG）进行量化评估，并收集积极情绪得分（PSS）评估心理适应性。

Result: 实验结果表明，大多数LLM在输赢方面都保持乐观，展现出比人类更强的适应高压对抗环境的能力。但PLG揭示了LLM技能的不稳定性，需要进一步研究。

Conclusion: 该论文提出了一种基于对抗性棋盘游戏评估大型语言模型（LLM）性能的框架，该框架通过Elo评分系统和性能循环图（PLG）对LLM的技术能力和心理适应性进行量化评估，实验结果表明LLM在高压对抗环境下表现出较强的适应性，但也暴露出其技能稳定性不足的问题。

Abstract: Adversarial board games, as a paradigmatic domain of strategic reasoning and
intelligence, have long served as both a popular competitive activity and a
benchmark for evaluating artificial intelligence (AI) systems. Building on this
foundation, we propose an adversarial benchmarking framework to assess the
comprehensive performance of Large Language Models (LLMs) through board games
competition, compensating the limitation of data dependency of the mainstream
Question-and-Answer (Q&A) based benchmark method. We introduce Qi Town, a
specialized evaluation platform that supports 5 widely played games and
involves 20 LLM-driven players. The platform employs both the Elo rating system
and a novel Performance Loop Graph (PLG) to quantitatively evaluate the
technical capabilities of LLMs, while also capturing Positive Sentiment Score
(PSS) throughout gameplay to assess mental fitness. The evaluation is
structured as a round-robin tournament, enabling systematic comparison across
players. Experimental results indicate that, despite technical differences,
most LLMs remain optimistic about winning and losing, demonstrating greater
adaptability to high-stress adversarial environments than humans. On the other
hand, the complex relationship between cyclic wins and losses in PLGs exposes
the instability of LLMs' skill play during games, warranting further
explanation and exploration.

</details>


### [4] [Fine-Tuning Small Language Models (SLMs) for Autonomous Web-based Geographical Information Systems (AWebGIS)](https://arxiv.org/abs/2508.04846)
*Mahdi Nazari Ashani,Ali Asghar Alesheikh,Saba Kazemi,Kimya Kheirkhah,Yasin Mohammadi,Fatemeh Rezaie,Amir Mahdi Manafi,Hedieh Zarkesh*

Main category: cs.AI

TL;DR: 客户端小型语言模型方法在自主式WebGIS中表现最佳，准确率高且无需服务器端推理。


<details>
  <summary>Details</summary>
Motivation: 现有自主式WebGIS依赖云端大型语言模型，存在网络依赖、隐私和可扩展性问题。

Method: 比较了三种方法：基于云端大型语言模型的在线方法、基于经典机器学习的离线方法和基于微调小型语言模型的客户端离线方法。

Result: 基于微调小型语言模型（T5-small）的客户端方法准确率最高，精确匹配准确率为0.93，Levenshtein相似度为0.99，ROUGE-1和ROUGE-L得分均为0.98。

Conclusion: 研究比较了三种实现自主式WebGIS的方法，基于微调小型语言模型的客户端方法在准确率、相似性和ROUGE得分上表现最佳，并能减少后端服务器负载。

Abstract: Autonomous web-based geographical information systems (AWebGIS) aim to
perform geospatial operations from natural language input, providing intuitive,
intelligent, and hands-free interaction. However, most current solutions rely
on cloud-based large language models (LLMs), which require continuous internet
access and raise users' privacy and scalability issues due to centralized
server processing. This study compares three approaches to enabling AWebGIS:
(1) a fully-automated online method using cloud-based LLMs (e.g., Cohere); (2)
a semi-automated offline method using classical machine learning classifiers
such as support vector machine and random forest; and (3) a fully autonomous
offline (client-side) method based on a fine-tuned small language model (SLM),
specifically T5-small model, executed in the client's web browser. The third
approach, which leverages SLMs, achieved the highest accuracy among all
methods, with an exact matching accuracy of 0.93, Levenshtein similarity of
0.99, and recall-oriented understudy for gisting evaluation ROUGE-1 and ROUGE-L
scores of 0.98. Crucially, this client-side computation strategy reduces the
load on backend servers by offloading processing to the user's device,
eliminating the need for server-based inference. These results highlight the
feasibility of browser-executable models for AWebGIS solutions.

</details>


### [5] [Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning](https://arxiv.org/abs/2508.04848)
*Chang Tian,Matthew B. Blaschko,Mingzhe Xing,Xiuxing Li,Yinliang Yue,Marie-Francine Moens*

Main category: cs.AI

TL;DR: 强化学习提升LLM推理能力有限，在非理想场景下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试忽略了大型语言模型在现实非理想场景下的推理性能。

Method: 使用强化学习微调三个大型语言模型和一个视觉语言模型，并在八个公共数据集上进行评估。

Result: 强化学习微调提高了理想场景下的推理性能，但在非理想场景（摘要推理、细粒度噪声抑制和上下文过滤）下性能显著下降。

Conclusion: 当前强化学习增强大型语言模型推理能力的方法在非理想场景下性能显著下降，暴露出其推理能力的局限性，现有方法难以有效解决这些问题。

Abstract: Reinforcement learning (RL) has become a key technique for enhancing the
reasoning abilities of large language models (LLMs), with policy-gradient
algorithms dominating the post-training stage because of their efficiency and
effectiveness. However, most existing benchmarks evaluate large-language-model
reasoning under idealized settings, overlooking performance in realistic,
non-ideal scenarios. We identify three representative non-ideal scenarios with
practical relevance: summary inference, fine-grained noise suppression, and
contextual filtering. We introduce a new research direction guided by
brain-science findings that human reasoning remains reliable under imperfect
inputs. We formally define and evaluate these challenging scenarios. We
fine-tune three LLMs and a state-of-the-art large vision-language model (LVLM)
using RL with a representative policy-gradient algorithm and then test their
performance on eight public datasets. Our results reveal that while RL
fine-tuning improves baseline reasoning under idealized settings, performance
declines significantly across all three non-ideal scenarios, exposing critical
limitations in advanced reasoning capabilities. Although we propose a
scenario-specific remediation method, our results suggest current methods leave
these reasoning deficits largely unresolved. This work highlights that the
reasoning abilities of large models are often overstated and underscores the
importance of evaluating models under non-ideal scenarios. The code and data
will be released at XXXX.

</details>


### [6] [ConfAgents: A Conformal-Guided Multi-Agent Framework for Cost-Efficient Medical Diagnosis](https://arxiv.org/abs/2508.04915)
*Huiya Zhao,Yinghao Zhu,Zixiang Wang,Yasha Wang,Junyi Gao,Liantao Ma*

Main category: cs.AI

TL;DR: Self-evolving AI agent HealthFlow surpasses existing methods in healthcare data analysis, highlighting the importance of self-evolving task-managers for scientific discovery.


<details>
  <summary>Details</summary>
Motivation: Addresses the limitation of static, predefined strategies in AI agents hindering their ability to become strategic planners in complex domains like healthcare.

Method: Introduces a novel meta-level evolution mechanism for AI agents to autonomously refine their problem-solving policies by distilling procedural successes and failures.  A new benchmark, EHRFlowBench, is also introduced for reproducible evaluation.

Result: HealthFlow significantly outperforms state-of-the-art agent frameworks.  The new benchmark, EHRFlowBench, facilitates reproducible evaluation.

Conclusion: HealthFlow, a self-evolving AI agent, significantly outperforms state-of-the-art agent frameworks in complex health data analysis tasks, demonstrating the effectiveness of self-evolving task-managers for scientific discovery.

Abstract: The efficacy of AI agents in healthcare research is hindered by their
reliance on static, predefined strategies. This creates a critical limitation:
agents can become better tool-users but cannot learn to become better strategic
planners, a crucial skill for complex domains like healthcare. We introduce
HealthFlow, a self-evolving AI agent that overcomes this limitation through a
novel meta-level evolution mechanism. HealthFlow autonomously refines its own
high-level problem-solving policies by distilling procedural successes and
failures into a durable, strategic knowledge base. To anchor our research and
facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark
featuring complex, realistic health data analysis tasks derived from
peer-reviewed clinical research. Our comprehensive experiments demonstrate that
HealthFlow's self-evolving approach significantly outperforms state-of-the-art
agent frameworks. This work marks a necessary shift from building better
tool-users to designing smarter, self-evolving task-managers, paving the way
for more autonomous and effective AI for scientific discovery.

</details>


### [7] [The Docking Game: Loop Self-Play for Fast, Dynamic, and Accurate Prediction of Flexible Protein--Ligand Binding](https://arxiv.org/abs/2508.05006)
*Youzhi Zhang,Yufei Li,Gaofeng Meng,Hongbin Liu,Jiebo Luo*

Main category: cs.AI

TL;DR: 利用博弈论框架和Loop Self-Play算法，提高分子对接精度，提升约10%。


<details>
  <summary>Details</summary>
Motivation: 现有多任务学习模型在配体对接方面性能不如蛋白质口袋对接，这是由于配体和蛋白质的结构复杂性不同。

Method: 提出了一种基于博弈论的Docking Game框架和Loop Self-Play算法，该算法通过两个玩家（配体和蛋白质）的迭代训练和相互适应来优化分子对接。

Result: 在公共基准数据集上，LoopPlay算法在预测准确结合模式方面比现有最先进方法提高了约10%。

Conclusion: 提出了一种新颖的博弈论框架Docking Game和Loop Self-Play算法，显著提高了分子对接的准确性，在公共基准数据集上取得了大约10%的改进。

Abstract: Molecular docking is a crucial aspect of drug discovery, as it predicts the
binding interactions between small-molecule ligands and protein pockets.
However, current multi-task learning models for docking often show inferior
performance in ligand docking compared to protein pocket docking. This
disparity arises largely due to the distinct structural complexities of ligands
and proteins. To address this issue, we propose a novel game-theoretic
framework that models the protein-ligand interaction as a two-player game
called the Docking Game, with the ligand docking module acting as the ligand
player and the protein pocket docking module as the protein player. To solve
this game, we develop a novel Loop Self-Play (LoopPlay) algorithm, which
alternately trains these players through a two-level loop. In the outer loop,
the players exchange predicted poses, allowing each to incorporate the other's
structural predictions, which fosters mutual adaptation over multiple
iterations. In the inner loop, each player dynamically refines its predictions
by incorporating its own predicted ligand or pocket poses back into its model.
We theoretically show the convergence of LoopPlay, ensuring stable
optimization. Extensive experiments conducted on public benchmark datasets
demonstrate that LoopPlay achieves approximately a 10\% improvement in
predicting accurate binding modes compared to previous state-of-the-art
methods. This highlights its potential to enhance the accuracy of molecular
docking in drug discovery.

</details>


### [8] [Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses](https://arxiv.org/abs/2508.05009)
*Bin Han,Robert Wolfe,Anat Caspi,Bill Howe*

Main category: cs.AI

TL;DR: LLM能有效整合城市空间数据，但需改进空间推理和数据格式处理能力。


<details>
  <summary>Details</summary>
Motivation: 传统的基于规则的方法和机器学习方法在处理城市空间数据整合方面存在局限性，LLM为解决这些问题提供了一种新的途径。

Method: 研究了LLM在空间数据整合中的应用，并采用了一种评审和改进的方法来纠正错误的初始响应。

Result: LLM在提供相关特征的情况下，能够生成高性能结果，结合评审和改进方法，可以有效纠正错误。

Conclusion: 大型语言模型(LLM)在整合大型、异构和噪声的城市空间数据集方面显示出潜力，尤其是在结合相关特征后，能够生成高性能结果，但仍需改进空间推理能力和处理多种数据格式的能力。

Abstract: We explore the application of large language models (LLMs) to empower domain
experts in integrating large, heterogeneous, and noisy urban spatial datasets.
Traditional rule-based integration methods are unable to cover all edge cases,
requiring manual verification and repair. Machine learning approaches require
collecting and labeling of large numbers of task-specific samples. In this
study, we investigate the potential of LLMs for spatial data integration. Our
analysis first considers how LLMs reason about environmental spatial
relationships mediated by human experience, such as between roads and
sidewalks. We show that while LLMs exhibit spatial reasoning capabilities, they
struggle to connect the macro-scale environment with the relevant computational
geometry tasks, often producing logically incoherent responses. But when
provided relevant features, thereby reducing dependence on spatial reasoning,
LLMs are able to generate high-performing results. We then adapt a
review-and-refine method, which proves remarkably effective in correcting
erroneous initial responses while preserving accurate responses. We discuss
practical implications of employing LLMs for spatial data integration in
real-world contexts and outline future research directions, including
post-training, multi-modal integration methods, and support for diverse data
formats. Our findings position LLMs as a promising and flexible alternative to
traditional rule-based heuristics, advancing the capabilities of adaptive
spatial data integration.

</details>


### [9] [Cognitive Duality for Adaptive Web Agents](https://arxiv.org/abs/2508.05081)
*Jiarun Liu,Chunhong Zhang,Zheng Hu*

Main category: cs.AI

TL;DR: CogniWeb:  A dual-process web agent achieving high efficiency and competitive performance in web navigation.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of web navigation in AGI evaluation by effectively integrating offline imitation learning and online exploration.

Method: Developed CogniWeb, a modular agent architecture that integrates offline imitation learning and online exploration based on dual-process theory.

Result: CogniWeb achieves 43.96% success rate on WebArena with 75% reduction in token usage compared to other methods.

Conclusion: CogniWeb, a modular web agent architecture inspired by dual-process theory, achieves competitive performance with significantly higher efficiency by adaptively switching between fast intuitive processing and deliberate reasoning.

Abstract: Web navigation represents a critical and challenging domain for evaluating
artificial general intelligence (AGI), demanding complex decision-making within
high-entropy, dynamic environments with combinatorially explosive action
spaces. Current approaches to building autonomous web agents either focus on
offline imitation learning or online exploration, but rarely integrate both
paradigms effectively. Inspired by the dual-process theory of human cognition,
we derive a principled decomposition into fast System 1 and slow System 2
cognitive processes. This decomposition provides a unifying perspective on
existing web agent methodologies, bridging the gap between offline learning of
intuitive reactive behaviors and online acquisition of deliberative planning
capabilities. We implement this framework in CogniWeb, a modular agent
architecture that adaptively toggles between fast intuitive processing and
deliberate reasoning based on task complexity. Our evaluation on WebArena
demonstrates that CogniWeb achieves competitive performance (43.96% success
rate) while maintaining significantly higher efficiency (75% reduction in token
usage).

</details>


### [10] [MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models](https://arxiv.org/abs/2508.05083)
*Dexuan Xu,Jieyi Wang,Zhongyan Chai,Yongzhi Cao,Hanpin Wang,Huamin Zhang,Yu Huang*

Main category: cs.AI

TL;DR: 提出MedMKEB基准测试，用于评估医学多模态大型语言模型的知识编辑能力，并揭示了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的医学多模态大型语言模型难以有效更新过时或不正确的信息，缺乏系统的多模态医学知识编辑基准。

Method: 构建了一个高质量的医学视觉问答数据集，并精心构建了编辑任务，包括反事实修正、语义泛化、知识迁移和对抗鲁棒性，并结合了人类专家验证。

Result: 实验证明了现有基于知识的编辑方法在医学中的局限性，突出了开发专业化编辑策略的必要性。MedMKEB将成为促进可信和高效医学知识编辑算法开发的标准基准。

Conclusion: MedMKEB，一个用于评估医学多模态大型语言模型知识编辑可靠性、通用性、局部性、可移植性和鲁棒性的综合基准测试，被提出以促进可信和高效的医学知识编辑算法的开发。

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly improved medical AI, enabling it to unify the understanding of
visual and textual information. However, as medical knowledge continues to
evolve, it is critical to allow these models to efficiently update outdated or
incorrect information without retraining from scratch. Although textual
knowledge editing has been widely studied, there is still a lack of systematic
benchmarks for multimodal medical knowledge editing involving image and text
modalities. To fill this gap, we present MedMKEB, the first comprehensive
benchmark designed to evaluate the reliability, generality, locality,
portability, and robustness of knowledge editing in medical multimodal large
language models. MedMKEB is built on a high-quality medical visual
question-answering dataset and enriched with carefully constructed editing
tasks, including counterfactual correction, semantic generalization, knowledge
transfer, and adversarial robustness. We incorporate human expert validation to
ensure the accuracy and reliability of the benchmark. Extensive single editing
and sequential editing experiments on state-of-the-art general and medical
MLLMs demonstrate the limitations of existing knowledge-based editing
approaches in medicine, highlighting the need to develop specialized editing
strategies. MedMKEB will serve as a standard benchmark to promote the
development of trustworthy and efficient medical knowledge editing algorithms.

</details>


### [11] [EasySize: Elastic Analog Circuit Sizing via LLM-Guided Heuristic Search](https://arxiv.org/abs/2508.05113)
*Xinyue Wu,Fan Hu,Shaik Jani Babu,Yi Zhao,Xinfei Guo*

Main category: cs.AI

TL;DR: EasySize是一个轻量级、高性能的模拟电路门级尺寸调整框架，可显著减少人工干预和计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计耗时且依赖经验，现有AI方法普遍存在模型过大、缺乏跨工艺节点可移植性等问题。

Method: EasySize利用性能指标的可达性（EOA）动态构建特定任务的损失函数，并结合全局差分进化（DE）和局部粒子群优化（PSO）算法进行高效启发式搜索。

Result: EasySize在5个运算放大器(Op-Amp)电路网表上，跨180nm、45nm和22nm工艺节点均取得良好性能，并在86.67%的任务中优于AutoCkt，同时减少了超过96.67%的仿真资源。

Conclusion: EasySize，一个基于微调Qwen3-8B模型的轻量级门级尺寸调整框架，在跨不同工艺节点、设计规范和电路拓扑结构的通用适用性方面表现出色，优于AutoCkt等现有方法。

Abstract: Analog circuit design is a time-consuming, experience-driven task in chip
development. Despite advances in AI, developing universal, fast, and stable
gate sizing methods for analog circuits remains a significant challenge. Recent
approaches combine Large Language Models (LLMs) with heuristic search
techniques to enhance generalizability, but they often depend on large model
sizes and lack portability across different technology nodes. To overcome these
limitations, we propose EasySize, the first lightweight gate sizing framework
based on a finetuned Qwen3-8B model, designed for universal applicability
across process nodes, design specifications, and circuit topologies. EasySize
exploits the varying Ease of Attainability (EOA) of performance metrics to
dynamically construct task-specific loss functions, enabling efficient
heuristic search through global Differential Evolution (DE) and local Particle
Swarm Optimization (PSO) within a feedback-enhanced flow. Although finetuned
solely on 350nm node data, EasySize achieves strong performance on 5
operational amplifier (Op-Amp) netlists across 180nm, 45nm, and 22nm technology
nodes without additional targeted training, and outperforms AutoCkt, a
widely-used Reinforcement Learning based sizing framework, on 86.67\% of tasks
with more than 96.67\% of simulation resources reduction. We argue that
EasySize can significantly reduce the reliance on human expertise and
computational resources in gate sizing, thereby accelerating and simplifying
the analog circuit design process. EasySize will be open-sourced at a later
date.

</details>


### [12] [Beyond Automation: Socratic AI, Epistemic Agency, and the Implications of the Emergence of Orchestrated Multi-Agent Learning Architectures](https://arxiv.org/abs/2508.05116)
*Peer-Benedikt Degen,Igor Asanov*

Main category: cs.AI

TL;DR: Socratic AI tutors improve student critical thinking;  suggests a shift towards orchestrated multi-agent systems in education.


<details>
  <summary>Details</summary>
Motivation: To evaluate the impact of a Socratic AI tutor on student research question development and to explore the potential of multi-agent systems (MAS) in higher education.

Method: Controlled experiment with 65 pre-service teacher students in Germany comparing interaction with a Socratic AI tutor to an uninstructed AI chatbot.

Result: Students using the Socratic AI tutor showed significantly greater support for critical, independent, and reflective thinking.

Conclusion: This study provides empirical evidence and a conceptual roadmap for hybrid learning ecosystems incorporating human-AI collaboration and pedagogical alignment, showing that a Socratic AI tutor significantly enhances students’ critical thinking skills compared to an uninstructed AI chatbot.

Abstract: Generative AI is no longer a peripheral tool in higher education. It is
rapidly evolving into a general-purpose infrastructure that reshapes how
knowledge is generated, mediated, and validated. This paper presents findings
from a controlled experiment evaluating a Socratic AI Tutor, a large language
model designed to scaffold student research question development through
structured dialogue grounded in constructivist theory. Conducted with 65
pre-service teacher students in Germany, the study compares interaction with
the Socratic Tutor to engagement with an uninstructed AI chatbot. Students
using the Socratic Tutor reported significantly greater support for critical,
independent, and reflective thinking, suggesting that dialogic AI can stimulate
metacognitive engagement and challenging recent narratives of de-skilling due
to generative AI usage. These findings serve as a proof of concept for a
broader pedagogical shift: the use of multi-agent systems (MAS) composed of
specialised AI agents. To conceptualise this, we introduce the notion of
orchestrated MAS, modular, pedagogically aligned agent constellations, curated
by educators, that support diverse learning trajectories through differentiated
roles and coordinated interaction. To anchor this shift, we propose an adapted
offer-and-use model, in which students appropriate instructional offers from
these agents. Beyond technical feasibility, we examine system-level
implications for higher education institutions and students, including funding
necessities, changes to faculty roles, curriculars, competencies and assessment
practices. We conclude with a comparative cost-effectiveness analysis
highlighting the scalability of such systems. In sum, this study contributes
both empirical evidence and a conceptual roadmap for hybrid learning ecosystems
that embed human-AI co-agency and pedagogical alignment.

</details>


### [13] [Graph-based Event Log Repair](https://arxiv.org/abs/2508.05145)
*Sebastiano Dissegna,Chiara Di Francescomarino,Massimiliano Ronzani*

Main category: cs.AI

TL;DR: 使用异构图神经网络修复流程挖掘中事件日志的缺失信息，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 流程挖掘中事件日志的质量至关重要，而现实世界中的事件日志往往存在缺失信息的问题。现有的基于模型的方法需要预先提供流程模型，而基于机器学习的方法通常只能修复部分事件属性。

Method: 开发了一个异构图神经网络模型，用于修复包含不完整事件的轨迹中缺失的属性。将该方法与基于自动编码器的最新方法进行了比较。

Result: 该模型在重建所有不同事件属性方面表现良好，优于现有方法。

Conclusion: 本文提出了一种基于异构图神经网络的模型，用于修复流程挖掘中事件日志中缺失的信息，并在合成日志和真实事件日志上进行了评估，结果表明该模型在重建所有不同事件属性方面表现良好。

Abstract: The quality of event logs in Process Mining is crucial when applying any form
of analysis to them. In real-world event logs, the acquisition of data can be
non-trivial (e.g., due to the execution of manual activities and related manual
recording or to issues in collecting, for each event, all its attributes), and
often may end up with events recorded with some missing information. Standard
approaches to the problem of trace (or log) reconstruction either require the
availability of a process model that is used to fill missing values by
leveraging different reasoning techniques or employ a Machine Learning/Deep
Learning model to restore the missing values by learning from similar cases. In
recent years, a new type of Deep Learning model that is capable of handling
input data encoded as graphs has emerged, namely Graph Neural Networks. Graph
Neural Network models, and even more so Heterogeneous Graph Neural Networks,
offer the advantage of working with a more natural representation of complex
multi-modal sequences like the execution traces in Process Mining, allowing for
more expressive and semantically rich encodings.
  In this work, we focus on the development of a Heterogeneous Graph Neural
Network model that, given a trace containing some incomplete events, will
return the full set of attributes missing from those events. We evaluate our
work against a state-of-the-art approach leveraging autoencoders on two
synthetic logs and four real event logs, on different types of missing values.
Different from state-of-the-art model-free approaches, which mainly focus on
repairing a subset of event attributes, the proposed approach shows very good
performance in reconstructing all different event attributes.

</details>


### [14] [QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering](https://arxiv.org/abs/2508.05197)
*Zhuohang Jiang,Pangjing Wu,Xu Yuan,Wenqi Fan,Qing Li*

Main category: cs.AI

TL;DR: QA-Dragon是一个改进的RAG系统，通过多模态和多跳推理显著提升了知识密集型VQA的准确性和知识覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法通常只从文本或图像中单独检索信息，限制了其处理需要多跳推理或最新事实知识的复杂查询。

Method: 提出了一种查询感知的动态RAG系统QA-Dragon，该系统包含领域路由器和搜索路由器，能够动态选择最优的检索策略，支持多模态、多轮和多跳推理。

Result: 在Meta CRAG-MM挑战赛中，QA-Dragon在单源任务上提升了5.06%，在多源任务上提升了6.35%，在多轮任务上提升了5.03%。

Conclusion: QA-Dragon系统在知识密集型VQA任务中显著提高了推理性能，在单源、多源和多轮任务上均取得了优于基线的成果。

Abstract: Retrieval-Augmented Generation (RAG) has been introduced to mitigate
hallucinations in Multimodal Large Language Models (MLLMs) by incorporating
external knowledge into the generation process, and it has become a widely
adopted approach for knowledge-intensive Visual Question Answering (VQA).
However, existing RAG methods typically retrieve from either text or images in
isolation, limiting their ability to address complex queries that require
multi-hop reasoning or up-to-date factual knowledge. To address this
limitation, we propose QA-Dragon, a Query-Aware Dynamic RAG System for
Knowledge-Intensive VQA. Specifically, QA-Dragon introduces a domain router to
identify the query's subject domain for domain-specific reasoning, along with a
search router that dynamically selects optimal retrieval strategies. By
orchestrating both text and image search agents in a hybrid setup, our system
supports multimodal, multi-turn, and multi-hop reasoning, enabling it to tackle
complex VQA tasks effectively. We evaluate our QA-Dragon on the Meta CRAG-MM
Challenge at KDD Cup 2025, where it significantly enhances the reasoning
performance of base models under challenging scenarios. Our framework achieves
substantial improvements in both answer accuracy and knowledge overlap scores,
outperforming baselines by 5.06% on the single-source task, 6.35% on the
multi-source task, and 5.03% on the multi-turn task.

</details>


### [15] [An Explainable Natural Language Framework for Identifying and Notifying Target Audiences In Enterprise Communication](https://arxiv.org/abs/2508.05267)
*Vítor N. Lourenço,Mohnish Dubey,Yunfei Bai,Audrey Depeige,Vivek Jain*

Main category: cs.AI

TL;DR: 利用RDF图数据库和LLM构建框架，提升大型维护组织沟通效率，解决信息过载和响应时间长的问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型维护组织中传统沟通方式导致的信息过载和响应时间过长的问题。

Method: 结合RDF图数据库和LLM，采用规划-协调架构，实现对自然语言查询的精确受众定位和透明推理。

Result: 能够让沟通负责人提出直观的查询，结合设备、制造商、维护工程师和设施等概念，提供可解释的结果，提高沟通效率，并保持对系统的信任。

Conclusion: 提出了一种结合RDF图数据库和LLM的新框架，用于解决大型维护组织中专家识别和跨复杂实体关系的沟通难题，提高沟通效率。

Abstract: In large-scale maintenance organizations, identifying subject matter experts
and managing communications across complex entities relationships poses
significant challenges -- including information overload and longer response
times -- that traditional communication approaches fail to address effectively.
We propose a novel framework that combines RDF graph databases with LLMs to
process natural language queries for precise audience targeting, while
providing transparent reasoning through a planning-orchestration architecture.
Our solution enables communication owners to formulate intuitive queries
combining concepts such as equipment, manufacturers, maintenance engineers, and
facilities, delivering explainable results that maintain trust in the system
while improving communication efficiency across the organization.

</details>


### [16] [A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents](https://arxiv.org/abs/2508.05311)
*Andrew Kiruluta*

Main category: cs.AI

TL;DR: 该论文提出了一种混合神经符号推理架构，通过结合决策树和大型语言模型，在多个推理基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有方法中符号和神经模块松散耦合的局限性，该研究提出了一种将决策树和随机森林嵌入到统一推理系统中的方法，以提高推理性能和可解释性。

Method: 该系统整合了基于决策树的模块和大型语言模型(LLM)代理，通过中心协调器维护信念状态一致性，并在结构化和非结构化输入上进行推理。

Result: 在ProofWriter、GSM8k和ARC等基准测试中，该系统分别在蕴含一致性、多步数学问题求解和抽象精度方面取得了显著的改进。

Conclusion: 该混合架构结合了基于决策树的符号推理和大型语言模型(LLM)的生成能力，在协调的多智能体框架内实现了强大的推理能力，并在推理基准测试中取得了优异的性能。

Abstract: We propose a hybrid architecture that integrates decision tree-based symbolic
reasoning with the generative capabilities of large language models (LLMs)
within a coordinated multi-agent framework. Unlike prior approaches that
loosely couple symbolic and neural modules, our design embeds decision trees
and random forests as callable oracles within a unified reasoning system.
Tree-based modules enable interpretable rule inference and causal logic, while
LLM agents handle abductive reasoning, generalization, and interactive
planning. A central orchestrator maintains belief state consistency and
mediates communication across agents and external tools, enabling reasoning
over both structured and unstructured inputs.
  The system achieves strong performance on reasoning benchmarks. On
\textit{ProofWriter}, it improves entailment consistency by +7.2\% through
logic-grounded tree validation. On GSM8k, it achieves +5.3\% accuracy gains in
multistep mathematical problems via symbolic augmentation. On \textit{ARC}, it
boosts abstraction accuracy by +6.0\% through integration of symbolic oracles.
Applications in clinical decision support and scientific discovery show how the
system encodes domain rules symbolically while leveraging LLMs for contextual
inference and hypothesis generation. This architecture offers a robust,
interpretable, and extensible solution for general-purpose neuro-symbolic
reasoning.

</details>


### [17] [The Term 'Agent' Has Been Diluted Beyond Utility and Requires Redefinition](https://arxiv.org/abs/2508.05338)
*Brinnae Bent*

Main category: cs.AI

TL;DR: 需要重新定义人工智能领域‘智能体’的概念，文章提出了一个新的框架。


<details>
  <summary>Details</summary>
Motivation: 人工智能领域中‘智能体’一词有多种解释，大型语言模型的发展加剧了这种歧义，对研究沟通、系统评估和可重复性以及政策制定造成巨大挑战。

Method: 历史分析和当代使用模式

Result: 提出一个框架，以清晰的最低要求定义系统何时可被认为是智能体，同时沿多维度刻画系统，为系统描述提供精确的词汇，同时保留该术语历史上的多方面特性。

Conclusion: 本文认为需要重新定义人工智能领域中‘智能体’这一术语，并提出了一个框架，以清晰的最低要求定义系统何时可被认为是智能体，同时沿环境交互、学习和适应、自主性、目标复杂性和时间连贯性等多维度刻画系统。

Abstract: The term 'agent' in artificial intelligence has long carried multiple
interpretations across different subfields. Recent developments in AI
capabilities, particularly in large language model systems, have amplified this
ambiguity, creating significant challenges in research communication, system
evaluation and reproducibility, and policy development. This paper argues that
the term 'agent' requires redefinition. Drawing from historical analysis and
contemporary usage patterns, we propose a framework that defines clear minimum
requirements for a system to be considered an agent while characterizing
systems along a multidimensional spectrum of environmental interaction,
learning and adaptation, autonomy, goal complexity, and temporal coherence.
This approach provides precise vocabulary for system description while
preserving the term's historically multifaceted nature. After examining
potential counterarguments and implementation challenges, we provide specific
recommendations for moving forward as a field, including suggestions for
terminology standardization and framework adoption. The proposed approach
offers practical tools for improving research clarity and reproducibility while
supporting more effective policy development.

</details>
