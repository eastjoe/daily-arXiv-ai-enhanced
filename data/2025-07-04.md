<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: 自我进化AI智能体STELLA在生物医学基准测试中表现出色，并随着经验积累持续提升性能。


<details>
  <summary>Details</summary>
Motivation: 生物医学数据的快速增长导致了研究领域的碎片化，而现有的AI智能体通常依赖于静态的工具集，限制了其适应性和可扩展性。

Method: 采用多智能体架构，通过不断发展的模板库和动态工具库来自主改进自身能力。

Result: 在Humanity's Last Exam、LAB-Bench: DBQA和LAB-Bench: LitQA基准测试中分别取得了约26%、54%和63%的准确率，优于现有领先模型。

Conclusion: STELLA，一个自我进化的AI智能体，在生物医学基准测试中取得了最先进的准确率，并且其性能随着经验的积累而系统性地提高。

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [2] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: HCVR:一种新的特征选择方法，结合参数间和参数与目标的相关性，在SPAMBASE数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了减少维度，提高分类器的性能。

Method: 提出了一种名为HCVR的轻量级基于规则的特征选择方法，该方法结合了参数到参数和参数到目标的相关性，通过反向消除冗余特征并保留相关特征。

Result: HCVR方法在SPAMBASE数据集上比传统的非迭代(CFS, mRMR, MI)和迭代(RFE, SFS, 遗传算法)特征选择技术取得了更好的性能。

Conclusion: HCVR方法在SPAMBASE数据集上表现出比传统非迭代和迭代特征选择方法更好的性能。

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [3] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: Survey on efficient LLM reasoning methods, categorized by controllability and adaptability, with benchmarks and future directions.


<details>
  <summary>Details</summary>
Motivation: Current LLMs are inefficient at reasoning, using fixed compute regardless of task complexity.  This survey aims to improve computational efficiency by exploring methods for adaptable and controllable inference.

Method: Comprehensive review and benchmarking of existing efficient test-time compute strategies for LLMs.

Result: A two-tiered taxonomy of TTC methods (L1 and L2) is presented, along with benchmarks highlighting performance/token usage trade-offs.  Emerging trends and future challenges are discussed.

Conclusion: This survey reviews efficient test-time compute (TTC) strategies for improving the computational efficiency of Large Language Model (LLM) reasoning, categorizing methods into L1-controllability (fixed compute budgets) and L2-adaptiveness (dynamically scaled inference).  It benchmarks leading LLMs, highlighting trade-offs between reasoning performance and token usage, and discusses future challenges in making LLMs more efficient.

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [4] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: SciGym基准测试显示，LLM在科学实验设计和分析方面能力有限，尤其在复杂系统中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 评估LLM的科学能力，特别是设计实验和解释结果的能力，但目前缺乏评估这些能力的有效方法。

Method: 提出SciGym基准，通过模拟生物系统来评估LLM在开放式科学发现任务中的迭代实验设计和分析能力。

Result: 评估了六个先进的LLM在137个小型系统上的表现，结果显示模型性能随着系统复杂性的增加而显著下降。

Conclusion: 大型语言模型(LLM)在科学实验设计和结果分析方面仍有很大的提升空间，尤其是在系统复杂性增加的情况下。

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [5] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: 神经科学能帮助改进AI的持续学习能力，实现AI对环境变化的快速适应。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型训练成本高昂且缓慢，动物却能快速适应环境变化，本文旨在探索如何将神经科学的学习机制应用于AI，以提高AI系统的适应性。

Method: 文献综述和比较分析

Result: 提出了神经科学见解如何改进AI持续学习和情境学习的方案，并指出了AI对神经科学研究的潜在贡献。

Conclusion: 本文探讨了人工智能从神经科学学习的可能性，并提出了一份研究议程，以促进人工智能和神经科学之间的交叉研究。

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [6] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: 审计研究数据能改进自动化招聘算法，平衡基准率方法存在缺陷，个体处理效应估计方法更有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅基于便利样本评估，存在选择偏差和标签偏差；审计研究数据能提供高质量数据，支持对歧视进行严格估计。

Method: 利用审计研究数据训练和评估自动化招聘算法，并提出基于个体处理效应估计方法的干预措施。

Result: 发现平衡基准率方法在传统度量下看似有效，但在适当度量下仍存在约10%的差异；基于个体处理效应估计方法的干预措施能进一步减少算法歧视。

Conclusion: 本文研究了如何利用审计研究数据改进自动化招聘算法的训练和评估，发现常用的基于平衡基准率的公平性干预方法在传统度量下看似达到了均等，但在适当的度量下仍存在约10%的差异，并提出基于个体处理效应估计方法的干预措施以进一步减少算法歧视。

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [7] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: 数据多样化策略能有效提升LLM的数学推理能力，Diversified-ThinkSolve方法表现最佳且效率高。


<details>
  <summary>Details</summary>
Motivation: 解决偏好学习中数学推理的挑战，提升大型语言模型的数学推理能力。

Method: 评估三种数据生成方法：温度采样、思维链提示和蒙特卡洛树搜索，并提出了一种新的结构化方法Diversified-ThinkSolve。

Result: Diversified-ThinkSolve方法在GSM8K和MATH数据集上分别提高了7.1%和4.2%的性能，计算开销仅为基线的1.03倍。

Conclusion: 通过数据多样化策略改进大型语言模型的数学推理能力，Diversified-ThinkSolve方法在GSM8K和MATH数据集上取得了最佳效果，且计算开销较小。

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [8] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: LLM在角色扮演中的言行不一致，需要改进其一致性以更好地用于行为研究。


<details>
  <summary>Details</summary>
Motivation: LLM作为角色扮演代理生成合成数据用于人类行为研究日益增多，确保其输出与其角色保持一致至关重要。

Method: 构建了一个评估框架，使用GenAgents角色库和信任博弈，测量LLM在角色扮演中信念与行为的一致性，并研究了不同因素（例如信念类型、信息呈现方式、预测时间）的影响。

Result: 研究揭示了LLM在角色扮演中言行存在系统性差异，即使编码了似是而非的信念，也可能无法一致地应用。

Conclusion: LLM扮演角色时，其言行不一致，尤其是在预测未来行为时；研究发现需要改进LLM在角色扮演中的行为一致性，以更好地应用于行为研究。

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [9] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: 使用多智能体Q学习算法研究空间囚徒困境中稀释和移动性的影响，发现固定更新规则和学习更新规则的游戏可能等效，且多种行为下种群间形成互利共生。


<details>
  <summary>Details</summary>
Motivation: 研究空间囚徒困境博弈中强化学习的静态智能体如何通过不同的机制（包括噪声注入、不同类型的学习算法和邻居收益知识）学习合作。

Method: 使用独立多智能体Q学习算法研究空间囚徒困境博弈中稀释和移动性的影响。

Result: 观察到一系列影响，包括具有固定更新规则的游戏在性质上可能等同于具有学习更新规则的游戏，以及在定义多个行动时，种群之间会形成一种互利共生的效应。

Conclusion: 研究发现，具有固定更新规则的游戏在性质上可能等同于具有学习更新规则的游戏，并且在定义多个行动时，种群之间会形成一种互利共生的效应。

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [10] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: NL2FLOW系统自动生成规划问题并评估LLM，结果显示直接推理更高效，顶级模型有效/最优计划生成成功率达86%/69%。


<details>
  <summary>Details</summary>
Motivation: 当前增强LLM规划和推理能力的进展受到可扩展、可靠数据生成和评估瓶颈的严重阻碍。

Method: 介绍了NL2FLOW系统，该系统能够自动生成自然语言、结构化中间表示和形式化PDDL三种形式的规划问题，并对生成的计划进行严格评估。利用该系统生成了一个包含2296个问题的自动化工作流生成领域数据集，并对多个开源的指令微调LLM进行了评估。

Result: 实验结果表明，高性能模型在生成有效计划和最优计划方面取得了显著成功，但中间翻译步骤可能会降低性能。

Conclusion: 该论文介绍了NL2FLOW系统，用于自动生成规划问题并评估LLM的规划能力，实验结果表明，最好的模型在生成有效计划和最优计划方面分别达到了86%和69%的成功率，并发现直接从自然语言推理到行动可能比引入中间翻译步骤更高效。

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [11] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 信仰修正研究应关注机制的能力而非仅限于约束。


<details>
  <summary>Details</summary>
Motivation: 当前信仰修正领域的研究存在不足，需要更全面的分析和新的研究视角。

Method: 分析现有研究方法的优缺点，并提出新的研究方向。

Result: 识别出信仰修正机制的多种能力，例如可塑性、等同性、教条性等，并指出不同修正机制拥有不同的能力组合。

Conclusion: 本文批判了信仰修正领域过多关注假设而忽略对现有方法分析的现状，并提出应关注修正机制的能力，例如是否能达到特定信念状态、是否能使两种条件同样可信等。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [12] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: OMS框架克服了现有LLM方法的局限性，实现了无需训练数据、在线多目标优化和自主质量评估的关键词生成。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的关键词生成方法存在依赖大规模查询关键词对数据、缺乏在线多目标性能监控和优化以及关键词选择质量控制薄弱等局限性。

Method: 提出了一种名为OMS的关键词生成框架，该框架无需训练数据，能够在线监控性能并据此调整，同时基于多个性能指标进行多目标优化，并能够自主评估关键词质量。

Result: 实验结果表明，OMS框架优于现有方法，消融实验和人工评估验证了各组件的有效性以及生成的关键词质量。

Conclusion: OMS框架在基准测试和真实广告活动中均优于现有方法，消融实验和人工评估也证实了各组件的有效性和关键词生成的质量。

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [13] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: AI原生自主实验室，支持复杂生物分子工程实验，多用户，高效率，可实现规模化科学服务。


<details>
  <summary>Details</summary>
Motivation: 实现能够独立进行复杂实验并服务于非专业人士的自主科学研究，需要人工智能驱动下的一种根本性范式转变。现有的自主实验系统仅限于目标单一、实验流程简单的领域。

Method: 该系统自主管理仪器，制定特定实验程序和优化启发式方法，同时服务于多个用户请求。其核心是模型、实验和仪器的协同设计理念，支持AI模型和自动化系统的共同进化。

Result: 该自主实验室支持核酸的基本功能（包括合成、转录、扩增和测序），并可应用于疾病诊断、药物开发和信息存储等领域。在无需人工干预的情况下，它能自主优化实验性能，达到与人类科学家相同的水平。

Conclusion: 该AI原生自主实验室支持复杂的、多目标的实验，并在多用户场景下显著提高了仪器利用率和实验效率，为生物材料研究克服专家依赖和资源障碍铺平了道路，为规模化的“科学即服务”建立了蓝图。

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [14] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: 用范畴论构建机器学习模型，提高可解释性，以多元线性回归为例，建立参数和残差之间的关系。


<details>
  <summary>Details</summary>
Motivation: 提升机器学习的可理解性和可解释性，以促进AI的更好社会应用。

Method: 使用范畴论，特别是伴随函子对，对多元线性回归模型进行建模，定义了参数和数据两个范畴，并通过高斯-马尔可夫伴随来描述信息流。

Result: 提出了一个基于范畴论的监督学习语义框架，明确描述了参数和残差之间的对偶信息流，并将其与最小二乘估计联系起来。

Conclusion: 本文通过范畴论的视角重新构建机器学习模型，为理解和构建AI系统提供了一个语义框架，并以多元线性回归模型为例，阐明了参数和残差之间的结构性相互作用。

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [15] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: 改进任务清晰度显著增强大型语言模型的Coq定理证明能力，效果优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 研究如何提高大型语言模型的推理能力，特别是针对Coq定理证明任务。

Method: 引入概念级度量评估任务清晰度，使用选择性概念展开丰富任务描述，并采用规划器-执行器架构。

Result: 添加结构化语义上下文后，清晰度评分提高1.85倍（44.5%提升至82.3%），证明成功率提高2.1倍（21.8%提升至45.8%），优于现有最佳技术Graph2Tac (33.2%)，微调小型模型后性能进一步提升至48.6%。

Conclusion: 提高大型语言模型任务清晰度可以增强其推理能力，实验证明在Coq定理证明中，添加结构化语义上下文可以显著提高模型的清晰度评分和证明成功率，优于现有技术。

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [16] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: 通过改进搜索策略和算子集，AI研究代理在MLE-bench上的表现得到显著提升，成功率提高近8%。


<details>
  <summary>Details</summary>
Motivation: 提高AI研究代理在MLE-bench上的性能，该基准测试代理在Kaggle竞赛中解决现实世界的机器学习问题。

Method: 将AI研究代理形式化为搜索策略，迭代地使用算子修改候选方案，系统地改变不同的算子集和搜索策略（贪婪、MCTS、进化）。

Result: 最佳搜索策略和算子集组合在MLE-bench lite上取得了最先进的结果，将获得Kaggle奖牌的成功率从39.6%提高到47.7%。

Conclusion: 研究表明，搜索策略和算子集的结合对于提高AI研究代理在MLE-bench上的性能至关重要，最佳组合将Kaggle奖牌成功率从39.6%提高到47.7%。

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [17] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: 本文分析了AI集体决策中责任扩散和差距的计算复杂性，结果表明其复杂度分别很高。


<details>
  <summary>Details</summary>
Motivation: 研究AI领域中集体决策责任的计算复杂性。

Method: 计算复杂性分析

Result: 无扩散和无差距决策机制集合的计算复杂度分别为Π2完全和Π3完全，两者的交集为Π2完全。

Conclusion: 本文探究了集体决策中责任的两个重要属性——扩散和差距——的计算复杂性，并证明了无扩散和无差距决策机制集合分别为Π2完全和Π3完全，而两者的交集为Π2完全。

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [18] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: 提出DynamiCare框架，模拟真实世界中迭代、交互的医疗诊断过程，并建立了相应的基准。


<details>
  <summary>Details</summary>
Motivation: 现有的医疗AI agent框架主要关注单轮任务，与真实世界迭代、不确定的诊断过程存在差距。

Method: 构建了MIMIC-Patient数据集，并提出了DynamiCare动态多Agent框架，模拟医生团队与病人系统交互的诊断过程。

Result: 通过大量实验，验证了DynamiCare框架的可行性和有效性，建立了首个动态临床决策基准。

Conclusion: DynamiCare框架有效模拟了动态的、多轮的临床诊断过程，为基于LLM的动态临床决策制定建立了首个基准。

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [19] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: LLM在迭代囚徒困境中表现出竞争力，并展现出不同公司模型的独特策略风格。


<details>
  <summary>Details</summary>
Motivation: 研究LLM是否构成一种新型战略智能，能够在竞争环境中推理目标。

Method: 通过在IPD锦标赛中加入不同终止概率，测试了来自OpenAI、Google和Anthropic的LLM与经典策略的竞争力。分析了近32000个模型提供的理由。

Result: LLM具有竞争力，表现出独特的策略特征：Google Gemini模型策略强硬；OpenAI模型合作性强；Anthropic Claude模型最宽容。模型的决策依赖于对时间范围和对手策略的推理。

Conclusion: 大型语言模型(LLM)在迭代囚徒困境(IPD)锦标赛中表现出高度竞争力，并展现出独特的策略特征。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [20] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: HiRA框架通过分离战略规划和专业执行，提高了复杂信息搜索任务的效率和答案质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于推理的方法使用单个模型处理高层规划和详细执行，导致推理效率低下且可扩展性有限。

Method: 提出了一种名为HiRA的分层框架，将战略规划与专业执行分离，并将复杂搜索任务分解为聚焦的子任务，分配给配备外部工具和推理能力的特定领域Agent，最终通过结构化集成机制协调结果。

Result: 在四个复杂的跨模态深度搜索基准测试中，HiRA显著优于最先进的RAG和基于Agent的系统，在答案质量和系统效率方面均有所提高。

Conclusion: HiRA框架显著优于现有RAG和基于Agent的系统，在答案质量和系统效率方面均有所提升。

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [21] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: 使用基于智能体的AI和人机交互，实现高效的硬件设计验证，在五个开源设计上取得了95%以上的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路日益复杂，其开发过程也日益复杂，传统的硬件设计验证方法费时费力。大型语言模型(LLM)的出现为硬件设计验证提供了新的可能性。

Method: 采用基于智能体的AI方法，结合人机交互(HITL)干预，进行动态、迭代和自我反思的端到端硬件设计和验证过程。

Result: 在五个开源设计上实现了超过95%的覆盖率，并减少了验证时间，同时展示了优越的性能、适应性和可配置性。

Conclusion: 提出了一种基于智能体的AI方法，用于硬件设计验证，该方法在五个开源设计上实现了超过95%的覆盖率，并减少了验证时间，同时展示了优越的性能、适应性和可配置性。

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [22] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu,Haoxi Li,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.AI

TL;DR: 针对长推理模型过度思考问题，提出TH2T两阶段微调策略，通过difficulty-hypnosis和redundancy-hypnosis，显著降低推理成本，提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有的长推理模型(LRMs)存在过度思考的问题，其主要原因是模型会像人类一样识别任务属性（难度级别）后再进行求解，导致推理过程缺乏针对性。

Method: 两阶段微调策略：首先，通过在模型输出的前缀中引入difficulty-hypnosis干预内部推理轨迹，结合异构的短长推理数据集，提高模型对任务难度的敏感性；其次，扩展redundancy-hypnosis到内部推理过程，引导模型识别冗余结构，生成更简洁的推理输出。

Result: 实验证明，TH2T显著降低了推理成本，同时保持了性能稳定性，输出结果表现出明显的难度感知能力和减少的冗余性。

Conclusion: TH2T策略显著降低了推理成本，同时保持了性能稳定性，尤其在简单任务上降低了超过70%，在困难任务上降低了40%。

Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [23] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard,Christof Imhof,Tansu Pancar,Ioan-Sorin Comsa,Martin Hlosta,Nicole Bergamin,Per Bergamin*

Main category: cs.AI

TL;DR: 利用机器学习算法和SHAP方法，对远程教育学生参与度进行检测，准确率达91%，并提出干预策略。


<details>
  <summary>Details</summary>
Motivation: 远程教育学生参与度下降会导致严重的长期后果，本文旨在检测远程教育中非强制性练习中的学生参与度下降情况。

Method: 使用Moodle平台的学生日志数据，训练和比较八种机器学习算法，并使用SHAP方法构建可解释机器学习框架。

Result: 实现了91%的平衡准确率，其中约85%的脱离学习的学生被正确检测。

Conclusion: 本文提出了一种基于机器学习的远程教育学生参与度检测方法，并实现了91%的平衡准确率，其中85%的脱离学习的学生被正确识别。该方法具有可解释性，并讨论了如何及时干预以最大限度地减少学生对在线学习中自愿任务的参与度下降。

Abstract: Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [24] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schmöcker,Lennart Kampmann,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 两种新颖的抽象丢弃方案OGA-IAAD和OGA-CAD，安全高效地提升MCTS性能。


<details>
  <summary>Details</summary>
Motivation: 改进蒙特卡洛树搜索(MCTS)，解决非精确抽象导致的近似误差问题。

Method: 提出两种抽象丢弃方案：OGA-IAAD 和 OGA-CAD。

Result: OGA-IAAD 和 OGA-CAD 两种方案均提升了MCTS性能，且不会导致性能下降。

Conclusion: 提出两种新颖的抽象丢弃方案，OGA-IAAD 和 OGA-CAD，在保证性能不下降的同时提高MCTS性能，OGA-IAAD适用于时间紧迫的场景，OGA-CAD则在相同迭代次数下提升性能。

Abstract: One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [25] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer,Xiaotong Ji,Rasul Tutunov,Anthony Bordg,Jun Wang,Haitham Bou Ammar*

Main category: cs.AI

TL;DR: 利用自生成目标条件马尔可夫决策过程和类似 MCTS 的算法，Bourbaki (7B) 在 PutnamBench 上取得了显著进展。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型 (LLM) 在自动定理证明 (ATP) 中进行推理的挑战，特别是对于需要复杂、多步骤推理的 PutnamBench 等基准测试。

Method: 提出了一种新的基于自生成目标条件马尔可夫决策过程 (sG-MDP) 的框架，该框架使智能体能够根据不断变化的证明状态生成和追求其子目标，并应用类似蒙特卡洛树搜索 (MCTS) 的算法来解决 sG-MDP。

Result: 在 PutnamBench 上取得了 state-of-the-art 的结果，解决了 26 个问题。

Conclusion: Bourbaki (7B) 在 PutnamBench 基准测试中解决了 26 个问题，实现了该规模模型的最新技术水平。

Abstract: Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>
