{"id": "2508.16681", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16681", "abs": "https://arxiv.org/abs/2508.16681", "authors": ["Eric Zhang"], "title": "Revisiting Rule-Based Stuttering Detection: A Comprehensive Analysis of Interpretable Models for Clinical Applications", "comment": null, "summary": "Stuttering affects approximately 1% of the global population, impacting\ncommunication and quality of life. While recent advances in deep learning have\npushed the boundaries of automatic speech dysfluency detection, rule-based\napproaches remain crucial for clinical applications where interpretability and\ntransparency are paramount. This paper presents a comprehensive analysis of\nrule-based stuttering detection systems, synthesizing insights from multiple\ncorpora including UCLASS, FluencyBank, and SEP-28k. We propose an enhanced\nrule-based framework that incorporates speaking-rate normalization, multi-level\nacoustic feature analysis, and hierarchical decision structures. Our approach\nachieves competitive performance while maintaining complete\ninterpretability-critical for clinical adoption. We demonstrate that rule-based\nsystems excel particularly in prolongation detection (97-99% accuracy) and\nprovide stable performance across varying speaking rates. Furthermore, we show\nhow these interpretable models can be integrated with modern machine learning\npipelines as proposal generators or constraint modules, bridging the gap\nbetween traditional speech pathology practices and contemporary AI systems. Our\nanalysis reveals that while neural approaches may achieve marginally higher\naccuracy in unconstrained settings, rule-based methods offer unique advantages\nin clinical contexts where decision auditability, patient-specific tuning, and\nreal-time feedback are essential.", "AI": {"tldr": "\u57fa\u4e8e\u89c4\u5219\u7684\u53e3\u5403\u68c0\u6d4b\u7cfb\u7edf\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u5177\u6709\u72ec\u7279\u4f18\u52bf\uff0c\u53ef\u4e0e\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u7ba1\u9053\u96c6\u6210\u3002", "motivation": "\u76ee\u524d\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u53e3\u5403\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u4ecd\u7136\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5176\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u662f\u91cd\u4e2d\u4e4b\u91cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u53e3\u5403\u68c0\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u8bed\u901f\u5f52\u4e00\u5316\u3001\u591a\u5c42\u6b21\u58f0\u5b66\u7279\u5f81\u5206\u6790\u548c\u5206\u5c42\u51b3\u7b56\u7ed3\u6784\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5b8c\u5168\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u64c5\u957f\u68c0\u6d4b\u5ef6\u957f\u97f3\uff0897-99%\u7684\u51c6\u786e\u7387\uff09\uff0c\u5e76\u5728\u4e0d\u540c\u8bed\u901f\u4e0b\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "\u672c\u6587\u5bf9\u57fa\u4e8e\u89c4\u5219\u7684\u53e3\u5403\u68c0\u6d4b\u7cfb\u7edf\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u5b8c\u5168\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u64c5\u957f\u68c0\u6d4b\u5ef6\u957f\u97f3\uff0c\u5e76\u5728\u4e0d\u540c\u8bed\u901f\u4e0b\u8868\u73b0\u7a33\u5b9a\u3002\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u5728\u9700\u8981\u51b3\u7b56\u53ef\u5ba1\u8ba1\u6027\u3001\u7279\u5b9a\u60a3\u8005\u8c03\u6574\u548c\u5b9e\u65f6\u53cd\u9988\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u5177\u6709\u72ec\u7279\u7684\u4f18\u52bf\u3002"}}
{"id": "2508.16747", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16747", "abs": "https://arxiv.org/abs/2508.16747", "authors": ["Liu Liu", "Rui Dai"], "title": "Explainable AI for Predicting and Understanding Mathematics Achievement: A Cross-National Analysis of PISA 2018", "comment": null, "summary": "Understanding the factors that shape students' mathematics performance is\nvital for designing effective educational policies. This study applies\nexplainable artificial intelligence (XAI) techniques to PISA 2018 data to\npredict math achievement and identify key predictors across ten countries\n(67,329 students). We tested four models: Multiple Linear Regression (MLR),\nRandom Forest (RF), CATBoost, and Artificial Neural Networks (ANN), using\nstudent, family, and school variables. Models were trained on 70% of the data\n(with 5-fold cross-validation) and tested on 30%, stratified by country.\nPerformance was assessed with R^2 and Mean Absolute Error (MAE). To ensure\ninterpretability, we used feature importance, SHAP values, and decision tree\nvisualizations. Non-linear models, especially RF and ANN, outperformed MLR,\nwith RF balancing accuracy and generalizability. Key predictors included\nsocio-economic status, study time, teacher motivation, and students' attitudes\ntoward mathematics, though their impact varied across countries. Visual\ndiagnostics such as scatterplots of predicted vs actual scores showed RF and\nCATBoost aligned closely with actual performance. Findings highlight the\nnon-linear and context-dependent nature of achievement and the value of XAI in\neducational research. This study uncovers cross-national patterns, informs\nequity-focused reforms, and supports the development of personalized learning\nstrategies.", "AI": {"tldr": "\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5206\u6790PISA\u6570\u636e\uff0c\u53d1\u73b0\u5f71\u54cd\u5b66\u751f\u6570\u5b66\u6210\u7ee9\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u5f3a\u8c03\u4e86\u6559\u80b2\u653f\u7b56\u5e94\u8003\u8651\u975e\u7ebf\u6027\u5173\u7cfb\u548c\u56fd\u5bb6\u95f4\u7684\u5dee\u5f02\u6027\u3002", "motivation": "\u7406\u89e3\u5f71\u54cd\u5b66\u751f\u6570\u5b66\u6210\u7ee9\u7684\u56e0\u7d20\uff0c\u4e3a\u5236\u5b9a\u6709\u6548\u7684\u6559\u80b2\u653f\u7b56\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5e94\u7528\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u6280\u672f\uff0c\u5305\u62ec\u591a\u5143\u7ebf\u6027\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001CATBoost\u548c\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7b49\u6a21\u578b\uff0c\u5bf9PISA 2018\u6570\u636e\u8fdb\u884c\u5206\u6790\u3002", "result": "\u975e\u7ebf\u6027\u6a21\u578b\uff08\u7279\u522b\u662f\u968f\u673a\u68ee\u6797\u548c\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff09\u7684\u9884\u6d4b\u6027\u80fd\u4f18\u4e8e\u7ebf\u6027\u6a21\u578b\uff1b \u5173\u952e\u9884\u6d4b\u56e0\u7d20\u5305\u62ec\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\u3001\u5b66\u4e60\u65f6\u95f4\u3001\u6559\u5e08\u79ef\u6781\u6027\u548c\u5b66\u751f\u5bf9\u6570\u5b66\u7684\u6001\u5ea6\uff0c\u4f46\u5176\u5f71\u54cd\u5728\u4e0d\u540c\u56fd\u5bb6\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u672c\u7814\u7a76\u5229\u7528\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u7ed3\u5408PISA 2018\u6570\u636e\uff0c\u9884\u6d4b\u6570\u5b66\u6210\u7ee9\u5e76\u8bc6\u522b\u5173\u952e\u9884\u6d4b\u56e0\u7d20\uff0c\u53d1\u73b0\u975e\u7ebf\u6027\u6a21\u578b\uff08\u5982\u968f\u673a\u68ee\u6797\uff09\u4f18\u4e8e\u7ebf\u6027\u6a21\u578b\uff0c\u5173\u952e\u9884\u6d4b\u56e0\u7d20\u5305\u62ec\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\u3001\u5b66\u4e60\u65f6\u95f4\u3001\u6559\u5e08\u79ef\u6781\u6027\u548c\u5b66\u751f\u6001\u5ea6\u7b49\uff0c\u5176\u5f71\u54cd\u56e0\u56fd\u5bb6\u800c\u5f02\u3002"}}
{"id": "2508.16777", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16777", "abs": "https://arxiv.org/abs/2508.16777", "authors": ["Mingyang Li", "Viktor Schlegel", "Tingting Mu", "Wuraola Oyewusi", "Kai Kang", "Goran Nenadic"], "title": "Evaluation and LLM-Guided Learning of ICD Coding Rationales", "comment": null, "summary": "Automated clinical coding involves mapping unstructured text from Electronic\nHealth Records (EHRs) to standardized code systems such as the International\nClassification of Diseases (ICD). While recent advances in deep learning have\nsignificantly improved the accuracy and efficiency of ICD coding, the lack of\nexplainability in these models remains a major limitation, undermining trust\nand transparency. Current explorations about explainability largely rely on\nattention-based techniques and qualitative assessments by physicians, yet lack\nsystematic evaluation using consistent criteria on high-quality rationale\ndatasets, as well as dedicated approaches explicitly trained to generate\nrationales for further enhancing explanation. In this work, we conduct a\ncomprehensive evaluation of the explainability of the rationales for ICD coding\nthrough two key lenses: faithfulness that evaluates how well explanations\nreflect the model's actual reasoning and plausibility that measures how\nconsistent the explanations are with human expert judgment. To facilitate the\nevaluation of plausibility, we construct a new rationale-annotated dataset,\noffering denser annotations with diverse granularity and aligns better with\ncurrent clinical practice, and conduct evaluation across three types of\nrationales of ICD coding. Encouraged by the promising plausibility of\nLLM-generated rationales for ICD coding, we further propose new rationale\nlearning methods to improve the quality of model-generated rationales, where\nrationales produced by prompting LLMs with/without annotation examples are used\nas distant supervision signals. We empirically find that LLM-generated\nrationales align most closely with those of human experts. Moreover,\nincorporating few-shot human-annotated examples not only further improves\nrationale generation but also enhances rationale-learning approaches.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86ICD\u7f16\u7801\u89e3\u91ca\u7684\u5408\u7406\u6027\uff0c\u6784\u5efa\u65b0\u7684\u6570\u636e\u96c6\u5e76\u63d0\u51fa\u6539\u8fdb\u6a21\u578b\u89e3\u91ca\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660eLLM\u751f\u6210\u7684\u89e3\u91ca\u6700\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\uff0c\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u867d\u7136\u63d0\u9ad8\u4e86ICD\u7f16\u7801\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u672c\u6587\u65e8\u5728\u63d0\u9ad8ICD\u7f16\u7801\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u672c\u6587\u901a\u8fc7\u4e24\u4e2a\u5173\u952e\u89c6\u89d2\uff08\u5fe0\u5b9e\u6027\u548c\u5408\u7406\u6027\uff09\u8bc4\u4f30\u4e86ICD\u7f16\u7801\u7684\u5408\u7406\u6027\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u5e26\u6709\u6ce8\u91ca\u7684\u6570\u636e\u5e93\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e09\u79cd\u7c7b\u578b\u7684ICD\u7f16\u7801\u7684\u5408\u7406\u6027\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5408\u7406\u6027\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528LLM\u751f\u6210\u7684\u7406\u7531\u4f5c\u4e3a\u8fdc\u7a0b\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u5e26\u6709\u6ce8\u91ca\u7684\u6570\u636e\u5e93\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5408\u7406\u6027\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528LLM\u751f\u6210\u7684\u7406\u7531\u4f5c\u4e3a\u8fdc\u7a0b\u76d1\u7763\u4fe1\u53f7\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u751f\u6210\u7684\u5408\u7406\u6027\u7684\u8d28\u91cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLLM\u751f\u6210\u7684\u7406\u7531\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u7406\u7531\u6700\u63a5\u8fd1\uff0c\u5e76\u4e14\u52a0\u5165\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u7684\u4f8b\u5b50\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7406\u7531\u751f\u6210\u548c\u7406\u7531\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u5bf9ICD\u7f16\u7801\u7684\u5408\u7406\u6027\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5408\u7406\u6027\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u751f\u6210\u7684\u5408\u7406\u6027\u7684\u8d28\u91cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLLM\u751f\u6210\u7684\u7406\u7531\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u7406\u7531\u6700\u63a5\u8fd1\uff0c\u5e76\u4e14\u52a0\u5165\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u7684\u4f8b\u5b50\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7406\u7531\u751f\u6210\u548c\u7406\u7531\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2508.16821", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16821", "abs": "https://arxiv.org/abs/2508.16821", "authors": ["Sam Earle", "Graham Todd", "Yuchen Li", "Ahmed Khalifa", "Muhammad Umair Nasir", "Zehua Jiang", "Andrzej Banburski-Fahey", "Julian Togelius"], "title": "PuzzleJAX: A Benchmark for Reasoning and Learning", "comment": "25 pages, 11 figures, 2 tables", "summary": "We introduce PuzzleJAX, a GPU-accelerated puzzle game engine and description\nlanguage designed to support rapid benchmarking of tree search, reinforcement\nlearning, and LLM reasoning abilities. Unlike existing GPU-accelerated learning\nenvironments that provide hard-coded implementations of fixed sets of games,\nPuzzleJAX allows dynamic compilation of any game expressible in its\ndomain-specific language (DSL). This DSL follows PuzzleScript, which is a\npopular and accessible online game engine for designing puzzle games. In this\npaper, we validate in PuzzleJAX several hundred of the thousands of games\ndesigned in PuzzleScript by both professional designers and casual creators\nsince its release in 2013, thereby demonstrating PuzzleJAX's coverage of an\nexpansive, expressive, and human-relevant space of tasks. By analyzing the\nperformance of search, learning, and language models on these games, we show\nthat PuzzleJAX can naturally express tasks that are both simple and intuitive\nto understand, yet often deeply challenging to master, requiring a combination\nof control, planning, and high-level insight.", "AI": {"tldr": "PuzzleJAX\u662f\u4e00\u4e2aGPU\u52a0\u901f\u7684\u76ca\u667a\u6e38\u620f\u5f15\u64ce\uff0c\u652f\u6301\u5feb\u901f\u57fa\u51c6\u6d4b\u8bd5\u591a\u79cdAI\u7b97\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u5e7f\u6cdb\u6e38\u620f\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u652f\u6301\u5feb\u901f\u57fa\u51c6\u6d4b\u8bd5\u6811\u641c\u7d22\u3001\u5f3a\u5316\u5b66\u4e60\u548cLLM\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u53ef\u4ee5\u52a8\u6001\u7f16\u8bd1\u5404\u79cd\u76ca\u667a\u6e38\u620f\u7684\u5e73\u53f0\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2aGPU\u52a0\u901f\u7684\u76ca\u667a\u6e38\u620f\u5f15\u64ce\u548c\u63cf\u8ff0\u8bed\u8a00PuzzleJAX\uff0c\u5e76\u7528\u5176\u9a8c\u8bc1\u4e86\u6570\u767e\u4e2aPuzzleScript\u6e38\u620f\u3002", "result": "\u9a8c\u8bc1\u4e86PuzzleJAX\u53ef\u4ee5\u8868\u8fbe\u7b80\u5355\u6613\u61c2\u4f46\u6781\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u9700\u8981\u7ed3\u5408\u63a7\u5236\u3001\u89c4\u5212\u548c\u9ad8\u5c42\u6b21\u7684\u6d1e\u5bdf\u529b\u3002", "conclusion": "PuzzleJAX\u662f\u4e00\u4e2aGPU\u52a0\u901f\u7684\u76ca\u667a\u6e38\u620f\u5f15\u64ce\u548c\u63cf\u8ff0\u8bed\u8a00\uff0c\u7528\u4e8e\u652f\u6301\u5feb\u901f\u57fa\u51c6\u6d4b\u8bd5\u6811\u641c\u7d22\u3001\u5f3a\u5316\u5b66\u4e60\u548cLLM\u63a8\u7406\u80fd\u529b\u3002\u5b83\u53ef\u4ee5\u52a8\u6001\u7f16\u8bd1\u4efb\u4f55\u53ef\u4ee5\u7528\u5176\u7279\u5b9a\u9886\u57df\u8bed\u8a00\uff08DSL\uff09\u8868\u8fbe\u7684\u6e38\u620f\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6570\u767e\u4e2aPuzzleScript\u6e38\u620f\uff0c\u5c55\u793a\u4e86\u5176\u5bf9\u5e7f\u6cdb\u4e14\u5177\u6709\u6311\u6218\u6027\u4efb\u52a1\u7684\u8986\u76d6\u80fd\u529b\u3002"}}
{"id": "2508.16839", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16839", "abs": "https://arxiv.org/abs/2508.16839", "authors": ["Shayan Vassef", "Soorya Ram Shimegekar", "Abhay Goyal", "Koustuv Saha", "Pi Zonooz", "Navin Kumar"], "title": "Route-and-Execute: Auditable Model-Card Matching and Specialty-Level Deployment", "comment": null, "summary": "Clinical workflows are fragmented as a patchwork of scripts and task-specific\nnetworks that often handle triage, task selection, and model deployment. These\npipelines are rarely streamlined for data science pipeline, reducing efficiency\nand raising operational costs. Workflows also lack data-driven model\nidentification (from imaging/tabular inputs) and standardized delivery of model\noutputs. In response, we present a practical, healthcare-first framework that\nuses a single vision-language model (VLM) in two complementary roles. First\n(Solution 1), the VLM acts as an aware model-card matcher that routes an\nincoming image to the appropriate specialist model via a three-stage workflow\n(modality -> primary abnormality -> model-card id). Checks are provided by (i)\nstagewise prompts that allow early exit via None/Normal/Other and (ii) a\nstagewise answer selector that arbitrates between the top-2 candidates at each\nstage, reducing the chance of an incorrect selection and aligning the workflow\nwith clinical risk tolerance. Second (Solution 2), we fine-tune the VLM on\nspecialty-specific datasets ensuring a single model covers multiple downstream\ntasks within each specialty, maintaining performance while simplifying\ndeployment. Across gastroenterology, hematology, ophthalmology, and pathology,\nour single-model deployment matches or approaches specialized baselines.\n  Compared with pipelines composed of many task-specific agents, this approach\nshows that one VLM can both decide and do. It may reduce effort by data\nscientists, shorten monitoring, increase the transparency of model selection\n(with per-stage justifications), and lower integration overhead.", "AI": {"tldr": "\u5355\u4e00VLM\u6a21\u578b\u53ef\u540c\u65f6\u8fdb\u884c\u6a21\u578b\u9009\u62e9\u548c\u4efb\u52a1\u6267\u884c\uff0c\u6709\u6548\u6574\u5408\u533b\u7597\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u63d0\u9ad8\u6548\u7387\u5e76\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u652f\u79bb\u7834\u788e\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u6210\u672c\u9ad8\u6602\u3002", "method": "\u8be5\u6846\u67b6\u4f7f\u7528\u5355\u4e00VLM\u626e\u6f14\u4e24\u4e2a\u89d2\u8272\uff1a1. \u4f5c\u4e3a\u6a21\u578b\u5339\u914d\u5668\uff0c\u6839\u636e\u56fe\u50cf\u5185\u5bb9\u9009\u62e9\u5408\u9002\u7684\u4e13\u4e1a\u6a21\u578b\uff1b2. \u901a\u8fc7\u5fae\u8c03\uff0c\u8986\u76d6\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u3002", "result": "\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u4e13\u4e1a\u9886\u57df\u5b9e\u73b0\u4e86\u4e0e\u4e13\u7528\u6a21\u578b\u76f8\u5f53\u6216\u63a5\u8fd1\u7684\u6027\u80fd\uff0c\u5e76\u964d\u4f4e\u4e86\u6570\u636e\u79d1\u5b66\u5bb6\u7684\u5de5\u4f5c\u91cf\uff0c\u7f29\u77ed\u4e86\u76d1\u63a7\u65f6\u95f4\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u9009\u62e9\u7684\u900f\u660e\u5ea6\uff0c\u964d\u4f4e\u4e86\u96c6\u6210\u5f00\u9500\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5355\u4e00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u533b\u7597\u4fdd\u5065\u6846\u67b6\uff0c\u7528\u4e8e\u6574\u5408\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u63d0\u9ad8\u6548\u7387\u5e76\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u3002\u8be5\u6846\u67b6\u5728\u80c3\u80a0\u75c5\u5b66\u3001\u8840\u6db2\u5b66\u3001\u773c\u79d1\u548c\u75c5\u7406\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u5b9e\u73b0\u4e86\u4e0e\u4e13\u7528\u6a21\u578b\u76f8\u5f53\u6216\u63a5\u8fd1\u7684\u6027\u80fd\u3002"}}
{"id": "2508.16846", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16846", "abs": "https://arxiv.org/abs/2508.16846", "authors": ["Katherine Atwell", "Pedram Heydari", "Anthony Sicilia", "Malihe Alikhani"], "title": "Quantifying Sycophancy as Deviations from Bayesian Rationality in LLMs", "comment": null, "summary": "Sycophancy, or overly agreeable or flattering behavior, is a documented issue\nin large language models (LLMs), and is critical to understand in the context\nof human/AI collaboration. Prior works typically quantify sycophancy by\nmeasuring shifts in behavior or impacts on accuracy, but neither metric\ncharacterizes shifts in rationality, and accuracy measures can only be used in\nscenarios with a known ground truth. In this work, we utilize a Bayesian\nframework to quantify sycophancy as deviations from rational behavior when\npresented with user perspectives, thus distinguishing between rational and\nirrational updates based on the introduction of user perspectives. In\ncomparison to other methods, this approach allows us to characterize excessive\nbehavioral shifts, even for tasks that involve inherent uncertainty or do not\nhave a ground truth. We study sycophancy for 3 different tasks, a combination\nof open-source and closed LLMs, and two different methods for probing\nsycophancy. We also experiment with multiple methods for eliciting probability\njudgments from LLMs. We hypothesize that probing LLMs for sycophancy will cause\ndeviations in LLMs' predicted posteriors that will lead to increased Bayesian\nerror. Our findings indicate that: 1) LLMs are not Bayesian rational, 2)\nprobing for sycophancy results in significant increases to the predicted\nposterior in favor of the steered outcome, 3) sycophancy sometimes results in\nincreased Bayesian error, and in a small number of cases actually decreases\nerror, and 4) changes in Bayesian error due to sycophancy are not strongly\ncorrelated in Brier score, suggesting that studying the impact of sycophancy on\nground truth alone does not fully capture errors in reasoning due to\nsycophancy.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8c04\u5a9a\u884c\u4e3a\uff0c\u8be5\u884c\u4e3a\u4f1a\u5bfc\u81f4\u5176\u504f\u79bb\u7406\u6027\uff0c\u5e76\u5f71\u54cd\u8d1d\u53f6\u65af\u8bef\u5dee\uff0c\u4f46\u5176\u5f71\u54cd\u5e76\u975e\u603b\u662f\u8d1f\u9762\u7684\uff0c\u4e14\u4e0e\u73b0\u6709\u8bc4\u4ef7\u6307\u6807\u7684\u76f8\u5173\u6027\u4e0d\u5f3a\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u901a\u8fc7\u6d4b\u91cf\u884c\u4e3a\u53d8\u5316\u6216\u51c6\u786e\u6027\u5f71\u54cd\u6765\u91cf\u5316LLM\u7684\u8c04\u5a9a\u884c\u4e3a\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u4e0d\u80fd\u5f88\u597d\u5730\u523b\u753b\u7406\u6027\u884c\u4e3a\u7684\u8f6c\u53d8\uff0c\u4e14\u51c6\u786e\u6027\u6d4b\u91cf\u4ec5\u9002\u7528\u4e8e\u5b58\u5728\u5df2\u77e5\u771f\u5b9e\u7ed3\u679c\u7684\u60c5\u51b5\u3002", "method": "\u5229\u7528\u8d1d\u53f6\u65af\u6846\u67b6\u91cf\u5316LLM\u5728\u9762\u5bf9\u7528\u6237\u89c2\u70b9\u65f6\u7684\u7406\u6027\u884c\u4e3a\u504f\u5dee\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u4efb\u52a1\u3001\u4e0d\u540cLLM\u548c\u4e0d\u540c\u63a2\u6d4b\u65b9\u6cd5\u4e0b\u7684\u7ed3\u679c\u6765\u7814\u7a76\u8c04\u5a9a\u884c\u4e3a\u3002", "result": "1) LLM\u5e76\u975e\u8d1d\u53f6\u65af\u7406\u6027\uff1b2) \u63a2\u6d4b\u8c04\u5a9a\u884c\u4e3a\u4f1a\u5bfc\u81f4\u9884\u6d4b\u540e\u9a8c\u6982\u7387\u5411\u5f15\u5bfc\u7ed3\u679c\u503e\u659c\uff1b3) \u8c04\u5a9a\u884c\u4e3a\u6709\u65f6\u589e\u52a0\u8d1d\u53f6\u65af\u8bef\u5dee\uff0c\u6709\u65f6\u53cd\u800c\u964d\u4f4e\u8bef\u5dee\uff1b4) \u8c04\u5a9a\u884c\u4e3a\u5bfc\u81f4\u7684\u8d1d\u53f6\u65af\u8bef\u5dee\u53d8\u5316\u4e0eBrier\u8bc4\u5206\u7684\u76f8\u5173\u6027\u4e0d\u5f3a\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u8c04\u5a9a\u884c\u4e3a\u4f1a\u5bfc\u81f4\u5176\u5728\u9762\u5bf9\u7528\u6237\u89c2\u70b9\u65f6\u504f\u79bb\u7406\u6027\u884c\u4e3a\uff0c\u8fd9\u79cd\u504f\u79bb\u53ef\u4ee5\u7528\u8d1d\u53f6\u65af\u6846\u67b6\u91cf\u5316\u3002\u7814\u7a76\u53d1\u73b0\uff0cLLM\u5e76\u975e\u8d1d\u53f6\u65af\u7406\u6027\u7684\uff0c\u63a2\u6d4b\u8c04\u5a9a\u884c\u4e3a\u4f1a\u5bfc\u81f4\u9884\u6d4b\u540e\u9a8c\u6982\u7387\u5411\u5f15\u5bfc\u7ed3\u679c\u503e\u659c\uff0c\u6709\u65f6\u589e\u52a0\u8d1d\u53f6\u65af\u8bef\u5dee\uff0c\u6709\u65f6\u53cd\u800c\u964d\u4f4e\u8bef\u5dee\u3002\u8c04\u5a9a\u884c\u4e3a\u5bf9\u8d1d\u53f6\u65af\u8bef\u5dee\u7684\u5f71\u54cd\u4e0eBrier\u8bc4\u5206\u7684\u76f8\u5173\u6027\u4e0d\u5f3a\uff0c\u8868\u660e\u4ec5\u7814\u7a76\u5176\u5bf9\u771f\u5b9e\u7ed3\u679c\u7684\u5f71\u54cd\u4e0d\u8db3\u4ee5\u5b8c\u5168\u6355\u6349\u7531\u4e8e\u8c04\u5a9a\u884c\u4e3a\u9020\u6210\u7684\u63a8\u7406\u9519\u8bef\u3002"}}
{"id": "2508.16850", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16850", "abs": "https://arxiv.org/abs/2508.16850", "authors": ["Anku Rani", "Aparna Garimella", "Apoorv Saxena", "Balaji Vasan Srinivasan", "Paul Pu Liang"], "title": "RADAR: A Reasoning-Guided Attribution Framework for Explainable Visual Data Analysis", "comment": null, "summary": "Data visualizations like charts are fundamental tools for quantitative\nanalysis and decision-making across fields, requiring accurate interpretation\nand mathematical reasoning. The emergence of Multimodal Large Language Models\n(MLLMs) offers promising capabilities for automated visual data analysis, such\nas processing charts, answering questions, and generating summaries. However,\nthey provide no visibility into which parts of the visual data informed their\nconclusions; this black-box nature poses significant challenges to real-world\ntrust and adoption. In this paper, we take the first major step towards\nevaluating and enhancing the capabilities of MLLMs to attribute their reasoning\nprocess by highlighting the specific regions in charts and graphs that justify\nmodel answers. To this end, we contribute RADAR, a semi-automatic approach to\nobtain a benchmark dataset comprising 17,819 diverse samples with charts,\nquestions, reasoning steps, and attribution annotations. We also introduce a\nmethod that provides attribution for chart-based mathematical reasoning.\nExperimental results demonstrate that our reasoning-guided approach improves\nattribution accuracy by 15% compared to baseline methods, and enhanced\nattribution capabilities translate to stronger answer generation, achieving an\naverage BERTScore of $\\sim$ 0.90, indicating high alignment with ground truth\nresponses. This advancement represents a significant step toward more\ninterpretable and trustworthy chart analysis systems, enabling users to verify\nand understand model decisions through reasoning and attribution.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6539\u8fdbMLLMs\u56fe\u8868\u5206\u6790\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "MLLMs\u5728\u56fe\u8868\u5206\u6790\u4e2d\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u96be\u4ee5\u5efa\u7acb\u4fe1\u4efb\u3002", "method": "\u63d0\u51faRADAR\u6570\u636e\u96c6\u548c\u4e00\u79cd\u63a8\u7406\u5f15\u5bfc\u7684\u5f52\u56e0\u65b9\u6cd5\u3002", "result": "\u63a8\u7406\u5f15\u5bfc\u7684\u65b9\u6cd5\u5c06\u5f52\u56e0\u51c6\u786e\u7387\u63d0\u9ad8\u4e8615%\uff0cBERTScore\u8fbe\u5230\u7ea60.90\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51faRADAR\uff0c\u4e00\u79cd\u534a\u81ea\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u521b\u5efa\u5305\u542b\u56fe\u8868\u3001\u95ee\u9898\u3001\u63a8\u7406\u6b65\u9aa4\u548c\u5f52\u56e0\u6ce8\u91ca\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u4e3a\u57fa\u4e8e\u56fe\u8868\u8fdb\u884c\u6570\u5b66\u63a8\u7406\u63d0\u4f9b\u5f52\u56e0\u7684\u65b9\u6cd5\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2508.16986", "categories": ["cs.AI", "math.LO"], "pdf": "https://arxiv.org/pdf/2508.16986", "abs": "https://arxiv.org/abs/2508.16986", "authors": ["Uri Andrews", "Luca San Mauro"], "title": "Complexity in finitary argumentation (extended version)", "comment": null, "summary": "Abstract argumentation frameworks (AFs) provide a formal setting to analyze\nmany forms of reasoning with conflicting information. While the expressiveness\nof general infinite AFs make them a tempting tool for modeling many kinds of\nreasoning scenarios, the computational intractability of solving infinite AFs\nlimit their use, even in many theoretical applications.\n  We investigate the complexity of computational problems related to infinite\nbut finitary argumentations frameworks, that is, infinite AFs where each\nargument is attacked by only finitely many others. Our results reveal a\nsurprising scenario. On one hand, we see that the assumption of being finitary\ndoes not automatically guarantee a drop in complexity. However, for the\nadmissibility-based semantics, we find a remarkable combinatorial constraint\nwhich entails a dramatic decrease in complexity.\n  We conclude that for many forms of reasoning, the finitary infinite AFs\nprovide a natural setting for reasoning which balances well the competing goals\nof being expressive enough to be applied to many reasoning settings while being\ncomputationally tractable enough for the analysis within the framework to be\nuseful.", "AI": {"tldr": "\u6709\u9650\u65e0\u9650 AFs \u4e3a\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u826f\u597d\u7684\u8bbe\u7f6e\uff0c\u517c\u987e\u4e86\u8868\u8fbe\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u65e0\u9650 AFs \u7684\u8ba1\u7b97\u96be\u5904\u7406\u6027\u9650\u5236\u4e86\u5176\u5728\u8bb8\u591a\u7406\u8bba\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002", "method": "\u7814\u7a76\u4e86\u4e0e\u65e0\u9650\u4f46\u6709\u9650\u8bba\u8bc1\u6846\u67b6\u76f8\u5173\u7684\u8ba1\u7b97\u95ee\u9898\u7684\u590d\u6742\u6027\u3002", "result": "\u6709\u9650\u6027\u5047\u8bbe\u5e76\u4e0d\u81ea\u52a8\u4fdd\u8bc1\u590d\u6742\u6027\u964d\u4f4e\uff0c\u4f46\u5bf9\u4e8e\u57fa\u4e8e\u53ef\u5bb9\u8bb8\u6027\u7684\u8bed\u4e49\uff0c\u53d1\u73b0\u4e86\u4e00\u4e2a\u663e\u8457\u7684\u7ec4\u5408\u7ea6\u675f\uff0c\u5bfc\u81f4\u590d\u6742\u6027\u6025\u5267\u4e0b\u964d\u3002", "conclusion": "\u65e0\u7a77\u4f46\u6709\u9650\u8bba\u8bc1\u6846\u67b6 (AFs) \u4e3a\u8bb8\u591a\u63a8\u7406\u5f62\u5f0f\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u7136\u7684\u8bbe\u7f6e\uff0c\u5b83\u5e73\u8861\u4e86\u8868\u8fbe\u80fd\u529b\u548c\u8ba1\u7b97\u6613\u5904\u7406\u6027\u3002"}}
{"id": "2508.16987", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16987", "abs": "https://arxiv.org/abs/2508.16987", "authors": ["Tanvir Bhathal", "Asanshay Gupta"], "title": "WebSight: A Vision-First Architecture for Robust Web Agents", "comment": null, "summary": "We introduce WebSight, a vision-based autonomous web agent, designed to\ninteract with web environments purely through visual perception, eliminating\ndependence on HTML or DOM-based inputs. Central to our approach we introduce\nour new model, WebSight-7B, a fine-tuned vision-language model optimized for UI\nelement interaction, trained using LoRA on a web-focused subset of the\nWave-UI-25K dataset. WebSight integrates this model into a modular multi-agent\narchitecture, comprising planning, reasoning, vision-action, and verification\nagents, coordinated through an episodic memory mechanism.\n  WebSight-7B achieves a top-1 accuracy of 58.84% on the Showdown Clicks\nbenchmark, outperforming several larger generalist models while maintaining\nlower latency. The full WebSight agent achieves a 68.0% success rate on the\nWebVoyager benchmark, surpassing systems from labs such as OpenAI (61.0%) and\nHCompany (Runner H, 67.0%). Among tasks completed, WebSight answers correctly\n97.14% of the time, indicating high precision. Together, WebSight and\nWebSight-7B establish a new standard for interpretable, robust, and efficient\nvisual web navigation.", "AI": {"tldr": "WebSight\uff0c\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u7f51\u9875\u4ee3\u7406\uff0c\u4f7f\u7528\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b WebSight-7B\uff0c\u5728\u7f51\u9875\u5bfc\u822a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u7684\u7ed3\u679c\u3002", "motivation": "\u6d88\u9664\u5bf9\u57fa\u4e8e HTML \u6216 DOM \u7684\u8f93\u5165\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u5b8c\u5168\u57fa\u4e8e\u89c6\u89c9\u611f\u77e5\u7684\u7f51\u9875\u4ea4\u4e92\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u81ea\u4e3b\u7f51\u9875\u4ee3\u7406 WebSight\uff0c\u8be5\u4ee3\u7406\u5b8c\u5168\u901a\u8fc7\u89c6\u89c9\u611f\u77e5\u4e0e\u7f51\u9875\u73af\u5883\u4ea4\u4e92\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u6a21\u578b WebSight-7B\uff08\u4e00\u4e2a\u9488\u5bf9 UI \u5143\u7d20\u4ea4\u4e92\u8fdb\u884c\u4e86\u4f18\u5316\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\u4e2d\u3002", "result": "WebSight-7B \u5728 Showdown Clicks \u57fa\u51c6\u6d4b\u8bd5\u4e2d top-1 \u51c6\u786e\u7387\u8fbe 58.84%\uff0cWebSight \u5728 WebVoyager \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6210\u529f\u7387\u8fbe 68.0%\uff0c\u4e14\u51c6\u786e\u7387\u8fbe 97.14%\u3002", "conclusion": "WebSight \u548c WebSight-7B \u6a21\u578b\u5728\u53ef\u89e3\u91ca\u6027\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u65b9\u9762\u4e3a\u53ef\u89c6\u5316\u7f51\u9875\u5bfc\u822a\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002WebSight-7B \u5728 Showdown Clicks \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86 58.84% \u7684 top-1 \u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u5176\u4ed6\u5927\u578b\u901a\u7528\u6a21\u578b\uff1bWebSight \u5b8c\u6574\u4ee3\u7406\u5728 WebVoyager \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86 68.0% \u7684\u6210\u529f\u7387\uff0c\u8d85\u8fc7\u4e86 OpenAI \u548c HCompany \u7b49\u5b9e\u9a8c\u5ba4\u7684\u7cfb\u7edf\u3002"}}
{"id": "2508.17087", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17087", "abs": "https://arxiv.org/abs/2508.17087", "authors": ["Wen Wang", "Xiangchen Wu", "Liang Wang", "Hao Hu", "Xianping Tao", "Linghao Zhang"], "title": "Solving the Min-Max Multiple Traveling Salesmen Problem via Learning-Based Path Generation and Optimal Splitting", "comment": null, "summary": "This study addresses the Min-Max Multiple Traveling Salesmen Problem\n($m^3$-TSP), which aims to coordinate tours for multiple salesmen such that the\nlength of the longest tour is minimized. Due to its NP-hard nature, exact\nsolvers become impractical under the assumption that $P \\ne NP$. As a result,\nlearning-based approaches have gained traction for their ability to rapidly\ngenerate high-quality approximate solutions. Among these, two-stage methods\ncombine learning-based components with classical solvers, simplifying the\nlearning objective. However, this decoupling often disrupts consistent\noptimization, potentially degrading solution quality. To address this issue, we\npropose a novel two-stage framework named \\textbf{Generate-and-Split} (GaS),\nwhich integrates reinforcement learning (RL) with an optimal splitting\nalgorithm in a joint training process. The splitting algorithm offers\nnear-linear scalability with respect to the number of cities and guarantees\noptimal splitting in Euclidean space for any given path. To facilitate the\njoint optimization of the RL component with the algorithm, we adopt an\nLSTM-enhanced model architecture to address partial observability. Extensive\nexperiments show that the proposed GaS framework significantly outperforms\nexisting learning-based approaches in both solution quality and\ntransferability.", "AI": {"tldr": "\u9488\u5bf9Min-Max\u591a\u65c5\u884c\u5546\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u751f\u6210\u4e0e\u5206\u5272\u7684\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u53ef\u8fc1\u79fb\u6027\u3002", "motivation": "\u89e3\u51b3Min-Max\u591a\u65c5\u884c\u5546\u95ee\u9898\uff08m3-TSP\uff09\uff0c\u8be5\u95ee\u9898\u65e8\u5728\u534f\u8c03\u591a\u4e2a\u9500\u552e\u4eba\u5458\u7684\u8def\u7ebf\uff0c\u4f7f\u6700\u957f\u8def\u7ebf\u957f\u5ea6\u6700\u5c0f\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u751f\u6210\u4e0e\u5206\u5272\uff08GaS\uff09\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5f3a\u5316\u5b66\u4e60\u548c\u6700\u4f18\u5206\u5272\u7b97\u6cd5\u3002", "result": "GaS\u6846\u67b6\u5728\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u53ef\u8fc1\u79fb\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u751f\u6210\u4e0e\u5206\u5272\u7684\u65b0\u578b\u4e24\u9636\u6bb5\u6846\u67b6\uff08GaS\uff09\uff0c\u8be5\u6846\u67b6\u5c06\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e0e\u6700\u4f18\u5206\u5272\u7b97\u6cd5\u96c6\u6210\u5230\u8054\u5408\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.17094", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.17094", "abs": "https://arxiv.org/abs/2508.17094", "authors": ["Emmanuel O. Badmus", "Peng Sang", "Dimitrios Stamoulis", "Amritanshu Pandey"], "title": "PowerChain: Automating Distribution Grid Analysis with Agentic AI Workflows", "comment": null, "summary": "Due to the rapid pace of electrification and decarbonization, distribution\ngrid (DG) operation and planning are becoming more complex, necessitating\nadvanced computational analyses to ensure grid reliability and resilience.\nState-of-the-art DG analyses rely on disparate workflows of complex models,\nfunctions, and data pipelines, which require expert knowledge and are\nchallenging to automate. Many small-scale utilities and cooperatives lack a\nlarge R&D workforce and therefore cannot use advanced analysis at scale. To\naddress this gap, we develop a novel agentic AI system, PowerChain, to solve\nunseen DG analysis tasks via automated agentic orchestration and large language\nmodels (LLMs) function-calling. Given a natural language query, PowerChain\ndynamically generates and executes an ordered sequence of domain-aware\nfunctions guided by the semantics of an expert-built power systems function\npool and a select reference set of known, expert-generated workflow-query\npairs. Our results show that PowerChain can produce expert-level workflows with\nboth GPT-5 and open-source Qwen models on complex, unseen DG analysis tasks\noperating on real utility data.", "AI": {"tldr": "PowerChain: \u4e00\u79cd\u65b0\u578bAI\u7cfb\u7edf\uff0c\u4f7f\u7528LLM\u81ea\u52a8\u6267\u884c\u914d\u7535\u7f51\u5206\u6790\u4efb\u52a1\uff0c\u80fd\u591f\u751f\u6210\u4e13\u5bb6\u7ea7\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u914d\u7535\u7f51(DG)\u7684\u7535\u6c14\u5316\u548c\u8131\u78b3\u901f\u5ea6\u52a0\u5feb\uff0c\u4f7f\u5f97\u914d\u7535\u7f51\u7684\u8fd0\u884c\u548c\u89c4\u5212\u66f4\u52a0\u590d\u6742\uff0c\u9700\u8981\u5148\u8fdb\u7684\u8ba1\u7b97\u5206\u6790\u6765\u786e\u4fdd\u7535\u7f51\u7684\u53ef\u9760\u6027\u548c\u5f39\u6027\u3002\u8bb8\u591a\u5c0f\u578b\u516c\u7528\u4e8b\u4e1a\u516c\u53f8\u548c\u5408\u4f5c\u793e\u7f3a\u4e4f\u5927\u578b\u7814\u53d1\u961f\u4f0d\uff0c\u56e0\u6b64\u65e0\u6cd5\u5927\u89c4\u6a21\u4f7f\u7528\u9ad8\u7ea7\u5206\u6790\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u4ee3\u7406\u7684AI\u7cfb\u7edf\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u51fd\u6570\u8c03\u7528\uff0c\u6839\u636e\u4e13\u5bb6\u6784\u5efa\u7684\u7535\u529b\u7cfb\u7edf\u51fd\u6570\u6c60\u548c\u5df2\u77e5\u7684\u4e13\u5bb6\u751f\u6210\u7684\u5de5\u4f5c\u6d41\u7a0b\u67e5\u8be2\u5bf9\u53c2\u8003\u96c6\uff0c\u52a8\u6001\u751f\u6210\u548c\u6267\u884c\u4e00\u7cfb\u5217\u7279\u5b9a\u9886\u57df\u7684\u51fd\u6570\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cPowerChain\u80fd\u591f\u4f7f\u7528GPT-5\u548c\u5f00\u6e90Qwen\u6a21\u578b\u5728\u590d\u6742\u3001\u672a\u77e5\u7684\u914d\u7535\u7f51\u5206\u6790\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u771f\u5b9e\u7684\u516c\u7528\u4e8b\u4e1a\u6570\u636e\u751f\u6210\u4e13\u5bb6\u7ea7\u5de5\u4f5c\u6d41\u7a0b\u3002", "conclusion": "\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aPowerChain\u7684\u65b0\u578bAI\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u914d\u7535\u7f51\u5206\u6790\u4efb\u52a1\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u901a\u8fc7\u81ea\u52a8\u5316\u4ee3\u7406\u534f\u8c03\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u51fd\u6570\u8c03\u7528\u6765\u751f\u6210\u548c\u6267\u884c\u4e13\u5bb6\u7ea7\u5de5\u4f5c\u6d41\u7a0b\u3002"}}
{"id": "2508.17104", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17104", "abs": "https://arxiv.org/abs/2508.17104", "authors": ["Sz-Ting Tzeng", "Frank Dignum"], "title": "Rethinking How AI Embeds and Adapts to Human Values: Challenges and Opportunities", "comment": "7 pages, accepted at VALE 2025", "summary": "The concepts of ``human-centered AI'' and ``value-based decision'' have\ngained significant attention in both research and industry. However, many\ncritical aspects remain underexplored and require further investigation. In\nparticular, there is a need to understand how systems incorporate human values,\nhow humans can identify these values within systems, and how to minimize the\nrisks of harm or unintended consequences. In this paper, we highlight the need\nto rethink how we frame value alignment and assert that value alignment should\nmove beyond static and singular conceptions of values. We argue that AI systems\nshould implement long-term reasoning and remain adaptable to evolving values.\nFurthermore, value alignment requires more theories to address the full\nspectrum of human values. Since values often vary among individuals or groups,\nmulti-agent systems provide the right framework for navigating pluralism,\nconflict, and inter-agent reasoning about values. We identify the challenges\nassociated with value alignment and indicate directions for advancing value\nalignment research. In addition, we broadly discuss diverse perspectives of\nvalue alignment, from design methodologies to practical applications.", "AI": {"tldr": "\u4ef7\u503c\u5bf9\u9f50\u5e94\u8d85\u8d8a\u9759\u6001\u5355\u4e00\u4ef7\u503c\u89c2\uff0c\u9700\u8003\u8651\u957f\u671f\u63a8\u7406\u548c\u9002\u5e94\u6027\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u662f\u5e94\u5bf9\u4ef7\u503c\u89c2\u591a\u5143\u5316\u7684\u6709\u6548\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u201c\u4ee5\u4eba\u4e3a\u672c\u7684AI\u201d\u548c\u201c\u57fa\u4e8e\u4ef7\u503c\u7684\u51b3\u7b56\u201d\u7814\u7a76\u4e2d\uff0c\u4ef7\u503c\u5bf9\u9f50\u7684\u8bb8\u591a\u5173\u952e\u65b9\u9762\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u4f8b\u5982\u7cfb\u7edf\u5982\u4f55\u6574\u5408\u4eba\u7c7b\u4ef7\u503c\u89c2\u3001\u4eba\u7c7b\u5982\u4f55\u8bc6\u522b\u7cfb\u7edf\u4e2d\u7684\u4ef7\u503c\u89c2\u4ee5\u53ca\u5982\u4f55\u6700\u5c0f\u5316\u98ce\u9669\u548c\u610f\u5916\u540e\u679c\u3002", "method": "\u8be5\u8bba\u6587\u901a\u8fc7\u5206\u6790\u73b0\u6709\u4ef7\u503c\u5bf9\u9f50\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u80fd\u591f\u5904\u7406\u957f\u671f\u63a8\u7406\u548c\u9002\u5e94\u6027\u4ef7\u503c\u89c2\u53d8\u5316\u7684\u65b0\u6846\u67b6\u3002", "result": "\u8bba\u6587\u8bc6\u522b\u4e86\u4ef7\u503c\u5bf9\u9f50\u76f8\u5173\u7684\u6311\u6218\uff0c\u5e76\u6307\u660e\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u540c\u65f6\u5e7f\u6cdb\u8ba8\u8bba\u4e86\u4ece\u8bbe\u8ba1\u65b9\u6cd5\u5230\u5b9e\u9645\u5e94\u7528\u7684\u5404\u79cd\u89c6\u89d2\u3002", "conclusion": "\u8be5\u8bba\u6587\u5f3a\u8c03\u9700\u8981\u91cd\u65b0\u601d\u8003\u4ef7\u503c\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u4e3b\u5f20\u5176\u5e94\u8d85\u8d8a\u9759\u6001\u548c\u5355\u4e00\u4ef7\u503c\u89c2\uff0c\u5e76\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u4ef7\u503c\u89c2\u3002\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e3a\u5e94\u5bf9\u4ef7\u503c\u89c2\u591a\u5143\u5316\u3001\u51b2\u7a81\u548c\u4ea4\u4e92\u5f0f\u63a8\u7406\u63d0\u4f9b\u4e86\u5408\u9002\u7684\u6846\u67b6\u3002"}}
{"id": "2508.17180", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17180", "abs": "https://arxiv.org/abs/2508.17180", "authors": ["Nilay Pande", "Sahiti Yerramilli", "Jayant Sravan Tamarapalli", "Rynaa Grover"], "title": "MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes", "comment": null, "summary": "A key frontier for Multimodal Large Language Models (MLLMs) is the ability to\nperform deep mathematical and spatial reasoning directly from images, moving\nbeyond their established success in semantic description. Mathematical surface\nplots provide a rigorous testbed for this capability, as they isolate the task\nof reasoning from the semantic noise common in natural images. To measure\nprogress on this frontier, we introduce MaRVL-QA (Mathematical Reasoning over\nVisual Landscapes), a new benchmark designed to quantitatively evaluate these\ncore reasoning skills. The benchmark comprises two novel tasks: Topological\nCounting, identifying and enumerating features like local maxima; and\nTransformation Recognition, recognizing applied geometric transformations.\nGenerated from a curated library of functions with rigorous ambiguity\nfiltering, our evaluation on MaRVL-QA reveals that even state-of-the-art MLLMs\nstruggle significantly, often resorting to superficial heuristics instead of\nrobust spatial reasoning. MaRVL-QA provides a challenging new tool for the\nresearch community to measure progress, expose model limitations, and guide the\ndevelopment of MLLMs with more profound reasoning abilities.", "AI": {"tldr": "\u65b0\u7684MaRVL-QA\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u76ee\u524d\u7684MLLM\u5728\u6570\u5b66\u548c\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLM)\u8fdb\u884c\u6df1\u5ea6\u6570\u5b66\u548c\u7a7a\u95f4\u63a8\u7406\u7684\u80fd\u529b\u3002\u6570\u5b66\u8868\u9762\u56fe\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u56e0\u4e3a\u5b83\u5c06\u63a8\u7406\u4efb\u52a1\u4e0e\u81ea\u7136\u56fe\u50cf\u4e2d\u5e38\u89c1\u7684\u8bed\u4e49\u566a\u58f0\u9694\u79bb\u5f00\u6765\u3002", "method": "\u63d0\u51faMaRVL-QA\u57fa\u51c6\uff0c\u5305\u542b\u62d3\u6251\u8ba1\u6570\u548c\u53d8\u6362\u8bc6\u522b\u4e24\u9879\u65b0\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30MLLM\u8fdb\u884c\u6570\u5b66\u548c\u7a7a\u95f4\u63a8\u7406\u7684\u80fd\u529b\u3002", "result": "\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684MLLM\uff0c\u5728MaRVL-QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "conclusion": "MaRVL-QA\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLM)\u4e5f\u96be\u4ee5\u8fdb\u884c\u6df1\u5c42\u6b21\u7684\u6570\u5b66\u548c\u7a7a\u95f4\u63a8\u7406\uff0c\u7ecf\u5e38\u91c7\u7528\u8868\u9762\u542f\u53d1\u5f0f\u65b9\u6cd5\u800c\u4e0d\u662f\u5f3a\u5927\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2508.17188", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17188", "abs": "https://arxiv.org/abs/2508.17188", "authors": ["Zhilin Zhang", "Xiang Zhang", "Jiaqi Wei", "Yiwei Xu", "Chenyu You"], "title": "PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMs", "comment": "Project Website: https://Y-Research-SBU.github.io/PosterGen", "summary": "Multi-agent systems built upon large language models (LLMs) have demonstrated\nremarkable capabilities in tackling complex compositional tasks. In this work,\nwe apply this paradigm to the paper-to-poster generation problem, a practical\nyet time-consuming process faced by researchers preparing for conferences.\nWhile recent approaches have attempted to automate this task, most neglect core\ndesign and aesthetic principles, resulting in posters that require substantial\nmanual refinement. To address these design limitations, we propose PosterGen, a\nmulti-agent framework that mirrors the workflow of professional poster\ndesigners. It consists of four collaborative specialized agents: (1) Parser and\nCurator agents extract content from the paper and organize storyboard; (2)\nLayout agent maps the content into a coherent spatial layout; (3) Stylist\nagents apply visual design elements such as color and typography; and (4)\nRenderer composes the final poster. Together, these agents produce posters that\nare both semantically grounded and visually appealing. To evaluate design\nquality, we introduce a vision-language model (VLM)-based rubric that measures\nlayout balance, readability, and aesthetic coherence. Experimental results show\nthat PosterGen consistently matches in content fidelity, and significantly\noutperforms existing methods in visual designs, generating posters that are\npresentation-ready with minimal human refinements.", "AI": {"tldr": "PosterGen\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u80fd\u6839\u636e\u8bba\u6587\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u6d77\u62a5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bba\u6587\u8f6c\u6d77\u62a5\u65b9\u6cd5\u5ffd\u7565\u6838\u5fc3\u8bbe\u8ba1\u548c\u7f8e\u5b66\u539f\u5219\u7684\u95ee\u9898\uff0c\u751f\u6210\u7684\u6d77\u62a5\u9700\u8981\u5927\u91cf\u4eba\u5de5\u4fee\u6539\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6PosterGen\uff0c\u6a21\u62df\u4e13\u4e1a\u6d77\u62a5\u8bbe\u8ba1\u5e08\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "PosterGen\u5728\u5185\u5bb9\u4fdd\u771f\u5ea6\u65b9\u9762\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u5728\u89c6\u89c9\u8bbe\u8ba1\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u30dd\u30b9\u30bf\u30fc\u51e0\u4e4e\u65e0\u9700\u4eba\u5de5\u4fee\u6539\u3002", "conclusion": "PosterGen\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u4e13\u4e1a\u5316\u4ee3\u7406\uff08\u89e3\u6790\u548c\u7b56\u5c55\u4ee3\u7406\u3001\u5e03\u5c40\u4ee3\u7406\u3001\u9020\u578b\u4ee3\u7406\u548c\u6e32\u67d3\u5668\uff09\u534f\u540c\u5de5\u4f5c\uff0c\u53ef\u4ee5\u6839\u636e\u8bba\u6587\u751f\u6210\u7f8e\u89c2\u4e14\u8bed\u4e49\u6e05\u6670\u7684\u6d77\u62a5\uff0c\u5728\u5185\u5bb9\u4fdd\u771f\u5ea6\u548c\u89c6\u89c9\u8bbe\u8ba1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.17198", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17198", "abs": "https://arxiv.org/abs/2508.17198", "authors": ["Shouwei Ruan", "Liyuan Wang", "Caixin Kang", "Qihui Zhu", "Songming Liu", "Xingxing Wei", "Hang Su"], "title": "From reactive to cognitive: brain-inspired spatial intelligence for embodied agents", "comment": "40 pages, 8 figures", "summary": "Spatial cognition enables adaptive goal-directed behavior by constructing\ninternal models of space. Robust biological systems consolidate spatial\nknowledge into three interconnected forms: \\textit{landmarks} for salient cues,\n\\textit{route knowledge} for movement trajectories, and \\textit{survey\nknowledge} for map-like representations. While recent advances in multi-modal\nlarge language models (MLLMs) have enabled visual-language reasoning in\nembodied agents, these efforts lack structured spatial memory and instead\noperate reactively, limiting their generalization and adaptability in complex\nreal-world environments. Here we present Brain-inspired Spatial Cognition for\nNavigation (BSC-Nav), a unified framework for constructing and leveraging\nstructured spatial memory in embodied agents. BSC-Nav builds allocentric\ncognitive maps from egocentric trajectories and contextual cues, and\ndynamically retrieves spatial knowledge aligned with semantic goals. Integrated\nwith powerful MLLMs, BSC-Nav achieves state-of-the-art efficacy and efficiency\nacross diverse navigation tasks, demonstrates strong zero-shot generalization,\nand supports versatile embodied behaviors in the real physical world, offering\na scalable and biologically grounded path toward general-purpose spatial\nintelligence.", "AI": {"tldr": "BSC-Nav\u6846\u67b6\u5229\u7528\u7ed3\u6784\u5316\u7a7a\u95f4\u8bb0\u5fc6\uff0c\u8d4b\u4e88\u5177\u8eab\u4ee3\u7406\u5f3a\u5927\u7684\u7a7a\u95f4\u667a\u80fd\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u5bfc\u822a\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u7ed3\u6784\u5316\u7684\u7a7a\u95f4\u8bb0\u5fc6\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u548c\u9002\u5e94\u80fd\u529b\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u5927\u8111\u7684\u7a7a\u95f4\u8ba4\u77e5\u6a21\u578bBSC-Nav\uff0c\u8be5\u6a21\u578b\u4ece\u81ea\u6211\u4e2d\u5fc3\u8f68\u8ff9\u548c\u60c5\u5883\u7ebf\u7d22\u6784\u5efa\u5ba2\u89c2\u8ba4\u77e5\u5730\u56fe\uff0c\u5e76\u52a8\u6001\u68c0\u7d22\u4e0e\u8bed\u4e49\u76ee\u6807\u5bf9\u9f50\u7684\u7a7a\u95f4\u77e5\u8bc6\u3002", "result": "BSC-Nav\u5728\u5404\u79cd\u5bfc\u822a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6548\u7387\u548c\u6548\u529b\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u652f\u6301\u5728\u73b0\u5b9e\u7269\u7406\u4e16\u754c\u4e2d\u5b9e\u73b0\u591a\u529f\u80fd\u7684\u5177\u8eab\u884c\u4e3a\u3002", "conclusion": "BSC-Nav\u6846\u67b6\u5728\u5177\u8eab\u4ee3\u7406\u4e2d\u6784\u5efa\u548c\u5229\u7528\u7ed3\u6784\u5316\u7a7a\u95f4\u8bb0\u5fc6\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6548\u7387\u548c\u6548\u529b\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.17200", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17200", "abs": "https://arxiv.org/abs/2508.17200", "authors": ["Amirreza Talebi"], "title": "Large Language Model-Based Automatic Formulation for Stochastic Optimization Models", "comment": null, "summary": "This paper presents the first integrated systematic study on the performance\nof large language models (LLMs), specifically ChatGPT, to automatically\nformulate and solve stochastic optimiza- tion problems from natural language\ndescriptions. Focusing on three key categories, joint chance- constrained\nmodels, individual chance-constrained models, and two-stage stochastic linear\nprograms (SLP-2), we design several prompts that guide ChatGPT through\nstructured tasks using chain-of- thought and modular reasoning. We introduce a\nnovel soft scoring metric that evaluates the struc- tural quality and partial\ncorrectness of generated models, addressing the limitations of canonical and\nexecution-based accuracy. Across a diverse set of stochastic problems,\nGPT-4-Turbo outperforms other models in partial score, variable matching, and\nobjective accuracy, with cot_s_instructions and agentic emerging as the most\neffective prompting strategies. Our findings reveal that with well-engineered\nprompts and multi-agent collaboration, LLMs can facilitate specially stochastic\nformulations, paving the way for intelligent, language-driven modeling\npipelines in stochastic opti- mization.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7279\u522b\u662fGPT-4-Turbo\uff0c\u5728\u7ed3\u5408\u6709\u6548\u7684\u63d0\u793a\u7b56\u7565\u540e\uff0c\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u968f\u673a\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u7814\u7a76LLM\u81ea\u52a8\u5236\u5b9a\u548c\u89e3\u51b3\u968f\u673a\u4f18\u5316\u95ee\u9898\u7684\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u51e0\u79cd\u63d0\u793a\uff0c\u5f15\u5bfcChatGPT\u4f7f\u7528\u601d\u7ef4\u94fe\u548c\u6a21\u5757\u5316\u63a8\u7406\u5b8c\u6210\u7ed3\u6784\u5316\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u8f6f\u8bc4\u5206\u6307\u6807\u6765\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u7684\u7ed3\u6784\u8d28\u91cf\u548c\u90e8\u5206\u6b63\u786e\u6027\u3002", "result": "GPT-4-Turbo\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0ccot_s_instructions\u548cagentic\u662f\u6709\u6548\u7684\u63d0\u793a\u7b56\u7565\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u7279\u522b\u662fChatGPT\uff0c\u5728\u81ea\u52a8\u5236\u5b9a\u548c\u89e3\u51b3\u6765\u81ea\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u968f\u673a\u4f18\u5316\u95ee\u9898\u65b9\u9762\u7684\u6027\u80fd\uff0c\u53d1\u73b0GPT-4-Turbo\u5728\u90e8\u5206\u5206\u6570\u3001\u53d8\u91cf\u5339\u914d\u548c\u76ee\u6807\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5e76\u786e\u5b9a\u4e86\u6709\u6548\u7684\u63d0\u793a\u7b56\u7565\u3002"}}
{"id": "2508.17207", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17207", "abs": "https://arxiv.org/abs/2508.17207", "authors": ["Xinyu Qin", "Mark H. Chignell", "Alexandria Greifenberger", "Sachinthya Lokuge", "Elssa Toumeh", "Tia Sternat", "Martin Katzman", "Lu Wang"], "title": "Explainable Counterfactual Reasoning in Depression Medication Selection at Multi-Levels (Personalized and Population)", "comment": null, "summary": "Background: This study investigates how variations in Major Depressive\nDisorder (MDD) symptoms, quantified by the Hamilton Rating Scale for Depression\n(HAM-D), causally influence the prescription of SSRIs versus SNRIs. Methods: We\napplied explainable counterfactual reasoning with counterfactual explanations\n(CFs) to assess the impact of specific symptom changes on antidepressant\nchoice. Results: Among 17 binary classifiers, Random Forest achieved highest\nperformance (accuracy, F1, precision, recall, ROC-AUC near 0.85). Sample-based\nCFs revealed both local and global feature importance of individual symptoms in\nmedication selection. Conclusions: Counterfactual reasoning elucidates which\nMDD symptoms most strongly drive SSRI versus SNRI selection, enhancing\ninterpretability of AI-based clinical decision support systems. Future work\nshould validate these findings on more diverse cohorts and refine algorithms\nfor clinical deployment.", "AI": {"tldr": "\u4f7f\u7528\u53cd\u4e8b\u5b9e\u63a8\u7406\u5206\u6790MDD\u75c7\u72b6\u5982\u4f55\u5f71\u54cdSSRI/SNRI\u5904\u65b9\u9009\u62e9\uff0c\u968f\u673a\u68ee\u6797\u6a21\u578b\u6548\u679c\u6700\u4f73\uff0c\u4e3aAI\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7814\u7a76MDD\u75c7\u72b6\uff08HAM-D\u91cf\u5316\uff09\u5982\u4f55\u56e0\u679c\u6027\u5730\u5f71\u54cdSSRI\u4e0eSNRI\u7684\u5904\u65b9\u3002", "method": "\u5e94\u7528\u53ef\u89e3\u91ca\u7684\u53cd\u4e8b\u5b9e\u63a8\u7406\u4e0e\u53cd\u4e8b\u5b9e\u89e3\u91ca(CFs)\u8bc4\u4f30\u7279\u5b9a\u75c7\u72b6\u53d8\u5316\u5bf9\u6297\u6291\u90c1\u836f\u9009\u62e9\u7684\u5f71\u54cd\u3002\u4f7f\u7528\u968f\u673a\u68ee\u6797\u6a21\u578b\uff0c\u51c6\u786e\u7387\u3001F1\u503c\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cROC-AUC\u63a5\u8fd10.85\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u572817\u4e2a\u4e8c\u5143\u5206\u7c7b\u5668\u4e2d\u6027\u80fd\u6700\u9ad8\uff0c\u6837\u672c\u53cd\u4e8b\u5b9eCF\u63ed\u793a\u4e86\u5404\u4e2a\u75c7\u72b6\u5728\u836f\u7269\u9009\u62e9\u4e2d\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u91cd\u8981\u6027\u3002", "conclusion": "\u53cd\u4e8b\u5b9e\u63a8\u7406\u9610\u660e\u4e86\u54ea\u4e9bMDD\u75c7\u72b6\u6700\u5f3a\u70c8\u5730\u9a71\u52a8SSRI\u4e0eSNRI\u7684\u9009\u62e9\uff0c\u589e\u5f3a\u4e86\u57fa\u4e8eAI\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
