<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Topos Theory for Generative AI and LLMs](https://arxiv.org/abs/2508.08293)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: 基于拓扑理论，提出新型LLM架构，并理论证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明Transformer是通用的序列到序列函数逼近器，而本文则探索利用LLM范畴构成拓扑的特性设计新型架构。

Method: 使用范畴论中的泛构造构建新型LLM架构，包括拉回、推前、（余）均衡子、指数对象和子对象分类器。

Result: 理论上证明了LLM范畴是（余）完全的，并构成一个拓扑，并提出了一种基于函子描述的反向传播的潜在实现。

Conclusion: 本文提出了一种基于拓扑理论设计新型分类生成式AI架构的方法，并理论上验证了该架构的有效性，包括LLM的范畴构成一个拓扑这一结论。

Abstract: We propose the design of novel categorical generative AI architectures
(GAIAs) using topos theory, a type of category that is ``set-like": a topos has
all (co)limits, is Cartesian closed, and has a subobject classifier. Previous
theoretical results on the Transformer model have shown that it is a universal
sequence-to-sequence function approximator, and dense in the space of all
continuous functions with compact support on the Euclidean space of embeddings
of tokens. Building on this theoretical result, we explore novel architectures
for LLMs that exploit the property that the category of LLMs, viewed as
functions, forms a topos. Previous studies of large language models (LLMs) have
focused on daisy-chained linear architectures or mixture-of-experts. In this
paper, we use universal constructions in category theory to construct novel LLM
architectures based on new types of compositional structures. In particular,
these new compositional structures are derived from universal properties of LLM
categories, and include pullback, pushout, (co) equalizers, exponential
objects, and subobject classifiers. We theoretically validate these new
compositional structures by showing that the category of LLMs is (co)complete,
meaning that all diagrams have solutions in the form of (co)limits. Building on
this completeness result, we then show that the category of LLMs forms a topos,
a ``set-like" category, which requires showing the existence of exponential
objects as well as subobject classifiers. We use a functorial characterization
of backpropagation to define a potential implementation of an LLM topos
architecture.

</details>


### [2] [Topos Causal Models](https://arxiv.org/abs/2508.08295)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: 提出拓扑因果模型 (TCM)，利用拓扑范畴理论解决复杂因果关系建模和干预问题，具有更强的表达能力和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有因果模型在处理复杂因果关系和干预方面存在局限性。

Method: 使用拓扑范畴的理论构建因果模型，利用其完备性、对偶完备性、子对象分类器和指数对象等属性解决因果推理问题。

Result: 构建了拓扑因果模型 (TCM)，该模型能够处理任意复杂的因果图，对因果干预进行分类表示，并允许对因果模型上的运算等价类进行推理。

Conclusion: 提出了一种新型因果模型：拓扑因果模型 (TCM)，利用了拓扑范畴的关键属性（完备性、对偶完备性、子对象分类器和指数对象），解决了因果推理中的多个问题，例如：子对象分类器允许对因果干预进行分类表示，极限和上极限允许求解任意复杂的因果图，指数对象允许对因果模型上的运算等价类进行推理。

Abstract: We propose topos causal models (TCMs), a novel class of causal models that
exploit the key properties of a topos category: they are (co)complete, meaning
all (co)limits exist, they admit a subobject classifier, and allow exponential
objects. The main goal of this paper is to show that these properties are
central to many applications in causal inference. For example, subobject
classifiers allow a categorical formulation of causal intervention, which
creates sub-models. Limits and colimits allow causal diagrams of arbitrary
complexity to be ``solved", using a novel interpretation of causal
approximation. Exponential objects enable reasoning about equivalence classes
of operations on causal models, such as covered edge reversal and causal
homotopy. Analogous to structural causal models (SCMs), TCMs are defined by a
collection of functions, each defining a ``local autonomous" causal mechanism
that assemble to induce a unique global function from exogenous to endogenous
variables. Since the category of TCMs is (co)complete, which we prove in this
paper, every causal diagram has a ``solution" in the form of a (co)limit: this
implies that any arbitrary causal model can be ``approximated" by some global
function with respect to the morphisms going into or out of the diagram.
Natural transformations are crucial in measuring the quality of approximation.
In addition, we show that causal interventions are modeled by subobject
classifiers: any sub-model is defined by a monic arrow into its parent model.
Exponential objects permit reasoning about entire classes of causal
equivalences and interventions. Finally, as TCMs form a topos, they admit an
internal logic defined as a Mitchell-Benabou language with an associated
Kripke-Joyal semantics. We show how to reason about causal models in TCMs using
this internal logic.

</details>


### [3] [An Efficient Application of Goal Programming to Tackle Multiobjective Problems with Recurring Fitness Landscapes](https://arxiv.org/abs/2508.08297)
*Rodrigo Lankaites Pinheiro,Dario Landa-Silva,Wasakorn Laesanklang,Ademir Aparecido Constantino*

Main category: cs.AI

TL;DR: 针对具有相似适应度景观的多目标问题，该方法利用目标规划和高效单目标算法，结合先进多目标算法的优势，快速有效地找到高质量解。


<details>
  <summary>Details</summary>
Motivation: 许多现实世界的应用需要决策者在考虑多个相互冲突的目标时评估解决方案的质量。即使对于现代多目标算法，获得高度约束的多目标问题的良好近似集通常也是一项艰巨的任务。在某些情况下，问题场景的多个实例在其适应度景观中存在相似之处。

Method: 该方法首先使用计算代价高的多目标算法求解一个给定问题场景的实例，得到一个好的近似集；然后使用目标规划和高效的单目标算法求解同一问题场景的其他实例。使用了三个基于目标的客观函数。

Result: 在带时间窗的多目标车辆路径问题的基准实例上，该方法能够在较短的计算时间内产生良好的结果。

Conclusion: 该方法结合了最先进的多目标算法的有效性和目标规划的效率，在问题实例具有相似适应度景观的场景中找到了良好的折衷方案。

Abstract: Many real-world applications require decision-makers to assess the quality of
solutions while considering multiple conflicting objectives. Obtaining good
approximation sets for highly constrained many-objective problems is often a
difficult task even for modern multiobjective algorithms. In some cases,
multiple instances of the problem scenario present similarities in their
fitness landscapes. That is, there are recurring features in the fitness
landscapes when searching for solutions to different problem instances. We
propose a methodology to exploit this characteristic by solving one instance of
a given problem scenario using computationally expensive multiobjective
algorithms to obtain a good approximation set and then using Goal Programming
with efficient single-objective algorithms to solve other instances of the same
problem scenario. We use three goal-based objective functions and show that on
benchmark instances of the multiobjective vehicle routing problem with time
windows, the methodology is able to produce good results in short computation
time. The methodology allows to combine the effectiveness of state-of-the-art
multiobjective algorithms with the efficiency of goal programming to find good
compromise solutions in problem scenarios where instances have similar fitness
landscapes.

</details>


### [4] [LLM-BI: Towards Fully Automated Bayesian Inference with Large Language Models](https://arxiv.org/abs/2508.08300)
*Yongchao Huang*

Main category: cs.AI

TL;DR: 大型语言模型可自动化贝叶斯推理流程，减少对专业统计知识的需求


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推理的广泛应用受到先验分布和似然函数规范的阻碍，这通常需要专业的统计知识。

Method: 提出LLM-BI，一个基于大型语言模型的贝叶斯推理流程，并通过两个贝叶斯线性回归实验验证了其可行性。实验一验证了LLM可以从自然语言中成功提取先验分布；实验二验证了LLM可以从单个高级问题描述中指定整个模型结构，包括先验和似然。

Result: 实验结果验证了LLM在自动化贝叶斯建模中的潜力。

Conclusion: LLM可以自动化贝叶斯建模的关键步骤，实现概率编程的自动化推理流程。

Abstract: A significant barrier to the widespread adoption of Bayesian inference is the
specification of prior distributions and likelihoods, which often requires
specialized statistical expertise. This paper investigates the feasibility of
using a Large Language Model (LLM) to automate this process. We introduce
LLM-BI (Large Language Model-driven Bayesian Inference), a conceptual pipeline
for automating Bayesian workflows. As a proof-of-concept, we present two
experiments focused on Bayesian linear regression. In Experiment I, we
demonstrate that an LLM can successfully elicit prior distributions from
natural language. In Experiment II, we show that an LLM can specify the entire
model structure, including both priors and the likelihood, from a single
high-level problem description. Our results validate the potential of LLMs to
automate key steps in Bayesian modeling, enabling the possibility of an
automated inference pipeline for probabilistic programming.

</details>


### [5] [First Ask Then Answer: A Framework Design for AI Dialogue Based on Supplementary Questioning with Large Language Models](https://arxiv.org/abs/2508.08308)
*Chuanruo Fu,Yuncheng Du*

Main category: cs.AI

TL;DR: FATA引导LLM先问问题再回答，以改进LLM在信息不完整情况下的响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在处理不完整或不明确的用户提供信息时，难以提供准确和可操作的答案。

Method: 提出了一种新的交互范式FATA，该范式引导LLM主动生成多维补充问题，并将用户提供的补充信息与原始查询集成，以改进响应质量。

Result: FATA在多个指标上优于基线方法约40%，并且变异系数比上下文增强专家提示低8%，表明其具有更高的稳定性。

Conclusion: FATA框架通过在LLM生成答案前提出补充问题来改进LLM的响应质量和相关性，实验结果表明其性能优于基线方法。

Abstract: Large Language Models (LLMs) often struggle to deliver accurate and
actionable answers when user-provided information is incomplete or
ill-specified. We propose a new interaction paradigm, First Ask Then Answer
(FATA), in which, through prompt words, LLMs are guided to proactively generate
multidimensional supplementary questions for users prior to response
generation. Subsequently, by integrating user-provided supplementary
information with the original query through sophisticated prompting techniques,
we achieve substantially improved response quality and relevance. In contrast
to existing clarification approaches -- such as the CLAM framework oriented to
ambiguity and the self-interrogation Self-Ask method -- FATA emphasizes
completeness (beyond mere disambiguation) and user participation (inviting
human input instead of relying solely on model-internal reasoning). It also
adopts a single-turn strategy: all clarifying questions are produced at once,
thereby reducing dialogue length and improving efficiency. Conceptually, FATA
uses the reasoning power of LLMs to scaffold user expression, enabling
non-expert users to formulate more comprehensive and contextually relevant
queries. To evaluate FATA, we constructed a multi-domain benchmark and compared
it with two controls: a baseline prompt (B-Prompt) and a context-enhanced
expert prompt (C-Prompt). Experimental results show that FATA outperforms
B-Prompt by approximately 40% in aggregate metrics and exhibits a coefficient
of variation 8% lower than C-Prompt, indicating superior stability.

</details>


### [6] [What Breaks Knowledge Graph based RAG? Empirical Insights into Reasoning under Incomplete Knowledge](https://arxiv.org/abs/2508.08344)
*Dongzhuoran Zhou,Yuqicheng Zhu,Xiaxia Wang,Hongkuan Zhou,Yuan He,Jiaoyan Chen,Evgeny Kharlamov,Steffen Staab*

Main category: cs.AI

TL;DR: 新方法评估了知识不完整情况下KG-RAG的推理能力，发现其能力有限，常依赖记忆，泛化能力也存在差异。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在不足，例如基准测试中包含可以直接使用KG中现有三元组回答的问题，以及评估指标和答案匹配标准不一致等问题。

Method: 提出了一种构建基准的通用方法和评估协议，用于系统地评估知识不完整情况下的KG-RAG方法。

Result: 实证结果表明，当前KG-RAG方法在知识缺失的情况下推理能力有限，经常依赖于内部记忆，并且泛化能力因设计而异。

Conclusion: 当前KG-RAG方法在知识缺失的情况下推理能力有限，经常依赖于内部记忆，并且泛化能力因设计而异。

Abstract: Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) is an
increasingly explored approach for combining the reasoning capabilities of
large language models with the structured evidence of knowledge graphs.
However, current evaluation practices fall short: existing benchmarks often
include questions that can be directly answered using existing triples in KG,
making it unclear whether models perform reasoning or simply retrieve answers
directly. Moreover, inconsistent evaluation metrics and lenient answer matching
criteria further obscure meaningful comparisons. In this work, we introduce a
general method for constructing benchmarks, together with an evaluation
protocol, to systematically assess KG-RAG methods under knowledge
incompleteness. Our empirical results show that current KG-RAG methods have
limited reasoning ability under missing knowledge, often rely on internal
memorization, and exhibit varying degrees of generalization depending on their
design.

</details>


### [7] [UrzaGPT: LoRA-Tuned Large Language Models for Card Selection in Collectible Card Games](https://arxiv.org/abs/2508.08382)
*Timo Bertram*

Main category: cs.AI

TL;DR: 本文介绍了UrzaGPT，一个用于Magic: The Gathering实时选牌的LLM模型，其性能优于零样本LLM，显示了LLM在CCG AI中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的AI模型在CCG任务(如构组套牌和游戏过程)中的表现远逊于人类玩家。

Method: 基于预训练的大型语言模型(LLM)，使用低秩自适应微调技术，并使用带注释的选牌日志数据集进行训练。

Result: 在Magic: The Gathering游戏中，UrzaGPT模型达到了66.2%的准确率，优于零样本LLM，但仍低于特定领域模型。

Conclusion: 使用LLM进行集换式卡牌游戏(CCG)选牌是可行的，并能创建高性能、通用且易于更新的选牌AI。

Abstract: Collectible card games (CCGs) are a difficult genre for AI due to their
partial observability, long-term decision-making, and evolving card sets. Due
to this, current AI models perform vastly worse than human players at CCG tasks
such as deckbuilding and gameplay. In this work, we introduce
$\textit{UrzaGPT}$, a domain-adapted large language model that recommends
real-time drafting decisions in $\textit{Magic: The Gathering}$. Starting from
an open-weight LLM, we use Low-Rank Adaptation fine-tuning on a dataset of
annotated draft logs. With this, we leverage the language modeling capabilities
of LLM, and can quickly adapt to different expansions of the game. We benchmark
$\textit{UrzaGPT}$ in comparison to zero-shot LLMs and the state-of-the-art
domain-specific model. Untuned, small LLMs like Llama-3-8B are completely
unable to draft, but the larger GPT-4o achieves a zero-shot performance of
$43\%$. Using UrzaGPT to fine-tune smaller models, we achieve an accuracy of
$66.2\%$ using only 10,000 steps. Despite this not reaching the capability of
domain-specific models, we show that solely using LLMs to draft is possible and
conclude that using LLMs can enable performant, general, and update-friendly
drafting AIs in the future.

</details>


### [8] [Bilevel MCTS for Amortized O(1) Node Selection in Classical Planning](https://arxiv.org/abs/2508.08385)
*Masataro Asai*

Main category: cs.AI

TL;DR: 改进MCTS算法，通过双层搜索和树折叠，将节点选择时间复杂度从O(logN)降到O(1)，提升经典规划效率。


<details>
  <summary>Details</summary>
Motivation: 经典规划问题中，MCTS 的节点选择步骤（时间复杂度为 O(log N)）成为瓶颈，本文旨在改进这一问题。

Method: 提出了一种改进的基于多臂老虎机 (MAB) 的蒙特卡洛树搜索 (MCTS) 方法，该方法包含双层搜索和树折叠技术。

Result: 通过双层搜索和树折叠技术，将节点选择的时间复杂度降低到 O(1)，提高了算法效率。

Conclusion: 本文提出了一种改进的基于多臂老虎机 (MAB) 的蒙特卡洛树搜索 (MCTS) 方法，用于经典规划问题，通过双层搜索和树折叠技术将节点选择的时间复杂度从 O(log N) 降低到 O(1)，从而提高了算法效率。

Abstract: We study an efficient implementation of Multi-Armed Bandit (MAB)-based
Monte-Carlo Tree Search (MCTS) for classical planning. One weakness of MCTS is
that it spends a significant time deciding which node to expand next. While
selecting a node from an OPEN list with $N$ nodes has $O(1)$ runtime complexity
with traditional array-based priority-queues for dense integer keys, the
tree-based OPEN list used by MCTS requires $O(\log N)$, which roughly
corresponds to the search depth $d$. In classical planning, $d$ is arbitrarily
large (e.g., $2^k-1$ in $k$-disk Tower-of-Hanoi) and the runtime for node
selection is significant, unlike in game tree search, where the cost is
negligible compared to the node evaluation (rollouts) because $d$ is inherently
limited by the game (e.g., $d\leq 361$ in Go). To improve this bottleneck, we
propose a bilevel modification to MCTS that runs a best-first search from each
selected leaf node with an expansion budget proportional to $d$, which achieves
amortized $O(1)$ runtime for node selection, equivalent to the traditional
queue-based OPEN list. In addition, we introduce Tree Collapsing, an
enhancement that reduces action selection steps and further improves the
performance.

</details>


### [9] [Solver-Aided Expansion of Loops to Avoid Generate-and-Test](https://arxiv.org/abs/2508.08442)
*Niklas Dewally,Özgür Akgün*

Main category: cs.AI

TL;DR: 提出一种新型编译方法，通过求解器计算必要组合，提高了约束建模语言的编译效率，尤其在归纳变量范围很大且存在选择性前提条件时。


<details>
  <summary>Details</summary>
Motivation: 标准方法生成所有归纳变量的组合，并使用部分求值来丢弃简化为结合-交换运算符的恒等元素的那些组合，这对于大多数组合最终无关紧要的问题效率低下。

Method: 使用求解器计算仅生成最终约束集所需的组合，避免完全枚举。

Result: 生成的模型与传统扁平化生成的模型相同，但编译速度可能显著提高。

Conclusion: 提出了一种避免完全枚举的方法，通过使用求解器仅计算生成最终约束集所需的组合，从而提高了将高级用户模型转换为求解器就绪形式的效率。

Abstract: Constraint modelling languages like MiniZinc and Essence rely on unrolling
loops (in the form of quantified expressions and comprehensions) during
compilation. Standard approaches generate all combinations of induction
variables and use partial evaluation to discard those that simplify to identity
elements of associative-commutative operators (e.g. true for conjunction, 0 for
summation). This can be inefficient for problems where most combinations are
ultimately irrelevant. We present a method that avoids full enumeration by
using a solver to compute only the combinations required to generate the final
set of constraints. The resulting model is identical to that produced by
conventional flattening, but compilation can be significantly faster. This
improves the efficiency of translating high-level user models into solver-ready
form, particularly when induction variables range over large domains with
selective preconditions.

</details>


### [10] [OverFill: Two-Stage Models for Efficient Language Model Decoding](https://arxiv.org/abs/2508.08446)
*Woojeong Kim,Junxiong Wang,Jing Nathan Yan,Mohamed Abdelfattah,Alexander M. Rush*

Main category: cs.AI

TL;DR: OverFill通过解耦预填充和解码阶段，提高了大型语言模型的推理效率和准确性，在基准测试中取得显著成果。


<details>
  <summary>Details</summary>
Motivation: 现有解码器模型统一处理预填充和解码阶段，导致效率低下，特别是对于长序列。

Method: 提出了一种名为OverFill的模型，该模型解耦预填充和解码阶段，分别使用完整模型和剪枝模型进行处理。

Result: 3B到1B的OverFill配置优于1B剪枝模型83.2%，8B到3B配置优于3B剪枝模型79.2%。OverFill与同等规模的从零开始训练的模型性能相当，但训练数据显著减少。

Conclusion: OverFill模型通过解耦预填充和解码阶段，优化了大型语言模型的推理效率和准确性，在标准基准测试中显著优于同等规模的模型。

Abstract: Large language models (LLMs) excel across diverse tasks but face significant
deployment challenges due to high inference costs. LLM inference comprises
prefill (compute-bound) and decode (memory-bound) stages, with decode
dominating latency particularly for long sequences. Current decoder-only models
handle both stages uniformly, despite their distinct computational profiles. We
propose OverFill, which decouples these stages to optimize accuracy-efficiency
tradeoffs. OverFill begins with a full model for prefill, processing system and
user inputs in parallel. It then switches to a dense pruned model, while
generating tokens sequentially. Leveraging more compute during prefill,
OverFill improves generation quality with minimal latency overhead. Our
3B-to-1B OverFill configuration outperforms 1B pruned models by 83.2%, while
the 8B-to-3B configuration improves over 3B pruned models by 79.2% on average
across standard benchmarks. OverFill matches the performance of same-sized
models trained from scratch, while using significantly less training data. Our
code is available at https://github.com/friendshipkim/overfill.

</details>


### [11] [A Fast GRASP Metaheuristic for the Trigger Arc TSP with MIP-Based Construction and Multi-Neighborhood Local Search](https://arxiv.org/abs/2508.08477)
*Joan Salvà Soler,Grégoire de Lambertye*

Main category: cs.AI

TL;DR: 一种新的基于GRASP的元启发式算法有效解决了触发弧旅行商问题（TA-TSP），并在MESS 2024竞赛中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 经典旅行商问题（TSP）无法有效解决动态弧成本问题，例如仓库操作中可压缩存储系统带来的问题，因此本文提出TA-TSP及其求解算法。

Method: 该算法结合了多种构造启发式算法和多领域局部搜索，利用混合整数规划（MIP）技术将TA-TSP转化为一系列定制的TSP实例，改进阶段应用2-Opt、Swap和Relocate算子。

Result: 在MESS 2024竞赛实例上，该算法在60秒内取得了平均最优差距0.77%和0.40%的优异结果；在较小的合成数据集上，该算法的结果比Gurobi求解器好11.3%。

Conclusion: 本文提出一种基于GRASP的元启发式算法，用于解决触发弧旅行商问题（TA-TSP），并在MESS 2024竞赛中取得了前三名的成绩，证明了该算法在具有状态相关旅行成本的实时路径规划应用中的适用性。

Abstract: The Trigger Arc Traveling Salesman Problem (TA-TSP) extends the classical TSP
by introducing dynamic arc costs that change when specific \textit{trigger}
arcs are traversed, modeling scenarios such as warehouse operations with
compactable storage systems. This paper introduces a GRASP-based metaheuristic
that combines multiple construction heuristics with a multi-neighborhood local
search. The construction phase uses mixed-integer programming (MIP) techniques
to transform the TA-TSP into a sequence of tailored TSP instances, while the
improvement phase applies 2-Opt, Swap, and Relocate operators. Computational
experiments on MESS 2024 competition instances achieved average optimality gaps
of 0.77\% and 0.40\% relative to the best-known solutions within a 60-second
limit. On smaller, synthetically generated datasets, the method produced
solutions 11.3\% better than the Gurobi solver under the same time constraints.
The algorithm finished in the top three at MESS 2024, demonstrating its
suitability for real-time routing applications with state-dependent travel
costs.

</details>


### [12] [Beyond Ordinal Preferences: Why Alignment Needs Cardinal Human Feedback](https://arxiv.org/abs/2508.08486)
*Parker Whitfill,Stewy Slocum*

Main category: cs.AI

TL;DR: 仅用序数比较无法训练最佳LLM模型，需用支付意愿等基数反馈数据。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM对齐技术依赖于基于序数比较的偏好目标，存在无法解决权衡和恢复最优模型的问题。

Method: 使用支付意愿法收集基数判断，并将其融入偏好微调中。

Result: 提出并验证了一种基于基数判断的模型选择方法，该方法在后续基准测试中优于仅使用序数比较的方法。

Conclusion: 本文证明仅依赖序数比较的算法无法系统地恢复最优模型，并提出了一种使用支付意愿收集基数判断的方法，实证结果表明该方法优于仅使用序数比较的方法。

Abstract: Alignment techniques for LLMs rely on optimizing preference-based objectives
-- where these preferences are typically elicited as ordinal, binary choices
between responses. Recent work has focused on improving label quality or
mitigating particular biases, but we identify a more fundamental limitation:
these methods collect the wrong kind of data. We prove an impossibility result:
no algorithm relying solely on ordinal comparisons can systematically recover
the most preferred model. Intuitively, ordinal data lacks the information
needed to resolve tradeoffs -- e.g., fixing a factual error on one prompt
versus improving style on another. We show that selecting the optimal model
requires recovering preferences over \emph{models} (rather than just
responses), which can only be identified given cardinal feedback about response
quality. To address this, we collect and publicly release a dataset of 25,000
cardinal judgments using willingness-to-pay elicitations, a well-established
tool from experimental economics. Empirically, we find that incorporating
cardinal feedback into preference fine-tuning allows models to prioritize
high-impact improvements and outperform ordinal-only methods on downstream
benchmarks, such as Arena-Hard.

</details>


### [13] [POMO+: Leveraging starting nodes in POMO for solving Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2508.08493)
*Szymon Jakubicz,Karol Kuźniak,Jan Wawszczak,Paweł Gora*

Main category: cs.AI

TL;DR: 改进的强化学习方法POMO+在VRP问题上表现更好，收敛更快，效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习的POMO方法在解决组合优化问题，例如车辆路径问题 (VRP) 的变体时，仍有改进空间。

Method: 改进的POMO (POMO+) 方法，利用初始节点更有效地寻找解决方案。

Result: POMO+方法在CVRPLIB数据集上，针对最多100个客户的案例，取得了比POMO更快的收敛速度和更好的结果。

Conclusion: POMO+方法在CVRPLIB数据集上，针对最多100个客户的案例，通过利用初始节点更有效地寻找解决方案，取得了比POMO更快的收敛速度和更好的结果。

Abstract: In recent years, reinforcement learning (RL) methods have emerged as a
promising approach for solving combinatorial problems. Among RL-based models,
POMO has demonstrated strong performance on a variety of tasks, including
variants of the Vehicle Routing Problem (VRP). However, there is room for
improvement for these tasks. In this work, we improved POMO, creating a method
(\textbf{POMO+}) that leverages the initial nodes to find a solution in a more
informed way. We ran experiments on our new model and observed that our
solution converges faster and achieves better results. We validated our models
on the CVRPLIB dataset and noticed improvements in problem instances with up to
100 customers. We hope that our research in this project can lead to further
advancements in the field.

</details>
