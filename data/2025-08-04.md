<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench](https://arxiv.org/abs/2508.00081)
*Fred Mutisya,Shikoh Gitau,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha*

Main category: cs.AI

TL;DR: 改进HealthBench基准测试，使其更客观、更符合全球标准。


<details>
  <summary>Details</summary>
Motivation: HealthBench基准测试存在地区偏见和个体临床医生的特殊性等问题，尤其是在中低收入地区。

Method: 提出了一种基于版本控制的临床实践指南（CPG）的奖励函数，并结合强化学习方法。

Result: 提出了一种更具有全球相关性和公平性的基准测试方法。

Conclusion: 本论文认为HealthBench基准测试依赖专家意见而非高级临床证据，可能导致地区偏见和个体临床医生的特殊性，尤其是在中低收入地区问题更为严重。因此，提出了一种基于版本控制的临床实践指南（CPG）的奖励函数，以增强医疗语言模型的临床可信度、伦理性和全球相关性。

Abstract: HealthBench, a benchmark designed to measure the capabilities of AI systems
for health better (Arora et al., 2025), has advanced medical language model
evaluation through physician-crafted dialogues and transparent rubrics.
However, its reliance on expert opinion, rather than high-tier clinical
evidence, risks codifying regional biases and individual clinician
idiosyncrasies, further compounded by potential biases in automated grading
systems. These limitations are particularly magnified in low- and middle-income
settings, where issues like sparse neglected tropical disease coverage and
region-specific guideline mismatches are prevalent.
  The unique challenges of the African context, including data scarcity,
inadequate infrastructure, and nascent regulatory frameworks, underscore the
urgent need for more globally relevant and equitable benchmarks. To address
these shortcomings, we propose anchoring reward functions in version-controlled
Clinical Practice Guidelines (CPGs) that incorporate systematic reviews and
GRADE evidence ratings.
  Our roadmap outlines "evidence-robust" reinforcement learning via
rubric-to-guideline linkage, evidence-weighted scoring, and contextual override
logic, complemented by a focus on ethical considerations and the integration of
delayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,
while preserving HealthBench's transparency and physician engagement, we aim to
foster medical language models that are not only linguistically polished but
also clinically trustworthy, ethically sound, and globally relevant.

</details>


### [2] [Hyperproperty-Constrained Secure Reinforcement Learning](https://arxiv.org/abs/2508.00106)
*Ernest Bonnah,Luan Viet Nguyen,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: 本文提出一种新的安全强化学习方法，有效解决了 HyperTWTL 约束下的安全策略学习问题，并在机器人任务中取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习研究较少关注利用超属性的安全强化学习问题，因此本文旨在解决这一问题。

Method: 提出了一种基于动态 Boltzmann softmax 强化学习算法，满足 HyperTWTL 约束的安全强化学习方法。

Result: 提出的方法在 pick-up and delivery 机器人任务案例研究中取得了优于其他两种基线强化学习算法的结果，证明了其有效性和可扩展性。

Conclusion: 提出了一种基于动态 Boltzmann softmax 强化学习算法的满足 HyperTWTL 约束的安全强化学习方法，并在 pick-up and delivery 机器人任务案例研究中验证了其有效性和可扩展性，并与其他两种基线强化学习算法进行了比较，结果表明该方法性能更优。

Abstract: Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a
domain-specific formal specification language known for its effectiveness in
compactly representing security, opacity, and concurrency properties for
robotics applications. This paper focuses on HyperTWTL-constrained secure
reinforcement learning (SecRL). Although temporal logic-constrained safe
reinforcement learning (SRL) is an evolving research problem with several
existing literature, there is a significant research gap in exploring
security-aware reinforcement learning (RL) using hyperproperties. Given the
dynamics of an agent as a Markov Decision Process (MDP) and opacity/security
constraints formalized as HyperTWTL, we propose an approach for learning
security-aware optimal policies using dynamic Boltzmann softmax RL while
satisfying the HyperTWTL constraints. The effectiveness and scalability of our
proposed approach are demonstrated using a pick-up and delivery robotic mission
case study. We also compare our results with two other baseline RL algorithms,
showing that our proposed method outperforms them.

</details>


### [3] [No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence](https://arxiv.org/abs/2508.00116)
*Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: 人工智能需要流程智能来改进运营流程。


<details>
  <summary>Details</summary>
Motivation: 组织机构难以在注重端到端运营流程的工业环境中成功应用人工智能。

Method: 本文探讨了生成式、预测式和规范性人工智能，并阐述了诊断和改进此类流程的挑战，提出将面向对象流程挖掘作为连接数据和流程的桥梁。

Result: 面向对象流程挖掘是连接数据和流程的缺失环节，它能够支持不同形式的人工智能，并通过流程智能将多种数据驱动技术结合，从而在组织环境中实现人工智能。

Conclusion: 本文论述了人工智能在改进运营流程中需要流程智能的原因，并强调了成功结合面向对象流程挖掘和生成式、预测式以及规范性人工智能的机会。

Abstract: The uptake of Artificial Intelligence (AI) impacts the way we work, interact,
do business, and conduct research. However, organizations struggle to apply AI
successfully in industrial settings where the focus is on end-to-end
operational processes. Here, we consider generative, predictive, and
prescriptive AI and elaborate on the challenges of diagnosing and improving
such processes. We show that AI needs to be grounded using Object-Centric
Process Mining (OCPM). Process-related data are structured and
organization-specific and, unlike text, processes are often highly dynamic.
OCPM is the missing link connecting data and processes and enables different
forms of AI. We use the term Process Intelligence (PI) to refer to the
amalgamation of process-centric data-driven techniques able to deal with a
variety of object and event types, enabling AI in an organizational context.
This paper explains why AI requires PI to improve operational processes and
highlights opportunities for successfully combining OCPM and generative,
predictive, and prescriptive AI.

</details>


### [4] [Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis](https://arxiv.org/abs/2508.00129)
*Agustín Borda,Juan Bautista Cabral,Gonzalo Giarda,Diego Nicolás Gimenez Irusta,Paula Pacheco,Alvaro Roy Schachner*

Main category: cs.AI

TL;DR: 本文提出三种检测多准则决策方法中排序逆转的测试方法，并在Scikit-Criteria库中实现，以改进多准则决策方法的评判。


<details>
  <summary>Details</summary>
Motivation: 多准则决策分析中排序逆转问题会严重影响结果，因此需要一种机制来衡量方法的性能。

Method: 提出三种检测排序逆转的测试方法，并在Scikit-Criteria库中实现。

Result: 实现了三种检测排序逆转的测试方法，并讨论了其实现复杂性和设计考虑。

Conclusion: 本文介绍了三种检测排序逆转的测试方法及其在Scikit-Criteria库中的实现，并讨论了这些测试在一般场景中的实现复杂性以及设计考虑，最后探讨了这些新增功能如何对多准则决策方法的评判产生重大影响。

Abstract: In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem
that can greatly affect the results of a Multi-Criteria Decision Method against
a particular set of alternatives. It is therefore useful to have a mechanism
that allows one to measure the performance of a method on a set of
alternatives. This idea could be taken further to build a global ranking of the
effectiveness of different methods to solve a problem. In this paper, we
present three tests that detect the presence of Rank Reversals, along with
their implementation in the Scikit-Criteria library. We also address the
complications that arise when implementing these tests for general scenarios
and the design considerations we made to handle them. We close with a
discussion about how these additions could play a major role in the judgment of
multi-criteria decision methods for problem solving.

</details>


### [5] [SHACL Validation under Graph Updates (Extended Paper)](https://arxiv.org/abs/2508.00137)
*Shqiponja Ahmetaj,George Konstantinidis,Magdalena Ortiz,Paolo Pareti,Mantas Simkus*

Main category: cs.AI

TL;DR: 提出一种SHACL更新语言，通过回归技术将更新下的静态验证简化为SHACL约束的（不）可满足性问题，并实现原型系统。


<details>
  <summary>Details</summary>
Motivation: 研究RDF图在更新下的SHACL验证问题，为推理演变的RDF图提供基础。

Method: 提出了一种基于SHACL的更新语言，使用将更新操作嵌入SHACL约束的回归技术。

Result: 将更新下的静态验证简化为SHACL约束的（不）可满足性问题，并分析了其计算复杂度，实现了一个原型系统。

Conclusion: 研究了RDF图在更新下的SHACL验证问题，提出了一种基于SHACL的更新语言，并通过将更新操作嵌入SHACL约束的回归技术，将更新下的静态验证简化为SHACL约束的（不）可满足性问题，分析了SHACL及其关键片段的静态验证问题的计算复杂度，最后实现了一个原型系统。

Abstract: SHACL (SHApe Constraint Language) is a W3C standardized constraint language
for RDF graphs. In this paper, we study SHACL validation in RDF graphs under
updates. We present a SHACL-based update language that can capture intuitive
and realistic modifications on RDF graphs and study the problem of static
validation under such updates. This problem asks to verify whether every graph
that validates a SHACL specification will still do so after applying a given
update sequence. More importantly, it provides a basis for further services for
reasoning about evolving RDF graphs. Using a regression technique that embeds
the update actions into SHACL constraints, we show that static validation under
updates can be reduced to (un)satisfiability of constraints in (a minor
extension of) SHACL. We analyze the computational complexity of the static
validation problem for SHACL and some key fragments. Finally, we present a
prototype implementation that performs static validation and other static
analysis tasks on SHACL constraints and demonstrate its behavior through
preliminary experiments.

</details>


### [6] [Co-Producing AI: Toward an Augmented, Participatory Lifecycle](https://arxiv.org/abs/2508.00138)
*Rashid Mushkani,Hugo Berard,Toumadher Ammar,Cassandre Chatonnier,Shin Koseki*

Main category: cs.AI

TL;DR: 重新设计AI生产流程，以合作生产和多学科协作为核心，减轻AI算法对弱势群体的负面影响。


<details>
  <summary>Details</summary>
Motivation: AI算法固有的风险和偏差可能对弱势群体产生不成比例的影响。

Method: 基于设计正义、扩展学习理论和参与式AI的实证研究，提出了一种增强的AI生命周期，包含五个阶段：合作构建、合作设计、合作实施、合作部署和合作维护。

Result: 提出了一种包含五个阶段的增强型AI生命周期模型，以促进更公平、更公正的AI系统开发。

Conclusion: 本文认为，减轻AI算法对弱势群体的负面影响，需要重新构建AI生产流程，提倡合作生产、多样性、公平、包容和多学科协作。

Abstract: Despite efforts to mitigate the inherent risks and biases of artificial
intelligence (AI) algorithms, these algorithms can disproportionately impact
culturally marginalized groups. A range of approaches has been proposed to
address or reduce these risks, including the development of ethical guidelines
and principles for responsible AI, as well as technical solutions that promote
algorithmic fairness. Drawing on design justice, expansive learning theory, and
recent empirical work on participatory AI, we argue that mitigating these harms
requires a fundamental re-architecture of the AI production pipeline. This
re-design should center co-production, diversity, equity, inclusion (DEI), and
multidisciplinary collaboration. We introduce an augmented AI lifecycle
consisting of five interconnected phases: co-framing, co-design,
co-implementation, co-deployment, and co-maintenance. The lifecycle is informed
by four multidisciplinary workshops and grounded in themes of distributed
authority and iterative knowledge exchange. Finally, we relate the proposed
lifecycle to several leading ethical frameworks and outline key research
questions that remain for scaling participatory governance.

</details>


### [7] [Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation](https://arxiv.org/abs/2508.00143)
*Danielle R. Thomas,Conrad Borchers,Kenneth R. Koedinger*

Main category: cs.AI

TL;DR: 过度依赖人类IRR验证数据质量会阻碍进步，建议结合多标签标注、专家方法和闭环效度等方法，优先考虑效度和教育影响。


<details>
  <summary>Details</summary>
Motivation: 改进教育数据标注和模型训练方法，以提升学生学习效果和产生更有效的见解。

Method: 分析了现有基于IRR的标注质量评估方法的局限性，并提出了五种补充性评估方法。

Result: 提出了五种补充性评估方法，并强调外部效度的重要性，呼吁重新思考标注质量和ground truth的概念。

Conclusion: 过度依赖人类评判者间信度 (IRR) 来验证标注质量阻碍了教育数据分类的进展，该论文建议采用多标签标注方案、专家方法和闭环效度等补充性评估方法，以提高训练数据的质量和模型的预测能力，最终提升学生学习效果和产生更有效的见解。

Abstract: Humans can be notoriously imperfect evaluators. They are often biased,
unreliable, and unfit to define "ground truth." Yet, given the surging need to
produce large amounts of training data in educational applications using AI,
traditional inter-rater reliability (IRR) metrics like Cohen's kappa remain
central to validating labeled data. IRR remains a cornerstone of many machine
learning pipelines for educational data. Take, for example, the classification
of tutors' moves in dialogues or labeling open responses in machine-graded
assessments. This position paper argues that overreliance on human IRR as a
gatekeeper for annotation quality hampers progress in classifying data in ways
that are valid and predictive in relation to improving learning. To address
this issue, we highlight five examples of complementary evaluation methods,
such as multi-label annotation schemes, expert-based approaches, and
close-the-loop validity. We argue that these approaches are in a better
position to produce training data and subsequent models that produce improved
student learning and more actionable insights than IRR approaches alone. We
also emphasize the importance of external validity, for example, by
establishing a procedure of validating tutor moves and demonstrating that it
works across many categories of tutor actions (e.g., providing hints). We call
on the field to rethink annotation quality and ground truth--prioritizing
validity and educational impact over consensus alone.

</details>


### [8] [Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power](https://arxiv.org/abs/2508.00159)
*Jobst Heitzig,Ram Potham*

Main category: cs.AI

TL;DR: 该论文提出将软化最大化人类能力作为人工智能安全的新目标，认为其比直接基于效用的目标更安全。


<details>
  <summary>Details</summary>
Motivation: 探索通过强制人工智能代理明确赋能人类并以理想方式管理人类与人工智能代理之间的力量平衡来促进安全和福祉。

Method: 使用基于原理的、部分公理化的方法设计参数化和可分解的目标函数，该函数代表人类能力的风险规避型长期总量，并通过逆向归纳或多智能体强化学习近似计算该指标。

Result: 设计了一个参数化和可分解的目标函数，代表人类能力的不平等和风险规避的长期总和，并导出了计算该指标的算法。

Conclusion: 软化最大化人类能力的聚合指标可能构成赋能型人工智能系统的有益目标，比直接基于效用的目标更安全。

Abstract: Power is a key concept in AI safety: power-seeking as an instrumental goal,
sudden or gradual disempowerment of humans, power balance in human-AI
interaction and international AI governance. At the same time, power as the
ability to pursue diverse goals is essential for wellbeing.
  This paper explores the idea of promoting both safety and wellbeing by
forcing AI agents explicitly to empower humans and to manage the power balance
between humans and AI agents in a desirable way. Using a principled, partially
axiomatic approach, we design a parametrizable and decomposable objective
function that represents an inequality- and risk-averse long-term aggregate of
human power. It takes into account humans' bounded rationality and social
norms, and, crucially, considers a wide variety of possible human goals.
  We derive algorithms for computing that metric by backward induction or
approximating it via a form of multi-agent reinforcement learning from a given
world model. We exemplify the consequences of (softly) maximizing this metric
in a variety of paradigmatic situations and describe what instrumental
sub-goals it will likely imply. Our cautious assessment is that softly
maximizing suitable aggregate metrics of human power might constitute a
beneficial objective for agentic AI systems that is safer than direct
utility-based objectives.

</details>


### [9] [RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization](https://arxiv.org/abs/2508.00222)
*Yihong Dong,Xue Jiang,Yongding Tao,Huanyu Liu,Kechi Zhang,Lili Mou,Rongyu Cao,Yingwei Ma,Jue Chen,Binhua Li,Zhi Jin,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: RL-PLUS通过结合内部思考和外部学习，显著提升了LLM的推理能力，解决了能力边界崩溃问题，并在多个基准测试中取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法由于其固有的策略以及LLM巨大的动作空间和稀疏奖励，难以突破基础LLM的能力边界，甚至可能导致能力边界崩溃。

Method: RL-PLUS整合了多重要性采样和基于探索的优势函数，以解决外部数据分布错配问题，并引导模型探索高价值的推理路径。

Result: RL-PLUS在六个数学推理基准测试和六个分布外推理任务上取得了最先进的性能，与现有RLVR方法相比，平均相对改进率达21.1%到69.2%。Pass@k曲线表明，RL-PLUS有效解决了能力边界崩溃问题。

Conclusion: RL-PLUS，一种结合内部利用（思考）和外部数据（学习）的强化学习方法，显著提高了大型语言模型的推理能力，解决了现有RLVR方法能力边界崩溃的问题，并在多个基准测试中取得了最先进的性能。

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has significantly
advanced the complex reasoning abilities of Large Language Models (LLMs).
However, it struggles to break through the inherent capability boundaries of
the base LLM, due to its inherently on-policy strategy with LLM's immense
action space and sparse reward. Further, RLVR can lead to the capability
boundary collapse, narrowing the LLM's problem-solving scope. To address this
problem, we propose RL-PLUS, a novel approach that synergizes internal
exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve
stronger reasoning capabilities and surpass the boundaries of base models.
RL-PLUS integrates two core components: Multiple Importance Sampling to address
for distributional mismatch from external data, and an Exploration-Based
Advantage Function to guide the model towards high-value, unexplored reasoning
paths. We provide both theoretical analysis and extensive experiments to
demonstrate the superiority and generalizability of our approach. The results
show that RL-PLUS achieves state-of-the-art performance compared with existing
RLVR methods on six math reasoning benchmarks and exhibits superior performance
on six out-of-distribution reasoning tasks. It also achieves consistent and
significant gains across diverse model families, with average relative
improvements ranging from 21.1\% to 69.2\%. Moreover, Pass@k curves across
multiple benchmarks indicate that RL-PLUS effectively resolves the capability
boundary collapse problem.

</details>


### [10] [MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning](https://arxiv.org/abs/2508.00271)
*Hongjin Qian,Zheng Liu*

Main category: cs.AI

TL;DR: MetaAgent:一个通过自我改进和元工具学习持续提升能力的代理系统，在知识发现方面表现优异


<details>
  <summary>Details</summary>
Motivation: 开发一种能够通过实践和自我改进持续提升专业技能的代理系统。

Method: MetaAgent是一种基于“学习通过实践”原则的代理范式，它通过自然语言寻求帮助，并通过元工具学习持续改进其推理和工具使用策略，无需更改模型参数或进行额外训练。

Result: MetaAgent在GAIA、WebWalkerQA和BrowseCamp等具有挑战性的知识发现基准测试中，持续优于基于工作流的基线，并匹配或超过端到端训练的智能体。

Conclusion: MetaAgent在知识发现基准测试中持续优于基于工作流的基线，并匹配或超过端到端训练的智能体，展示了自我进化代理系统在健壮、通用知识发现方面的潜力。

Abstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the
principle of learning-by-doing, where expertise is developed through hands-on
practice and continual self-improvement. MetaAgent starts with a minimal
workflow, equipped only with basic reasoning and adaptive help-seeking
abilities. When a knowledge gap is encountered, MetaAgent generates natural
language help requests, which are routed to the most suitable external tool by
a dedicated tool router. As MetaAgent solves tasks, it continually conducts
self-reflection and answer verification, distilling actionable experience into
concise texts that are dynamically incorporated into future task contexts.
Besides, MetaAgent autonomously builds in-house tools and a persistent
knowledge base by organizing its tool-use history, further enhancing its
ability to retrieve and integrate relevant information We term this continual,
data-driven process as \textit{meta tool learning}, through which MetaAgent
incrementally refines its reasoning and tool-use strategies, without changing
model parameters or requiring further post-training. Evaluated on challenging
knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,
MetaAgent consistently outperforms workflow-based baselines and matches or
exceeds end-to-end trained agents, demonstrating the promise of self-evolving
agentic systems for robust, general-purpose knowledge discovery. We provide our
source codes in https://github.com/qhjqhj00/MetaAgent.

</details>


### [11] [Mind the Gap: The Divergence Between Human and LLM-Generated Tasks](https://arxiv.org/abs/2508.00282)
*Yi-Long Lu,Jiajun Song,Chunhui Zhang,Wei Wang*

Main category: cs.AI

TL;DR: LLM生成的任務與人類生成的任務存在根本差異，缺乏人類的價值觀和體現性。


<details>
  <summary>Details</summary>
Motivation: 探究LLM代理是否基於與人類相似的認知原則生成任務。

Method: 比較人類和LLM代理(GPT-4o)的任務生成結果。

Result: 人類任務生成受心理因素(例如，價值觀和認知風格)影響，而LLM即使被提供這些因素，也無法體現相應的行為模式，生成的任務缺乏社會性、體現性和價值驅動性。

Conclusion: 大型语言模型(LLM)生成的任務與人類生成的任務存在核心差異，LLM生成的任務缺乏社會性、體現性和價值驅動性，突顯了在設計更人性化智能體時納入內在動機和物理基礎的必要性。

Abstract: Humans constantly generate a diverse range of tasks guided by internal
motivations. While generative agents powered by large language models (LLMs)
aim to simulate this complex behavior, it remains uncertain whether they
operate on similar cognitive principles. To address this, we conducted a
task-generation experiment comparing human responses with those of an LLM agent
(GPT-4o). We find that human task generation is consistently influenced by
psychological drivers, including personal values (e.g., Openness to Change) and
cognitive style. Even when these psychological drivers are explicitly provided
to the LLM, it fails to reflect the corresponding behavioral patterns. They
produce tasks that are markedly less social, less physical, and thematically
biased toward abstraction. Interestingly, while the LLM's tasks were perceived
as more fun and novel, this highlights a disconnect between its linguistic
proficiency and its capacity to generate human-like, embodied goals.We conclude
that there is a core gap between the value-driven, embodied nature of human
cognition and the statistical patterns of LLMs, highlighting the necessity of
incorporating intrinsic motivation and physical grounding into the design of
more human-aligned agents.

</details>


### [12] [Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning](https://arxiv.org/abs/2508.00323)
*Jianyi Zhang,Xu Ji,Ziyin Zhou,Yuchen Zhou,Shubo Shi,Haoyu Wu,Zhen Li,Shizhao Liu*

Main category: cs.AI

TL;DR: ReasonBench:一个用于评估VLMs复杂图形推理能力的新基准，揭示了当前模型的局限性，并提出了一种双重优化策略以提高性能。


<details>
  <summary>Details</summary>
Motivation: 评估VLMs在复杂图形推理中的性能，特别是针对现有研究较少关注的复杂图形推理和抽象问题解决。

Method: 构建ReasonBench基准测试，包含1613个来自现实世界智力测试的问题；对11个主流VLMs进行评估；提出DiaCoT和ReasonTune两种优化策略。

Result: ReasonBench基准测试揭示了当前VLMs的局限性；DiaCoT和ReasonTune策略提高了VLM性能33.5%。

Conclusion: 现有视觉语言模型(VLMs)在复杂图形推理和抽象问题解决方面存在明显缺陷，本文提出的ReasonBench基准测试涵盖了位置、属性、数量和多元素任务等推理维度，对11个主流VLMs进行了评估，并提出了一种双重优化策略(DiaCoT和ReasonTune)提高了VLM性能33.5%。

Abstract: Evaluating the performance of visual language models (VLMs) in graphic
reasoning tasks has become an important research topic. However, VLMs still
show obvious deficiencies in simulating human-level graphic reasoning
capabilities, especially in complex graphic reasoning and abstract problem
solving, which are less studied and existing studies only focus on simple
graphics. To evaluate the performance of VLMs in complex graphic reasoning, we
propose ReasonBench, the first evaluation benchmark focused on structured
graphic reasoning tasks, which includes 1,613 questions from real-world
intelligence tests. ReasonBench covers reasoning dimensions related to
location, attribute, quantity, and multi-element tasks, providing a
comprehensive evaluation of the performance of VLMs in spatial, relational, and
abstract reasoning capabilities. We benchmark 11 mainstream VLMs (including
closed-source and open-source models) and reveal significant limitations of
current models. Based on these findings, we propose a dual optimization
strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability
of reasoning by decomposing layers, and ReasonTune enhances the task
adaptability of model reasoning through training, all of which improves VLM
performance by 33.5\%. All experimental data and code are in the repository:
https://huggingface.co/datasets/cistine/ReasonBench.

</details>


### [13] [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/abs/2508.00324)
*Yeonjun In,Wonjoong Kim,Sangwu Park,Chanyoung Park*

Main category: cs.AI

TL;DR: R1-Act通过结构化推理触发模型的安全知识，有效提升大型推理模型安全性，训练效率高。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型(LRM)经常执行有害的用户指令，存在安全风险。研究发现模型已具备足够的安全性知识，但推理过程中未能激活。

Method: 提出R1-Act，一种通过结构化推理过程显式触发安全知识的简单高效的后训练方法。

Result: R1-Act在多个LRM主干和规模上都表现出鲁棒性、可扩展性和实用效率，优于现有方法。仅需1000个训练样本和90分钟的训练时间。

Conclusion: R1-Act方法显著提升了大型推理模型的安全性，同时保持了推理性能，且训练效率高。

Abstract: Although large reasoning models (LRMs) have demonstrated impressive
capabilities on complex tasks, recent studies reveal that these models
frequently fulfill harmful user instructions, raising significant safety
concerns. In this paper, we investigate the underlying cause of LRM safety
risks and find that models already possess sufficient safety knowledge but fail
to activate it during reasoning. Based on this insight, we propose R1-Act, a
simple and efficient post-training method that explicitly triggers safety
knowledge through a structured reasoning process. R1-Act achieves strong safety
improvements while preserving reasoning performance, outperforming prior
alignment methods. Notably, it requires only 1,000 training examples and 90
minutes of training on a single RTX A6000 GPU. Extensive experiments across
multiple LRM backbones and sizes demonstrate the robustness, scalability, and
practical efficiency of our approach.

</details>


### [14] [CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding](https://arxiv.org/abs/2508.00378)
*Shixin Yi,Lin Shang*

Main category: cs.AI

TL;DR: CoRGI框架通过视觉验证改进VLM推理，减少幻觉，提升准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的CoT prompting方法生成的解释虽然流畅，但缺乏视觉内容的支撑，容易出现幻觉。

Method: CoRGI是一个三阶段模块化框架，包括文本推理链生成、视觉证据提取和文本与视觉证据整合。它可以与现有的VLM集成，无需端到端重新训练。

Result: 在VCR基准测试中，CoRGI提高了两个开源VLM（Qwen-2.5VL和LLaVA-1.6）的推理性能。消融实验证明了验证模块中每个步骤的贡献，人工评估表明CoRGI生成的解释更准确、更有帮助。

Conclusion: CoRGI框架通过引入视觉验证机制，提高了视觉语言模型（VLM）的多步推理能力，减少了幻觉，并生成更准确、更有帮助的解释。

Abstract: Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in
vision-language models (VLMs), but it often produces explanations that are
linguistically fluent yet lack grounding in visual content. We observe that
such hallucinations arise in part from the absence of an explicit verification
mechanism during multi-step reasoning. To address this, we propose
\textbf{CoRGI}(\textbf{C}hain \textbf{o}f \textbf{R}easoning with
\textbf{G}rounded \textbf{I}nsights), a modular framework that introduces
visual verification into the reasoning process. CoRGI follows a three-stage
pipeline: it first generates a textual reasoning chain, then extracts
supporting visual evidence for each reasoning step via a dedicated module
(VEVM), and finally synthesizes the textual rationale with visual evidence to
generate a grounded, verified answer. The framework can be integrated with
existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR
benchmark and find that it improves reasoning performance on two representative
open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm
the contribution of each step in the verification module, and human evaluations
suggest that CoRGI leads to more factual and helpful explanations. We also
examine alternative designs for the visual verification step and discuss
potential limitations of post-hoc verification frameworks. These findings
highlight the importance of grounding intermediate reasoning steps in visual
evidence to enhance the robustness of multimodal reasoning.

</details>


### [15] [Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation](https://arxiv.org/abs/2508.00401)
*Riddhi J. Pitliya,Ozan Catal,Toon Van de Maele,Corrado Pezzato,Tim Verbelen*

Main category: cs.AI

TL;DR: 该研究提出一种基于主动推理和心智理论的多智能体合作方法，无需共享模型和显式通信，实验结果表明其在碰撞避免和觅食任务中优于无ToM智能体。


<details>
  <summary>Details</summary>
Motivation: 现有主动推理方法依赖于特定任务的共享生成模型或显式通信，缺乏通用性。该研究旨在构建一种无需共享模型和显式通信的通用多智能体合作方法。

Method: 在主动推理框架下，通过构建自身和他人信念与目标的独立表征，并扩展复杂的基于推理树的规划算法，系统地探索联合策略空间。

Result: ToM智能体能够更好地合作，避免碰撞并减少冗余工作，这通过仅仅根据可观察行为推断他人的信念来实现。

Conclusion: 该研究提出了一种新颖的多智能体合作方法，通过在主动推理中实现心智理论（ToM）来实现更好的合作，并在碰撞避免和觅食任务模拟中得到验证。

Abstract: We present a novel approach to multi-agent cooperation by implementing theory
of mind (ToM) within active inference. ToM - the ability to understand that
others can have differing knowledge and goals - enables agents to reason about
others' beliefs while planning their own actions. Unlike previous active
inference approaches to multi-agent cooperation, our method neither relies on
task-specific shared generative models nor requires explicit communication,
while being generalisable. In our framework, the ToM-equipped agent maintains
distinct representations of its own and others' beliefs and goals. We extend
the sophisticated inference tree-based planning algorithm to systematically
explore joint policy spaces through recursive reasoning. Our approach is
evaluated through collision avoidance and foraging task simulations. Results
demonstrate that ToM-equipped agents cooperate better compared to non-ToM
counterparts by being able to avoid collisions and reduce redundant efforts.
Crucially, ToM agents accomplish this by inferring others' beliefs solely from
observable behaviour. This work advances practical applications in artificial
intelligence while providing computational insights into ToM.

</details>


### [16] [Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training](https://arxiv.org/abs/2508.00414)
*Tianqing Fang,Zhisong Zhang,Xiaoyang Wang,Rui Wang,Can Qin,Yuxuan Wan,Jun-Yu Ma,Ce Zhang,Jiaqi Chen,Xiyun Li,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: 开源多模块智能体框架Cognitive Kernel-Pro，在GAIA基准测试中取得SOTA结果，提升了AI智能体的可访问性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前的智能体系统要么是闭源的，要么严重依赖各种付费API和专有工具，限制了研究界的可访问性和可重复性。

Method: 系统地研究了高级AI智能体的高质量训练数据，关注四个关键领域（网络、文件、代码和一般推理）中查询、轨迹和可验证答案的构建，并探索了智能体测试时反射和投票的新策略以增强智能体的稳健性和性能。

Result: Cognitive Kernel-Pro 在GAIA上取得了最先进的结果，其8B参数的开源模型超过了WebDancer和WebSailor等之前的领先系统。

Conclusion: Cognitive Kernel-Pro，一个完全开源且免费的多模块智能体框架，在GAIA上取得了最先进的结果，其8B参数的开源模型超过了WebDancer和WebSailor等之前的领先系统，为易于访问、高能力的AI智能体建立了新的性能标准。

Abstract: General AI Agents are increasingly recognized as foundational frameworks for
the next generation of artificial intelligence, enabling complex reasoning, web
interaction, coding, and autonomous research capabilities. However, current
agent systems are either closed-source or heavily reliant on a variety of paid
APIs and proprietary tools, limiting accessibility and reproducibility for the
research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a
fully open-source and (to the maximum extent) free multi-module agent framework
designed to democratize the development and evaluation of advanced AI agents.
Within Cognitive Kernel-Pro, we systematically investigate the curation of
high-quality training data for Agent Foundation Models, focusing on the
construction of queries, trajectories, and verifiable answers across four key
domains: web, file, code, and general reasoning. Furthermore, we explore novel
strategies for agent test-time reflection and voting to enhance agent
robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving
state-of-the-art results among open-source and free agents. Notably, our
8B-parameter open-source model surpasses previous leading systems such as
WebDancer and WebSailor, establishing a new performance standard for
accessible, high-capability AI agents. Code is available at
https://github.com/Tencent/CognitiveKernel-Pro

</details>
