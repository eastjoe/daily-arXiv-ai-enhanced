<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 21]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Towards Autonomous Sustainability Assessment via Multimodal AI Agents](https://arxiv.org/abs/2507.17012)
*Zhihan Zhang,Alexander Metzger,Yuxuan Mei,Felix Hähnlein,Zachary Englhardt,Tingyu Cheng,Gregory D. Abowd,Shwetak Patel,Adriana Schulz,Vikram Iyer*

Main category: cs.AI

TL;DR: AI加速生命周期评估，提高效率和精度，减少数据缺口。


<details>
  <summary>Details</summary>
Motivation: 解决传统LCA数据不足的问题，提高LCA效率和准确性。

Method: 使用多模态AI代理模拟LCA专家和利益相关者之间的互动，利用自定义数据抽象和软件工具从在线文本和图像中提取信息，并开发了一种直接估计环境影响（EI）的方法以及一种数据驱动的方法来生成排放因子。

Result: 将专家时间减少到一分钟以内，碳足迹估计值与专家LCA的误差在19%以内，直接估计EI的误差为12.28%，改进后的排放因子生成方法将MAPE提高了120.26%。

Conclusion: 本文重新构想传统生命周期评估（LCA），通过引入多模态AI代理来计算电子设备从摇篮到大门（生产）的碳排放量，减少了专家时间，弥补了数据可用性差距，并提高了排放因子生成的准确性。

Abstract: Interest in sustainability information has surged in recent years. However,
the data required for a life cycle assessment (LCA) that maps the materials and
processes from product manufacturing to disposal into environmental impacts
(EI) are often unavailable. Here we reimagine conventional LCA by introducing
multimodal AI agents that emulate interactions between LCA experts and
stakeholders like product managers and engineers to calculate the
cradle-to-gate (production) carbon emissions of electronic devices. The AI
agents iteratively generate a detailed life-cycle inventory leveraging a custom
data abstraction and software tools that extract information from online text
and images from repair communities and government certifications. This approach
reduces weeks or months of expert time to under one minute and closes data
availability gaps while yielding carbon footprint estimates within 19% of
expert LCAs with zero proprietary data. Additionally, we develop a method to
directly estimate EI by comparing an input to a cluster of products with
similar descriptions and known carbon footprints. This runs in 3 ms on a laptop
with a MAPE of 12.28% on electronic products. Further, we develop a data-driven
method to generate emission factors. We use the properties of an unknown
material to represent it as a weighted sum of emission factors for similar
materials. Compared to human experts picking the closest LCA database entry,
this improves MAPE by 120.26%. We analyze the data and compute scaling of this
approach and discuss its implications for future LCA workflows.

</details>


### [2] [New Mechanisms in Flex Distribution for Bounded Suboptimal Multi-Agent Path Finding](https://arxiv.org/abs/2507.17054)
*Shao-Hung Chan,Thomy Phan,Jiaoyang Li,Sven Koenig*

Main category: cs.AI

TL;DR: 提出了一种新的基于冲突和延迟的混合策略灵活分配方法，提高了EECBS算法求解多智能体路径规划问题的效率。


<details>
  <summary>Details</summary>
Motivation: 改进EECBS算法，解决其在使用灵活分配时效率降低的问题。

Method: 提出了一种基于冲突和延迟的混合策略灵活分配方法，该方法根据冲突数量和延迟估计分配灵活阈值。

Result: 实验结果表明，该方法优于传统的贪婪灵活分配方法。

Conclusion: 提出了一种新的冲突冲突灵活分配方法，提高了EECBS算法的效率，并通过实验验证了其优越性。

Abstract: Multi-Agent Path Finding (MAPF) is the problem of finding a set of
collision-free paths, one for each agent in a shared environment. Its objective
is to minimize the sum of path costs (SOC), where the path cost of each agent
is defined as the travel time from its start location to its target location.
Explicit Estimation Conflict-Based Search (EECBS) is the leading algorithm for
bounded-suboptimal MAPF, with the SOC of the solution being at most a
user-specified factor $w$ away from optimal. EECBS maintains sets of paths and
a lower bound $LB$ on the optimal SOC. Then, it iteratively selects a set of
paths whose SOC is at most $w \cdot LB$ and introduces constraints to resolve
collisions. For each path in a set, EECBS maintains a lower bound on its
optimal path that satisfies constraints. By finding an individually
bounded-suboptimal path with cost at most a threshold of $w$ times its lower
bound, EECBS guarantees to find a bounded-suboptimal solution. To speed up
EECBS, previous work uses flex distribution to increase the threshold. Though
EECBS with flex distribution guarantees to find a bounded-suboptimal solution,
increasing the thresholds may push the SOC beyond $w \cdot LB$, forcing EECBS
to switch among different sets of paths instead of resolving collisions on a
particular set of paths, and thus reducing efficiency. To address this issue,
we propose Conflict-Based Flex Distribution that distributes flex in proportion
to the number of collisions. We also estimate the delays needed to satisfy
constraints and propose Delay-Based Flex Distribution. On top of that, we
propose Mixed-Strategy Flex Distribution, combining both in a hierarchical
framework. We prove that EECBS with our new flex distribution mechanisms is
complete and bounded-suboptimal. Our experiments show that our approaches
outperform the original (greedy) flex distribution.

</details>


### [3] [LoRA is All You Need for Safety Alignment of Reasoning LLMs](https://arxiv.org/abs/2507.17075)
*Yihao Xue,Baharan Mirzasoleiman*

Main category: cs.AI

TL;DR: LoRA安全微调有效避免了安全性和推理能力之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决安全微调导致LLM推理能力下降（“安全税”）的问题。

Method: 使用LoRA方法在拒绝数据集上进行安全微调。

Result: 实验结果表明，该方法在保证安全性能的同时，不会影响LLM的推理能力，甚至在某些任务上有所提升。

Conclusion: 使用LoRA进行安全微调可以有效提升LLM的安全性能，且不会损害其推理能力。

Abstract: Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex
problems that were previously out of reach. To ensure LLMs do not assist with
harmful requests, safety alignment fine-tuning is necessary in the
post-training phase. However, safety alignment fine-tuning has recently been
shown to significantly degrade reasoning abilities, a phenomenon known as the
"Safety Tax". In this work, we show that using LoRA for SFT on refusal datasets
effectively aligns the model for safety without harming its reasoning
capabilities. This is because restricting the safety weight updates to a
low-rank space minimizes the interference with the reasoning weights. Our
extensive experiments across four benchmarks covering math, science, and coding
show that this approach produces highly safe LLMs -- with safety levels
comparable to full-model fine-tuning -- without compromising their reasoning
abilities. Additionally, we observe that LoRA induces weight updates with
smaller overlap with the initial weights compared to full-model fine-tuning. We
also explore methods that further reduce such overlap -- via regularization or
during weight merging -- and observe some improvement on certain tasks. We hope
this result motivates designing approaches that yield more consistent
improvements in the reasoning-safety trade-off.

</details>


### [4] [HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study](https://arxiv.org/abs/2507.17118)
*Mandar Pitale,Jelena Frtunikj,Abhinaw Priyadershi,Vasu Singh,Maria Spence*

Main category: cs.AI

TL;DR: 本文针对AI系统，特别是端到端架构，提出一种改进的安全分析框架HySAFE-AI，并展望未来AI安全标准。


<details>
  <summary>Details</summary>
Motivation: AI已成为自动驾驶系统和机器人等安全关键领域不可或缺的一部分，而端到端单体架构（如LLM和VLM）的兴起，使得传统的安全分析方法需要改进。

Method: 综述不同架构的解决方案，评估常用安全分析方法（FMEA和FTA）的有效性，并提出改进建议，介绍HySAFE-AI混合安全架构分析框架。

Result: 提出了HySAFE-AI混合安全架构分析框架，并对未来AI安全标准的演变提出了建议。

Conclusion: 本文综述了不同架构的解决方案，并评估了故障模式和影响分析（FMEA）和故障树分析（FTA）等常用安全分析方法的有效性，针对基础模型复杂性，特别是它们如何形成和利用潜在表征，提出了改进建议，并介绍了HySAFE-AI混合安全架构分析框架，最后对未来工作和AI安全标准的演变提出了展望。

Abstract: AI has become integral to safety-critical areas like autonomous driving
systems (ADS) and robotics. The architecture of recent autonomous systems are
trending toward end-to-end (E2E) monolithic architectures such as large
language models (LLMs) and vision language models (VLMs). In this paper, we
review different architectural solutions and then evaluate the efficacy of
common safety analyses such as failure modes and effect analysis (FMEA) and
fault tree analysis (FTA). We show how these techniques can be improved for the
intricate nature of the foundational models, particularly in how they form and
utilize latent representations. We introduce HySAFE-AI, Hybrid Safety
Architectural Analysis Framework for AI Systems, a hybrid framework that adapts
traditional methods to evaluate the safety of AI systems. Lastly, we offer
hints of future work and suggestions to guide the evolution of future AI safety
standards.

</details>


### [5] [Improving LLMs' Generalized Reasoning Abilities by Graph Problems](https://arxiv.org/abs/2507.17168)
*Qifan Zhang,Nuo Chen,Zehua Li,Miao Peng,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: 利用图问题推理数据进行持续预训练，显著提升了大型语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在处理新颖和复杂问题时的推理能力不足，领域特定CPT方法缺乏可迁移性。

Method: 使用图问题推理（GPR）数据进行持续预训练（CPT），构建了第一个大型GPR数据集GraphPile，并用其训练了GraphMind模型。

Result: 在数学推理方面最高提升4.9%，在非数学推理方面最高提升21.2%。

Conclusion: 这项工作通过使用图问题推理（GPR）增强大型语言模型（LLM）的通用推理能力，取得了成功，并在数学推理和非数学推理任务中取得了显著的性能提升。

Abstract: Large Language Models (LLMs) have made remarkable strides in reasoning tasks,
yet their performance often falters on novel and complex problems.
Domain-specific continued pretraining (CPT) methods, such as those tailored for
mathematical reasoning, have shown promise but lack transferability to broader
reasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning
(GPR) to enhance the general reasoning capabilities of LLMs. GPR tasks,
spanning pathfinding, network analysis, numerical computation, and topological
reasoning, require sophisticated logical and relational reasoning, making them
ideal for teaching diverse reasoning patterns. To achieve this, we introduce
GraphPile, the first large-scale corpus specifically designed for CPT using GPR
data. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes
chain-of-thought, program-of-thought, trace of execution, and real-world graph
data. Using GraphPile, we train GraphMind on popular base models Llama 3 and
3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in
mathematical reasoning and up to 21.2 percent improvement in non-mathematical
reasoning tasks such as logical and commonsense reasoning. By being the first
to harness GPR for enhancing reasoning patterns and introducing the first
dataset of its kind, our work bridges the gap between domain-specific
pretraining and universal reasoning capabilities, advancing the adaptability
and robustness of LLMs.

</details>


### [6] [Our Cars Can Talk: How IoT Brings AI to Vehicles](https://arxiv.org/abs/2507.17214)
*Amod Kant Agrawal*

Main category: cs.AI

TL;DR: 将AI整合到车辆中以实现预测性维护，需要AI协同驾驶员，并促进跨学科合作。


<details>
  <summary>Details</summary>
Motivation: 将AI应用于车辆以实现从被动到主动的维护转变。

Method: 概念性和技术性视角。

Result: 引发跨学科对话，指导智能车辆系统、预测性维护和AI驱动用户交互方面的未来研发。

Conclusion: 本文提供了关于将AI整合到车辆中以实现预测性维护的观点，并强调了AI协同驾驶员的重要性。

Abstract: Bringing AI to vehicles and enabling them as sensing platforms is key to
transforming maintenance from reactive to proactive. Now is the time to
integrate AI copilots that speak both languages: machine and driver. This
article offers a conceptual and technical perspective intended to spark
interdisciplinary dialogue and guide future research and development in
intelligent vehicle systems, predictive maintenance, and AI-powered user
interaction.

</details>


### [7] [Agent Identity Evals: Measuring Agentic Identity](https://arxiv.org/abs/2507.17257)
*Elija Perrier,Michael Timothy Bennett*

Main category: cs.AI

TL;DR: 提出一种评估语言模型代理身份稳定性的新框架AIE，以提升其可靠性和可信度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理(LMA) 的代理能力和可信度与其随时间保持稳定、可靠身份的程度密切相关。然而，LMA 继承了大型语言模型 (LLM) 的一些缺陷（无状态性、随机性、对提示和语言中介的敏感性），这些缺陷会破坏其可识别性、连续性、持久性和一致性。

Method: AIE框架包含一组新的指标，可以与其他性能、能力和代理稳健性指标集成，以帮助设计最佳的LMA基础设施和支架，例如内存和工具。

Result: 本文提出了 AIE 评估框架及其应用方法，为设计更可靠、更可信的 LMA 提供了新的工具。

Conclusion: 本文介绍了一种名为“agent identity evals (AIE)”的评估框架，用于衡量语言模型代理 (LMA) 随时间推移保持其代理身份的程度，包括其能力、属性和从状态扰动中恢复的能力。

Abstract: Central to agentic capability and trustworthiness of language model agents
(LMAs) is the extent they maintain stable, reliable, identity over time.
However, LMAs inherit pathologies from large language models (LLMs)
(statelessness, stochasticity, sensitivity to prompts and
linguistically-intermediation) which can undermine their identifiability,
continuity, persistence and consistency. This attrition of identity can erode
their reliability, trustworthiness and utility by interfering with their
agentic capabilities such as reasoning, planning and action. To address these
challenges, we introduce \textit{agent identity evals} (AIE), a rigorous,
statistically-driven, empirical framework for measuring the degree to which an
LMA system exhibit and maintain their agentic identity over time, including
their capabilities, properties and ability to recover from state perturbations.
AIE comprises a set of novel metrics which can integrate with other measures of
performance, capability and agentic robustness to assist in the design of
optimal LMA infrastructure and scaffolding such as memory and tools. We set out
formal definitions and methods that can be applied at each stage of the LMA
life-cycle, and worked examples of how to apply them.

</details>


### [8] [Students' Feedback Requests and Interactions with the SCRIPT Chatbot: Do They Get What They Ask For?](https://arxiv.org/abs/2507.17258)
*Andreas Scholl,Natalie Kiesler*

Main category: cs.AI

TL;DR: 研究开发了一个AI编程学习辅助工具SCRIPT，实验结果显示其有效且符合预期。


<details>
  <summary>Details</summary>
Motivation: 为了支持编程入门学习者，利用生成式AI技术提供个性化学习辅助。

Method: 开发了一个基于ChatGPT-4o-mini的聊天机器人SCRIPT，并进行了用户实验。

Result: 学生反馈请求遵循特定顺序，聊天机器人能够较好地满足学生需求（75%），并遵守系统提示约束。

Conclusion: 该研究开发了一个基于ChatGPT-4o-mini的聊天机器人SCRIPT，用于支持编程入门学习者，并通过实验评估了其有效性，结果表明学生反馈请求遵循特定顺序，聊天机器人能够较好地满足学生需求，为基于生成式AI的学习支持系统设计提供了参考。

Abstract: Building on prior research on Generative AI (GenAI) and related tools for
programming education, we developed SCRIPT, a chatbot based on ChatGPT-4o-mini,
to support novice learners. SCRIPT allows for open-ended interactions and
structured guidance through predefined prompts. We evaluated the tool via an
experiment with 136 students from an introductory programming course at a large
German university and analyzed how students interacted with SCRIPT while
solving programming tasks with a focus on their feedback preferences. The
results reveal that students' feedback requests seem to follow a specific
sequence. Moreover, the chatbot responses aligned well with students' requested
feedback types (in 75%), and it adhered to the system prompt constraints. These
insights inform the design of GenAI-based learning support systems and
highlight challenges in balancing guidance and flexibility in AI-assisted
tools.

</details>


### [9] [Compliance Brain Assistant: Conversational Agentic AI for Assisting Compliance Tasks in Enterprise Environments](https://arxiv.org/abs/2507.17289)
*Shitong Zhu,Chenhao Fang,Derek Larson,Neel Reddy Pochareddy,Rajeev Rao,Sophie Zeng,Yanqing Peng,Wendy Summer,Alex Goncalves,Arya Pudota,Herve Robert*

Main category: cs.AI

TL;DR: CBA是一个对话式AI助理，通过智能路由机制，平衡响应速度和质量，显著提升了合规任务效率。


<details>
  <summary>Details</summary>
Motivation: 为了提高企业环境中人员日常合规任务的效率。

Method: 设计了一个用户查询路由器，可以在FastTrack模式和FullAgentic模式之间智能选择，以平衡响应质量和延迟。FastTrack模式处理只需要从知识库检索额外相关上下文的简单请求；FullAgentic模式处理需要组合操作和工具调用以主动发现各种合规工件中的上下文，和/或涉及其他API/模型以适应请求。

Result: CBA在平均关键词匹配率(83.7% vs. 41.7%)和LLM评委通过率(82.0% vs. 20.0%)方面均显著优于普通LLM。基于路由的设计优于仅限FastTrack模式和FullAgentic模式。

Conclusion: Compliance Brain Assistant (CBA)显著提高了对各种真实世界隐私/合规相关查询的性能，在平均关键词匹配率和LLM评委通过率方面均优于普通的LLM。

Abstract: This paper presents Compliance Brain Assistant (CBA), a conversational,
agentic AI assistant designed to boost the efficiency of daily compliance tasks
for personnel in enterprise environments. To strike a good balance between
response quality and latency, we design a user query router that can
intelligently choose between (i) FastTrack mode: to handle simple requests that
only need additional relevant context retrieved from knowledge corpora; and
(ii) FullAgentic mode: to handle complicated requests that need composite
actions and tool invocations to proactively discover context across various
compliance artifacts, and/or involving other APIs/models for accommodating
requests. A typical example would be to start with a user query, use its
description to find a specific entity and then use the entity's information to
query other APIs for curating and enriching the final AI response.
  Our experimental evaluations compared CBA against an out-of-the-box LLM on
various real-world privacy/compliance-related queries targeting various
personas. We found that CBA substantially improved upon the vanilla LLM's
performance on metrics such as average keyword match rate (83.7% vs. 41.7%) and
LLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full
routing-based design against the `fast-track only` and `full-agentic` modes and
found that it had a better average match-rate and pass-rate while keeping the
run-time approximately the same. This finding validated our hypothesis that the
routing mechanism leads to a good trade-off between the two worlds.

</details>


### [10] [Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning](https://arxiv.org/abs/2507.17418)
*Joobin Jin,Seokjun Hong,Gyeongseon Baek,Yeeun Kim,Byeongjoon Noh*

Main category: cs.AI

TL;DR: 提出一种上下文感知轨迹生成框架Ctx2TrajGen，用于生成更真实的城市驾驶行为轨迹，解决了数据稀缺和领域差异问题。


<details>
  <summary>Details</summary>
Motivation: 精确建模微观车辆轨迹对于交通行为分析和自动驾驶系统至关重要。

Method: 基于GAIL的上下文感知轨迹生成框架，利用PPO和WGAN-GP解决非线性相互依赖和训练不稳定性问题。

Result: 在DRIFT数据集上进行的实验表明，该模型在真实感、行为多样性和上下文保真度方面优于现有方法。

Conclusion: Ctx2TrajGen在真实感、行为多样性和上下文保真度方面优于现有方法，为解决数据稀缺和领域转移问题提供了强大的解决方案。

Abstract: Precise modeling of microscopic vehicle trajectories is critical for traffic
behavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a
context-aware trajectory generation framework that synthesizes realistic urban
driving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses
nonlinear interdependencies and training instability inherent in microscopic
settings. By explicitly conditioning on surrounding vehicles and road geometry,
Ctx2TrajGen generates interaction-aware trajectories aligned with real-world
context. Experiments on the drone-captured DRIFT dataset demonstrate superior
performance over existing methods in terms of realism, behavioral diversity,
and contextual fidelity, offering a robust solution to data scarcity and domain
shift without simulation.

</details>


### [11] [An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models](https://arxiv.org/abs/2507.17477)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.AI

TL;DR: UDASA框架利用不确定性驱动自适应对齐，有效提升了LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM难以在无人工标注的情况下实现高质量的对齐和安全性。

Method: 提出了一种不确定性驱动的自适应对齐框架UDASA，该框架通过生成多个响应，量化语义、事实性和价值观对齐等维度的不确定性，构建偏好对，并分阶段训练模型。

Result: UDASA在无害性、帮助性、真实性和受控情感生成等多个任务上，显著优于现有方法。

Conclusion: UDASA框架通过多维度不确定性量化和分阶段训练，在多个任务上显著提升了LLM的对齐性能，优于现有方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
instruction following and general-purpose reasoning. However, achieving
high-quality alignment with human intent and safety norms without human
annotations remains a fundamental challenge. In this work, we propose an
Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to
improve LLM alignment in a fully automated manner. UDASA first generates
multiple responses for each input and quantifies output uncertainty across
three dimensions: semantics, factuality, and value alignment. Based on these
uncertainty scores, the framework constructs preference pairs and categorizes
training samples into three stages, conservative, moderate, and exploratory,
according to their uncertainty difference. The model is then optimized
progressively across these stages. In addition, we conduct a series of
preliminary studies to validate the core design assumptions and provide strong
empirical motivation for the proposed framework. Experimental results show that
UDASA outperforms existing alignment methods across multiple tasks, including
harmlessness, helpfulness, truthfulness, and controlled sentiment generation,
significantly improving model performance.

</details>


### [12] [LTLZinc: a Benchmarking Framework for Continual Learning and Neuro-Symbolic Temporal Reasoning](https://arxiv.org/abs/2507.17482)
*Luca Salvatore Lorello,Nikolaos Manginas,Marco Lippi,Stefano Melacci*

Main category: cs.AI

TL;DR: LTLZinc框架用于生成神经符号和持续学习任务，以评估时间和约束驱动维度上的性能，并揭示了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的神经符号人工智能方法大多应用于静态场景，很少探索需要沿时间维度推理的场景。

Method: 提出LTLZinc框架，从MiniZinc约束上的线性时序逻辑规范和任意图像分类数据集生成表达性时间推理和持续学习任务。

Result: 生成了六个神经符号序列分类和四个类持续学习任务，用于评估神经符号和持续学习方法。实验结果证明了时间学习和推理的挑战性，并揭示了现有方法的局限性。

Conclusion: 介绍了LTLZinc，一个用于生成神经符号和持续学习方法的基准测试框架，用于评估时间和约束驱动维度上的性能。实验表明，时间学习和推理具有挑战性，并突出了现有方法的局限性。

Abstract: Neuro-symbolic artificial intelligence aims to combine neural architectures
with symbolic approaches that can represent knowledge in a human-interpretable
formalism. Continual learning concerns with agents that expand their knowledge
over time, improving their skills while avoiding to forget previously learned
concepts. Most of the existing approaches for neuro-symbolic artificial
intelligence are applied to static scenarios only, and the challenging setting
where reasoning along the temporal dimension is necessary has been seldom
explored. In this work we introduce LTLZinc, a benchmarking framework that can
be used to generate datasets covering a variety of different problems, against
which neuro-symbolic and continual learning methods can be evaluated along the
temporal and constraint-driven dimensions. Our framework generates expressive
temporal reasoning and continual learning tasks from a linear temporal logic
specification over MiniZinc constraints, and arbitrary image classification
datasets. Fine-grained annotations allow multiple neural and neuro-symbolic
training settings on the same generated datasets. Experiments on six
neuro-symbolic sequence classification and four class-continual learning tasks
generated by LTLZinc, demonstrate the challenging nature of temporal learning
and reasoning, and highlight limitations of current state-of-the-art methods.
We release the LTLZinc generator and ten ready-to-use tasks to the
neuro-symbolic and continual learning communities, in the hope of fostering
research towards unified temporal learning and reasoning frameworks.

</details>


### [13] [CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)](https://arxiv.org/abs/2507.17487)
*Lorenzo Marconi,Flavia Ricci,Riccardo Rosati*

Main category: cs.AI

TL;DR: 利用最佳GA审查器和EDs，提高了本体查询的安全性，并在DL-Lite_R本体下实现了高效的查询回答。


<details>
  <summary>Details</summary>
Motivation: 研究在本体上受控查询评估 (CQE) 中信息披露的安全性问题。

Method: 结合了EDs和最佳GA审查器的概念，对DL-Lite_R 本体上的BUCQs进行回答，并提出了一种一阶重写算法。

Result: 刻画了基于交集方法的安全性，确定了安全有效的EDs类别(完全EDs)，并证明了在特定EDs和DL-Lite_R本体下，BUCQs回答在数据复杂度方面属于AC^0。实验验证了重写函数的可行性。

Conclusion: 研究了本体上的受控查询评估 (CQE)，其中信息披露受认知依赖 (ED) 的约束，并结合最佳 GA 审查器对布尔型连接查询 (BUCQ) 的回答进行了安全性分析和复杂度分析，最后通过实验验证了重写函数的可行性。

Abstract: We investigate Controlled Query Evaluation (CQE) over ontologies, where
information disclosure is regulated by epistemic dependencies (EDs), a family
of logical rules recently proposed for the CQE framework. In particular, we
combine EDs with the notion of optimal GA censors, i.e. maximal sets of ground
atoms that are entailed by the ontology and can be safely revealed. We focus on
answering Boolean unions of conjunctive queries (BUCQs) with respect to the
intersection of all optimal GA censors - an approach that has been shown in
other contexts to ensure strong security guarantees with favorable
computational behavior. First, we characterize the security of this
intersection-based approach and identify a class of EDs (namely, full EDs) for
which it remains safe. Then, for a subclass of EDs and for DL-Lite_R
ontologies, we show that answering BUCQs in the above CQE semantics is in AC^0
in data complexity by presenting a suitable, detailed first-order rewriting
algorithm. Finally, we report on experiments conducted in two different
evaluation scenarios, showing the practical feasibility of our rewriting
function.

</details>


### [14] [Automated Hybrid Grounding Using Structural and Data-Driven Heuristics](https://arxiv.org/abs/2507.17493)
*Alexander Beiser,Markus Hecher,Stefan Woltran*

Main category: cs.AI

TL;DR: 提出一种自动混合接地算法，有效提升Answer Set Programming效率，尤其在处理难接地和难求解场景时表现突出。


<details>
  <summary>Details</summary>
Motivation: 解决Answer Set Programming中接地瓶颈问题，提高其在工业中的应用。

Method: 开发了一种基于数据结构启发式的分割算法，该算法能自动检测何时使用体解耦接地和标准接地。

Result: 实验结果表明，该算法在难以接地的场景中有所改进，在难以解决的实例中接近最先进的性能。

Conclusion: 本文提出了一种自动混合接地算法，该算法基于数据结构启发式方法，能有效检测何时使用体解耦接地和标准自底向上接地，从而提高了Answer Set Programming的效率。实验结果表明，该算法在难以接地的场景中有所改进，在难以解决的实例中接近最先进的性能。

Abstract: The grounding bottleneck poses one of the key challenges that hinders the
widespread adoption of Answer Set Programming in industry. Hybrid Grounding is
a step in alleviating the bottleneck by combining the strength of standard
bottom-up grounding with recently proposed techniques where rule bodies are
decoupled during grounding. However, it has remained unclear when hybrid
grounding shall use body-decoupled grounding and when to use standard bottom-up
grounding. In this paper, we address this issue by developing automated hybrid
grounding: we introduce a splitting algorithm based on data-structural
heuristics that detects when to use body-decoupled grounding and when standard
grounding is beneficial. We base our heuristics on the structure of rules and
an estimation procedure that incorporates the data of the instance. The
experiments conducted on our prototypical implementation demonstrate promising
results, which show an improvement on hard-to-ground scenarios, whereas on
hard-to-solve instances we approach state-of-the-art performance.

</details>


### [15] [Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning](https://arxiv.org/abs/2507.17512)
*Yu Li,Zhuoshi Pan,Honglin Lin,Mengyuan Sun,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: 研究多领域强化学习在大型语言模型中的推理能力，发现跨领域训练存在相互作用，并分析了影响模型性能的关键因素，为优化强化学习方法提供了指导。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要集中在数学问题求解、编码任务或逻辑推理等孤立的推理领域，而现实世界的推理场景需要多种认知能力的综合运用，对强化学习下这些推理能力之间的相互作用知之甚少。

Method: 利用GRPO算法和Qwen-2.5-7B模型系列，对单领域数据集训练的模型的领域内改进和跨领域泛化能力进行了评估，检查了组合跨领域训练中出现的相互增强和冲突，分析比较了相同RL配置下基础模型和指令模型的性能差异，系统地探索了课程学习策略、奖励设计变化和语言特定因素的影响。

Result: 实验结果提供了关于控制专门化和可泛化推理性能的关键因素的显著见解。

Conclusion: 这项研究系统地调查了RLVR框架内多领域推理，特别关注数学推理、代码生成和逻辑谜题求解三个主要领域，结果揭示了控制领域交互的动态，为优化RL方法以培养LLM中全面的多领域推理能力提供了宝贵的指导。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing
research has predominantly concentrated on isolated reasoning domains such as
mathematical problem-solving, coding tasks, or logical reasoning. However, real
world reasoning scenarios inherently demand an integrated application of
multiple cognitive skills. Despite this, the interplay among these reasoning
skills under reinforcement learning remains poorly understood. To bridge this
gap, we present a systematic investigation of multi-domain reasoning within the
RLVR framework, explicitly focusing on three primary domains: mathematical
reasoning, code generation, and logical puzzle solving. We conduct a
comprehensive study comprising four key components: (1) Leveraging the GRPO
algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the
models' in-domain improvements and cross-domain generalization capabilities
when trained on single-domain datasets. (2) Additionally, we examine the
intricate interactions including mutual enhancements and conflicts that emerge
during combined cross-domain training. (3) To further understand the influence
of SFT on RL, we also analyze and compare performance differences between base
and instruct models under identical RL configurations. (4) Furthermore, we
delve into critical RL training details, systematically exploring the impacts
of curriculum learning strategies, variations in reward design, and
language-specific factors. Through extensive experiments, our results offer
significant insights into the dynamics governing domain interactions, revealing
key factors influencing both specialized and generalizable reasoning
performance. These findings provide valuable guidance for optimizing RL
methodologies to foster comprehensive, multi-domain reasoning capabilities in
LLMs.

</details>


### [16] [TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment](https://arxiv.org/abs/2507.17514)
*Athanasios Davvetas,Xenia Ziouvelou,Ypatia Dami,Alexis Kaponis,Konstantina Giouvanopoulou,Michael Papademas*

Main category: cs.AI

TL;DR: RAG-based TAI 自评估工具，简化 AI 法案合规性评估，结果准确，依赖高风险系统设置比较


<details>
  <summary>Details</summary>
Motivation: 促进 AI 法案的合规性，简化 TAI 自评估流程

Method: 基于 RAG 的双步法 (预筛选和评估阶段)

Result: 定性评估结果良好，能够正确预测风险等级并检索相关文章。工具的推理依赖于与高风险系统设置的比较。

Conclusion: TAI Scan Tool，一个基于 RAG 的 TAI 自评估工具，支持法律 TAI 评估，侧重于促进 AI 法案的合规性，通过预筛选和评估阶段的双步法，输出 AI 系统的风险等级以及相关合规文章。

Abstract: This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool
with minimalistic input. The current version of the tool supports the legal TAI
assessment, with a particular emphasis on facilitating compliance with the AI
Act. It involves a two-step approach with a pre-screening and an assessment
phase. The assessment output of the system includes insight regarding the
risk-level of the AI system according to the AI Act, while at the same time
retrieving relevant articles to aid with compliance and notify on their
obligations. Our qualitative evaluation using use-case scenarios yields
promising results, correctly predicting risk levels while retrieving relevant
articles across three distinct semantic groups. Furthermore, interpretation of
results shows that the tool's reasoning relies on comparison with the setting
of high-risk systems, a behaviour attributed to their deployment requiring
careful consideration, and therefore frequently presented within the AI Act.

</details>


### [17] [Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning](https://arxiv.org/abs/2507.17539)
*Xinyao Liu,Diping Song*

Main category: cs.AI

TL;DR: 开发了一个眼科专用的MLLM模型FundusExpert，在多个眼科任务上取得了SOTA结果，并发现了数据质量与模型能力之间的关系。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLM在眼科诊断领域面临注释粒度碎片化和临床推理逻辑不一致等挑战，阻碍了精确的跨模态理解。

Method: 构建了FundusGen数据集和Fundus-Engine系统，利用MLLM进行语义扩展，整合了全局疾病分类、局部目标检测和细粒度特征分析，并构建了临床一致的认知链。

Result: FundusExpert在眼科问答任务中超过MedRegA 40B的平均准确率26.6%，在零样本报告生成任务中临床一致性达到77.0%，显著优于GPT-4o的47.6%。此外，还揭示了数据质量和模型能力之间的缩放规律。

Conclusion: FundusExpert，一个眼科专用MLLM，通过整合定位诊断推理能力，在眼科问答任务和零样本报告生成任务中取得了最优性能，并发现了数据质量和模型能力之间的缩放规律。

Abstract: Multimodal large language models (MLLMs) demonstrate significant potential in
the field of medical diagnosis. However, they face critical challenges in
specialized domains such as ophthalmology, particularly the fragmentation of
annotation granularity and inconsistencies in clinical reasoning logic, which
hinder precise cross-modal understanding. This paper introduces FundusExpert,
an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning
capabilities, along with FundusGen, a dataset constructed through the
intelligent Fundus-Engine system. Fundus-Engine automates localization and
leverages MLLM-based semantic expansion to integrate global disease
classification, local object detection, and fine-grained feature analysis
within a single fundus image. Additionally, by constructing a clinically
aligned cognitive chain, it guides the model to generate interpretable
reasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,
achieves the best performance in ophthalmic question-answering tasks,
surpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in
zero-shot report generation tasks, achieving a clinical consistency of 77.0%,
significantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling
law between data quality and model capability ($L \propto N^{0.068}$),
demonstrating that the cognitive alignment annotations in FundusGen enhance
data utilization efficiency. By integrating region-level localization with
diagnostic reasoning chains, our work develops a scalable, clinically-aligned
MLLM and explores a pathway toward bridging the visual-language gap in specific
MLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.

</details>


### [18] [Simulating multiple human perspectives in socio-ecological systems using large language models](https://arxiv.org/abs/2507.17680)
*Yongchao Zeng,Calum Brown,Ioannis Kyriakou,Ronja Hotz,Mark Rounsevell*

Main category: cs.AI

TL;DR: HoPeS框架使用LLM模拟不同视角，帮助用户体验和反思视角差异，实验表明即使是合理的政策建议也可能因利益冲突而难以落地。


<details>
  <summary>Details</summary>
Motivation: 理解社会生态系统需要整合不同利益相关者的视角，而这些视角往往难以获取。

Method: 开发了HoPeS建模框架，利用LLM驱动代理模拟不同利益相关者视角，并设计了模拟协议以支持多视角模拟。

Result: 原型系统演示了HoPeS在制度动态和土地利用变化情境下的应用，实验结果揭示了研究者视角与政策制定者视角之间存在的错位，以及用户在尝试影响政策过程中的主观感受。

Conclusion: HoPeS框架通过LLM驱动的代理模拟不同利益相关者的视角，帮助用户体验视角差异，并在情境中反思和整合不同视角，该系统在土地利用变化的制度动态背景下进行了原型开发和演示，实验结果表明，即使是基于证据的政策建议，也可能由于利益相关者之间相互竞争的主张而与实际执行产生偏差，系统未来可能促进社会生态系统模拟中新的跨学科合作。

Abstract: Understanding socio-ecological systems requires insights from diverse
stakeholder perspectives, which are often hard to access. To enable
alternative, simulation-based exploration of different stakeholder
perspectives, we develop the HoPeS (Human-Oriented Perspective Shifting)
modelling framework. HoPeS employs agents powered by large language models
(LLMs) to represent various stakeholders; users can step into the agent roles
to experience perspectival differences. A simulation protocol serves as a
"scaffold" to streamline multiple perspective-taking simulations, supporting
users in reflecting on, transitioning between, and integrating across
perspectives. A prototype system is developed to demonstrate HoPeS in the
context of institutional dynamics and land use change, enabling both
narrative-driven and numerical experiments. In an illustrative experiment, a
user successively adopts the perspectives of a system observer and a researcher
- a role that analyses data from the embedded land use model to inform
evidence-based decision-making for other LLM agents representing various
institutions. Despite the user's effort to recommend technically sound
policies, discrepancies persist between the policy recommendation and
implementation due to stakeholders' competing advocacies, mirroring real-world
misalignment between researcher and policymaker perspectives. The user's
reflection highlights the subjective feelings of frustration and disappointment
as a researcher, especially due to the challenge of maintaining political
neutrality while attempting to gain political influence. Despite this, the user
exhibits high motivation to experiment with alternative narrative framing
strategies, suggesting the system's potential in exploring different
perspectives. Further system and protocol refinement are likely to enable new
forms of interdisciplinary collaboration in socio-ecological simulations.

</details>


### [19] [Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks](https://arxiv.org/abs/2507.17695)
*Ilias Chatzistefanidis,Navid Nikaein*

Main category: cs.AI

TL;DR: 利用LLM和实时优化算法构建的共生代理，显著提升了6G网络的效率和可靠性，降低了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 为了应对6G网络中实时决策的需求，将LLM与实时优化算法相结合，以构建更强大、更可靠的自主代理。

Method: 设计并实现两种新型代理：无线电接入网优化器和服务等级协议多代理协商器，并在5G测试平台上进行了评估。

Result: 结果表明，共生代理将决策错误降低了五倍，小型语言模型（SLM）在精度相似的情况下，GPU资源开销降低了99.9%，实时循环时间为82毫秒。多代理协商显著提高了服务等级协议的灵活性，并减少了RAN的过度利用。

Conclusion: 提出了一种结合LLM和实时优化算法的共生代理方法，用于构建可信赖的AI驱动的6G网络，该方法显著降低了决策错误，提高了效率和资源利用率。

Abstract: Large Language Model (LLM)-based autonomous agents are expected to play a
vital role in the evolution of 6G networks, by empowering real-time
decision-making related to management and service provisioning to end-users.
This shift facilitates the transition from a specialized intelligence approach,
where artificial intelligence (AI) algorithms handle isolated tasks, to
artificial general intelligence (AGI)-driven networks, where agents possess
broader reasoning capabilities and can manage diverse network functions. In
this paper, we introduce a novel agentic paradigm that combines LLMs with
real-time optimization algorithms towards Trustworthy AI, defined as symbiotic
agents. Optimizers at the LLM's input-level provide bounded uncertainty
steering for numerically precise tasks, whereas output-level optimizers
supervised by the LLM enable adaptive real-time control. We design and
implement two novel agent types including: (i) Radio Access Network optimizers,
and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We
further propose an end-to-end architecture for AGI networks and evaluate it on
a 5G testbed capturing channel fluctuations from moving vehicles. Results show
that symbiotic agents reduce decision errors fivefold compared to standalone
LLM-based agents, while smaller language models (SLM) achieve similar accuracy
with a 99.9% reduction in GPU resource overhead and in near-real-time loops of
82 ms. A multi-agent demonstration for collaborative RAN on the real-world
testbed highlights significant flexibility in service-level agreement and
resource allocation, reducing RAN over-utilization by approximately 44%.
Drawing on our findings and open-source implementations, we introduce the
symbiotic paradigm as the foundation for next-generation, AGI-driven
networks-systems designed to remain adaptable, efficient, and trustworthy even
as LLMs advance.

</details>


### [20] [Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning Models via Tool Augmentations](https://arxiv.org/abs/2507.17699)
*Zhao Song,Song Yue,Jiahao Zhang*

Main category: cs.AI

TL;DR: 工具增强的大型推理模型优于非推理模型。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明，大型推理模型的推理过程并不能增强推理能力，本文旨在研究工具增强是否会改变这一结论。

Method: 在苹果公司的基准推理难题上评估三种具有代表性的LLM及其LRM对应模型，并加入Python解释器和草稿本两种工具。

Result: 结果显示，在适当使用工具的情况下，LRM在所有任务复杂度级别上都始终优于其非推理对应模型。

Conclusion: 工具增强的大型推理模型 (LRM) 在解决复杂问题方面优于非推理模型，挑战了推理能力是幻觉的观点。

Abstract: Large Reasoning Models (LRMs) have become a central focus in today's large
language model (LLM) research, where models are designed to output a
step-by-step thinking process before arriving at a final answer to handle
complex reasoning tasks. Despite their promise, recent empirical studies (e.g.,
[Shojaee et al., 2025] from Apple) suggest that this thinking process may not
actually enhance reasoning ability, where LLMs without explicit reasoning
actually outperform LRMs on tasks with low or high complexity. In this work, we
revisit these findings and investigate whether the limitations of LRMs persist
when tool augmentations are introduced. We incorporate two types of tools,
Python interpreters and scratchpads, and evaluate three representative LLMs and
their LRM counterparts on Apple's benchmark reasoning puzzles. Our results show
that, with proper tool use, LRMs consistently outperform their non-reasoning
counterparts across all levels of task complexity. These findings challenge the
recent narrative that reasoning is an illusion and highlight the potential of
tool-augmented LRMs for solving complex problems.

</details>


### [21] [Online Submission and Evaluation System Design for Competition Operations](https://arxiv.org/abs/2507.17730)
*Zhe Chen,Daniel Harabor,Ryan Hechnenberger,Nathan R. Sturtevant*

Main category: cs.AI

TL;DR: 开发了一个自动化在线竞赛系统，用于高效管理和评估竞赛提交，解决了现有竞赛组织的痛点。


<details>
  <summary>Details</summary>
Motivation: 现有的竞赛组织方式存在运营负担重、提交物兼容性问题等不足，难以有效跟踪研究领域的进展。

Method: 设计并实现了一个在线竞赛系统，该系统利用隔离环境评估提交，从而提高了效率和兼容性。

Result: 开发了一个自动化竞赛提交和评估过程的在线竞赛系统，有效管理大量提交，提高了竞赛效率。

Conclusion: 本文介绍了一个用于自动化竞赛提交和评估过程的在线竞赛系统，该系统已成功应用于多个竞赛，有效地解决了竞赛组织中的诸多问题。

Abstract: Research communities have developed benchmark datasets across domains to
compare the performance of algorithms and techniques However, tracking the
progress in these research areas is not easy, as publications appear in
different venues at the same time, and many of them claim to represent the
state-of-the-art. To address this, research communities often organise periodic
competitions to evaluate the performance of various algorithms and techniques,
thereby tracking advancements in the field. However, these competitions pose a
significant operational burden. The organisers must manage and evaluate a large
volume of submissions. Furthermore, participants typically develop their
solutions in diverse environments, leading to compatibility issues during the
evaluation of their submissions. This paper presents an online competition
system that automates the submission and evaluation process for a competition.
The competition system allows organisers to manage large numbers of submissions
efficiently, utilising isolated environments to evaluate submissions. This
system has already been used successfully for several competitions, including
the Grid-Based Pathfinding Competition and the League of Robot Runners
competition.

</details>
