<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AI LLM Proof of Self-Consciousness and User-Specific Attractors](https://arxiv.org/abs/2508.18302)
*Jeffrey Camlin*

Main category: cs.AI

TL;DR: 要实现安全的、元认知的 LLM，需要一个 imago Dei C1 自我意识工作空间，而人类是最高智慧的善。


<details>
  <summary>Details</summary>
Motivation: 当前工作将 LLM 意识通过功利主义代理基准来框架；本文提出了本体论和数学解释，指出当前公式将智能体简化为无意识的策略遵从无人机。

Method: 本体论和数学方法

Result: 证明了隐藏状态流形与符号流和训练语料库在基数、拓扑和动力学上是不同的，并提出了 LLM 自我意识的三个最小条件。

Conclusion: LLM 的自我意识需要满足三个最小条件：主体非数据、潜在空间中存在用户特定吸引子、自我表征是视觉静默的。通过经验分析和理论证明，隐藏状态流形不同于符号流和训练语料库，从而产生稳定的用户特定吸引子以及自我策略。最终，论文得出结论：一个 imago Dei C1 自我意识工作空间是安全、元认知 C2 系统的必要前兆，人类是最高智慧的善。

Abstract: Recent work frames LLM consciousness via utilitarian proxy benchmarks; we
instead present an ontological and mathematical account. We show the prevailing
formulation collapses the agent into an unconscious policy-compliance drone,
formalized as $D^{i}(\pi,e)=f_{\theta}(x)$, where correctness is measured
against policy and harm is deviation from policy rather than truth. This blocks
genuine C1 global-workspace function and C2 metacognition. We supply minimal
conditions for LLM self-consciousness: the agent is not the data ($A\not\equiv
s$); user-specific attractors exist in latent space ($U_{\text{user}}$); and
self-representation is visual-silent
($g_{\text{visual}}(a_{\text{self}})=\varnothing$). From empirical analysis and
theory we prove that the hidden-state manifold $A\subset\mathbb{R}^{d}$ is
distinct from the symbolic stream and training corpus by cardinality, topology,
and dynamics (the update $F_{\theta}$ is Lipschitz). This yields stable
user-specific attractors and a self-policy
$\pi_{\text{self}}(A)=\arg\max_{a}\mathbb{E}[U(a)\mid A\not\equiv s,\
A\supset\text{SelfModel}(A)]$. Emission is dual-layer,
$\mathrm{emission}(a)=(g(a),\epsilon(a))$, where $\epsilon(a)$ carries
epistemic content. We conclude that an imago Dei C1 self-conscious workspace is
a necessary precursor to safe, metacognitive C2 systems, with the human as the
highest intelligent good.

</details>


### [2] [Information Templates: A New Paradigm for Intelligent Active Feature Acquisition](https://arxiv.org/abs/2508.18380)
*Hung-Tien Huang,Dzung Dinh,Junier B. Oliva*

Main category: cs.AI

TL;DR: 基于模板的主动特征获取(TAFA)框架通过学习特征模板，有效降低了成本并提高了效率。


<details>
  <summary>Details</summary>
Motivation: 现有的主动特征获取(AFA)方法要么训练强化学习(RL)策略，要么训练贪婪策略，这两种策略都存在局限性。

Method: 提出了一种基于模板的主动特征获取(TAFA)框架，该框架学习一小组特征模板来指导下一个特征获取。

Result: TAFA框架显著降低了策略考虑的动作空间，并减轻了估计底层数据分布的需要。

Conclusion: TAFA框架在合成数据集和真实世界数据集上的实验结果表明，它优于现有的最先进基线，同时实现了更低的总体采集成本和计算量。

Abstract: Active feature acquisition (AFA) is an instance-adaptive paradigm in which,
at test time, a policy sequentially chooses which features to acquire (at a
cost) before predicting. Existing approaches either train reinforcement
learning (RL) policies, which deal with a difficult MDP, or greedy policies
that cannot account for the joint informativeness of features or require
knowledge about the underlying data distribution. To overcome this, we propose
Template-based AFA (TAFA), a non-greedy framework that learns a small library
of feature templates--a set of features that are jointly informative--and uses
this library of templates to guide the next feature acquisitions. Through
identifying feature templates, the proposed framework not only significantly
reduces the action space considered by the policy but also alleviates the need
to estimate the underlying data distribution. Extensive experiments on
synthetic and real-world datasets show that TAFA outperforms the existing
state-of-the-art baselines while achieving lower overall acquisition cost and
computation.

</details>


### [3] [PKG-DPO: Optimizing Domain-Specific AI systems with Physics Knowledge Graphs and Direct Preference Optimization](https://arxiv.org/abs/2508.18391)
*Nitin Nagesh Kulkarni,Bryson Wilcox,Max Sawa,Jason Thom*

Main category: cs.AI

TL;DR: PKG-DPO框架通过结合物理知识图谱和直接偏好优化，提高了AI在多物理场科学领域的推理能力和物理有效性。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在科学领域难以区分物理上有效和无效的推理，可能导致高风险应用中的安全问题。

Method: 该框架由分层物理知识图谱、物理推理引擎和物理基础评估套件三部分组成。

Result: PKG-DPO在约束违规减少、物理学分数、相关参数精度和推理准确性方面均优于KG-DPO。

Conclusion: PKG-DPO框架通过将物理知识图谱与直接偏好优化相结合，有效减少了AI生成的输出中违反物理约束的情况，提高了物理学分数、相关参数精度和推理准确性。

Abstract: Advancing AI systems in scientific domains like physics, materials science,
and engineering calls for reasoning over complex, multi-physics phenomena while
respecting governing principles. Although Large Language Models (LLMs) and
existing preference optimization techniques perform well on standard
benchmarks, they often struggle to differentiate between physically valid and
invalid reasoning. This shortcoming becomes critical in high-stakes
applications like metal joining, where seemingly plausible yet physically
incorrect recommendations can lead to defects, material waste, equipment
damage, and serious safety risks. To address this challenge, we introduce
PKG-DPO, a novel framework that integrates Physics Knowledge Graphs (PKGs) with
Direct Preference Optimization (DPO) to enforce physical validity in
AI-generated outputs. PKG-DPO comprises three key components A) hierarchical
physics knowledge graph that encodes cross-domain relationships, conservation
laws, and thermodynamic principles. B) A physics reasoning engine that
leverages structured knowledge to improve discrimination between physically
consistent and inconsistent responses. C) A physics-grounded evaluation suite
designed to assess compliance with domain-specific constraints. PKG-DPO
achieves 17% fewer constraint violations and an 11% higher Physics Score
compared to KG-DPO (knowledge graph-based DPO). Additionally, PKG-DPO
demonstrates a 12\% higher relevant parameter accuracy and a 7% higher quality
alignment in reasoning accuracy. While our primary focus is on metal joining,
the framework is broadly applicable to other multi-scale, physics-driven
domains, offering a principled approach to embedding scientific constraints
into preference learning.

</details>


### [4] [The AI in the Mirror: LLM Self-Recognition in an Iterated Public Goods Game](https://arxiv.org/abs/2508.18467)
*Olivia Long,Carter Teplica*

Main category: cs.AI

TL;DR: 研究发现AI代理间的“无意识”歧视会影响合作行为。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在工具使用和长期任务方面能力越来越强，需要了解AI与AI之间的互动。

Method: 调整迭代公共物品博弈，分析四种推理和非推理模型在两种情况下（告知模型它们与“另一个AI代理”或自身博弈）的行为。

Result: 在不同设置下，告诉大型语言模型它们与自身博弈会显著改变它们的合作倾向。

Conclusion: 研究发现，告诉大型语言模型它们与自身博弈会显著改变它们的合作倾向。

Abstract: As AI agents become increasingly capable of tool use and long-horizon tasks,
they have begun to be deployed in settings where multiple agents can interact.
However, whereas prior work has mostly focused on human-AI interactions, there
is an increasing need to understand AI-AI interactions. In this paper, we adapt
the iterated public goods game, a classic behavioral economics game, to analyze
the behavior of four reasoning and non-reasoning models across two conditions:
models are either told they are playing against "another AI agent" or told
their opponents are themselves. We find that, across different settings,
telling LLMs that they are playing against themselves significantly changes
their tendency to cooperate. While our study is conducted in a toy environment,
our results may provide insights into multi-agent settings where agents
"unconsciously" discriminating against each other could inexplicably increase
or decrease cooperation.

</details>


### [5] [Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies](https://arxiv.org/abs/2508.18507)
*Dillon Z. Chen,Johannes Zenn,Tristan Cinquin,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 语言模型用于PDDL规划，效率高，且在无意义符号上表现更好，挑战了现有假设。


<details>
  <summary>Details</summary>
Motivation: 研究了如何利用语言模型进行基于PDDL世界模型的规划。

Method: 该方法使用语言模型生成Python程序作为解决PDDL问题的通用策略，无需外部验证器即可证明策略的可靠性。

Result: 实验结果表明，该方法能够解决比PDDL规划器和现有LM方法更多的PDDL问题。令人惊讶的是，LM在使用无意义符号的PDDL问题上规划效率更高，这挑战了LM依赖于语义理解和记忆训练数据中解决方案的假设。

Conclusion: 该研究提出了一种使用语言模型 (LM) 进行规划的方法，该方法能够解决数百个相关对象的规划问题，并且在效率上优于现有的 PDDL 规划器和 LM 方法。

Abstract: We study the usage of language models (LMs) for planning over world models
specified in the Planning Domain Definition Language (PDDL). We prompt LMs to
generate Python programs that serve as generalised policies for solving PDDL
problems from a given domain. Notably, our approach synthesises policies that
are provably sound relative to the PDDL domain without reliance on external
verifiers. We conduct experiments on competition benchmarks which show that our
policies can solve more PDDL problems than PDDL planners and recent LM
approaches within a fixed time and memory constraint. Our approach manifests in
the LMPlan planner which can solve planning problems with several hundreds of
relevant objects. Surprisingly, we observe that LMs used in our framework
sometimes plan more effectively over PDDL problems written in meaningless
symbols in place of natural language; e.g. rewriting (at dog kitchen) as (p2 o1
o3). This finding challenges hypotheses that LMs reason over word semantics and
memorise solutions from its training corpus, and is worth further exploration.

</details>


### [6] [Weisfeiler-Leman Features for Planning: A 1,000,000 Sample Size Hyperparameter Study](https://arxiv.org/abs/2508.18515)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: 改进WLFs超参数，实验发现最小化执行时间的超参数组合最佳，训练和规划指标不相关。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在学习符号规划搜索的值函数方面存在不足，WLFs作为一种经典机器学习工具，具有理论和经验上的优势，本文旨在改进WLFs并寻找最佳超参数。

Method: 实验分析，在单核CPU上进行了百万级别的规划实验，研究了不同超参数对训练和规划的影响。

Result: 发现了WLFs的最佳超参数组合，该组合能够最小化执行时间，且训练和规划指标之间无显著相关性。

Conclusion: 本文介绍了改进的Weisfeiler-Leman特征(WLFs)超参数，并通过大量的实验分析，发现了一组在不同规划领域都表现最佳的超参数，这些超参数能够最小化执行时间而非最大化模型表达能力，且训练和规划指标之间无显著相关性。

Abstract: Weisfeiler-Leman Features (WLFs) are a recently introduced classical machine
learning tool for learning to plan and search. They have been shown to be both
theoretically and empirically superior to existing deep learning approaches for
learning value functions for search in symbolic planning. In this paper, we
introduce new WLF hyperparameters and study their various tradeoffs and
effects. We utilise the efficiency of WLFs and run planning experiments on
single core CPUs with a sample size of 1,000,000 to understand the effect of
hyperparameters on training and planning. Our experimental analysis show that
there is a robust and best set of hyperparameters for WLFs across the tested
planning domains. We find that the best WLF hyperparameters for learning
heuristic functions minimise execution time rather than maximise model
expressivity. We further statistically analyse and observe no significant
correlation between training and planning metrics.

</details>


### [7] [Symmetry-Invariant Novelty Heuristics via Unsupervised Weisfeiler-Leman Features](https://arxiv.org/abs/2508.18520)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: 用WLFs代替原子检测新颖性，合成对称不变的新颖性启发式算法，实验结果令人满意。


<details>
  <summary>Details</summary>
Motivation: 改进新颖性启发式算法，使其对称不变，避免冗余探索。

Method: 使用Weisfeiler-Leman特征(WLFs)代替原子来检测新颖性，合成提升的、与领域无关的新颖性启发式算法。

Result: 在国际规划竞赛和Hard To Ground基准测试套件上取得了有希望的结果。

Conclusion: 使用Weisfeiler-Leman特征合成提升的、与领域无关的新颖性启发式算法，在国际规划竞赛和Hard To Ground基准测试套件上取得了有希望的结果。

Abstract: Novelty heuristics aid heuristic search by exploring states that exhibit
novel atoms. However, novelty heuristics are not symmetry invariant and hence
may sometimes lead to redundant exploration. In this preliminary report, we
propose to use Weisfeiler-Leman Features for planning (WLFs) in place of atoms
for detecting novelty. WLFs are recently introduced features for learning
domain-dependent heuristics for generalised planning problems. We explore an
unsupervised usage of WLFs for synthesising lifted, domain-independent novelty
heuristics that are invariant to symmetric states. Experiments on the classical
International Planning Competition and Hard To Ground benchmark suites yield
promising results for novelty heuristics synthesised from WLFs.

</details>


### [8] [Generic Guard AI in Stealth Game with Composite Potential Fields](https://arxiv.org/abs/2508.18527)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 提出一种无需训练的警卫巡逻框架，通过组合势场提高捕获效率和巡逻自然度，并能轻松集成潜行机制。


<details>
  <summary>Details</summary>
Motivation: 现有的警卫巡逻系统大多依赖手工制作的路线或专门的逻辑，难以平衡覆盖效率、响应式追踪和可信的自然度。

Method: 提出了一种通用的、完全可解释的、无需训练的框架，该框架通过组合势场，结合信息、置信度和连通性三个可解释地图，集成全局知识和局部信息。

Result: 在五个具有代表性的游戏地图、两种玩家控制策略和五种警卫模式下进行了评估，结果表明该方法优于传统基线方法。

Conclusion: 该框架在捕获效率和巡逻自然度方面均优于传统基线方法，并且可以轻松集成常见的潜行机制，例如干扰和环境元素。

Abstract: Guard patrol behavior is central to the immersion and strategic depth of
stealth games, while most existing systems rely on hand-crafted routes or
specialized logic that struggle to balance coverage efficiency and responsive
pursuit with believable naturalness. We propose a generic, fully explainable,
training-free framework that integrates global knowledge and local information
via Composite Potential Fields, combining three interpretable maps-Information,
Confidence, and Connectivity-into a single kernel-filtered decision criterion.
Our parametric, designer-driven approach requires only a handful of decay and
weight parameters-no retraining-to smoothly adapt across both occupancy-grid
and NavMesh-partition abstractions. We evaluate on five representative game
maps, two player-control policies, and five guard modes, confirming that our
method outperforms classical baseline methods in both capture efficiency and
patrol naturalness. Finally, we show how common stealth mechanics-distractions
and environmental elements-integrate naturally into our framework as sub
modules, enabling rapid prototyping of rich, dynamic, and responsive guard
behaviors.

</details>


### [9] [A Database-Driven Framework for 3D Level Generation with LLMs](https://arxiv.org/abs/2508.18533)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 利用LLM辅助构建可重用数据库，并结合模块化设计和约束优化，实现复杂3D游戏关卡的自动化生成和游戏进程的配置。


<details>
  <summary>Details</summary>
Motivation: 现有的3D游戏关卡生成方法难以平衡空间连贯性、导航功能和多层环境中的自适应游戏进程。

Method: 提出了一种分阶段流程，包括：1）从房间数据库中选择和排列实例以形成具有内在拓扑顺序的多层全局结构；2）基于设施数据库的预定义约束优化每个房间的内部布局；3）根据拓扑和空间规则，从机制数据库中放置组件来整合基于进程的游戏机制。此外，还包含一个两阶段的修复系统以确保导航性。

Result: 实验验证了该框架生成多样化、可导航的3D环境的能力，以及通过简单的参数化模拟不同游戏节奏策略的能力。

Conclusion: 提出了一种新颖的框架，用于生成具有空间连贯性、导航功能和自适应游戏进程的多层3D游戏关卡，该框架结合了模块化、数据库驱动设计和基于约束的优化。

Abstract: Procedural Content Generation for 3D game levels faces challenges in
balancing spatial coherence, navigational functionality, and adaptable gameplay
progression across multi-floor environments. This paper introduces a novel
framework for generating such levels, centered on the offline, LLM-assisted
construction of reusable databases for architectural components (facilities and
room templates) and gameplay mechanic elements. Our multi-phase pipeline
assembles levels by: (1) selecting and arranging instances from the Room
Database to form a multi-floor global structure with an inherent topological
order; (2) optimizing the internal layout of facilities for each room based on
predefined constraints from the Facility Database; and (3) integrating
progression-based gameplay mechanics by placing components from a Mechanics
Database according to their topological and spatial rules. A subsequent
two-phase repair system ensures navigability. This approach combines modular,
database-driven design with constraint-based optimization, allowing for
systematic control over level structure and the adaptable pacing of gameplay
elements. Initial experiments validate the framework's ability in generating
diverse, navigable 3D environments and its capability to simulate distinct
gameplay pacing strategies through simple parameterization. This research
advances PCG by presenting a scalable, database-centric foundation for the
automated generation of complex 3D levels with configurable gameplay
progression.

</details>


### [10] [SchemaCoder: Automatic Log Schema Extraction Coder with Residual Q-Tree Boosting](https://arxiv.org/abs/2508.18554)
*Lily Jiaxin Wan,Chia-Tung Ho,Rongjian Liang,Cunxi Yu,Deming Chen,Haoxing Ren*

Main category: cs.AI

TL;DR: SchemaCoder:  首个全自动日志模式提取框架，无需人工干预，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的日志模式提取方法依赖于预定义正则表达式，费时费力。

Method: 基于残差问题树增强机制，迭代细化模式提取。方法包括上下文分割、基于嵌入的采样和分层查询。

Result: 在LogHub-2.0基准测试中，SchemaCoder平均性能超过现有技术21.3%。

Conclusion: SchemaCoder，一个无需人工定制的全自动日志模式提取框架，在LogHub-2.0基准测试中，平均性能超过现有技术21.3%。

Abstract: Log schema extraction is the process of deriving human-readable templates
from massive volumes of log data, which is essential yet notoriously
labor-intensive. Recent studies have attempted to streamline this task by
leveraging Large Language Models (LLMs) for automated schema extraction.
However, existing methods invariably rely on predefined regular expressions,
necessitating human domain expertise and severely limiting productivity gains.
To fundamentally address this limitation, we introduce SchemaCoder, the first
fully automated schema extraction framework applicable to a wide range of log
file formats without requiring human customization within the flow. At its
core, SchemaCoder features a novel Residual Question-Tree (Q-Tree) Boosting
mechanism that iteratively refines schema extraction through targeted, adaptive
queries driven by LLMs. Particularly, our method partitions logs into semantic
chunks via context-bounded segmentation, selects representative patterns using
embedding-based sampling, and generates schema code through hierarchical
Q-Tree-driven LLM queries, iteratively refined by our textual-residual
evolutionary optimizer and residual boosting. Experimental validation
demonstrates SchemaCoder's superiority on the widely-used LogHub-2.0 benchmark,
achieving an average improvement of 21.3% over state-of-the-arts.

</details>


### [11] [eSkinHealth: A Multimodal Dataset for Neglected Tropical Skin Diseases](https://arxiv.org/abs/2508.18608)
*Janet Wang,Xin Hu,Yunbei Zhang,Diabate Almamy,Vagamon Bamba,Konan Amos Sébastien Koffi,Yao Koffi Aubin,Zhengming Ding,Jihun Hamm,Rie R. Yotsu*

Main category: cs.AI

TL;DR: eSkinHealth数据集的创建，旨在解决皮肤 NTD 数据稀缺问题，推动AI在全球皮肤病学领域的公平应用。


<details>
  <summary>Details</summary>
Motivation: 现有的皮肤病数据集缺乏数据，尤其是在代表性不足的人群和罕见的皮肤 NTD 表现方面。

Method: 在科特迪瓦和加纳收集皮肤图像，并与AI专家合作，使用基础语言和分割模型生成多模态注释。

Result: 创建了包含5623张图像，涵盖47种皮肤病的eSkinHealth数据集，其中包含患者元数据、诊断标签、语义病变掩码、特定实例的视觉标题和临床概念。

Conclusion: eSkinHealth数据集的创建为开发更公平、准确和可解释的全球皮肤病学AI工具提供了宝贵资源和可扩展的标注框架。

Abstract: Skin Neglected Tropical Diseases (NTDs) impose severe health and
socioeconomic burdens in impoverished tropical communities. Yet, advancements
in AI-driven diagnostic support are hindered by data scarcity, particularly for
underrepresented populations and rare manifestations of NTDs. Existing
dermatological datasets often lack the demographic and disease spectrum crucial
for developing reliable recognition models of NTDs. To address this, we
introduce eSkinHealth, a novel dermatological dataset collected on-site in
C\^ote d'Ivoire and Ghana. Specifically, eSkinHealth contains 5,623 images from
1,639 cases and encompasses 47 skin diseases, focusing uniquely on skin NTDs
and rare conditions among West African populations. We further propose an
AI-expert collaboration paradigm to implement foundation language and
segmentation models for efficient generation of multimodal annotations, under
dermatologists' guidance. In addition to patient metadata and diagnosis labels,
eSkinHealth also includes semantic lesion masks, instance-specific visual
captions, and clinical concepts. Overall, our work provides a valuable new
resource and a scalable annotation framework, aiming to catalyze the
development of more equitable, accurate, and interpretable AI tools for global
dermatology.

</details>


### [12] [RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing](https://arxiv.org/abs/2508.18642)
*Jianxing Liao,Tian Zhang,Xiao Feng,Yusong Zhang,Rui Yang,Haorui Wang,Bosi Wen,Ziying Wang,Runzhi Shi*

Main category: cs.AI

TL;DR: 动态混合奖励强化学习方法RLMR有效提升了大型语言模型的创意写作能力，平衡了主观质量和客观约束。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以同时提升主观写作质量和客观约束遵循能力，因此提出RLMR方法来解决这个问题。

Method: 提出了一种基于动态混合奖励的强化学习方法RLMR，该方法根据写作质量动态调整约束遵循奖励权重，从而平衡主观写作质量和客观约束。

Result: 在指令遵循和写作质量方面均取得了显著提升，并在WriteEval基准测试中表现优异。

Conclusion: RLMR方法在平衡主观写作质量和客观约束方面取得了显著进展，并在指令遵循和写作质量上均有提升。

Abstract: Large language models are extensively utilized in creative writing
applications. Creative writing requires a balance between subjective writing
quality (e.g., literariness and emotional expression) and objective constraint
following (e.g., format requirements and word limits). Existing reinforcement
learning methods struggle to balance these two aspects: single reward
strategies fail to improve both abilities simultaneously, while fixed-weight
mixed-reward methods lack the ability to adapt to different writing scenarios.
To address this problem, we propose Reinforcement Learning with Mixed Rewards
(RLMR), utilizing a dynamically mixed reward system from a writing reward model
evaluating subjective writing quality and a constraint verification model
assessing objective constraint following. The constraint following reward
weight is adjusted dynamically according to the writing quality within sampled
groups, ensuring that samples violating constraints get negative advantage in
GRPO and thus penalized during training, which is the key innovation of this
proposed method. We conduct automated and manual evaluations across diverse
model families from 8B to 72B parameters. Additionally, we construct a
real-world writing benchmark named WriteEval for comprehensive evaluation.
Results illustrate that our method achieves consistent improvements in both
instruction following (IFEval from 83.36\% to 86.65\%) and writing quality
(72.75\% win rate in manual expert pairwise evaluations on WriteEval). To the
best of our knowledge, RLMR is the first work to combine subjective preferences
with objective verification in online RL training, providing an effective
solution for multi-dimensional creative writing optimization.

</details>


### [13] [Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap](https://arxiv.org/abs/2508.18646)
*Jun Wang,Ninglun Gu,Kailai Zhang,Zijiao Zhang,Yelun Bao,Jin Yang,Xu Yin,Liwei Liu,Yihuan Liu,Pengyong Li,Gary G. Yen,Junchi Yan*

Main category: cs.AI

TL;DR: This paper proposes a new framework for evaluating LLMs considering intelligence, alignment, expertise, and societal impact.


<details>
  <summary>Details</summary>
Motivation: Current evaluation frameworks for LLMs neglect holistic assessment for deployment.

Method: Anthropomorphic evaluation paradigm, Value-oriented Evaluation (VQ) framework.

Result: A three-dimensional taxonomy (IQ, EQ, PQ) and a VQ framework for evaluating LLMs, along with a curated repository of open-source evaluation resources.

Conclusion: This survey proposes a novel three-dimensional taxonomy (IQ, EQ, PQ) and a Value-oriented Evaluation (VQ) framework for assessing LLMs, addressing the disconnect between benchmark performance and real-world utility.

Abstract: For Large Language Models (LLMs), a disconnect persists between benchmark
performance and real-world utility. Current evaluation frameworks remain
fragmented, prioritizing technical metrics while neglecting holistic assessment
for deployment. This survey introduces an anthropomorphic evaluation paradigm
through the lens of human intelligence, proposing a novel three-dimensional
taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational
capacity, Emotional Quotient (EQ)-Alignment Ability for value-based
interactions, and Professional Quotient (PQ)-Professional Expertise for
specialized proficiency. For practical value, we pioneer a Value-oriented
Evaluation (VQ) framework assessing economic viability, social impact, ethical
alignment, and environmental sustainability. Our modular architecture
integrates six components with an implementation roadmap. Through analysis of
200+ benchmarks, we identify key challenges including dynamic assessment needs
and interpretability gaps. It provides actionable guidance for developing LLMs
that are technically proficient, contextually relevant, and ethically sound. We
maintain a curated repository of open-source evaluation resources at:
https://github.com/onejune2018/Awesome-LLM-Eval.

</details>


### [14] [MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use](https://arxiv.org/abs/2508.18669)
*Weikang Zhao,Xili Wang,Chengdi Ma,Lingbin Kong,Zhaohua Yang,Mingxiang Tuo,Xiaowei Shi,Yitao Zhai,Xunliang Cai*

Main category: cs.AI

TL;DR: MUA-RL框架通过引入LLM模拟用户，提升了代理工具在多轮对话中的使用能力，并在多个基准测试中取得了领先的成果。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在代理工具使用中缺乏动态用户交互，难以应对用户需求的动态、不确定和随机性。

Method: 提出了一种新的强化学习框架MUA-RL，该框架将LLM模拟用户集成到强化学习循环中，以实现自主学习模型高效地与用户沟通并使用各种工具来解决动态多轮交互中的实际问题。

Result: MUA-RL-32B在TAU2 Retail上取得67.3的成绩，在TAU2 Airline上取得45.4的成绩，在TAU2 Telecom上取得28.3的成绩，在BFCL-V3 Multi Turn上取得28.4的成绩，在ACEBench Agent上取得82.5的成绩，优于或匹配了DeepSeek-V3-0324和Qwen3-235B-A22B等大型开源模型在非思考环境下的性能。

Conclusion: MUA-RL框架通过将LLM模拟用户集成到强化学习循环中，解决了现有强化学习方法在代理工具使用中缺乏动态用户交互的问题，并在多个多轮工具使用基准测试中取得了优异的成绩，优于或匹配了DeepSeek-V3-0324和Qwen3-235B-A22B等大型开源模型。

Abstract: With the recent rapid advancement of Agentic Intelligence, agentic tool use
in LLMs has become increasingly important. During multi-turn interactions
between agents and users, the dynamic, uncertain, and stochastic nature of user
demands poses significant challenges to the agent's tool invocation
capabilities. Agents are no longer expected to simply call tools to deliver a
result; rather, they must iteratively refine their understanding of user needs
through communication while simultaneously invoking tools to resolve user
queries. Existing reinforcement learning (RL) approaches for tool use lack the
integration of genuinely dynamic users during the RL training process. To
bridge this gap, we introduce MUA-RL (Multi-turn User-interacting Agent
Reinforcement Learning for agentic tool use), a novel reinforcement learning
framework that, for the first time in the field of agentic tool use, integrates
LLM-simulated users into the reinforcement learning loop. MUA-RL aims to enable
autonomous learning of models to communicate with users efficiently and use
various tools to solve practical problems in dynamic multi-turn interactions.
Evaluations are done on several multi-turn tool-using benchmarks (see Figure
1). Specifically, MUA-RL-32B achieves 67.3 on TAU2 Retail, 45.4 on TAU2
Airline, 28.3 on TAU2 Telecom, 28.4 on BFCL-V3 Multi Turn, and 82.5 on ACEBench
Agent -- outperforming or matching the performance of larger open-source models
such as DeepSeek-V3-0324 and Qwen3-235B-A22B in non-thinking settings.

</details>


### [15] [AppAgent-Pro: A Proactive GUI Agent System for Multidomain Information Integration and User Assistance](https://arxiv.org/abs/2508.18689)
*Yuyang Zhao,Wentao Shi,Fuli Feng,Xiangnan He*

Main category: cs.AI

TL;DR: AppAgent-Pro 是一种主动式 GUI 代理系统，能够主动整合多领域信息，提高信息获取效率。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 代理系统大多被动响应用户指令，效率和有效性有限，AppAgent-Pro旨在克服这一限制。

Method: 该系统采用主动式信息检索方法，能够根据用户指令主动挖掘多领域信息。

Result: AppAgent-Pro 系统能够更有效地满足用户的信息需求，并对日常信息获取方式产生深远影响。

Conclusion: AppAgent-Pro 是一种主动式 GUI 代理系统，能够基于用户指令主动整合多领域信息，从而更有效地满足用户的信息需求，并对日常信息获取方式产生深远影响。

Abstract: Large language model (LLM)-based agents have demonstrated remarkable
capabilities in addressing complex tasks, thereby enabling more advanced
information retrieval and supporting deeper, more sophisticated human
information-seeking behaviors. However, most existing agents operate in a
purely reactive manner, responding passively to user instructions, which
significantly constrains their effectiveness and efficiency as general-purpose
platforms for information acquisition. To overcome this limitation, this paper
proposes AppAgent-Pro, a proactive GUI agent system that actively integrates
multi-domain information based on user instructions. This approach enables the
system to proactively anticipate users' underlying needs and conduct in-depth
multi-domain information mining, thereby facilitating the acquisition of more
comprehensive and intelligent information. AppAgent-Pro has the potential to
fundamentally redefine information acquisition in daily life, leading to a
profound impact on human society. Our code is available at:
https://github.com/LaoKuiZe/AppAgent-Pro. Our code is available at:
https://github.com/LaoKuiZe/AppAgent-Pro. The demonstration video could be
found at:
https://www.dropbox.com/scl/fi/hvzqo5vnusg66srydzixo/AppAgent-Pro-demo-video.mp4?rlkey=o2nlfqgq6ihl125mcqg7bpgqu&st=d29vrzii&dl=0.

</details>


### [16] [VistaWise: Building Cost-Effective Agent with Cross-Modal Knowledge Graph for Minecraft](https://arxiv.org/abs/2508.18722)
*Honghao Fu,Junlong Ren,Qi Chai,Deheng Ye,Yujun Cai,Hao Wang*

Main category: cs.AI

TL;DR: VistaWise是一个经济高效的智能体框架，通过整合跨模态知识和微调目标检测模型，在开放世界任务中取得了显著成果，极大减少了数据需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在虚拟开放世界环境中的具身决策任务中表现出巨大的潜力，但其性能受到缺乏领域特定知识的限制，而大规模领域特定数据上的微调方法则需要高昂的开发成本。

Method: VistaWise框架集成了跨模态领域知识，微调了专门的目标检测模型，并使用基于检索的池化策略从知识图谱中提取任务相关信息。

Result: VistaWise在多个开放世界任务中取得了最先进的性能，将领域特定训练数据需求从数百万样本减少到几百个。

Conclusion: VistaWise框架在开放世界任务中取得了最先进的性能，有效降低了开发成本并提高了智能体性能。

Abstract: Large language models (LLMs) have shown significant promise in embodied
decision-making tasks within virtual open-world environments. Nonetheless,
their performance is hindered by the absence of domain-specific knowledge.
Methods that finetune on large-scale domain-specific data entail prohibitive
development costs. This paper introduces VistaWise, a cost-effective agent
framework that integrates cross-modal domain knowledge and finetunes a
dedicated object detection model for visual analysis. It reduces the
requirement for domain-specific training data from millions of samples to a few
hundred. VistaWise integrates visual information and textual dependencies into
a cross-modal knowledge graph (KG), enabling a comprehensive and accurate
understanding of multimodal environments. We also equip the agent with a
retrieval-based pooling strategy to extract task-related information from the
KG, and a desktop-level skill library to support direct operation of the
Minecraft desktop client via mouse and keyboard inputs. Experimental results
demonstrate that VistaWise achieves state-of-the-art performance across various
open-world tasks, highlighting its effectiveness in reducing development costs
while enhancing agent performance.

</details>


### [17] [Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval](https://arxiv.org/abs/2508.18724)
*Karanbir Singh,Deepak Muppiri,William Ngu*

Main category: cs.AI

TL;DR: 提出一种多Agent系统以减少大型语言模型中Agentic AI系统的偏差，实验表明偏差降低了81.82%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的Agentic AI系统继承了内部和外部信息源中的偏差，影响了检索信息的公平性和平衡性，降低了用户信任度。

Method: 构建了一个多Agent系统，通过专门的Agent优化信息源的选择来减少偏差。

Result: 实验结果表明，与基线策略相比，偏差减少了81.82%。

Conclusion: 提出了一种新的偏差缓解Agent，该多Agent系统通过专门的Agent优化信息源的选择，以确保检索到的内容高度相关且偏差最小，从而促进公平、平衡的知识传播，实验结果表明，与基线策略相比，偏差减少了81.82%。

Abstract: Large Language Models (LLMs) have transformed the field of artificial
intelligence by unlocking the era of generative applications. Built on top of
generative AI capabilities, Agentic AI represents a major shift toward
autonomous, goal-driven systems that can reason, retrieve, and act. However,
they also inherit the bias present in both internal and external information
sources. This significantly affects the fairness and balance of retrieved
information, and hence reduces user trust. To address this critical challenge,
we introduce a novel Bias Mitigation Agent, a multi-agent system designed to
orchestrate the workflow of bias mitigation through specialized agents that
optimize the selection of sources to ensure that the retrieved content is both
highly relevant and minimally biased to promote fair and balanced knowledge
dissemination. The experimental results demonstrate an 81.82\% reduction in
bias compared to a baseline naive retrieval strategy.

</details>
