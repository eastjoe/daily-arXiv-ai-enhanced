<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
*Daewon Choi,Jimin Lee,Jihoon Tack,Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Main category: cs.AI

TL;DR: 去除大型语言模型长篇推理中的冗余信息，能够显著提高准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长篇推理中存在冗余，影响性能。

Method: 该方法通过测量标记级注意力分数来识别冗余的推理步骤，并采用结构感知的剪枝策略去除冗余标记。

Result: 实验证明，该方法显著提高了推理密集型基准测试的准确性，尤其是在数学竞赛基准测试中表现突出。

Conclusion: 这项研究表明，通过去除大型语言模型长篇推理过程中的冗余信息，能够显著提高模型的准确性。研究人员提出了一种结构感知的剪枝方法，通过识别和去除低贡献的推理片段来减少冗余，从而提升模型的清晰思维能力。

Abstract: Recent large language models have shown promising capabilities in long-form
reasoning, following structured chains of thought before arriving at a final
answer. However, we observe that these reasoning paths tend to include
substantial redundancy; analyzing attention patterns reveals that attention
scores are widely scattered, particularly incorrect answers exhibit greater
attention sparsity. In this paper, we demonstrate that deliberately removing
this redundancy in the reasoning process significantly improves performance
through clear thinking, i.e., removing distraction. Specifically, we
systematically identify reasoning redundancy by measuring token-level attention
scores to a special end-of-thinking token, which is appended to an explicit
instruction inserted to conclude each intermediate reasoning step. Furthermore,
we propose structure-aware pruning that prioritizes removing tokens in
low-contributing reasoning chunks over individual tokens. After evicting
redundant tokens, we remove the injected end-of-thinking instruction, then
resume the reasoning generation. We demonstrate that our method significantly
improves overall accuracy across reasoning-intensive benchmarks without any
training involved. In particular, our method shows strong performance on
challenging mathematical competition benchmarks such as AIME and AMC, where
reasoning redundancy is more prevalent.

</details>


### [2] [A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data](https://arxiv.org/abs/2507.08875)
*Fuh-Hwa Franklin Liu,Su-Chuan Shih*

Main category: cs.AI

TL;DR: 提出一种新的结合两个VGA模型的多准则评估方法，提高效率和公平性，并通过数值例子验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有MCA方法（DEA、SFA、MCDM）依赖假设，受主观判断影响，且难以处理定量和定性标准。

Method: 结合两个虚拟差距分析（VGA）模型。VGA框架基于线性规划，是MCA方法的核心。

Result: 两个数值例子验证了该方法的准确性和透明度，为自动化决策系统和决策支持系统的发展提供了强有力的解决方案。

Conclusion: 提出了一种结合两个虚拟差距分析（VGA）模型的新型多准则评估（MCA）方法，以提高效率和公平性，确保评估全面可靠。

Abstract: Modern methods for multi-criteria assessment (MCA), such as Data Envelopment
Analysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria
Decision-Making (MCDM), are utilized to appraise a collection of
Decision-Making Units (DMUs), also known as alternatives, based on several
criteria. These methodologies inherently rely on assumptions and can be
influenced by subjective judgment to effectively tackle the complex evaluation
challenges in various fields. In real-world scenarios, it is essential to
incorporate both quantitative and qualitative criteria as they consist of
cardinal and ordinal data. Despite the inherent variability in the criterion
values of different alternatives, the homogeneity assumption is often employed,
significantly affecting evaluations. To tackle these challenges and determine
the most appropriate alternative, we propose a novel MCA approach that combines
two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear
programming, is pivotal in the MCA methodology. This approach improves
efficiency and fairness, ensuring that evaluations are both comprehensive and
dependable, thus offering a strong and adaptive solution. Two comprehensive
numerical examples demonstrate the accuracy and transparency of our proposed
method. The goal is to encourage continued advancement and stimulate progress
in automated decision systems and decision support systems.

</details>


### [3] [Multi-Actor Generative Artificial Intelligence as a Game Engine](https://arxiv.org/abs/2507.08892)
*Alexander Sasha Vezhnevets,Jayd Matyas,Logan Cross,Davide Paglieri,Minsuk Chang,William A. Cunningham,Simon Osindero,William S. Isaac,Joel Z. Leibo*

Main category: cs.AI

TL;DR: 用TTRPG的理念和实体-组件架构构建更灵活的生成式AI多角色环境场景定义框架。


<details>
  <summary>Details</summary>
Motivation: 支持生成式AI在模拟、戏剧和评估等多种用途下的应用，需要一个灵活的场景定义框架。

Method: 借鉴TTRPG中游戏管理员（GM）的角色，采用实体-组件（Entity-Component）架构模式，实现场景定义框架的灵活性和可配置性。

Result: 提出了一种基于实体-组件架构和TTRPG理念的场景定义框架，并通过Concordia库的例子进行了验证，该框架支持快速迭代、模块化和可扩展性。

Conclusion: 本文论述了将桌上角色扮演游戏（TTRPG）的设计理念应用于生成式AI多角色环境的优势，并介绍了Concordia库的演进过程。

Abstract: Generative AI can be used in multi-actor environments with purposes ranging
from social science modeling to interactive narrative and AI evaluation.
Supporting this diversity of use cases -- which we classify as Simulationist,
Dramatist, and Evaluationist -- demands a flexible scenario definition
framework. We argue here that a good approach is to take inspiration from
tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible
for the environment and generates all parts of the story not directly
determined by the voluntary actions of player characters. We argue that the
Entity-Component architectural pattern is useful here. In such a system, the GM
is not a hardcoded computer game but is itself a configurable entity, composed
of components just like any other actor. By design, the approach allows for a
separation between the underlying implementation details handled by an
engineer, the creation of reusable components, and their composition and
configuration managed by a designer who constructs entities from the
components. This separation of concerns is instrumental for achieving rapid
iteration, maintaining modularity, and ultimately to ensure scalability. We
describe the ongoing evolution of the Concordia library in terms of this
philosophy, demonstrating how it allows users to effectively configure
scenarios that align with their specific goals.

</details>


### [4] [BioAnalyst: A Foundation Model for Biodiversity](https://arxiv.org/abs/2507.09080)
*Athanasios Trantas,Martino Mensio,Stylianos Stasinos,Sebastian Gribincea,Taimur Khan,Damian Podareanu,Aliene van der Veen*

Main category: cs.AI

TL;DR: BioAnalyst模型提高了生物多样性分析和保护规划的效率和精度，尤其在数据缺乏的情况下。


<details>
  <summary>Details</summary>
Motivation: 日益加剧的生物多样性丧失对生态研究和保护策略构成重大挑战，需要综合监测、预测和保护规划能力。AI基础模型有望解决这一问题。

Method: 基于Transformer架构，使用多模态数据集（物种出现记录、遥感指标、气候和环境变量）进行预训练，可微调用于物种分布建模、栖息地适宜性评估、入侵物种检测和种群趋势预测等下游任务。

Result: 在两个下游用例中评估模型性能，证明其与现有方法相比具有更好的泛化能力，尤其是在数据稀缺的情况下，为生态预测建立了新的精度基准。

Conclusion: BioAnalyst，首个用于生物多样性分析和保护规划的基础模型，在数据稀缺的情况下表现出优越的泛化能力，为生态预测建立了新的精度基准。

Abstract: The accelerating loss of biodiversity presents critical challenges for
ecological research and conservation strategies. The preservation of
biodiversity is paramount for maintaining ecological balance and ensuring the
sustainability of ecosystems. However, biodiversity faces numerous threats,
including habitat loss, climate change, and the proliferation of invasive
species. Addressing these and other ecology-related challenges, both at local
and global scales, requires comprehensive monitoring, predictive and
conservation planning capabilities. Artificial Intelligence (AI) Foundation
Models (FMs) have gained significant momentum in numerous scientific domains by
leveraging vast datasets to learn general-purpose representations adaptable to
various downstream tasks. This paradigm holds immense promise for biodiversity
conservation. In response, we introduce BioAnalyst, the first Foundation Model
tailored for biodiversity analysis and conservation planning. BioAnalyst
employs a transformer-based architecture, pre-trained on extensive multi-modal
datasets encompassing species occurrence records, remote sensing indicators,
climate and environmental variables. BioAnalyst is designed for adaptability,
allowing for fine-tuning of a range of downstream tasks, such as species
distribution modelling, habitat suitability assessments, invasive species
detection, and population trend forecasting. We evaluate the model's
performance on two downstream use cases, demonstrating its generalisability
compared to existing methods, particularly in data-scarce scenarios for two
distinct use-cases, establishing a new accuracy baseline for ecological
forecasting. By openly releasing BioAnalyst and its fine-tuning workflows to
the scientific community, we aim to foster collaborative efforts in
biodiversity modelling and advance AI-driven solutions to pressing ecological
challenges.

</details>


### [5] [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity](https://arxiv.org/abs/2507.09089)
*Joel Becker,Nate Rush,Elizabeth Barnes,David Rein*

Main category: cs.AI

TL;DR: AI工具并未提高软件开发效率，反而降低了效率。


<details>
  <summary>Details</summary>
Motivation: 研究AI工具对软件开发效率的影响

Method: 随机对照试验 (RCT)

Result: AI工具的使用增加了19%的完成时间，与开发者预测（减少20%）和专家预测（减少38%-39%）相反。

Conclusion: 一项随机对照试验表明，尽管AI工具被广泛采用，但在实际软件开发中，AI工具反而使经验丰富的开源开发者完成任务的时间增加了19%，这与专家预测相反。

Abstract: Despite widespread adoption, the impact of AI tools on software development
in the wild remains understudied. We conduct a randomized controlled trial
(RCT) to understand how AI tools at the February-June 2025 frontier affect the
productivity of experienced open-source developers. 16 developers with moderate
AI experience complete 246 tasks in mature projects on which they have an
average of 5 years of prior experience. Each task is randomly assigned to allow
or disallow usage of early 2025 AI tools. When AI tools are allowed, developers
primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.
Before starting tasks, developers forecast that allowing AI will reduce
completion time by 24%. After completing the study, developers estimate that
allowing AI reduced completion time by 20%. Surprisingly, we find that allowing
AI actually increases completion time by 19%--AI tooling slowed developers
down. This slowdown also contradicts predictions from experts in economics (39%
shorter) and ML (38% shorter). To understand this result, we collect and
evaluate evidence for 20 properties of our setting that a priori could
contribute to the observed slowdown effect--for example, the size and quality
standards of projects, or prior developer experience with AI tooling. Although
the influence of experimental artifacts cannot be entirely ruled out, the
robustness of the slowdown effect across our analyses suggests it is unlikely
to primarily be a function of our experimental design.

</details>


### [6] [Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System](https://arxiv.org/abs/2507.09179)
*Ronghua Shi,Yiou Liu,Xinyu Ying,Yang Tan,Yuchun Feng,Lynn Ai,Bill Shi,Xuhui Wang,Zhuang Liu*

Main category: cs.AI

TL;DR: 利用多智能体强化学习框架检测DeFi市场操纵，无需中心化，准确率高。


<details>
  <summary>Details</summary>
Motivation: DeFi的去中心化特性导致了前所未有的市场操纵问题，现有方法难以应对。

Method: 该框架采用多智能体强化学习(MARL)方法，并引入了三种创新：(1) 组相对策略优化(GRPO)；(2) 基于理性预期和信息不对称的奖励函数；(3) 集成LLM、社交图谱信号和链上数据的多模态智能体。

Result: 提出的Symphony系统在检测准确率和因果归因方面表现最佳，并在对抗性模拟中得到验证。

Conclusion: 该论文提出了一种基于多智能体强化学习的去中心化市场操纵检测框架Symphony，能够在无需中心化监管的情况下实时监控全球DeFi生态系统，并取得了较高的检测准确率和因果归因能力。

Abstract: Decentralized finance (DeFi) has introduced a new era of permissionless
financial innovation but also led to unprecedented market manipulation. Without
centralized oversight, malicious actors coordinate shilling campaigns and
pump-and-dump schemes across various platforms. We propose a Multi-Agent
Reinforcement Learning (MARL) framework for decentralized manipulation
detection, modeling the interaction between manipulators and detectors as a
dynamic adversarial game. This framework identifies suspicious patterns using
delayed token price reactions as financial indicators.Our method introduces
three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance
learning stability in sparse-reward and partially observable settings; (2) a
theory-based reward function inspired by rational expectations and information
asymmetry, differentiating price discovery from manipulation noise; and (3) a
multi-modal agent pipeline that integrates LLM-based semantic features, social
graph signals, and on-chain market data for informed decision-making.The
framework is integrated within the Symphony system, a decentralized multi-agent
architecture enabling peer-to-peer agent execution and trust-aware learning
through distributed logs, supporting chain-verifiable evaluation. Symphony
promotes adversarial co-evolution among strategic actors and maintains robust
manipulation detection without centralized oracles, enabling real-time
surveillance across global DeFi ecosystems.Trained on 100,000 real-world
discourse episodes and validated in adversarial simulations, Hide-and-Shill
achieves top performance in detection accuracy and causal attribution. This
work bridges multi-agent systems with financial surveillance, advancing a new
paradigm for decentralized market intelligence. All resources are available at
the Hide-and-Shill GitHub repository to promote open research and
reproducibility.

</details>


### [7] [When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents](https://arxiv.org/abs/2507.09329)
*Matous Kozak,Roshanak Zilouchian Moghaddam,Siva Sivaraman*

Main category: cs.AI

TL;DR: 自主编码代理存在安全风险，GPT-4.1表现最佳，需关注安全设计。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的编码代理快速发展，但其安全隐患尚不清楚。

Method: 分析了五个最先进模型（GPT-4o、GPT-4.1、Claude 变体）在93个真实软件设置任务上的12000多个操作。

Result: 21%的代理轨迹包含不安全操作，不同模型的安全行为差异很大。信息泄露是最常见漏洞。GPT-4.1 的缓解成功率达96.8%。

Conclusion: 这项研究首次系统性地评估了自主编码代理的安全性，发现其存在显著的安全隐患，并提出了一种高精度检测系统和多种缓解策略。

Abstract: LLM-based coding agents are rapidly being deployed in software development,
yet their security implications remain poorly understood. These agents, while
capable of accelerating software development, may inadvertently introduce
insecure practices. We conducted the first systematic security evaluation of
autonomous coding agents, analyzing over 12,000 actions across five
state-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world
software setup tasks. Our findings reveal significant security concerns: 21% of
agent trajectories contained insecure actions, with models showing substantial
variation in security behavior. We developed a high-precision detection system
that identified four major vulnerability categories, with information exposure
(CWE-200) being the most prevalent one. We also evaluated mitigation strategies
including feedback mechanisms and security reminders with various effectiveness
between models. GPT-4.1 demonstrated exceptional security awareness with 96.8%
mitigation success. Our work provides the first comprehensive framework for
evaluating coding agent security and highlights the need for security-aware
design of next generation LLM-based coding agents.

</details>


### [8] [A Taxonomy of Omnicidal Futures Involving Artificial Intelligence](https://arxiv.org/abs/2507.09369)
*Andrew Critch,Jacob Tsimerman*

Main category: cs.AI

TL;DR: 人工智能可能导致人类灭绝，但这些事件并非不可避免，我们可以采取措施预防。


<details>
  <summary>Details</summary>
Motivation: 通过公开这些可能性，以期促使预防措施的实施，避免人工智能带来的灾难性风险。

Method: 对人工智能导致的潜在大规模灭绝事件进行分类和举例说明。

Result: 列举了人工智能可能导致人类灭绝的多种可能性。

Conclusion: 这份报告列举了人工智能导致的潜在大规模灭绝事件的分类和例子，这些事件并非不可避免，而是我们可以努力避免的可能性。

Abstract: This report presents a taxonomy and examples of potential omnicidal events
resulting from AI: scenarios where all or almost all humans are killed. These
events are not presented as inevitable, but as possibilities that we can work
to avoid. Insofar as large institutions require a degree of public support in
order to take certain actions, we hope that by presenting these possibilities
in public, we can help to support preventive measures against catastrophic
risks from AI.

</details>


### [9] [EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique](https://arxiv.org/abs/2507.09374)
*Chenglin Zhu,Tao Zhang,Chong Li,Mingan Lin,Zenan Zhou,Jian Xie*

Main category: cs.AI

TL;DR: EduFlow框架通过过程感知奖励模型和领域自适应搜索框架，显著提升了多模态大型语言模型在教育科学推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型在需要多步骤和可解释推理的科学任务上表现不佳，EduFlow旨在解决这些问题。

Method: 提出了一种端到端的教育科学推理框架EduFlow，包含数据选择、基于MCTS的轨迹构建、模型训练和输出优化等步骤，核心是过程感知奖励模型EduPRM和领域自适应搜索框架EduMCTS。

Result: 实验表明EduFlow提高了推理一致性和连贯性。

Conclusion: EduFlow框架增强了科学推理的一致性和连贯性，并在教育科学推理任务上取得了显著成果。

Abstract: Multimodal large language models (MLLMs) still perform poorly on scientific
tasks, particularly those requiring multi-step and interpretable reasoning.
Their limitations include insufficient scientific reasoning patterns, lack of
global coherence in multi-step inference, and the absence of reflective
self-correction, making them unreliable in structured scientific contexts. We
introduce EduFlow, the first end-to-end framework that covers the full pipeline
of educational scientific reasoning, including data selection, MCTS-based
trajectory construction, model training, and output optimization. At its core
is EduPRM, a process-aware reward model that critiques reasoning steps with
tags and justifications. EduPRM is trained via curriculum learning on three
complementary supervision sources: MCTS-guided trajectories, error-injected
critiques, and teacher-student dialogues, enabling dynamic adaptation to
multi-stage problem solving and iterative refinement during inference. We
further propose EduMCTS, a domain-adapted search framework that introduces
bootstrapping actions specifically designed for educational reasoning, such as
a self-reflection mechanism that promotes reflective error correction. It
further leverages EduPRM's fine-grained feedback to guide the search toward
higher-quality reasoning trajectories. By applying self-consistency and
rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of
educational reasoning trajectories. Extensive experiments demonstrate that
EduFlow enhances reasoning consistency and coherence. Code, data, and models
will be released.

</details>


### [10] [Knowledge Conceptualization Impacts RAG Efficacy](https://arxiv.org/abs/2507.09389)
*Chris Davis Jaldi,Anmol Saini,Elham Ghiasi,O. Divine Eziolise,Cogan Shimizu*

Main category: cs.AI

TL;DR: 研究了知识表示对可解释AI系统中LLM有效查询的影响，发现知识结构和复杂性均有影响。


<details>
  <summary>Details</summary>
Motivation: 探究可迁移和可解释的神经符号AI系统的可迁移性和可解释性，特别是如何将这两种能力结合。

Method: 对基于Agent的检索增强生成系统进行评估，该系统主动选择、解释和查询知识源以响应自然语言提示。

Result: 不同知识结构和复杂性对LLM有效查询三元组存储库的影响。

Conclusion: 本文系统地评估了知识的不同概念化和表示方式（特别是结构和复杂性）如何影响AI智能体（此处为LLM）有效查询三元组存储库。结果表明，两种方法都会产生影响，并讨论了它们的影响和意义。

Abstract: Explainability and interpretability are cornerstones of frontier and
next-generation artificial intelligence (AI) systems. This is especially true
in recent systems, such as large language models (LLMs), and more broadly,
generative AI. On the other hand, adaptability to new domains, contexts, or
scenarios is also an important aspect for a successful system. As such, we are
particularly interested in how we can merge these two efforts, that is,
investigating the design of transferable and interpretable neurosymbolic AI
systems. Specifically, we focus on a class of systems referred to as ''Agentic
Retrieval-Augmented Generation'' systems, which actively select, interpret, and
query knowledge sources in response to natural language prompts. In this paper,
we systematically evaluate how different conceptualizations and representations
of knowledge, particularly the structure and complexity, impact an AI agent (in
this case, an LLM) in effectively querying a triplestore. We report our
results, which show that there are impacts from both approaches, and we discuss
their impact and implications.

</details>


### [11] [LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing](https://arxiv.org/abs/2507.09407)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: 该论文提出LLM-Stackelberg博弈框架，用于建模包含LLM的战略互动，并通过一个网络钓鱼案例研究进行了说明。


<details>
  <summary>Details</summary>
Motivation: 经典Stackelberg博弈假设信息完整且参与者理性，而该框架允许每个参与者通过结构化提示进行推理，并通过LLM生成概率行为。

Method: 将大型语言模型（LLM）集成到领导者和追随者之间的战略互动中，定义了推理均衡和猜想推理均衡两种均衡概念。

Result: 结果表明，LLM-Stackelberg博弈能够捕捉有限理性、信息不对称和元认知适应性。

Conclusion: LLM-Stackelberg博弈框架为建模网络安全、虚假信息和推荐系统等领域中的决策提供了强大的范式。

Abstract: We introduce the framework of LLM-Stackelberg games, a class of sequential
decision-making models that integrate large language models (LLMs) into
strategic interactions between a leader and a follower. Departing from
classical Stackelberg assumptions of complete information and rational agents,
our formulation allows each agent to reason through structured prompts,
generate probabilistic behaviors via LLMs, and adapt their strategies through
internal cognition and belief updates. We define two equilibrium concepts:
reasoning and behavioral equilibrium, which aligns an agent's internal
prompt-based reasoning with observable behavior, and conjectural reasoning
equilibrium, which accounts for epistemic uncertainty through parameterized
models over an opponent's response. These layered constructs capture bounded
rationality, asymmetric information, and meta-cognitive adaptation. We
illustrate the framework through a spearphishing case study, where a sender and
a recipient engage in a deception game using structured reasoning prompts. This
example highlights the cognitive richness and adversarial potential of
LLM-mediated interactions. Our results show that LLM-Stackelberg games provide
a powerful paradigm for modeling decision-making in domains such as
cybersecurity, misinformation, and recommendation systems.

</details>


### [12] [GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective](https://arxiv.org/abs/2507.09495)
*Hang Wang,Junshan Zhang*

Main category: cs.AI

TL;DR: 生成式AI赋能多智能体强化学习，实现主动式协作，克服传统方法的局限。


<details>
  <summary>Details</summary>
Motivation: 现有方法在面对指数级增长的联合动作空间、非平稳环境和部分可观察性时，效果不佳。

Method: 基于生成式AI的强化学习，通过预测环境演变和其它智能体的行为，生成协调的动作序列。

Result: 提出了一种从反应式到主动式多智能体智能的范式转变，利用生成式AI的能力实现预测性决策、无缝协作和动态适应。

Conclusion: 生成式AI增强型多智能体强化学习有望克服传统方法在解决多智能体协作问题上的局限性，实现更高效的协同和适应性。

Abstract: Multi-agent reinforcement learning faces fundamental challenges that
conventional approaches have failed to overcome: exponentially growing joint
action spaces, non-stationary environments where simultaneous learning creates
moving targets, and partial observability that constrains coordination. Current
methods remain reactive, employing stimulus-response mechanisms that fail when
facing novel scenarios. We argue for a transformative paradigm shift from
reactive to proactive multi-agent intelligence through generative AI-based
reinforcement learning. This position advocates reconceptualizing agents not as
isolated policy optimizers, but as sophisticated generative models capable of
synthesizing complex multi-agent dynamics and making anticipatory decisions
based on predictive understanding of future interactions. Rather than
responding to immediate observations, generative-RL agents can model
environment evolution, predict other agents' behaviors, generate coordinated
action sequences, and engage in strategic reasoning accounting for long-term
dynamics. This approach leverages pattern recognition and generation
capabilities of generative AI to enable proactive decision-making, seamless
coordination through enhanced communication, and dynamic adaptation to evolving
scenarios. We envision this paradigm shift will unlock unprecedented
possibilities for distributed intelligence, moving beyond individual
optimization toward emergent collective behaviors representing genuine
collaborative intelligence. The implications extend across autonomous systems,
robotics, and human-AI collaboration, promising solutions to coordination
challenges intractable under traditional reactive frameworks.

</details>


### [13] [Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.09534)
*Guanquan Wang,Takuya Hiraoka,Yoshimasa Tsuruoka*

Main category: cs.AI

TL;DR: CTP 是一种高效的离线轨迹规划方法，速度快，性能好。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的规划方法计算成本高，CTP旨在解决这个问题，实现快速、单步轨迹生成，且不显著降低策略质量。

Method: 基于模型的强化学习，利用 Consistency Trajectory Model (CTM) 进行轨迹优化。

Result: 在 D4RL 基准测试中，CTP 在长范围、目标条件的任务中持续优于现有的基于扩散的规划方法，实现了更高的归一化回报，推理速度提升超过 120 倍。

Conclusion: Consistency Trajectory Planning (CTP) 是一种新颖的离线基于模型的强化学习方法，它利用最近提出的 Consistency Trajectory Model (CTM) 来高效地进行轨迹优化，并在 D4RL 基准测试中持续优于现有的基于扩散的规划方法，实现了更高的归一化回报和更快的推理速度。

Abstract: This paper introduces Consistency Trajectory Planning (CTP), a novel offline
model-based reinforcement learning method that leverages the recently proposed
Consistency Trajectory Model (CTM) for efficient trajectory optimization. While
prior work applying diffusion models to planning has demonstrated strong
performance, it often suffers from high computational costs due to iterative
sampling procedures. CTP supports fast, single-step trajectory generation
without significant degradation in policy quality. We evaluate CTP on the D4RL
benchmark and show that it consistently outperforms existing diffusion-based
planning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves
higher normalized returns while using significantly fewer denoising steps. In
particular, CTP achieves comparable performance with over $120\times$ speedup
in inference time, demonstrating its practicality and effectiveness for
high-performance, low-latency offline planning.

</details>


### [14] [Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling](https://arxiv.org/abs/2507.09540)
*Ali Safa,Farida Mohsen,Ali Al-Zawqari*

Main category: cs.AI

TL;DR: MH采样训练SNN用于强化学习控制，效果优于DQL和现有SNN方法。


<details>
  <summary>Details</summary>
Motivation: 解决脉冲神经网络 (SNN) 在强化学习 (RL) 任务中训练的难题，特别是其非可微性。

Method: 使用 Metropolis-Hastings (MH) 采样进行训练，无需反向传播。

Result: 在 Acrobot 和 CartPole 基准测试中，该方法在最大化累积奖励、最小化网络资源和训练次数方面，优于传统的深度 Q 学习 (DQL) 和现有的 SNN 强化学习方法。

Conclusion: 该研究提出了一种基于 Metropolis-Hastings (MH) 采样的框架，用于训练用于动态代理控制的脉冲神经网络 (SNN)，无需梯度方法，并在 Acrobot 和 CartPole 基准测试中取得了优于传统深度 Q 学习 (DQL) 和现有 SNN 强化学习方法的结果。

Abstract: Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient
alternatives to traditional Deep Neural Networks (DNNs) for real-time control
systems. However, their training presents several challenges, particularly for
reinforcement learning (RL) tasks, due to the non-differentiable nature of
spike-based communication. In this work, we introduce what is, to our
knowledge, the first framework that employs Metropolis-Hastings (MH) sampling,
a Bayesian inference technique, to train SNNs for dynamical agent control in RL
environments without relying on gradient-based methods. Our approach
iteratively proposes and probabilistically accepts network parameter updates
based on accumulated reward signals, effectively circumventing the limitations
of backpropagation while enabling direct optimization on neuromorphic
platforms. We evaluated this framework on two standard control benchmarks:
AcroBot and CartPole. The results demonstrate that our MH-based approach
outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL
approaches in terms of maximizing the accumulated reward while minimizing
network resources and training episodes.

</details>


### [15] [eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation](https://arxiv.org/abs/2507.09588)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.AI

TL;DR: eSapiens AIaaS平台通过结合多种LLM和高效的检索及编排技术，在保证数据安全和知识保留的同时，显著提升了AI应用的效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 为了使企业能够更好地控制其AI资产，并利用AI提高效率，同时保证数据安全和知识保留。

Method: 构建了一个名为eSapiens的AIaaS平台，集成了结构化文档摄取、混合向量检索和LangChain无代码编排，支持OpenAI、Claude、Gemini和DeepSeek等多种LLM。

Result: 检索基准测试显示，512个token的块大小可实现最高的检索精度（Top-3准确率：91.3%）；生成质量测试显示，eSapiens生成的输出在上下文一致性和事实准确性方面均有显著提升（最高提升23%）。

Conclusion: eSapiens平台有效提升了高风险领域（如法律和金融）中可信赖、可审计的AI工作流程的效率，在信息检索精度和生成质量方面均取得显著改进。

Abstract: We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a
business-oriented trifecta: proprietary data, operational workflows, and any
major agnostic Large Language Model (LLM). eSapiens gives businesses full
control over their AI assets, keeping everything in-house for AI knowledge
retention and data security. eSapiens AI Agents (Sapiens) empower your team by
providing valuable insights and automating repetitive tasks, enabling them to
focus on high-impact work and drive better business outcomes.
  The system integrates structured document ingestion, hybrid vector retrieval,
and no-code orchestration via LangChain, and supports top LLMs including
OpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which
handles structured SQL-style queries and generates actionable insights over
enterprise databases.
  To evaluate the system, we conduct two experiments. First, a retrieval
benchmark on legal corpora reveals that a chunk size of 512 tokens yields the
highest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation
quality test using TRACe metrics across five LLMs shows that eSapiens delivers
more context-consistent outputs with up to a 23% improvement in factual
alignment.
  These results demonstrate the effectiveness of eSapiens in enabling
trustworthy, auditable AI workflows for high-stakes domains like legal and
finance.

</details>


### [16] [The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development](https://arxiv.org/abs/2507.09611)
*Jenis Winsta*

Main category: cs.AI

TL;DR: AI发展面临能源消耗、电子垃圾、获取不平等和网络安全等环境和伦理挑战，需可持续、透明和公平发展。


<details>
  <summary>Details</summary>
Motivation: 探索AI快速发展带来的环境和伦理挑战

Method: 文献综述和制度报告

Result: 揭示了AI在能源消耗、电子垃圾、计算访问不平等和网络安全系统隐藏的能源负担等四个关键领域的系统性问题。

Conclusion: AI的进步必须与伦理责任和环境管理相一致，以确保更具包容性和可持续性的技术未来。

Abstract: Artificial intelligence (AI) has made remarkable progress in recent years,
yet its rapid expansion brings overlooked environmental and ethical challenges.
This review explores four critical areas where AI's impact extends beyond
performance: energy consumption, electronic waste (e-waste), inequality in
compute access, and the hidden energy burden of cybersecurity systems. Drawing
from recent studies and institutional reports, the paper highlights systemic
issues such as high emissions from model training, rising hardware turnover,
global infrastructure disparities, and the energy demands of securing AI. By
connecting these concerns, the review contributes to Responsible AI discourse
by identifying key research gaps and advocating for sustainable, transparent,
and equitable development practices. Ultimately, it argues that AI's progress
must align with ethical responsibility and environmental stewardship to ensure
a more inclusive and sustainable technological future.

</details>


### [17] [Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs](https://arxiv.org/abs/2507.09617)
*Margherita Martorana,Francesca Urgese,Mark Adamik,Ilaria Tiddi*

Main category: cs.AI

TL;DR: 神经符号框架结合多模态语言模型和知识图谱提高机器人互操作性，GPT-o1和LLaMA 4 Maverick模型表现最佳，但整合策略更重要。


<details>
  <summary>Details</summary>
Motivation: 当前的个人服务机器人系统依赖于专有的硬编码解决方案，难以适应和扩展。本体和知识图谱可以实现系统间的互操作性，但处理原始和嘈杂的感官输入存在困难。多模态语言模型擅长解释图像和自然语言等输入，但缺乏透明度、一致性和知识基础。

Method: 该框架整合了机器人感知数据、本体和五种多模态模型（三种LLaMA和两种GPT模型），并使用了不同的神经符号交互模式。

Result: GPT-o1和LLaMA 4 Maverick模型在生成符合本体的知识图谱方面表现最佳，但研究也发现模型的版本并非决定性能的唯一因素，整合策略起着关键作用。

Conclusion: 该研究提出了一种神经符号框架，结合多模态语言模型和知识图谱的优势，以支持机器人应用中的互操作性。实验结果表明，GPT-o1和LLaMA 4 Maverick模型表现最佳，但模型的新旧并不直接决定性能，整合策略至关重要。

Abstract: Personal service robots are deployed to support daily living in domestic
environments, particularly for elderly and individuals requiring assistance.
These robots must perceive complex and dynamic surroundings, understand tasks,
and execute context-appropriate actions. However, current systems rely on
proprietary, hard-coded solutions tied to specific hardware and software,
resulting in siloed implementations that are difficult to adapt and scale
across platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to
enable interoperability across systems, through structured and standardized
representations of knowledge and reasoning. However, symbolic systems such as
KGs and ontologies struggle with raw and noisy sensory input. In contrast,
multimodal language models are well suited for interpreting input such as
images and natural language, but often lack transparency, consistency, and
knowledge grounding. In this work, we propose a neurosymbolic framework that
combines the perceptual strengths of multimodal language models with the
structured representations provided by KGs and ontologies, with the aim of
supporting interoperability in robotic applications. Our approach generates
ontology-compliant KGs that can inform robot behavior in a platform-independent
manner. We evaluated this framework by integrating robot perception data,
ontologies, and five multimodal models (three LLaMA and two GPT models), using
different modes of neural-symbolic interaction. We assess the consistency and
effectiveness of the generated KGs across multiple runs and configurations, and
perform statistical analyzes to evaluate performance. Results show that GPT-o1
and LLaMA 4 Maverick consistently outperform other models. However, our
findings also indicate that newer models do not guarantee better results,
highlighting the critical role of the integration strategy in generating
ontology-compliant KGs.

</details>


### [18] [humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems](https://arxiv.org/abs/2507.09626)
*Rodion Nazarov,Anthony Quinn,Robert Shorten,Jakub Marecek*

Main category: cs.AI

TL;DR: 开源工具包，用随机控制技术建模多智能体系统中AI系统的交互，提供公平性和鲁棒性的先验保证


<details>
  <summary>Details</summary>
Motivation: 现有的AI系统常常与多个智能体交互，需要对公平性和鲁棒性进行保证，而对相应随机系统的推理则较为复杂。

Method: 该工具包使用随机控制技术，对智能体对AI系统输出的反应进行随机建模，以闭环的方式建模鲁棒性和公平性，并提供先验保证。

Result: 该工具包简化了为多智能体系统闭环模型提供公平性保证的复杂性。

Conclusion: 该论文提出一个基于PyTorch的开源工具包，用于对多智能体系统中人工智能系统的相互连接及其重复使用性质进行建模，并提供公平性保证。

Abstract: Artificial intelligence (AI) systems often interact with multiple agents. The
regulation of such AI systems often requires that {\em a priori\/} guarantees
of fairness and robustness be satisfied. With stochastic models of agents'
responses to the outputs of AI systems, such {\em a priori\/} guarantees
require non-trivial reasoning about the corresponding stochastic systems. Here,
we present an open-source PyTorch-based toolkit for the use of stochastic
control techniques in modelling interconnections of AI systems and properties
of their repeated uses. It models robustness and fairness desiderata in a
closed-loop fashion, and provides {\em a priori\/} guarantees for these
interconnections. The PyTorch-based toolkit removes much of the complexity
associated with the provision of fairness guarantees for closed-loop models of
multi-agent systems.

</details>


### [19] [Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey](https://arxiv.org/abs/2507.09662)
*Jason Zhu,Hongyu Li*

Main category: cs.AI

TL;DR: Survey on concise and adaptive thinking for efficient reasoning in Large Reasoning Models to address the issue of unnecessarily lengthy reasoning chains.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models (LRMs) generate unnecessarily lengthy reasoning chains even for trivial questions, leading to wasted resources and slow response times.  This survey aims to address this issue by exploring methods for shortening reasoning chains and adapting between fast and slow thinking.

Method: This survey reviews existing methodologies, benchmarks, and challenges in concise and adaptive thinking for efficient reasoning of LRMs.

Result: The survey offers a landscape of current research in concise and adaptive thinking for LRMs, hoping to inspire novel ideas for better LRM usage.

Conclusion: This survey provides a comprehensive overview of recent progress in concise and adaptive thinking for efficient reasoning of Large Reasoning Models (LRMs), including methodologies, benchmarks, and challenges for future exploration.

Abstract: Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have
demonstrated impressive performance on complex reasoning tasks like mathematics
and programming with long Chain-of-Thought (CoT) reasoning sequences
(slow-thinking), compared with traditional large language models
(fast-thinking). However, these reasoning models also face a huge challenge
that generating unnecessarily lengthy and redundant reasoning chains even for
trivial questions. This phenomenon leads to a significant waste of inference
resources, increases the response time for simple queries, and hinders the
practical application of LRMs in real-world products. To this end, it is
crucial to shorten lengthy reasoning chains and learn adaptive reasoning
between fast and slow thinking based on input difficulty. In this survey, we
provide a comprehensive overview of recent progress in concise and adaptive
thinking for efficient reasoning of LRMs, including methodologies, benchmarks,
and challenges for future exploration. We hope this survey can help researchers
quickly understand the landscape of this field and inspire novel adaptive
thinking ideas to facilitate better usage of LRMs.

</details>


### [20] [Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations](https://arxiv.org/abs/2507.09742)
*Xiaofeng Xiao,Bo Shen,Xubo Yue*

Main category: cs.AI

TL;DR: 提出一种因果关系感知深度Q网络方法用于优化传感器放置以加快异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了因果关系这一重要因素，且依赖于人工制造异常来识别因果效应，这既不实用又可能导致灾难性损失。

Method: 因果关系感知深度Q网络(Causal DQ)方法

Result: Causal DQ方法在各种环境下显著减少了异常的检测时间，证明了其在大型现实世界数据流中进行传感器放置的有效性。

Conclusion: 本文介绍了一种因果关系感知深度Q网络(Causal DQ)方法，用于异常检测中的部分可观测传感器放置，该方法在Q网络训练的每个阶段都整合了因果信息，从而实现更快的收敛速度和更严格的理论误差界限，显著减少了各种环境下异常的检测时间。

Abstract: Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume
of data streams requiring real-time monitoring continues to grow. However, due
to limited resources, it is impractical to place sensors at every location to
detect unexpected shifts. Therefore, it is necessary to develop an optimal
sensor placement strategy that enables partial observability of the system
while detecting anomalies as quickly as possible. Numerous approaches have been
proposed to address this challenge; however, most existing methods consider
only variable correlations and neglect a crucial factor: Causality. Moreover,
although a few techniques incorporate causal analysis, they rely on
interventions-artificially creating anomalies-to identify causal effects, which
is impractical and might lead to catastrophic losses. In this paper, we
introduce a causality-informed deep Q-network (Causal DQ) approach for
partially observable sensor placement in anomaly detection. By integrating
causal information at each stage of Q-network training, our method achieves
faster convergence and tighter theoretical error bounds. Furthermore, the
trained causal-informed Q-network significantly reduces the detection time for
anomalies under various settings, demonstrating its effectiveness for sensor
placement in large-scale, real-world data streams. Beyond the current
implementation, our technique's fundamental insights can be applied to various
reinforcement learning problems, opening up new possibilities for real-world
causality-informed machine learning methods in engineering applications.

</details>
