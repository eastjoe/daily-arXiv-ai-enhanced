{"id": "2509.03536", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.03536", "abs": "https://arxiv.org/abs/2509.03536", "authors": ["Weizhi Chen", "Ziwei Wang", "Leyang Yang", "Sheng Zhou", "Xiaoxuan Tang", "Jiajun Bu", "Yong Li", "Wei Jiang"], "title": "PG-Agent: An Agent Powered by Page Graph", "comment": "Paper accepted to ACM MM 2025", "summary": "Graphical User Interface (GUI) agents possess significant commercial and\nsocial value, and GUI agents powered by advanced multimodal large language\nmodels (MLLMs) have demonstrated remarkable potential. Currently, existing GUI\nagents usually utilize sequential episodes of multi-step operations across\npages as the prior GUI knowledge, which fails to capture the complex transition\nrelationship between pages, making it challenging for the agents to deeply\nperceive the GUI environment and generalize to new scenarios. Therefore, we\ndesign an automated pipeline to transform the sequential episodes into page\ngraphs, which explicitly model the graph structure of the pages that are\nnaturally connected by actions. To fully utilize the page graphs, we further\nintroduce Retrieval-Augmented Generation (RAG) technology to effectively\nretrieve reliable perception guidelines of GUI from them, and a tailored\nmulti-agent framework PG-Agent with task decomposition strategy is proposed to\nbe injected with the guidelines so that it can generalize to unseen scenarios.\nExtensive experiments on various benchmarks demonstrate the effectiveness of\nPG-Agent, even with limited episodes for page graph construction.", "AI": {"tldr": "\u5229\u7528\u9875\u9762\u56fe\u548c\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u6784\u5efa\u66f4\u6709\u6548\u7684GUI\u667a\u80fd\u4f53PG-Agent\uff0c\u63d0\u5347\u5176\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709GUI\u667a\u80fd\u4f53\u96be\u4ee5\u6355\u6349\u9875\u9762\u95f4\u590d\u6742\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u5176\u5bf9GUI\u73af\u5883\u7684\u7406\u89e3\u548c\u6cdb\u5316\u80fd\u529b", "method": "\u5c06\u5e8f\u5217\u64cd\u4f5c\u8f6c\u5316\u4e3a\u9875\u9762\u56fe\uff0c\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u4ece\u9875\u9762\u56fe\u4e2d\u68c0\u7d22GUI\u611f\u77e5\u6307\u5bfc\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u4efb\u52a1\u5206\u89e3\u7684PG-Agent\u591a\u667a\u80fd\u4f53\u6846\u67b6", "result": "\u5b9e\u9a8c\u8868\u660ePG-Agent\u6709\u6548\uff0c\u5373\u4f7f\u9875\u9762\u56fe\u6784\u5efa\u6570\u636e\u6709\u9650", "conclusion": "\u9875\u9762\u56fe\u548cRAG\u6280\u672f\u7ed3\u5408\u80fd\u6709\u6548\u63d0\u5347GUI\u667a\u80fd\u4f53\u7684\u6cdb\u5316\u80fd\u529b"}}
{"id": "2509.03548", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03548", "abs": "https://arxiv.org/abs/2509.03548", "authors": ["Jo\u00e3o P. Arroyo", "Jo\u00e3o G. Rodrigues", "Daniel Lawand", "Denis D. Mau\u00e1", "Junkyu Lee", "Radu Marinescu", "Alex Gray", "Eduardo R. Laurentino", "Fabio G. Cozman"], "title": "Multilinear and Linear Programs for Partially Identifiable Queries in Quasi-Markovian Structural Causal Models", "comment": "Accepted at the Causal Abstractions and Representations (CAR)\n  workshop of the 41st Conference on Uncertainty in Artificial Intelligence\n  (UAI 2025)", "summary": "We investigate partially identifiable queries in a class of causal models. We\nfocus on acyclic Structural Causal Models that are quasi-Markovian (that is,\neach endogenous variable is connected with at most one exogenous confounder).\nWe look into scenarios where endogenous variables are observed (and a\ndistribution over them is known), while exogenous variables are not fully\nspecified. This leads to a representation that is in essence a Bayesian network\nwhere the distribution of root variables is not uniquely determined. In such\ncircumstances, it may not be possible to precisely compute a probability value\nof interest. We thus study the computation of tight probability bounds, a\nproblem that has been solved by multilinear programming in general, and by\nlinear programming when a single confounded component is intervened upon. We\npresent a new algorithm to simplify the construction of such programs by\nexploiting input probabilities over endogenous variables. For scenarios with a\nsingle intervention, we apply column generation to compute a probability bound\nthrough a sequence of auxiliary linear integer programs, thus showing that a\nrepresentation with polynomial cardinality for exogenous variables is possible.\nExperiments show column generation techniques to be superior to existing\nmethods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u56e0\u679c\u6a21\u578b\u4e2d\u90e8\u5206\u53ef\u8bc6\u522b\u67e5\u8be2\u7684\u6982\u7387\u754c\u9650\u8ba1\u7b97\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u5229\u7528\u5185\u751f\u53d8\u91cf\u4e0a\u7684\u8f93\u5165\u6982\u7387\u7b80\u5316\u591a\u7ebf\u6027\u89c4\u5212\u7684\u6784\u5efa\uff0c\u5e76\u901a\u8fc7\u5217\u751f\u6210\u6280\u672f\u8ba1\u7b97\u6982\u7387\u754c\u9650\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u5728\u5185\u751f\u53d8\u91cf\u53ef\u89c2\u6d4b\u4f46\u5916\u751f\u53d8\u91cf\u672a\u5b8c\u5168\u6307\u5b9a\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u8ba1\u7b97\u611f\u5174\u8da3\u6982\u7387\u7684\u7d27\u81f4\u754c\u9650\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u5229\u7528\u5185\u751f\u53d8\u91cf\u4e0a\u7684\u8f93\u5165\u6982\u7387\u7b80\u5316\u591a\u7ebf\u6027\u89c4\u5212\u7684\u6784\u5efa\uff0c\u5e76\u4f7f\u7528\u5217\u751f\u6210\u6280\u672f\u8ba1\u7b97\u6982\u7387\u754c\u9650\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8ba1\u7b97\u90e8\u5206\u53ef\u8bc6\u522b\u67e5\u8be2\u7684\u6982\u7387\u754c\u9650\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5bf9\u56e0\u679c\u63a8\u65ad\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.03550", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03550", "abs": "https://arxiv.org/abs/2509.03550", "authors": ["Tonghe Li", "Jixin Liu", "Weili Zeng", "Hao Jiang"], "title": "Diffusion-RL Based Air Traffic Conflict Detection and Resolution Method", "comment": "59 pages,13 figures, 3 tables", "summary": "In the context of continuously rising global air traffic, efficient and safe\nConflict Detection and Resolution (CD&R) is paramount for air traffic\nmanagement. Although Deep Reinforcement Learning (DRL) offers a promising\npathway for CD&R automation, existing approaches commonly suffer from a\n\"unimodal bias\" in their policies. This leads to a critical lack of\ndecision-making flexibility when confronted with complex and dynamic\nconstraints, often resulting in \"decision deadlocks.\" To overcome this\nlimitation, this paper pioneers the integration of diffusion probabilistic\nmodels into the safety-critical task of CD&R, proposing a novel autonomous\nconflict resolution framework named Diffusion-AC. Diverging from conventional\nmethods that converge to a single optimal solution, our framework models its\npolicy as a reverse denoising process guided by a value function, enabling it\nto generate a rich, high-quality, and multimodal action distribution. This core\narchitecture is complemented by a Density-Progressive Safety Curriculum (DPSC),\na training mechanism that ensures stable and efficient learning as the agent\nprogresses from sparse to high-density traffic environments. Extensive\nsimulation experiments demonstrate that the proposed method significantly\noutperforms a suite of state-of-the-art DRL benchmarks. Most critically, in the\nmost challenging high-density scenarios, Diffusion-AC not only maintains a high\nsuccess rate of 94.1% but also reduces the incidence of Near Mid-Air Collisions\n(NMACs) by approximately 59% compared to the next-best-performing baseline,\nsignificantly enhancing the system's safety margin. This performance leap stems\nfrom its unique multimodal decision-making capability, which allows the agent\nto flexibly switch to effective alternative maneuvers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiffusion-AC\u7684\u65b0\u578b\u81ea\u4e3b\u51b2\u7a81\u89e3\u51b3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u6269\u6563\u6982\u7387\u6a21\u578b\u514b\u670d\u4e86\u73b0\u6709\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u7a7a\u4e2d\u4ea4\u901a\u51b2\u7a81\u68c0\u6d4b\u4e0e\u89e3\u51b3\u4e2d\u7684\u5355\u5cf0\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u4e0e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u52a8\u6001\u7ea6\u675f\u65f6\u7f3a\u4e4f\u51b3\u7b56\u7075\u6d3b\u6027\uff0c\u5bb9\u6613\u51fa\u73b0\u51b3\u7b56\u6b7b\u9501\u3002", "method": "\u5c06\u6269\u6563\u6982\u7387\u6a21\u578b\u4e0e\u5bc6\u5ea6\u6e10\u8fdb\u5f0f\u5b89\u5168\u8bfe\u7a0b(DPSC)\u76f8\u7ed3\u5408\uff0c\u751f\u6210\u591a\u5cf0\u52a8\u4f5c\u5206\u5e03\uff0c\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u51b3\u7b56\u3002", "result": "\u5728\u9ad8\u5bc6\u5ea6\u573a\u666f\u4e0b\uff0cDiffusion-AC\u6210\u529f\u7387\u8fbe94.1%\uff0cNMACs\u53d1\u751f\u7387\u964d\u4f4e\u7ea659%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Diffusion-AC\u7684\u591a\u5cf0\u51b3\u7b56\u80fd\u529b\u4f7f\u5176\u80fd\u591f\u7075\u6d3b\u5207\u6362\u6709\u6548\u89c4\u907f\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u4e2d\u4ea4\u901a\u51b2\u7a81\u89e3\u51b3\u7684\u5b89\u5168\u6027\u4e0e\u6548\u7387\u3002"}}
{"id": "2509.03581", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03581", "abs": "https://arxiv.org/abs/2509.03581", "authors": ["Davide Paglieri", "Bart\u0142omiej Cupia\u0142", "Jonathan Cook", "Ulyana Piterbarg", "Jens Tuyls", "Edward Grefenstette", "Jakob Nicolaus Foerster", "Jack Parker-Holder", "Tim Rockt\u00e4schel"], "title": "Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents", "comment": null, "summary": "Training large language models (LLMs) to reason via reinforcement learning\n(RL) significantly improves their problem-solving capabilities. In agentic\nsettings, existing methods like ReAct prompt LLMs to explicitly plan before\nevery action; however, we demonstrate that always planning is computationally\nexpensive and degrades performance on long-horizon tasks, while never planning\nfurther limits performance. To address this, we introduce a conceptual\nframework formalizing dynamic planning for LLM agents, enabling them to\nflexibly decide when to allocate test-time compute for planning. We propose a\nsimple two-stage training pipeline: (1) supervised fine-tuning on diverse\nsynthetic data to prime models for dynamic planning, and (2) RL to refine this\ncapability in long-horizon environments. Experiments on the Crafter environment\nshow that dynamic planning agents trained with this approach are more\nsample-efficient and consistently achieve more complex objectives.\nAdditionally, we demonstrate that these agents can be effectively steered by\nhuman-written plans, surpassing their independent capabilities. To our\nknowledge, this work is the first to explore training LLM agents for dynamic\ntest-time compute allocation in sequential decision-making tasks, paving the\nway for more efficient, adaptive, and controllable agentic systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u5e8f\u5217\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684RL\u65b9\u6cd5\u8981\u4e48\u603b\u662f\u89c4\u5212\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u8981\u4e48\u4ece\u4e0d\u89c4\u5212\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a1. \u5728\u591a\u6837\u5316\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff1b2. \u5728\u957f\u65f6\u5e8f\u73af\u5883\u4e2d\u4f7f\u7528RL\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u52a8\u6001\u89c4\u5212\u667a\u80fd\u4f53\u6837\u672c\u6548\u7387\u66f4\u9ad8\uff0c\u80fd\u5b8c\u6210\u66f4\u590d\u6742\u7684\u76ee\u6807\uff0c\u5e76\u80fd\u6709\u6548\u5730\u54cd\u5e94\u4eba\u7c7b\u8ba1\u5212\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u63a2\u7d22\u4e86\u5728\u5e8f\u5217\u51b3\u7b56\u4efb\u52a1\u4e2d\u8bad\u7ec3LLM\u667a\u80fd\u4f53\u8fdb\u884c\u52a8\u6001\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u5206\u914d\uff0c\u4e3a\u66f4\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u548c\u53ef\u63a7\u7684\u667a\u80fd\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.03626", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03626", "abs": "https://arxiv.org/abs/2509.03626", "authors": ["Zahra Zehtabi Sabeti Moghaddam", "Zeinab Dehghani", "Maneeha Rani", "Koorosh Aslansefat", "Bhupesh Kumar Mishra", "Rameez Raja Kureshi", "Dhavalkumar Thakker"], "title": "Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE", "comment": null, "summary": "Generative AI, such as Large Language Models (LLMs), has achieved impressive\nprogress but still produces hallucinations and unverifiable claims, limiting\nreliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves\naccuracy by grounding outputs in external knowledge, especially in domains like\nhealthcare, where precision is vital. However, RAG remains opaque and\nessentially a black box, heavily dependent on data quality. We developed a\nmethod-agnostic, perturbation-based framework that provides token and\ncomponent-level interoperability for Graph RAG using SMILE and named it as\nKnowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing\nsimilarities, and training weighted linear surrogates, KG-SMILE identifies the\ngraph entities and relations most influential to generated outputs, thereby\nmaking RAG more transparent. We evaluate KG-SMILE using comprehensive\nattribution metrics, including fidelity, faithfulness, consistency, stability,\nand accuracy. Our findings show that KG-SMILE produces stable, human-aligned\nexplanations, demonstrating its capacity to balance model effectiveness with\ninterpretability and thereby fostering greater transparency and trust in\nmachine learning technologies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aKG-SMILE\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6a21\u578b\uff08RAG\uff09\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u5176\u5728\u533b\u7597\u7b49\u9886\u57df\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u5f0fAI\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u548c\u4e0d\u53ef\u9a8c\u8bc1\u7684\u65ad\u8a00\u95ee\u9898\uff0c\u8be5\u8bba\u6587\u65e8\u5728\u63d0\u9ad8RAG\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u4e0e\u65b9\u6cd5\u65e0\u5173\u7684\u3001\u57fa\u4e8e\u6270\u52a8\u7684\u6846\u67b6KG-SMILE\uff0c\u901a\u8fc7\u63a7\u5236\u6270\u52a8\u3001\u8ba1\u7b97\u76f8\u4f3c\u6027\u548c\u8bad\u7ec3\u52a0\u6743\u7ebf\u6027\u4ee3\u7406\u6a21\u578b\u6765\u8bc6\u522b\u5bf9\u751f\u6210\u8f93\u51fa\u5f71\u54cd\u6700\u5927\u7684\u56fe\u5b9e\u4f53\u548c\u5173\u7cfb\u3002", "result": "KG-SMILE\u80fd\u591f\u4ea7\u751f\u7a33\u5b9a\u4e14\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u4e00\u81f4\u7684\u89e3\u91ca\uff0c\u5728\u6a21\u578b\u6709\u6548\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "KG-SMILE\u63d0\u9ad8\u4e86RAG\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\uff0c\u589e\u5f3a\u4e86\u4eba\u4eec\u5bf9\u673a\u5668\u5b66\u4e60\u6280\u672f\u7684\u4fe1\u4efb\u3002"}}
{"id": "2509.03636", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03636", "abs": "https://arxiv.org/abs/2509.03636", "authors": ["Jacqueline Maasch", "John Kalantari", "Kia Khezeli"], "title": "CausalARC: Abstract Reasoning with Causal World Models", "comment": null, "summary": "Reasoning requires adaptation to novel problem settings under limited data\nand distribution shift. This work introduces CausalARC: an experimental testbed\nfor AI reasoning in low-data and out-of-distribution regimes, modeled after the\nAbstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is\nsampled from a fully specified causal world model, formally expressed as a\nstructural causal model. Principled data augmentations provide observational,\ninterventional, and counterfactual feedback about the world model in the form\nof few-shot, in-context learning demonstrations. As a proof-of-concept, we\nillustrate the use of CausalARC for four language model evaluation settings:\n(1) abstract reasoning with test-time training, (2) counterfactual reasoning\nwith in-context learning, (3) program synthesis, and (4) causal discovery with\nlogical reasoning.", "AI": {"tldr": "CausalARC:\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u5728\u4f4e\u6570\u636e\u548c\u5206\u5e03\u5916\u73af\u5883\u4e0b\u63a8\u7406\u80fd\u529b\u7684\u5b9e\u9a8c\u5e73\u53f0\uff0c\u5b83\u57fa\u4e8eARC\uff0c\u5e76\u4f7f\u7528\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u751f\u6210\u6837\u672c\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u4f4e\u6570\u636e\u548c\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "method": "\u6784\u5efaCausalARC\u5e73\u53f0\uff0c\u5229\u7528\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u751f\u6210\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u89c2\u5bdf\u6027\u3001\u5e72\u9884\u6027\u548c\u53cd\u4e8b\u5b9e\u6027\u53cd\u9988\u3002", "result": "\u5728\u56db\u4e2a\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff1a\u62bd\u8c61\u63a8\u7406\u3001\u53cd\u4e8b\u5b9e\u63a8\u7406\u3001\u7a0b\u5e8f\u5408\u6210\u548c\u56e0\u679c\u53d1\u73b0\u3002", "conclusion": "CausalARC\u4e3a\u8bc4\u4f30AI\u5728\u4f4e\u6570\u636e\u548c\u5206\u5e03\u5916\u73af\u5883\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2509.03644", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03644", "abs": "https://arxiv.org/abs/2509.03644", "authors": ["Fran\u00e7ois Olivier", "Zied Bouraoui"], "title": "Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations", "comment": "To appear in Proceedings of Machine Learning Research, 19th\n  Conference on Neurosymbolic Learning and Reasoning, 2025", "summary": "Despite significant progress in natural language understanding, Large\nLanguage Models (LLMs) remain error-prone when performing logical reasoning,\noften lacking the robust mental representations that enable human-like\ncomprehension. We introduce a prototype neurosymbolic system, Embodied-LM, that\ngrounds understanding and logical reasoning in schematic representations based\non image schemas-recurring patterns derived from sensorimotor experience that\nstructure human cognition. Our system operationalizes the spatial foundations\nof these cognitive structures using declarative spatial reasoning within Answer\nSet Programming. Through evaluation on logical deduction problems, we\ndemonstrate that LLMs can be guided to interpret scenarios through embodied\ncognitive structures, that these structures can be formalized as executable\nprograms, and that the resulting representations support effective logical\nreasoning with enhanced interpretability. While our current implementation\nfocuses on spatial primitives, it establishes the computational foundation for\nincorporating more complex and dynamic representations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edfEmbodied-LM\uff0c\u901a\u8fc7\u5c06\u56fe\u50cf\u6a21\u5f0f\u878d\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u589e\u5f3a\u5176\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684LLM\u5728\u903b\u8f91\u63a8\u7406\u65b9\u9762\u5bb9\u6613\u51fa\u9519\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u822c\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u8be5\u7cfb\u7edf\u4f7f\u7528\u57fa\u4e8e\u56fe\u50cf\u6a21\u5f0f\u7684\u56fe\u89e3\u8868\u793a\uff0c\u5e76\u5229\u7528Answer Set Programming\u8fdb\u884c\u58f0\u660e\u5f0f\u7a7a\u95f4\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEmbodied-LM\u80fd\u591f\u5f15\u5bfcLLM\u66f4\u597d\u5730\u7406\u89e3\u573a\u666f\uff0c\u5e76\u8fdb\u884c\u66f4\u6709\u6548\u7684\u903b\u8f91\u63a8\u7406\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "Embodied-LM\u4e3a\u5c06\u66f4\u590d\u6742\u548c\u52a8\u6001\u7684\u8868\u793a\u878d\u5165LLM\u5960\u5b9a\u4e86\u8ba1\u7b97\u57fa\u7840\u3002"}}
{"id": "2509.03646", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03646", "abs": "https://arxiv.org/abs/2509.03646", "authors": ["Haozhe Wang", "Qixin Xu", "Che Liu", "Junhong Wu", "Fangzhen Lin", "Wenhu Chen"], "title": "Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning", "comment": "Preprint", "summary": "Reinforcement Learning (RL) has proven highly effective at enhancing the\ncomplex reasoning abilities of Large Language Models (LLMs), yet underlying\nmechanisms driving this success remain largely opaque. Our analysis reveals\nthat puzzling phenomena like ``aha moments\", ``length-scaling'' and entropy\ndynamics are not disparate occurrences but hallmarks of an emergent reasoning\nhierarchy, akin to the separation of high-level strategic planning from\nlow-level procedural execution in human cognition. We uncover a compelling\ntwo-phase dynamic: initially, a model is constrained by procedural correctness\nand must improve its low-level skills. The learning bottleneck then decisively\nshifts, with performance gains being driven by the exploration and mastery of\nhigh-level strategic planning. This insight exposes a core inefficiency in\nprevailing RL algorithms like GRPO, which apply optimization pressure\nagnostically and dilute the learning signal across all tokens. To address this,\nwe propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that\nconcentrates optimization efforts on high-impact planning tokens. HICRA\nsignificantly outperforms strong baselines, demonstrating that focusing on this\nstrategic bottleneck is key to unlocking advanced reasoning. Furthermore, we\nvalidate semantic entropy as a superior compass for measuring strategic\nexploration over misleading metrics such as token-level entropy.", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u5176\u673a\u5236\u5728\u4e8e\u5206\u5c42\u63a8\u7406\uff1a\u5148\u63d0\u5347\u4f4e\u7ea7\u6280\u80fd\uff0c\u518d\u8f6c\u5411\u9ad8\u7ea7\u7b56\u7565\u89c4\u5212\u3002HICRA\u7b97\u6cd5\u901a\u8fc7\u5173\u6ce8\u9ad8\u7ea7\u89c4\u5212\u4ee4\u724c\u63d0\u5347\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u540e\u51fa\u73b0\u201caha moments\u201d\u548c\u201clength-scaling\u201d\u7b49\u73b0\u8c61\uff0c\u673a\u5236\u4e0d\u660e\u3002", "method": "\u5206\u6790\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u673a\u5236\uff0c\u63d0\u51faHICRA\u7b97\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u9ad8\u7ea7\u89c4\u5212\u4ee4\u724c\u7684\u4f18\u5316\u3002", "result": "HICRA\u7b97\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u7b97\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u8bed\u4e49\u71b5\u4f5c\u4e3a\u8861\u91cf\u7b56\u7565\u63a2\u7d22\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u9ad8\u7ea7\u7b56\u7565\u89c4\u5212\u662f\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u5173\u952e\u74f6\u9888\uff0c\u9488\u5bf9\u6027\u4f18\u5316\u7b56\u7565\u89c4\u5212\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2509.03649", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03649", "abs": "https://arxiv.org/abs/2509.03649", "authors": ["Davide Italo Serramazza", "Nikos Papadeas", "Zahraa Abdallah", "Georgiana Ifrim"], "title": "An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series Classification", "comment": null, "summary": "Explainable AI (XAI) has become an increasingly important topic for\nunderstanding and attributing the predictions made by complex Time Series\nClassification (TSC) models. Among attribution methods, SHapley Additive\nexPlanations (SHAP) is widely regarded as an excellent attribution method; but\nits computational complexity, which scales exponentially with the number of\nfeatures, limits its practicality for long time series. To address this, recent\nstudies have shown that aggregating features via segmentation, to compute a\nsingle attribution value for a group of consecutive time points, drastically\nreduces SHAP running time. However, the choice of the optimal segmentation\nstrategy remains an open question. In this work, we investigated eight\ndifferent Time Series Segmentation algorithms to understand how segment\ncompositions affect the explanation quality. We evaluate these approaches using\ntwo established XAI evaluation methodologies: InterpretTime and AUC Difference.\nThrough experiments on both Multivariate (MTS) and Univariate Time Series\n(UTS), we find that the number of segments has a greater impact on explanation\nquality than the specific segmentation method. Notably, equal-length\nsegmentation consistently outperforms most of the custom time series\nsegmentation algorithms. Furthermore, we introduce a novel attribution\nnormalisation technique that weights segments by their length and we show that\nit consistently improves attribution quality.", "AI": {"tldr": "\"\u672c\u6587\u7814\u7a76\u4e86\u65f6\u95f4\u5e8f\u5217\u5206\u5272\u7b97\u6cd5\u5bf9\u53ef\u89e3\u91caAI\u4e2dSHAP\u65b9\u6cd5\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7b49\u957f\u5206\u5272\u4f18\u4e8e\u5176\u4ed6\u81ea\u5b9a\u4e49\u7b97\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f52\u4e00\u5316\u6280\u672f\"", "motivation": "\"\u73b0\u6709SHAP\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u957f\u65f6\u5e8f\u6570\u636e\u4e2d\u7684\u5e94\u7528\uff0c\u672c\u6587\u65e8\u5728\u7814\u7a76\u6700\u4f18\u5206\u5272\u7b56\u7565\"", "method": "\"\u7814\u7a76\u4e86\u516b\u79cd\u65f6\u95f4\u5e8f\u5217\u5206\u5272\u7b97\u6cd5\uff0c\u4f7f\u7528InterpretTime\u548cAUC Difference\u4e24\u79cd\u65b9\u6cd5\u8bc4\u4f30\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f52\u4e00\u5316\u6280\u672f\"", "result": "\"\u7b49\u957f\u5206\u5272\u4f18\u4e8e\u5176\u4ed6\u7b97\u6cd5\uff0c\u5206\u5272\u6570\u91cf\u6bd4\u5177\u4f53\u7b97\u6cd5\u5bf9\u89e3\u91ca\u8d28\u91cf\u5f71\u54cd\u66f4\u5927\uff0c\u65b0\u7684\u5f52\u4e00\u5316\u6280\u672f\u63d0\u5347\u4e86\u5c5e\u6027\u8d28\u91cf\"", "conclusion": "\"\u7b49\u957f\u5206\u5272\u662f\u5904\u7406\u957f\u65f6\u5e8f\u6570\u636eSHAP\u89e3\u91ca\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u65b0\u7684\u5f52\u4e00\u5316\u6280\u672f\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u89e3\u91ca\u8d28\u91cf\""}}
{"id": "2509.03728", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.03728", "abs": "https://arxiv.org/abs/2509.03728", "authors": ["Wesley Hanwen Deng", "Sunnie S. Y. Kim", "Akshita Jha", "Ken Holstein", "Motahhare Eslami", "Lauren Wilcox", "Leon A Gatys"], "title": "PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming", "comment": null, "summary": "Recent developments in AI governance and safety research have called for\nred-teaming methods that can effectively surface potential risks posed by AI\nmodels. Many of these calls have emphasized how the identities and backgrounds\nof red-teamers can shape their red-teaming strategies, and thus the kinds of\nrisks they are likely to uncover. While automated red-teaming approaches\npromise to complement human red-teaming by enabling larger-scale exploration of\nmodel behavior, current approaches do not consider the role of identity. As an\ninitial step towards incorporating people's background and identities in\nautomated red-teaming, we develop and evaluate a novel method, PersonaTeaming,\nthat introduces personas in the adversarial prompt generation process to\nexplore a wider spectrum of adversarial strategies. In particular, we first\nintroduce a methodology for mutating prompts based on either \"red-teaming\nexpert\" personas or \"regular AI user\" personas. We then develop a dynamic\npersona-generating algorithm that automatically generates various persona types\nadaptive to different seed prompts. In addition, we develop a set of new\nmetrics to explicitly measure the \"mutation distance\" to complement existing\ndiversity measurements of adversarial prompts. Our experiments show promising\nimprovements (up to 144.1%) in the attack success rates of adversarial prompts\nthrough persona mutation, while maintaining prompt diversity, compared to\nRainbowPlus, a state-of-the-art automated red-teaming method. We discuss the\nstrengths and limitations of different persona types and mutation methods,\nshedding light on future opportunities to explore complementarities between\nautomated and human red-teaming approaches.", "AI": {"tldr": "PersonaTeaming\u65b9\u6cd5\u901a\u8fc7\u5728\u5bf9\u6297\u6027\u63d0\u793a\u751f\u6210\u8fc7\u7a0b\u4e2d\u5f15\u5165\u89d2\u8272\u6765\u6539\u8fdbAI\u6a21\u578b\u7684\u7ea2\u961f\u6d4b\u8bd5\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u7ea2\u961f\u65b9\u6cd5\u5ffd\u7565\u4e86\u8eab\u4efd\u5bf9\u7ea2\u961f\u7b56\u7565\u7684\u5f71\u54cd\uff0cPersonaTeaming\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u89d2\u8272\u6765\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\u3002", "method": "\u63d0\u51faPersonaTeaming\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u201c\u7ea2\u961f\u4e13\u5bb6\u201d\u6216\u201c\u666e\u901aAI\u7528\u6237\u201d\u89d2\u8272\u5bf9\u63d0\u793a\u8fdb\u884c\u53d8\u5f02\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u52a8\u6001\u89d2\u8272\u751f\u6210\u7b97\u6cd5\u548c\u65b0\u7684\u5ea6\u91cf\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0eRainbowPlus\u76f8\u6bd4\uff0cPersonaTeaming\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5bf9\u6297\u6027\u63d0\u793a\u7684\u653b\u51fb\u6210\u529f\u7387\uff08\u6700\u9ad8\u8fbe144.1%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63d0\u793a\u7684\u591a\u6837\u6027\u3002", "conclusion": "PersonaTeaming\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u7ea2\u961f\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u63a2\u7d22\u81ea\u52a8\u5316\u548c\u4eba\u5de5\u7ea2\u961f\u65b9\u6cd5\u7684\u4e92\u8865\u6027\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.03730", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.03730", "abs": "https://arxiv.org/abs/2509.03730", "authors": ["Pengrui Han", "Rafal Kocielnik", "Peiyang Song", "Ramit Debnath", "Dean Mobbs", "Anima Anandkumar", "R. Michael Alvarez"], "title": "The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs", "comment": "We make public all code and source data at\n  https://github.com/psychology-of-AI/Personality-Illusion", "summary": "Personality traits have long been studied as predictors of human\nbehavior.Recent advances in Large Language Models (LLMs) suggest similar\npatterns may emerge in artificial systems, with advanced LLMs displaying\nconsistent behavioral tendencies resembling human traits like agreeableness and\nself-regulation. Understanding these patterns is crucial, yet prior work\nprimarily relied on simplified self-reports and heuristic prompting, with\nlittle behavioral validation. In this study, we systematically characterize LLM\npersonality across three dimensions: (1) the dynamic emergence and evolution of\ntrait profiles throughout training stages; (2) the predictive validity of\nself-reported traits in behavioral tasks; and (3) the impact of targeted\ninterventions, such as persona injection, on both self-reports and behavior.\nOur findings reveal that instructional alignment (e.g., RLHF, instruction\ntuning) significantly stabilizes trait expression and strengthens trait\ncorrelations in ways that mirror human data. However, these self-reported\ntraits do not reliably predict behavior, and observed associations often\ndiverge from human patterns. While persona injection successfully steers\nself-reports in the intended direction, it exerts little or inconsistent effect\non actual behavior. By distinguishing surface-level trait expression from\nbehavioral consistency, our findings challenge assumptions about LLM\npersonality and underscore the need for deeper evaluation in alignment and\ninterpretability.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5c55\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u6027\u683c\u7279\u5f81\u7684\u884c\u4e3a\u503e\u5411\uff0c\u4f46\u5176\u81ea\u6211\u62a5\u544a\u7684\u6027\u683c\u7279\u5f81\u4e0e\u5b9e\u9645\u884c\u4e3a\u7684\u76f8\u5173\u6027\u8f83\u5f31\uff0c\u4e2a\u6027\u6ce8\u5165\u7b49\u5e72\u9884\u63aa\u65bd\u5bf9\u884c\u4e3a\u7684\u5f71\u54cd\u6709\u9650\u3002", "motivation": "\u7814\u7a76LLM\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6027\u683c\u7279\u5f81\u7684\u52a8\u6001\u53d8\u5316\u3001\u81ea\u6211\u62a5\u544a\u7279\u5f81\u7684\u9884\u6d4b\u6548\u5ea6\u4ee5\u53ca\u5e72\u9884\u63aa\u65bd\u7684\u5f71\u54cd\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u523b\u753bLLM\u7684\u6027\u683c\u7279\u5f81\uff0c\u5305\u62ec\u8bad\u7ec3\u9636\u6bb5\u7279\u5f81\u7684\u6f14\u53d8\u3001\u81ea\u6211\u62a5\u544a\u7279\u5f81\u7684\u9884\u6d4b\u6548\u5ea6\u4ee5\u53ca\u4e2a\u6027\u6ce8\u5165\u7b49\u5e72\u9884\u63aa\u65bd\u7684\u5f71\u54cd\u3002", "result": "\u6307\u4ee4\u5bf9\u9f50\uff08\u5982RLHF\uff09\u80fd\u7a33\u5b9aLLM\u7684\u6027\u683c\u8868\u8fbe\u5e76\u589e\u5f3a\u7279\u5f81\u76f8\u5173\u6027\uff0c\u4f46\u81ea\u6211\u62a5\u544a\u7684\u6027\u683c\u7279\u5f81\u4e0d\u80fd\u53ef\u9760\u9884\u6d4b\u884c\u4e3a\uff0c\u4e2a\u6027\u6ce8\u5165\u4e3b\u8981\u5f71\u54cd\u81ea\u6211\u62a5\u544a\u800c\u975e\u884c\u4e3a\u3002", "conclusion": "LLM\u7684\u6027\u683c\u8868\u73b0\u4e0e\u4eba\u7c7b\u5b58\u5728\u5dee\u5f02\uff0c\u9700\u8981\u66f4\u6df1\u5165\u5730\u8bc4\u4f30LLM\u7684\u5bf9\u9f50\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.03736", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03736", "abs": "https://arxiv.org/abs/2509.03736", "authors": ["James Mooney", "Josef Woldense", "Zheng Robert Jia", "Shirley Anugrah Hayati", "My Ha Nguyen", "Vipul Raheja", "Dongyeop Kang"], "title": "Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation", "comment": "25 pages, 9 figures, 7 tables", "summary": "The impressive capabilities of Large Language Models (LLMs) have fueled the\nnotion that synthetic agents can serve as substitutes for real participants in\nhuman-subject research. In an effort to evaluate the merits of this claim,\nsocial science researchers have largely focused on whether LLM-generated survey\ndata corresponds to that of a human counterpart whom the LLM is prompted to\nrepresent. In contrast, we address a more fundamental question: Do agents\nmaintain internal consistency, retaining similar behaviors when examined under\ndifferent experimental settings? To this end, we develop a study designed to\n(a) reveal the agent's internal state and (b) examine agent behavior in a basic\ndialogue setting. This design enables us to explore a set of behavioral\nhypotheses to assess whether an agent's conversation behavior is consistent\nwith what we would expect from their revealed internal state. Our findings on\nthese hypotheses show significant internal inconsistencies in LLMs across model\nfamilies and at differing model sizes. Most importantly, we find that, although\nagents may generate responses matching those of their human counterparts, they\nfail to be internally consistent, representing a critical gap in their\ncapabilities to accurately substitute for real participants in human-subject\nresearch. Our simulation code and data are publicly accessible.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u751f\u6210\u7684\u4ee3\u7406\u662f\u5426\u53ef\u4ee5\u66ff\u4ee3\u4eba\u7c7b\u53c2\u4e0e\u8005\uff1f\u7814\u7a76\u8868\u660e\uff0cLLM\u4ee3\u7406\u5728\u4e0d\u540c\u5b9e\u9a8c\u73af\u5883\u4e0b\u884c\u4e3a\u4e0d\u4e00\u81f4\uff0c\u65e0\u6cd5\u51c6\u786e\u66ff\u4ee3\u4eba\u7c7b\u53c2\u4e0e\u8005\u3002", "motivation": "\u8bc4\u4f30LLM\u4ee3\u7406\u662f\u5426\u53ef\u4ee5\u66ff\u4ee3\u4eba\u7c7b\u53c2\u4e0e\u8005\u8fdb\u884c\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u3002", "method": "\u8bbe\u8ba1\u5b9e\u9a8c\uff0c\u63ed\u793aLLM\u4ee3\u7406\u7684\u5185\u90e8\u72b6\u6001\uff0c\u5e76\u5728\u57fa\u672c\u5bf9\u8bdd\u73af\u5883\u4e2d\u68c0\u67e5\u5176\u884c\u4e3a\uff0c\u9a8c\u8bc1\u884c\u4e3a\u5047\u8bbe\u3002", "result": "\u53d1\u73b0LLM\u4ee3\u7406\u5728\u4e0d\u540c\u6a21\u578b\u7cfb\u5217\u548c\u5927\u5c0f\u4e0a\u5b58\u5728\u663e\u8457\u7684\u5185\u90e8\u4e0d\u4e00\u81f4\u6027\uff0c\u867d\u7136\u5b83\u4eec\u751f\u6210\u7684\u56de\u5e94\u53ef\u80fd\u4e0e\u4eba\u7c7b\u76f8\u4f3c\uff0c\u4f46\u5176\u5185\u90e8\u884c\u4e3a\u4e0d\u4e00\u81f4\uff0c\u65e0\u6cd5\u66ff\u4ee3\u4eba\u7c7b\u53c2\u4e0e\u8005\u3002", "conclusion": "LLM\u4ee3\u7406\u5728\u5185\u90e8\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u5173\u952e\u5dee\u8ddd\uff0c\u4e0d\u80fd\u51c6\u786e\u66ff\u4ee3\u4eba\u7c7b\u53c2\u4e0e\u8005\u8fdb\u884c\u7814\u7a76\u3002"}}
{"id": "2509.03768", "categories": ["cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.03768", "abs": "https://arxiv.org/abs/2509.03768", "authors": ["Connor Walker", "Koorosh Aslansefat", "Mohammad Naveed Akram", "Yiannis Papadopoulos"], "title": "RAGuard: A Novel Approach for in-context Safe Retrieval Augmented Generation for LLMs", "comment": null, "summary": "Accuracy and safety are paramount in Offshore Wind (OSW) maintenance, yet\nconventional Large Language Models (LLMs) often fail when confronted with\nhighly specialised or unexpected scenarios. We introduce RAGuard, an enhanced\nRetrieval-Augmented Generation (RAG) framework that explicitly integrates\nsafety-critical documents alongside technical manuals.By issuing parallel\nqueries to two indices and allocating separate retrieval budgets for knowledge\nand safety, RAGuard guarantees both technical depth and safety coverage. We\nfurther develop a SafetyClamp extension that fetches a larger candidate pool,\n\"hard-clamping\" exact slot guarantees to safety. We evaluate across sparse\n(BM25), dense (Dense Passage Retrieval) and hybrid retrieval paradigms,\nmeasuring Technical Recall@K and Safety Recall@K. Both proposed extensions of\nRAG show an increase in Safety Recall@K from almost 0\\% in RAG to more than\n50\\% in RAGuard, while maintaining Technical Recall above 60\\%. These results\ndemonstrate that RAGuard and SafetyClamp have the potential to establish a new\nstandard for integrating safety assurance into LLM-powered decision support in\ncritical maintenance contexts.", "AI": {"tldr": "RAGuard\u6846\u67b6\u901a\u8fc7\u6574\u5408\u5b89\u5168\u5173\u952e\u6587\u6863\u548c\u6280\u672f\u624b\u518c\uff0c\u589e\u5f3a\u4e86RAG\u6a21\u578b\u5728\u6d77\u4e0a\u98ce\u7535\u7ef4\u62a4\u4e2d\u7684\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709LLM\u5728\u4e13\u4e1a\u6216\u610f\u5916\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faRAGuard\u6846\u67b6\uff0c\u5e76\u884c\u67e5\u8be2\u77e5\u8bc6\u548c\u5b89\u5168\u7d22\u5f15\uff0c\u4f7f\u7528SafetyClamp\u6269\u5c55\u786e\u4fdd\u5b89\u5168\u8986\u76d6\u3002", "result": "RAGuard\u548cSafetyClamp\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u53ec\u56de\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u6280\u672f\u53ec\u56de\u7387\u3002", "conclusion": "RAGuard\u548cSafetyClamp\u4e3a\u5173\u952e\u7ef4\u62a4\u573a\u666f\u4e2d\u57fa\u4e8eLLM\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u589e\u52a0\u4e86\u5b89\u5168\u4fdd\u969c\uff0c\u5177\u6709\u5efa\u7acb\u65b0\u6807\u51c6\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.03811", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03811", "abs": "https://arxiv.org/abs/2509.03811", "authors": ["Yongzhi Qi", "Jiaheng Yin", "Jianshen Zhang", "Dongyang Geng", "Zhengyu Chen", "Hao Hu", "Wei Qi", "Zuo-Jun Max Shen"], "title": "Leveraging LLM-Based Agents for Intelligent Supply Chain Planning", "comment": null, "summary": "In supply chain management, planning is a critical concept. The movement of\nphysical products across different categories, from suppliers to warehouse\nmanagement, to sales, and logistics transporting them to customers, entails the\ninvolvement of many entities. It covers various aspects such as demand\nforecasting, inventory management, sales operations, and replenishment. How to\ncollect relevant data from an e-commerce platform's perspective, formulate\nlong-term plans, and dynamically adjust them based on environmental changes,\nwhile ensuring interpretability, efficiency, and reliability, is a practical\nand challenging problem. In recent years, the development of AI technologies,\nespecially the rapid progress of large language models, has provided new tools\nto address real-world issues. In this work, we construct a Supply Chain\nPlanning Agent (SCPA) framework that can understand domain knowledge,\ncomprehend the operator's needs, decompose tasks, leverage or create new tools,\nand return evidence-based planning reports. We deploy this framework in\nJD.com's real-world scenario, demonstrating the feasibility of LLM-agent\napplications in the supply chain. It effectively reduced labor and improved\naccuracy, stock availability, and other key metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f9b\u5e94\u94fe\u8ba1\u5212\u4ee3\u7406\uff08SCPA\uff09\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63d0\u9ad8\u4e86\u4eac\u4e1c\u4f9b\u5e94\u94fe\u8ba1\u5212\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u4e86\u4eba\u5de5\u6210\u672c\u5e76\u6539\u5584\u4e86\u5173\u952e\u6307\u6807\u3002", "motivation": "\u89e3\u51b3\u7535\u5546\u5e73\u53f0\u4f9b\u5e94\u94fe\u8ba1\u5212\u4e2d\u6570\u636e\u6536\u96c6\u3001\u957f\u671f\u89c4\u5212\u548c\u52a8\u6001\u8c03\u6574\u7684\u96be\u9898\uff0c\u5728\u4fdd\u8bc1\u53ef\u89e3\u91ca\u6027\u3001\u6548\u7387\u548c\u53ef\u9760\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5229\u7528AI\u6280\u672f\u4f18\u5316\u4f9b\u5e94\u94fe\u7ba1\u7406\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4f9b\u5e94\u94fe\u8ba1\u5212\u4ee3\u7406\uff08SCPA\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u7406\u89e3\u9886\u57df\u77e5\u8bc6\u3001\u7406\u89e3\u64cd\u4f5c\u5458\u7684\u9700\u6c42\u3001\u5206\u89e3\u4efb\u52a1\u3001\u5229\u7528\u6216\u521b\u5efa\u65b0\u7684\u5de5\u5177\uff0c\u5e76\u8fd4\u56de\u57fa\u4e8e\u8bc1\u636e\u7684\u8ba1\u5212\u62a5\u544a\u3002\u5e76\u5728\u4eac\u4e1c\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u90e8\u7f72\u3002", "result": "\u8be5\u6846\u67b6\u6709\u6548\u964d\u4f4e\u4e86\u4eba\u5de5\u6210\u672c\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u6539\u5584\u4e86\u5e93\u5b58\u53ef\u7528\u6027\u548c\u5176\u4ed6\u5173\u952e\u6307\u6807\u3002", "conclusion": "LLM\u4ee3\u7406\u5728\u4f9b\u5e94\u94fe\u7ba1\u7406\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2509.03817", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.03817", "abs": "https://arxiv.org/abs/2509.03817", "authors": ["Wei Yang", "Jesse Thomason"], "title": "Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning", "comment": null, "summary": "Multi-agent systems of large language models (LLMs) show promise for complex\nreasoning, but their effectiveness is often limited by fixed collaboration\nprotocols. These frameworks typically focus on macro-level orchestration while\noverlooking agents' internal deliberative capabilities. This critical\nmeta-cognitive blindspot treats agents as passive executors unable to adapt\ntheir strategy based on internal cognitive states like uncertainty or\nconfidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where\nagents learn a decentralized policy over a set of high-level meta-cognitive\nactions: Persist, Refine, and Concede. To overcome the instability of\ntraditional policy gradients in this setting, we develop SoftRankPO, a novel\nreinforcement learning algorithm. SoftRankPO stabilizes training by shaping\nadvantages based on the rank of rewards mapped through smooth normal quantiles,\nmaking the learning process robust to reward variance. Experiments show that\nMPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across\nfive mathematical and general reasoning benchmarks compared to six\nstate-of-the-art heuristic and learning-based multi-agent reasoning algorithms.\nOur work presents a paradigm for learning adaptive, meta-cognitive policies for\nmulti-agent LLM systems, shifting the focus from designing fixed protocols to\nlearning dynamic, deliberative strategies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5143\u7b56\u7565\u534f\u5546\u6846\u67b6(MPDF)\u548c\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5SoftRankPO\uff0c\u7528\u4e8e\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e864-5%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u534f\u4f5c\u534f\u8bae\u56fa\u5b9a\uff0c\u5ffd\u7565\u4e86\u667a\u80fd\u4f53\u7684\u5185\u7701\u80fd\u529b\uff0c\u8be5\u8bba\u6587\u65e8\u5728\u63d0\u5347\u667a\u80fd\u4f53\u6839\u636e\u81ea\u8eab\u8ba4\u77e5\u72b6\u6001\uff08\u5982\u4e0d\u786e\u5b9a\u6027\u6216\u7f6e\u4fe1\u5ea6\uff09\u81ea\u9002\u5e94\u8c03\u6574\u7b56\u7565\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5143\u7b56\u7565\u534f\u5546\u6846\u67b6(MPDF)\uff0c\u5176\u4e2d\u667a\u80fd\u4f53\u5b66\u4e60\u4e00\u7ec4\u5143\u8ba4\u77e5\u52a8\u4f5c\uff08\u575a\u6301\u3001\u7ec6\u5316\u3001\u8ba9\u6b65\uff09\u7684\u53bb\u4e2d\u5fc3\u5316\u7b56\u7565\uff1b\u5f00\u53d1SoftRankPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u5956\u52b1\u6392\u5e8f\u7684\u4f18\u52bf\u5851\u9020\u6765\u7a33\u5b9a\u7b56\u7565\u68af\u5ea6\u8bad\u7ec3\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u5b66\u548c\u4e00\u822c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e0e\u73b0\u6709\u516d\u79cd\u7b97\u6cd5\u76f8\u6bd4\uff0cMPDF\u7ed3\u5408SoftRankPO\u53d6\u5f97\u4e864-5%\u7684\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b66\u4e60\u5927\u578b\u8bed\u8a00\u6a21\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u81ea\u9002\u5e94\u5143\u8ba4\u77e5\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u79cd\u8303\u4f8b\uff0c\u4ece\u8bbe\u8ba1\u56fa\u5b9a\u534f\u8bae\u8f6c\u5411\u5b66\u4e60\u52a8\u6001\u7684\u3001\u6709\u601d\u8003\u7684\u7b56\u7565\u3002"}}
{"id": "2509.03827", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03827", "abs": "https://arxiv.org/abs/2509.03827", "authors": ["Pierre Le Coz", "Jia An Liu", "Debarun Bhattacharjya", "Georgina Curto", "Serge Stinckwich"], "title": "What Would an LLM Do? Evaluating Policymaking Capabilities of Large Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly being adopted in high-stakes\ndomains. Their capacity to process vast amounts of unstructured data, explore\nflexible scenarios, and handle a diversity of contextual factors can make them\nuniquely suited to provide new insights for the complexity of social\npolicymaking. This article evaluates whether LLMs' are aligned with domain\nexperts (and among themselves) to inform social policymaking on the subject of\nhomelessness alleviation - a challenge affecting over 150 million people\nworldwide. We develop a novel benchmark comprised of decision scenarios with\npolicy choices across four geographies (South Bend, USA; Barcelona, Spain;\nJohannesburg, South Africa; Macau SAR, China). The policies in scope are\ngrounded in the conceptual framework of the Capability Approach for human\ndevelopment. We also present an automated pipeline that connects the\nbenchmarked policies to an agent-based model, and we explore the social impact\nof the recommended policies through simulated social scenarios. The paper\nresults reveal promising potential to leverage LLMs for social policy making.\nIf responsible guardrails and contextual calibrations are introduced in\ncollaboration with local domain experts, LLMs can provide humans with valuable\ninsights, in the form of alternative policies at scale.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u5e2e\u52a9\u89e3\u51b3\u5168\u7403\u6027\u95ee\u9898\uff0c\u4f8b\u5982\u65e0\u5bb6\u53ef\u5f52\u8005\u95ee\u9898\u4e0a\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LLM\u3001\u60c5\u666f\u6a21\u62df\u548c\u57fa\u4e8e\u4e3b\u4f53\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u793e\u4f1a\u653f\u7b56\u5236\u5b9a(\u4ee5\u89e3\u51b3\u65e0\u5bb6\u53ef\u5f52\u95ee\u9898\u4e3a\u4f8b)\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u63a2\u7d22\u5176\u4e0e\u9886\u57df\u4e13\u5bb6\u7684\u610f\u89c1\u4e00\u81f4\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u8de8\u56db\u4e2a\u5730\u533a(\u5357\u672c\u5fb7\u3001\u5df4\u585e\u7f57\u90a3\u3001\u7ea6\u7ff0\u5185\u65af\u5821\u548c\u6fb3\u95e8)\u653f\u7b56\u9009\u62e9\u7684\u51b3\u7b56\u60c5\u666f\u57fa\u51c6\uff0c\u5e76\u5c06\u57fa\u51c6\u653f\u7b56\u4e0e\u57fa\u4e8e\u4e3b\u4f53\u7684\u6a21\u578b\u8fde\u63a5\u8d77\u6765\uff0c\u4ee5\u6a21\u62df\u63a8\u8350\u653f\u7b56\u7684\u793e\u4f1a\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5728\u8d1f\u8d23\u4efb\u7684\u4fdd\u969c\u548c\u4e0e\u5f53\u5730\u9886\u57df\u4e13\u5bb6\u7684\u5408\u4f5c\u4e0b\uff0cLLM\u53ef\u4ee5\u4e3a\u4eba\u7c7b\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u66ff\u4ee3\u653f\u7b56\u89c1\u89e3\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6709\u6f5c\u529b\u8f85\u52a9\u793e\u4f1a\u653f\u7b56\u5236\u5b9a\uff0c\u4f46\u9700\u8981\u4e0e\u9886\u57df\u4e13\u5bb6\u5408\u4f5c\uff0c\u5e76\u8bbe\u7f6e\u76f8\u5e94\u7684\u4fdd\u969c\u63aa\u65bd\u3002"}}
