<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AI-Powered Math Tutoring: Platform for Personalized and Adaptive Education](https://arxiv.org/abs/2507.12484)
*Jarosław A. Chudziak,Adam Kostka*

Main category: cs.AI

TL;DR: 开发了一个新的AI数学辅导平台，提供自适应、个性化和工具辅助的学习体验。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅导系统存在局限性，往往直接提供答案，缺乏深度思考和结构化的教学工具和策略，尤其是在数学领域。

Method: 该研究介绍了一个新颖的多智能体AI辅导平台，结合自适应和个性化反馈、结构化课程生成和教科书知识检索。

Result: 该系统允许学生学习新主题，识别和解决自身的弱点，有效地复习考试，并练习无限数量的个性化练习。

Conclusion: 本文介绍了一个新的多智能体AI辅导平台，该平台结合了自适应和个性化反馈、结构化课程生成和教科书知识检索，从而实现模块化、工具辅助的学习过程。

Abstract: The growing ubiquity of artificial intelligence (AI), in particular large
language models (LLMs), has profoundly altered the way in which learners gain
knowledge and interact with learning material, with many claiming that AI
positively influences their learning achievements. Despite this advancement,
current AI tutoring systems face limitations associated with their reactive
nature, often providing direct answers without encouraging deep reflection or
incorporating structured pedagogical tools and strategies. This limitation is
most apparent in the field of mathematics, in which AI tutoring systems remain
underdeveloped. This research addresses the question: How can AI tutoring
systems move beyond providing reactive assistance to enable structured,
individualized, and tool-assisted learning experiences? We introduce a novel
multi-agent AI tutoring platform that combines adaptive and personalized
feedback, structured course generation, and textbook knowledge retrieval to
enable modular, tool-assisted learning processes. This system allows students
to learn new topics while identifying and targeting their weaknesses, revise
for exams effectively, and practice on an unlimited number of personalized
exercises. This article contributes to the field of artificial intelligence in
education by introducing a novel platform that brings together pedagogical
agents and AI-driven components, augmenting the field with modular and
effective systems for teaching mathematics.

</details>


### [2] [MR-LDM -- The Merge-Reactive Longitudinal Decision Model: Game Theoretic Human Decision Modeling for Interactive Sim Agents](https://arxiv.org/abs/2507.12494)
*Dustin Holley,Jovin D'sa,Hossein Nourkhiz Mahjoub,Gibran Ali*

Main category: cs.AI

TL;DR: 提出一种改进的高速公路合并场景模拟博弈论模型，具有高效计算能力，可用于大规模自动驾驶模拟。


<details>
  <summary>Details</summary>
Motivation: 为了改进自动驾驶汽车技术开发中对真实世界驾驶员行为的模拟，特别是高速公路合并场景下的战术决策建模，该模型旨在提高模拟的真实性和可解释性。

Method: 提出了一种博弈论模型，用于改进高速公路合并场景的模拟，该模型具有改进的效用函数和滞后行为，并结合了底层动力学模型。

Result: 该模型在真实数据集上验证了其对复杂交互的良好重现性，并被集成到高保真模拟环境中，具有足够的计算效率，可用于大规模模拟以支持自动驾驶汽车的开发。

Conclusion: 该模型在真实数据集上验证了其对复杂交互的良好重现性，并被集成到高保真模拟环境中，具有足够的计算效率，可用于大规模模拟以支持自动驾驶汽车的开发。

Abstract: Enhancing simulation environments to replicate real-world driver behavior,
i.e., more humanlike sim agents, is essential for developing autonomous vehicle
technology. In the context of highway merging, previous works have studied the
operational-level yielding dynamics of lag vehicles in response to a merging
car at highway on-ramps. Other works focusing on tactical decision modeling
generally consider limited action sets or utilize payoff functions with large
parameter sets and limited payoff bounds. In this work, we aim to improve the
simulation of the highway merge scenario by targeting a game theoretic model
for tactical decision-making with improved payoff functions and lag actions. We
couple this with an underlying dynamics model to have a unified decision and
dynamics model that can capture merging interactions and simulate more
realistic interactions in an explainable and interpretable fashion. The
proposed model demonstrated good reproducibility of complex interactions when
validated on a real-world dataset. The model was finally integrated into a high
fidelity simulation environment and confirmed to have adequate computation time
efficiency for use in large-scale simulations to support autonomous vehicle
development.

</details>


### [3] [A Survey of Explainable Reinforcement Learning: Targets, Methods and Needs](https://arxiv.org/abs/2507.12599)
*Léo Saulières*

Main category: cs.AI

TL;DR: 对可解释强化学习(XRL)的250多篇论文进行了综述，提出了新的分类方法，并确定了该领域未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 解释强化学习模型的内部机制和行为，提高AI模型的可解释性。

Method: 文献综述和分类法

Result: 对XRL领域进行了全面综述，提出了一个新的分类方法，并指出了未来研究方向。

Conclusion: 本文综述了可解释强化学习（XRL）领域250余篇论文，提出了一种基于“是什么”和“如何”两个问题的直观分类法，对现有方法进行了分类和总结，并指出了该领域未来发展方向和需要关注的领域。

Abstract: The success of recent Artificial Intelligence (AI) models has been
accompanied by the opacity of their internal mechanisms, due notably to the use
of deep neural networks. In order to understand these internal mechanisms and
explain the output of these AI models, a set of methods have been proposed,
grouped under the domain of eXplainable AI (XAI). This paper focuses on a
sub-domain of XAI, called eXplainable Reinforcement Learning (XRL), which aims
to explain the actions of an agent that has learned by reinforcement learning.
We propose an intuitive taxonomy based on two questions "What" and "How". The
first question focuses on the target that the method explains, while the second
relates to the way the explanation is provided. We use this taxonomy to provide
a state-of-the-art review of over 250 papers. In addition, we present a set of
domains close to XRL, which we believe should get attention from the community.
Finally, we identify some needs for the field of XRL.

</details>


### [4] [Fly, Fail, Fix: Iterative Game Repair with Reinforcement Learning and Large Multimodal Models](https://arxiv.org/abs/2507.12666)
*Alex Zook,Josef Spjut,Jonathan Tremblay*

Main category: cs.AI

TL;DR: 利用强化学习和大型多模态模型迭代改进游戏设计，实现AI辅助游戏设计。


<details>
  <summary>Details</summary>
Motivation: 现代生成系统难以捕捉静态规则和内容如何转化为动态玩家行为，该框架旨在弥合这一差距。

Method: 将强化学习智能体与大型多模态模型相结合，通过智能体玩游戏，模型分析游戏行为轨迹并修改游戏配置，迭代改进游戏设计。

Result: LMM能够根据RL智能体提供的行为轨迹迭代改进游戏机制。

Conclusion: 大型多模态模型(LMM)可以根据强化学习(RL)智能体的行为轨迹迭代改进游戏机制，为AI辅助游戏设计提供实用且可扩展的工具。

Abstract: Game design hinges on understanding how static rules and content translate
into dynamic player behavior - something modern generative systems that inspect
only a game's code or assets struggle to capture. We present an automated
design iteration framework that closes this gap by pairing a reinforcement
learning (RL) agent, which playtests the game, with a large multimodal model
(LMM), which revises the game based on what the agent does. In each loop the RL
player completes several episodes, producing (i) numerical play metrics and/or
(ii) a compact image strip summarising recent video frames. The LMM designer
receives a gameplay goal and the current game configuration, analyses the play
traces, and edits the configuration to steer future behaviour toward the goal.
We demonstrate results that LMMs can reason over behavioral traces supplied by
RL agents to iteratively refine game mechanics, pointing toward practical,
scalable tools for AI-assisted game design.

</details>


### [5] [Benchmarking Deception Probes via Black-to-White Performance Boosts](https://arxiv.org/abs/2507.12691)
*Avi Parrack,Carlo Leonardo Attubato,Stefan Heimersheim*

Main category: cs.AI

TL;DR: 现有 AI 欺骗检测方法效果不佳


<details>
  <summary>Details</summary>
Motivation: 评估现有欺骗探测器的有效性和对简单对抗策略的鲁棒性

Method: 对比白盒监控和黑盒监控，评估现有欺骗探测器的性能

Result: 白盒监控优于黑盒监控，但提升有限

Conclusion: 现有检测 AI 助手欺骗行为的探测器效果有限，白盒监控略优于黑盒监控，但提升幅度不大

Abstract: AI assistants will occasionally respond deceptively to user queries.
Recently, linear classifiers (called "deception probes") have been trained to
distinguish the internal activations of a language model during deceptive
versus honest responses. However, it's unclear how effective these probes are
at detecting deception in practice, nor whether such probes are resistant to
simple counter strategies from a deceptive assistant who wishes to evade
detection. In this paper, we compare white-box monitoring (where the monitor
has access to token-level probe activations) to black-box monitoring (without
such access). We benchmark deception probes by the extent to which the white
box monitor outperforms the black-box monitor, i.e. the black-to-white
performance boost. We find weak but encouraging black-to-white performance
boosts from existing deception probes.

</details>


### [6] [Imitating Mistakes in a Learning Companion AI Agent for Online Peer Learning](https://arxiv.org/abs/2507.12801)
*Sosui Moribe,Taketoshi Ushiama*

Main category: cs.AI

TL;DR: AI同伴学习系统，专注相同能力水平同伴的错误学习，以英语写作为例。


<details>
  <summary>Details</summary>
Motivation: 近年来，同伴学习作为一种促进学习者自发性思维的方法受到了关注，其有效性已得到大量研究证实。然而，人与人之间的同伴学习存在局限性，并非总是有效。有效的同伴学习需要能力水平相当的同伴。

Method: 本研究关注英语写作，假设与学习者能力水平相同的同伴会犯同样的错误。

Result: 本研究通过英语作文的例子验证了这种方法的可行性。

Conclusion: 这项研究旨在开发一个AI代理作为学习伙伴，随时随地实现同伴学习。

Abstract: In recent years, peer learning has gained attention as a method that promotes
spontaneous thinking among learners, and its effectiveness has been confirmed
by numerous studies. This study aims to develop an AI Agent as a learning
companion that enables peer learning anytime and anywhere. However, peer
learning between humans has various limitations, and it is not always
effective. Effective peer learning requires companions at the same proficiency
levels. In this study, we assume that a learner's peers with the same
proficiency level as the learner make the same mistakes as the learner does and
focus on English composition as a specific example to validate this approach.

</details>


### [7] [MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models](https://arxiv.org/abs/2507.12806)
*Zhiwei Liu,Jielin Qiu,Shiyu Wang,Jianguo Zhang,Zuxin Liu,Roshan Ram,Haolin Chen,Weiran Yao,Huan Wang,Shelby Heinecke,Silvio Savarese,Caiming Xiong*

Main category: cs.AI

TL;DR: MCPEval是一个用于自动评估LLM智能体的开放源码框架，它标准化了指标，并能与原生工具无缝集成，提高了评估效率和可重复性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法依赖于静态基准和劳动密集型数据收集，限制了实际评估。

Method: 提出了一种基于模型上下文协议（MCP）的开放源码框架MCPEval，用于自动化端到端任务生成和对不同领域的LLM智能体的深度评估。

Result: 实证结果表明，该框架能够有效地揭示细微的、特定领域的性能。

Conclusion: 介绍了一种名为MCPEval的开放源码框架，用于自动评估大型语言模型（LLM）智能体的端到端任务生成和深度评估，该框架标准化了指标，并能与原生智能体工具无缝集成，消除了构建评估流程的人工工作。

Abstract: The rapid rise of Large Language Models (LLMs)-based intelligent agents
underscores the need for robust, scalable evaluation frameworks. Existing
methods rely on static benchmarks and labor-intensive data collection, limiting
practical assessment. We introduce \oursystemname, an open-source Model Context
Protocol (MCP)-based framework that automates end-to-end task generation and
deep evaluation of LLM agents across diverse domains. MCPEval standardizes
metrics, seamlessly integrates with native agent tools, and eliminates manual
effort in building evaluation pipelines. Empirical results across five
real-world domains show its effectiveness in revealing nuanced, domain-specific
performance. We publicly release MCPEval
https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and
standardized LLM agent evaluation.

</details>


### [8] [Emotional Support with LLM-based Empathetic Dialogue Generation](https://arxiv.org/abs/2507.12820)
*Shiquan Wang,Ruiyu Fang,Zhongjiang He,Shuangyong Song,Yongxiang Li*

Main category: cs.AI

TL;DR: 利用LLM和有效的自适应方法，在情感支持对话任务中取得了第二名的成绩。


<details>
  <summary>Details</summary>
Motivation: 为了满足日益增长的精神健康支持需求，提供有效的情感帮助。

Method: 利用大型语言模型，结合参数高效的低秩自适应和全参数微调策略。

Result: 该模型在NLPCC 2025 Task 8竞赛中获得第二名。

Conclusion: 该论文提出了一种结合大型语言模型、提示工程和微调技术的解决方案，用于情感支持对话任务，该模型在NLPCC 2025 Task 8竞赛中获得第二名。

Abstract: Emotional Support Conversation (ESC) aims to provide empathetic and effective
emotional assistance through dialogue, addressing the growing demand for mental
health support. This paper presents our solution for the NLPCC 2025 Task 8 ESC
evaluation, where we leverage large-scale language models enhanced by prompt
engineering and finetuning techniques. We explore both parameter-efficient
Low-Rank Adaptation and full-parameter fine-tuning strategies to improve the
model's ability to generate supportive and contextually appropriate responses.
Our best model ranked second in the competition, highlighting the potential of
combining LLMs with effective adaptation methods for ESC tasks. Future work
will focus on further enhancing emotional understanding and response
personalization to build more practical and reliable emotional support systems.

</details>


### [9] [Assessing adaptive world models in machines with novel games](https://arxiv.org/abs/2507.12821)
*Lance Ying,Katherine M. Collins,Prafull Sharma,Cedric Colas,Kaiya Ivy Zhao,Adrian Weller,Zenna Tavares,Phillip Isola,Samuel J. Gershman,Jacob D. Andreas,Thomas L. Griffiths,Francois Chollet,Kelsey R. Allen,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 论文批判了现有AI对world model评估的不足，并提出了一种新的基于“新颖游戏”的评估框架，以促进AI系统具备类人的快速适应和鲁棒泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI对world model的理解和评估存在局限性，未能捕捉人类高效学习和适应能力的本质。

Method: 提出了一种新的基于精心设计的游戏套件的评估范式，这些游戏具有真正的、深刻的和持续更新的新颖性，用于评估AI系统快速构建和改进内部环境表征的能力。

Result: 提出了一种新的评估world model的框架，该框架基于一系列精心设计的游戏，这些游戏具有持续变化的游戏结构，并提出相应的评估指标。

Conclusion: 该论文认为，人工智能领域对world model的评估过于狭隘，缺乏对模型在陌生环境中学习效率和有效性的考量，并提出了一种基于“新颖游戏”的新评估框架。

Abstract: Human intelligence exhibits a remarkable capacity for rapid adaptation and
effective problem-solving in novel and unfamiliar contexts. We argue that this
profound adaptability is fundamentally linked to the efficient construction and
refinement of internal representations of the environment, commonly referred to
as world models, and we refer to this adaptation mechanism as world model
induction. However, current understanding and evaluation of world models in
artificial intelligence (AI) remains narrow, often focusing on static
representations learned from training on a massive corpora of data, instead of
the efficiency and efficacy of models in learning these representations through
interaction and exploration within a novel environment. In this Perspective, we
provide a view of world model induction drawing on decades of research in
cognitive science on how humans learn and adapt so efficiently; we then call
for a new evaluation framework for assessing adaptive world models in AI.
Concretely, we propose a new benchmarking paradigm based on suites of carefully
designed games with genuine, deep and continually refreshing novelty in the
underlying game structures -- we refer to this kind of games as novel games. We
detail key desiderata for constructing these games and propose appropriate
metrics to explicitly challenge and evaluate the agent's ability for rapid
world model induction. We hope that this new evaluation framework will inspire
future evaluation efforts on world models in AI and provide a crucial step
towards developing AI systems capable of the human-like rapid adaptation and
robust generalization -- a critical component of artificial general
intelligence.

</details>


### [10] [Information-Theoretic Aggregation of Ethical Attributes in Simulated-Command](https://arxiv.org/abs/2507.12862)
*Hussein Abbass,Taylan Akay,Harrison Tolley*

Main category: cs.AI

TL;DR: 将伦理决策的权重计算自动化，提高AI辅助军事决策效率


<details>
  <summary>Details</summary>
Motivation: 在AI时代，人工评估海量场景中的伦理决策是低效且不可行的，因此需要一种自动化的方法。

Method: 本文采用多准则决策方法，利用熵的概念自动计算伦理属性的权重。

Result: 提出了一种在生成式模拟中动态加权伦理属性的方法，将人工判断从循环中移除，提高效率。

Conclusion: 本文提出了一种将人工判断从仿真决策循环中移除的方法，通过预先设计伦理度量空间，让模拟环境探索该空间，最后将少量选项反馈给人类指挥官进行选择。

Abstract: In the age of AI, human commanders need to use the computational powers
available in today's environment to simulate a very large number of scenarios.
Within each scenario, situations occur where different decision design options
could have ethical consequences. Making these decisions reliant on human
judgement is both counter-productive to the aim of exploring very large number
of scenarios in a timely manner and infeasible when considering the workload
needed to involve humans in each of these choices. In this paper, we move human
judgement outside the simulation decision cycle. Basically, the human will
design the ethical metric space, leaving it to the simulated environment to
explore the space. When the simulation completes its testing cycles, the
testing environment will come back to the human commander with a few options to
select from. The human commander will then exercise human-judgement to select
the most appropriate course of action, which will then get executed
accordingly. We assume that the problem of designing metrics that are
sufficiently granular to assess the ethical implications of decisions is
solved. Subsequently, the fundamental problem we look at in this paper is how
to weight ethical decisions during the running of these simulations; that is,
how to dynamically weight the ethical attributes when agents are faced with
decision options with ethical implications during generative simulations. The
multi-criteria decision making literature has started to look at nearby
problems, where the concept of entropy has been used to determine the weights
during aggregation. We draw from that literature different approaches to
automatically calculate the weights for ethical attributes during
simulation-based testing and evaluation.

</details>


### [11] [Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework](https://arxiv.org/abs/2507.12872)
*Rishane Dassanayake,Mario Demetroudi,James Walpole,Lindley Lentati,Jason R. Brown,Edward James Young*

Main category: cs.AI

TL;DR: 人工智能系统操纵风险日益增加，本文提供了一个评估和减轻此类风险的框架，供人工智能公司在部署前使用。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能系统能力的快速发展，其操纵和欺骗人类行为的能力也日益增强，对企业安全构成重大威胁，但目前缺乏相应的评估和缓解框架。

Method: 本文提供了一个基于三个核心论点（无力性、可控性和可信度）的操纵风险安全案例框架。

Result: 本文提出了一个用于评估和减轻操纵风险的框架，为人工智能公司提供了具体的证据要求、评估方法和实施考虑。

Conclusion: 本文首次提出了一种将操纵风险纳入人工智能安全治理的系统方法，为人工智能公司评估和降低这些风险提供了一个具体的框架。

Abstract: Frontier AI systems are rapidly advancing in their capabilities to persuade,
deceive, and influence human behaviour, with current models already
demonstrating human-level persuasion and strategic deception in specific
contexts. Humans are often the weakest link in cybersecurity systems, and a
misaligned AI system deployed internally within a frontier company may seek to
undermine human oversight by manipulating employees. Despite this growing
threat, manipulation attacks have received little attention, and no systematic
framework exists for assessing and mitigating these risks. To address this, we
provide a detailed explanation of why manipulation attacks are a significant
threat and could lead to catastrophic outcomes. Additionally, we present a
safety case framework for manipulation risk, structured around three core lines
of argument: inability, control, and trustworthiness. For each argument, we
specify evidence requirements, evaluation methodologies, and implementation
considerations for direct application by AI companies. This paper provides the
first systematic methodology for integrating manipulation risk into AI safety
governance, offering AI companies a concrete foundation to assess and mitigate
these threats before deployment.

</details>


### [12] [VAR-MATH: Probing True Mathematical Reasoning in Large Language Models via Symbolic Multi-Instance Benchmarks](https://arxiv.org/abs/2507.12885)
*Jian Yao,Ran Cheng,Kay Chen Tan*

Main category: cs.AI

TL;DR: VAR-MATH框架通过符号化评估方法，有效揭示了强化学习在提升大语言模型数学推理能力方面的局限性，许多模型的改进仅仅是过度拟合的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在解决数学推理问题时，容易出现过度拟合基准测试模式的情况，难以评估模型的真正推理能力。

Method: 提出了一种名为VAR-MATH的符号评估框架，通过将固定数值问题转换为符号模板并要求模型求解每个模板的多个实例，从而减轻数据泄露，提高评估鲁棒性。将AMC23和AIME24基准测试转换为其符号对应版本VAR-AMC23和VAR-AIME24。

Result: 实验结果表明，在变异版本上，强化学习训练的模型性能下降显著，尤其是在小型模型上，平均下降幅度在AMC23上为48.0%，在AIME24上为58.3%。

Conclusion: VAR-MATH框架揭示了强化学习训练的语言模型在数学推理方面存在过度拟合基准测试模式的问题，许多现有方法依赖于表面启发式，难以泛化到具体的数值形式之外。

Abstract: Recent advances in reinforcement learning (RL) have led to substantial
improvements in the mathematical reasoning abilities of large language models
(LLMs), as measured by standard benchmarks. However, these gains often persist
even when models are trained with flawed signals, such as random or inverted
rewards, raising a fundamental question: do such improvements reflect true
reasoning, or are they merely artifacts of overfitting to benchmark-specific
patterns? To address this question, we take an evaluation-centric perspective
and identify two critical shortcomings in existing protocols. First,
\emph{benchmark contamination} arises from the public availability of test
problems, increasing the risk of data leakage. Second, \emph{evaluation
fragility} stems from the reliance on single-instance assessments, which are
highly sensitive to stochastic outputs and fail to capture reasoning
consistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic
evaluation framework designed to probe genuine reasoning ability. By converting
fixed numerical problems into symbolic templates and requiring models to solve
multiple instantiations of each, VAR-MATH enforces consistent reasoning across
structurally equivalent variants, thereby mitigating contamination and
improving evaluation robustness. We apply VAR-MATH to transform two popular
benchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and
VAR-AIME24. Experimental results reveal substantial performance drops for
RL-trained models on the variabilized versions, especially for smaller models,
with average declines of 48.0\% on AMC23 and 58.3\% on AIME24. These findings
suggest that many existing RL methods rely on superficial heuristics and fail
to generalize beyond specific numerical forms. Overall, VAR-MATH offers a
principled, contamination-resistant evaluation paradigm for mathematical
reasoning.

</details>


### [13] [A Translation of Probabilistic Event Calculus into Markov Decision Processes](https://arxiv.org/abs/2507.12989)
*Lyris Xu,Fabio Aurelio D'Asaro,Luke Dickens*

Main category: cs.AI

TL;DR: This paper extends Probabilistic Event Calculus with goal-directed reasoning by translating it into Markov Decision Processes, combining the strengths of both formalisms.


<details>
  <summary>Details</summary>
Motivation: PEC lacks mechanisms for goal-directed reasoning. This paper aims to bridge this gap by leveraging the extensive tools and algorithms available for MDPs.

Method: The paper develops a formal translation of PEC into MDPs, introducing the concept of "action-taking situations" to handle PEC's flexible action semantics.  It also details methods for mapping learned MDP policies back into human-readable PEC representations.

Result: The resulting PEC-MDP formalism allows the application of MDP algorithms to PEC domains, supporting both temporal reasoning and objective-driven planning while maintaining interpretability.

Conclusion: This paper presents a novel translation from Probabilistic Event Calculus (PEC) domains to Markov Decision Processes (MDPs), enabling goal-directed reasoning within the PEC framework while preserving its interpretability and expressive power.

Abstract: Probabilistic Event Calculus (PEC) is a logical framework for reasoning about
actions and their effects in uncertain environments, which enables the
representation of probabilistic narratives and computation of temporal
projections. The PEC formalism offers significant advantages in
interpretability and expressiveness for narrative reasoning. However, it lacks
mechanisms for goal-directed reasoning. This paper bridges this gap by
developing a formal translation of PEC domains into Markov Decision Processes
(MDPs), introducing the concept of "action-taking situations" to preserve PEC's
flexible action semantics. The resulting PEC-MDP formalism enables the
extensive collection of algorithms and theoretical tools developed for MDPs to
be applied to PEC's interpretable narrative domains. We demonstrate how the
translation supports both temporal reasoning tasks and objective-driven
planning, with methods for mapping learned policies back into human-readable
PEC representations, maintaining interpretability while extending PEC's
capabilities.

</details>


### [14] [Exploiting Constraint Reasoning to Build Graphical Explanations for Mixed-Integer Linear Programming](https://arxiv.org/abs/2507.13007)
*Roger Xavier Lera-Leri,Filippo Bistaffa,Athina Georgara,Juan Antonio Rodriguez-Aguilar*

Main category: cs.AI

TL;DR: X-MILP 使用约束推理技术为 MILP 问题生成对比解释，通过计算 IIS 并将其表示为原因图来回答用户查询。


<details>
  <summary>Details</summary>
Motivation: 为了响应对可信赖 AI 的需求，开发用于优化的对比解释技术，特别是针对形式化为 MILP 的特定决策过程的解决方案。

Method: 该方法基于约束推理技术，将用户的查询编码为附加约束，然后计算不可约不可行子系统 (IIS) 来确定答案。

Result: 在已知优化问题的实例上测试了该方法，评估了计算解释的经验难度。

Conclusion: X-MILP 是一种用于构建 MILP 对比解释的领域无关方法，它基于约束推理技术，将用户的查询编码为附加约束，通过计算不可约不可行子系统 (IIS) 来确定答案，并以“原因图”的形式呈现解释。

Abstract: Following the recent push for trustworthy AI, there has been an increasing
interest in developing contrastive explanation techniques for optimisation,
especially concerning the solution of specific decision-making processes
formalised as MILPs. Along these lines, we propose X-MILP, a domain-agnostic
approach for building contrastive explanations for MILPs based on constraint
reasoning techniques. First, we show how to encode the queries a user makes
about the solution of an MILP problem as additional constraints. Then, we
determine the reasons that constitute the answer to the user's query by
computing the Irreducible Infeasible Subsystem (IIS) of the newly obtained set
of constraints. Finally, we represent our explanation as a "graph of reasons"
constructed from the IIS, which helps the user understand the structure among
the reasons that answer their query. We test our method on instances of
well-known optimisation problems to evaluate the empirical hardness of
computing explanations.

</details>


### [15] [Prediction of Highway Traffic Flow Based on Artificial Intelligence Algorithms Using California Traffic Data](https://arxiv.org/abs/2507.13112)
*Junseong Lee,Jaegwan Cho,Yoonju Cho,Seoyoon Choi,Yejin Shin*

Main category: cs.AI

TL;DR: 基于机器学习的交通流预测模型，在加州78号公路数据测试中，10分钟数据间隔的MLR和RF模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 旨在解决全球交通拥堵问题。

Method: 使用多元线性回归(MLR)和随机森林(RF)算法，分析了不同数据收集间隔（30秒到15分钟）下的交通数据。

Result: MLR和RF模型在10分钟数据收集间隔下性能最佳，R^2, MAE和RMSE指标验证了这一点。

Conclusion: 该研究提出了一种基于机器学习的交通流预测模型，并通过实验证明了该模型在预测加州78号公路交通流方面的有效性。

Abstract: The study "Prediction of Highway Traffic Flow Based on Artificial
Intelligence Algorithms Using California Traffic Data" presents a machine
learning-based traffic flow prediction model to address global traffic
congestion issues. The research utilized 30-second interval traffic data from
California Highway 78 over a five-month period from July to November 2022,
analyzing a 7.24 km westbound section connecting "Melrose Dr" and "El-Camino
Real" in the San Diego area. The study employed Multiple Linear Regression
(MLR) and Random Forest (RF) algorithms, analyzing data collection intervals
ranging from 30 seconds to 15 minutes. Using R^2, MAE, and RMSE as performance
metrics, the analysis revealed that both MLR and RF models performed optimally
with 10-minute data collection intervals. These findings are expected to
contribute to future traffic congestion solutions and efficient traffic
management.

</details>


### [16] [From Roots to Rewards: Dynamic Tree Reasoning with RL](https://arxiv.org/abs/2507.13142)
*Ahmed Bahloul,Simon Malberg*

Main category: cs.AI

TL;DR: 动态强化学习框架改进树结构推理，提升效率和准确性


<details>
  <summary>Details</summary>
Motivation: 现有的基于树的推理方法（如ProbTree）存在推理树固定和节点计算效率低的问题，难以处理复杂的知识整合和错误传播。

Method: 动态强化学习框架，增量构建推理树，基于实时置信度估计学习最优策略。

Result: 该框架在保持ProbTree概率严谨性的同时，通过选择性扩展和集中资源分配，提高了求解质量和计算效率，为树结构推理建立了新的范例。

Conclusion: 本文提出了一种动态强化学习框架，将基于树的推理转化为自适应过程，从而提高了基于概率树的推理方法的解决方案质量和计算效率。

Abstract: Modern language models address complex questions through chain-of-thought
(CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al.,
2021), yet struggle with error propagation and knowledge integration.
Tree-structured reasoning methods, particularly the Probabilistic
Tree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues
by decomposing questions into hierarchical structures and selecting answers
through confidence-weighted aggregation of parametric and retrieved knowledge
(Yao et al., 2023). However, ProbTree's static implementation introduces two
key limitations: (1) the reasoning tree is fixed during the initial
construction phase, preventing dynamic adaptation to intermediate results, and
(2) each node requires exhaustive evaluation of all possible solution
strategies, creating computational inefficiency. We present a dynamic
reinforcement learning (Sutton and Barto, 2018) framework that transforms
tree-based reasoning into an adaptive process. Our approach incrementally
constructs the reasoning tree based on real-time confidence estimates, while
learning optimal policies for action selection (decomposition, retrieval, or
aggregation). This maintains ProbTree's probabilistic rigor while improving
both solution quality and computational efficiency through selective expansion
and focused resource allocation. The work establishes a new paradigm for
treestructured reasoning that balances the reliability of probabilistic
frameworks with the flexibility required for real-world question answering
systems.

</details>


### [17] [Black Box Deployed -- Functional Criteria for Artificial Moral Agents in the LLM Era](https://arxiv.org/abs/2507.13175)
*Matthew E. Brophy*

Main category: cs.AI

TL;DR: 大型语言模型的出现需要重新评估人工道德智能体，文章提出了十项新的评估标准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的复杂性和不透明性使得传统的道德评估标准不再适用。

Method: 本文通过分析大型语言模型的特性，结合哲学和技术伦理的观点，提出了新的评估标准。并用自动公共汽车的例子进行了说明。

Result: 提出了十项评估基于LLM的AMA的新标准。

Conclusion: 本文认为，大型语言模型（LLM）的出现使得评估人工道德智能体（AMA）的传统哲学标准过时，并提出了十项功能性标准来评估基于LLM的AMA，包括道德一致性、语境敏感性、规范完整性、元伦理意识、系统弹性、可信度、可纠正性、部分透明性、功能自主性和道德想象力。

Abstract: The advancement of powerful yet opaque large language models (LLMs)
necessitates a fundamental revision of the philosophical criteria used to
evaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the
assumption of transparent architectures, which LLMs defy due to their
stochastic outputs and opaque internal states. This paper argues that
traditional ethical criteria are pragmatically obsolete for LLMs due to this
mismatch. Engaging with core themes in the philosophy of technology, this paper
proffers a revised set of ten functional criteria to evaluate LLM-based
artificial moral agents: moral concordance, context sensitivity, normative
integrity, metaethical awareness, system resilience, trustworthiness,
corrigibility, partial transparency, functional autonomy, and moral
imagination. These guideposts, applied to what we term "SMA-LLS" (Simulating
Moral Agency through Large Language Systems), aim to steer AMAs toward greater
alignment and beneficial societal integration in the coming years. We
illustrate these criteria using hypothetical scenarios involving an autonomous
public bus (APB) to demonstrate their practical applicability in morally
salient contexts.

</details>
