{"id": "2509.05323", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.05323", "abs": "https://arxiv.org/abs/2509.05323", "authors": ["Adam Cole", "Mick Grierson"], "title": "Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts", "comment": "3rd international workshop on eXplainable AI for the Arts (XAIxArts)\n  at the ACM Creativity and Cognition Conference 2025", "summary": "This paper presents an artistic and technical investigation into the\nattention mechanisms of video diffusion transformers. Inspired by early video\nartists who manipulated analog video signals to create new visual aesthetics,\nthis study proposes a method for extracting and visualizing cross-attention\nmaps in generative video models. Built on the open-source Wan model, our tool\nprovides an interpretable window into the temporal and spatial behavior of\nattention in text-to-video generation. Through exploratory probes and an\nartistic case study, we examine the potential of attention maps as both\nanalytical tools and raw artistic material. This work contributes to the\ngrowing field of Explainable AI for the Arts (XAIxArts), inviting artists to\nreclaim the inner workings of AI as a creative medium.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u89c6\u9891\u6269\u6563Transformer\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u53d6\u548c\u53ef\u89c6\u5316\u751f\u6210\u89c6\u9891\u6a21\u578b\u4e2d\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\u7684\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u827a\u672f\u521b\u4f5c\u3002", "motivation": "\u53d7\u65e9\u671f\u89c6\u9891\u827a\u672f\u5bb6\u521b\u4f5c\u7075\u611f\u7684\u542f\u53d1\uff0c\u65e8\u5728\u63a2\u7d22\u6ce8\u610f\u529b\u673a\u5236\u5728\u6587\u672c\u751f\u6210\u89c6\u9891\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u5206\u6790\u5de5\u5177\u548c\u827a\u672f\u7d20\u6750\u3002", "method": "\u57fa\u4e8e\u5f00\u6e90Wan\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5de5\u5177\u6765\u63d0\u53d6\u548c\u53ef\u89c6\u5316\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\uff0c\u5e76\u901a\u8fc7\u63a2\u7d22\u6027\u63a2\u6d4b\u548c\u827a\u672f\u6848\u4f8b\u7814\u7a76\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u4e86\u6ce8\u610f\u529b\u56fe\u4f5c\u4e3a\u5206\u6790\u5de5\u5177\u548c\u827a\u672f\u7d20\u6750\u7684\u6f5c\u529b\uff0c\u4e3a\u827a\u672f\u9886\u57df\u7684XAI\u505a\u51fa\u4e86\u8d21\u732e\u3002", "conclusion": "\u6ce8\u610f\u529b\u673a\u5236\u53ef\u88ab\u827a\u672f\u5bb6\u7528\u4f5c\u521b\u9020\u6027\u5a92\u4ecb\u3002"}}
{"id": "2509.05324", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05324", "abs": "https://arxiv.org/abs/2509.05324", "authors": ["Rongqian Chen", "Shu Hong", "Rifatul Islam", "Mahdi Imani", "G. Gary Tan", "Tian Lan"], "title": "Perception Graph for Cognitive Attack Reasoning in Augmented Reality", "comment": "Accepted by ACM MobiHoc XR Security workshop 2025", "summary": "Augmented reality (AR) systems are increasingly deployed in tactical\nenvironments, but their reliance on seamless human-computer interaction makes\nthem vulnerable to cognitive attacks that manipulate a user's perception and\nseverely compromise user decision-making. To address this challenge, we\nintroduce the Perception Graph, a novel model designed to reason about human\nperception within these systems. Our model operates by first mimicking the\nhuman process of interpreting key information from an MR environment and then\nrepresenting the outcomes using a semantically meaningful structure. We\ndemonstrate how the model can compute a quantitative score that reflects the\nlevel of perception distortion, providing a robust and measurable method for\ndetecting and analyzing the effects of such cognitive attacks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u611f\u77e5\u56fe\u201d\u7684\u65b0\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u5206\u6790\u589e\u5f3a\u73b0\u5b9e\u7cfb\u7edf\u4e2d\u8ba4\u77e5\u653b\u51fb\u5bf9\u4eba\u7c7b\u611f\u77e5\u7684\u5f71\u54cd\u3002", "motivation": "\u589e\u5f3a\u73b0\u5b9e\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u64cd\u7eb5\u7528\u6237\u611f\u77e5\u7684\u8ba4\u77e5\u653b\u51fb\uff0c\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u8be5\u6a21\u578b\u6a21\u62df\u4eba\u7c7b\u89e3\u91ca\u589e\u5f3a\u73b0\u5b9e\u73af\u5883\u4e2d\u5173\u952e\u4fe1\u606f\u7684\u8fc7\u7a0b\uff0c\u5e76\u4f7f\u7528\u8bed\u4e49\u7ed3\u6784\u8868\u793a\u7ed3\u679c\uff0c\u6700\u7ec8\u8ba1\u7b97\u4e00\u4e2a\u91cf\u5316\u5206\u6570\u6765\u53cd\u6620\u611f\u77e5\u626d\u66f2\u7684\u7a0b\u5ea6\u3002", "result": "\u8be5\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u4e14\u53ef\u8861\u91cf\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u5206\u6790\u8ba4\u77e5\u653b\u51fb\u7684\u5f71\u54cd\u3002", "conclusion": "\u611f\u77e5\u56fe\u6a21\u578b\u4e3a\u589e\u5f3a\u73b0\u5b9e\u7cfb\u7edf\u4e2d\u8ba4\u77e5\u653b\u51fb\u7684\u68c0\u6d4b\u548c\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.05325", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05325", "abs": "https://arxiv.org/abs/2509.05325", "authors": ["Liming Xu", "Yunbo Long", "Alexandra Brintrup"], "title": "SynDelay: A Synthetic Dataset for Delivery Delay Prediction", "comment": "This paper incldues 1 figure and 2 tables", "summary": "Artificial intelligence (AI) is transforming supply chain management, yet\nprogress in predictive tasks -- such as delivery delay prediction -- remains\nconstrained by the scarcity of high-quality, openly available datasets.\nExisting datasets are often proprietary, small, or inconsistently maintained,\nhindering reproducibility and benchmarking. We present SynDelay, a synthetic\ndataset designed for delivery delay prediction. Generated using an advanced\ngenerative model trained on real-world data, SynDelay preserves realistic\ndelivery patterns while ensuring privacy. Although not entirely free of noise\nor inconsistencies, it provides a challenging and practical testbed for\nadvancing predictive modelling. To support adoption, we provide baseline\nresults and evaluation metrics as initial benchmarks, serving as reference\npoints rather than state-of-the-art claims. SynDelay is publicly available\nthrough the Supply Chain Data Hub, an open initiative promoting dataset sharing\nand benchmarking in supply chain AI. We encourage the community to contribute\ndatasets, models, and evaluation practices to advance research in this area.\nAll code is openly accessible at https://supplychaindatahub.org.", "AI": {"tldr": "\u5408\u6210\u6570\u636e\u96c6SynDelay\u7528\u4e8e\u6539\u8fdb\u9012\u9001\u5ef6\u8bef\u9884\u6d4b", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u7a00\u7f3a\u3001\u8d28\u91cf\u4e0d\u9ad8\uff0c\u963b\u788d\u9012\u9001\u5ef6\u8bef\u9884\u6d4b\u6a21\u578b\u7684\u5f00\u53d1\u548c\u57fa\u51c6\u6d4b\u8bd5", "method": "\u4f7f\u7528\u9ad8\u7ea7\u751f\u6210\u6a21\u578b\uff0c\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u751f\u6210\u5408\u6210\u6570\u636e\u96c6SynDelay\uff0c\u4fdd\u7559\u771f\u5b9e\u9012\u9001\u6a21\u5f0f\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u548c\u5b9e\u7528\u6027\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u51c6\u7ed3\u679c\u548c\u8bc4\u4f30\u6307\u6807", "conclusion": "SynDelay\u516c\u5f00\u53ef\u7528\uff0c\u9f13\u52b1\u793e\u533a\u8d21\u732e\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u8bc4\u4f30\u5b9e\u8df5\uff0c\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u7814\u7a76"}}
{"id": "2509.05330", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05330", "abs": "https://arxiv.org/abs/2509.05330", "authors": ["Seyed Muhammad Hossein Mousavi", "Atiye Ilanloo"], "title": "MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset", "comment": null, "summary": "Automatic emotion recognition has become increasingly important with the rise\nof AI, especially in fields like healthcare, education, and automotive systems.\nHowever, there is a lack of multimodal datasets, particularly involving body\nmotion and physiological signals, which limits progress in the field. To\naddress this, the MVRS dataset is introduced, featuring synchronized recordings\nfrom 13 participants aged 12 to 60 exposed to VR based emotional stimuli\n(relaxation, fear, stress, sadness, joy). Data were collected using eye\ntracking (via webcam in a VR headset), body motion (Kinect v2), and EMG and GSR\nsignals (Arduino UNO), all timestamp aligned. Participants followed a unified\nprotocol with consent and questionnaires. Features from each modality were\nextracted, fused using early and late fusion techniques, and evaluated with\nclassifiers to confirm the datasets quality and emotion separability, making\nMVRS a valuable contribution to multimodal affective computing.", "AI": {"tldr": "MVRS\u6570\u636e\u96c6\u6536\u96c6\u4e8613\u540d\u53c2\u4e0e\u8005\u5728VR\u73af\u5883\u4e0b\u60c5\u7eea\u523a\u6fc0\u4e0b\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u5305\u62ec\u773c\u52a8\u8ffd\u8e2a\u3001\u80a2\u4f53\u52a8\u4f5c\u3001\u808c\u7535\u548c\u76ae\u80a4\u7535\u53cd\u5e94\uff0c\u5e76\u8fdb\u884c\u4e86\u7279\u5f81\u63d0\u53d6\u3001\u878d\u5408\u548c\u5206\u7c7b\u8bc4\u4f30\uff0c\u4e3a\u591a\u6a21\u6001\u60c5\u611f\u8ba1\u7b97\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002", "motivation": "\u7f3a\u4e4f\u5305\u542b\u80a2\u4f53\u8fd0\u52a8\u548c\u751f\u7406\u4fe1\u53f7\u7684\u591a\u6a21\u6001\u60c5\u611f\u6570\u636e\u96c6\u9650\u5236\u4e86\u81ea\u52a8\u60c5\u611f\u8bc6\u522b\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u6536\u96c6\u4e8613\u540d\u53c2\u4e0e\u8005\u5728VR\u73af\u5883\u4e0b\u89c2\u770b\u60c5\u7eea\u523a\u6fc0\u89c6\u9891\u65f6\u7684\u591a\u6a21\u6001\u6570\u636e\uff08\u773c\u52a8\u3001\u80a2\u4f53\u8fd0\u52a8\u3001\u808c\u7535\u3001\u76ae\u80a4\u7535\uff09\uff0c\u91c7\u7528\u65e9\u671f\u548c\u665a\u671f\u878d\u5408\u6280\u672f\u878d\u5408\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u5206\u7c7b\u5668\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6784\u5efa\u4e86MVRS\u6570\u636e\u96c6\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6570\u636e\u8d28\u91cf\u548c\u60c5\u7eea\u53ef\u5206\u79bb\u6027\u3002", "conclusion": "MVRS\u6570\u636e\u96c6\u4e3a\u591a\u6a21\u6001\u60c5\u611f\u8ba1\u7b97\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.05346", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05346", "abs": "https://arxiv.org/abs/2509.05346", "authors": ["Bo Yuan", "Jiazi Hu"], "title": "Benchmarking Large Language Models for Personalized Guidance in AI-Enhanced Learning", "comment": null, "summary": "While Large Language Models (LLMs) are increasingly envisioned as intelligent\nassistants for personalized learning, systematic head-to-head evaluations\nwithin authentic learning scenarios remain limited. This study conducts an\nempirical comparison of three state-of-the-art LLMs on a tutoring task that\nsimulates a realistic learning setting. Using a dataset comprising a student's\nanswers to ten questions of mixed formats with correctness labels, each LLM is\nrequired to (i) analyze the quiz to identify underlying knowledge components,\n(ii) infer the student's mastery profile, and (iii) generate targeted guidance\nfor improvement. To mitigate subjectivity and evaluator bias, we employ Gemini\nas a virtual judge to perform pairwise comparisons along various dimensions:\naccuracy, clarity, actionability, and appropriateness. Results analyzed via the\nBradley-Terry model indicate that GPT-4o is generally preferred, producing\nfeedback that is more informative and better structured than its counterparts,\nwhile DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower\nconsistency. These findings highlight the feasibility of deploying LLMs as\nadvanced teaching assistants for individualized support and provide\nmethodological guidance for future empirical research on LLM-driven\npersonalized learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u5b9e\u8bc1\u6bd4\u8f83\u4e86\u4e09\u4e2a\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u6a21\u62df\u771f\u5b9e\u5b66\u4e60\u73af\u5883\u7684\u8f85\u5bfc\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u8868\u660eGPT-4o\u8868\u73b0\u6700\u4f73\uff0c\u5176\u53cd\u9988\u66f4\u5177\u4fe1\u606f\u6027\u548c\u7ed3\u6784\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u5b66\u4e60\u573a\u666f\u4e2d\u4e2a\u6027\u5316\u5b66\u4e60\u8f85\u52a9\u4f5c\u7528\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u5305\u542b\u5b66\u751f\u7b54\u6848\u53ca\u5176\u6b63\u786e\u6027\u6807\u7b7e\u7684\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e09\u4e2aLLM\u5728\u5206\u6790\u6d4b\u8bd5\u3001\u63a8\u65ad\u5b66\u751f\u638c\u63e1\u60c5\u51b5\u548c\u751f\u6210\u9488\u5bf9\u6027\u6307\u5bfc\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u4f7f\u7528Gemini\u8fdb\u884c\u6210\u5bf9\u6bd4\u8f83\u3002", "result": "GPT-4o\u603b\u4f53\u8868\u73b0\u6700\u4f73\uff0cDeepSeek-V3\u548cGLM-4.5\u8868\u73b0\u4e0d\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u5c06LLM\u4f5c\u4e3a\u9ad8\u7ea7\u6559\u5b66\u52a9\u7406\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u6cd5\u6307\u5bfc\u3002"}}
{"id": "2509.05363", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.05363", "abs": "https://arxiv.org/abs/2509.05363", "authors": ["Lijie Ding", "Changwoo Do"], "title": "SasAgent: Multi-Agent AI System for Small-Angle Scattering Data Analysis", "comment": "8 pages, 7 figures", "summary": "We introduce SasAgent, a multi-agent AI system powered by large language\nmodels (LLMs) that automates small-angle scattering (SAS) data analysis by\nleveraging tools from the SasView software and enables user interaction via\ntext input. SasAgent features a coordinator agent that interprets user prompts\nand delegates tasks to three specialized agents for scattering length density\n(SLD) calculation, synthetic data generation, and experimental data fitting.\nThese agents utilize LLM-friendly tools to execute tasks efficiently. These\ntools, including the model data tool, Retrieval-Augmented Generation (RAG)\ndocumentation tool, bump fitting tool, and SLD calculator tool, are derived\nfrom the SasView Python library. A user-friendly Gradio-based interface\nenhances user accessibility. Through diverse examples, we demonstrate\nSasAgent's ability to interpret complex prompts, calculate SLDs, generate\naccurate scattering data, and fit experimental datasets with high precision.\nThis work showcases the potential of LLM-driven AI systems to streamline\nscientific workflows and enhance automation in SAS research.", "AI": {"tldr": "SasAgent\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684AI\u7cfb\u7edf\uff0c\u81ea\u52a8\u5316\u5c0f\u89d2\u6563\u5c04(SAS)\u6570\u636e\u5206\u6790\u3002", "motivation": "\u7b80\u5316SAS\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\uff0c\u63d0\u9ad8\u81ea\u52a8\u5316\u6c34\u5e73\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u534f\u8c03\u4e09\u4e2a\u4e13\u4e1a\u4ee3\u7406\uff08SLD\u8ba1\u7b97\u3001\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u5b9e\u9a8c\u6570\u636e\u62df\u5408\uff09\uff0c\u5e76\u901a\u8fc7Gradio\u754c\u9762\u8fdb\u884c\u7528\u6237\u4ea4\u4e92\u3002", "result": "SasAgent\u80fd\u591f\u89e3\u91ca\u590d\u6742\u7684\u63d0\u793a\uff0c\u8ba1\u7b97SLD\uff0c\u751f\u6210\u51c6\u786e\u7684\u6563\u5c04\u6570\u636e\uff0c\u5e76\u9ad8\u7cbe\u5ea6\u62df\u5408\u5b9e\u9a8c\u6570\u636e\u96c6\u3002", "conclusion": "LLM\u9a71\u52a8\u7684AI\u7cfb\u7edf\u5728\u7b80\u5316\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.05375", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05375", "abs": "https://arxiv.org/abs/2509.05375", "authors": ["Arend Hintze"], "title": "Characterizing Fitness Landscape Structures in Prompt Engineering", "comment": null, "summary": "While prompt engineering has emerged as a crucial technique for optimizing\nlarge language model performance, the underlying optimization landscape remains\npoorly understood. Current approaches treat prompt optimization as a black-box\nproblem, applying sophisticated search algorithms without characterizing the\nlandscape topology they navigate. We present a systematic analysis of fitness\nlandscape structures in prompt engineering using autocorrelation analysis\nacross semantic embedding spaces. Through experiments on error detection tasks\nwith two distinct prompt generation strategies -- systematic enumeration (1,024\nprompts) and novelty-driven diversification (1,000 prompts) -- we reveal\nfundamentally different landscape topologies. Systematic prompt generation\nyields smoothly decaying autocorrelation, while diversified generation exhibits\nnon-monotonic patterns with peak correlation at intermediate semantic\ndistances, indicating rugged, hierarchically structured landscapes.\nTask-specific analysis across 10 error detection categories reveals varying\ndegrees of ruggedness across different error types. Our findings provide an\nempirical foundation for understanding the complexity of optimization in prompt\nengineering landscapes.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u63d0\u793a\u5de5\u7a0b\u7684\u4f18\u5316\u76ee\u6807\u51fd\u6570\u5177\u6709\u590d\u6742\u62d3\u6251\u7ed3\u6784\uff0c\u4e0d\u540c\u63d0\u793a\u751f\u6210\u7b56\u7565\u5bfc\u81f4\u4e0d\u540c\u7ed3\u6784\u3002", "motivation": "\u5f53\u524d\u63d0\u793a\u5de5\u7a0b\u4f18\u5316\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u76ee\u6807\u51fd\u6570\u7ed3\u6784\u7684\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u81ea\u76f8\u5173\u5206\u6790\u548c\u8bed\u4e49\u5d4c\u5165\u7a7a\u95f4\u7814\u7a76\u4e0d\u540c\u63d0\u793a\u751f\u6210\u7b56\u7565\u4e0b\u7684\u76ee\u6807\u51fd\u6570\u62d3\u6251\u7ed3\u6784\u3002", "result": "\u7cfb\u7edf\u63d0\u793a\u751f\u6210\u7b56\u7565\u4ea7\u751f\u5e73\u6ed1\u8870\u51cf\u7684\u81ea\u76f8\u5173\uff0c\u800c\u591a\u6837\u5316\u7b56\u7565\u4ea7\u751f\u975e\u5355\u8c03\u6a21\u5f0f\uff0c\u8868\u660e\u591a\u6837\u5316\u7b56\u7565\u4e0b\u7684\u76ee\u6807\u51fd\u6570\u66f4\u590d\u6742\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u63d0\u793a\u5de5\u7a0b\u4f18\u5316\u590d\u6742\u6027\u63d0\u4f9b\u4e86\u7ecf\u9a8c\u57fa\u7840\u3002"}}
{"id": "2509.05378", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.05378", "abs": "https://arxiv.org/abs/2509.05378", "authors": ["Andreas Motzfeldt", "Joakim Edin", "Casper L. Christensen", "Christian Hardmeier", "Lars Maal\u00f8e", "Anna Rogers"], "title": "Code Like Humans: A Multi-Agent Solution for Medical Coding", "comment": "EMNLP Findings 2025", "summary": "In medical coding, experts map unstructured clinical notes to alphanumeric\ncodes for diagnoses and procedures. We introduce Code Like Humans: a new\nagentic framework for medical coding with large language models. It implements\nofficial coding guidelines for human experts, and it is the first solution that\ncan support the full ICD-10 coding system (+70K labels). It achieves the best\nperformance to date on rare diagnosis codes (fine-tuned discriminative\nclassifiers retain an advantage for high-frequency codes, to which they are\nlimited). Towards future work, we also contribute an analysis of system\nperformance and identify its `blind spots' (codes that are systematically\nundercoded).", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d4b\u80fd\u533b\u5b66\u7f16\u7801\uff0c\u5b9e\u73b0ICD-10\u7f16\u7801\u7cfb\u7edf\u5168\u8986\u76d6\uff0c\u5bf9\u7f55\u89c1\u75c5\u75c7\u7f16\u7801\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u6539\u8fdb\u533b\u5b66\u7f16\u7801\u4e2d\u4e13\u5bb6\u5c06\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u7b14\u8bb0\u6620\u5c04\u5230\u8bca\u65ad\u548c\u624b\u672f\u7684\u5b57\u6bcd\u6570\u5b57\u4ee3\u7801\u7684\u6d41\u7a0b\u3002", "method": "\u63d0\u51faCode Like Humans\u6846\u67b6\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u533b\u5b66\u7f16\u7801\u6846\u67b6\uff0c\u9075\u5faa\u4eba\u7c7b\u4e13\u5bb6\u4f7f\u7528\u7684\u5b98\u65b9\u7f16\u7801\u6307\u5357\uff0c\u652f\u6301ICD-10\u7f16\u7801\u7cfb\u7edf\uff087\u4e07\u591a\u4e2a\u6807\u7b7e\uff09\u3002", "result": "\u5728\u7f55\u89c1\u8bca\u65ad\u4ee3\u7801\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff08\u9ad8\u9891\u4ee3\u7801\u4ecd\u53d7\u76ca\u4e8e\u5fae\u8c03\u7684\u5224\u522b\u5f0f\u5206\u7c7b\u5668\uff09\u3002", "conclusion": "\u5206\u6790\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u5e76\u786e\u5b9a\u4e86\u5176\u4e0d\u8db3\uff08\u7cfb\u7edf\u6027\u6f0f\u7801\u4ee3\u7801\uff09\u3002"}}
{"id": "2509.05381", "categories": ["cs.AI", "cs.LG", "68T01, 68T20, 68Q87"], "pdf": "https://arxiv.org/pdf/2509.05381", "abs": "https://arxiv.org/abs/2509.05381", "authors": ["Madhava Gaikwad"], "title": "Murphys Laws of AI Alignment: Why the Gap Always Wins", "comment": "21 pages", "summary": "Large language models are increasingly aligned to human preferences through\nreinforcement learning from human feedback (RLHF) and related methods such as\nDirect Preference Optimization (DPO), Constitutional AI, and RLAIF. While\neffective, these methods exhibit recurring failure patterns i.e., reward\nhacking, sycophancy, annotator drift, and misgeneralization. We introduce the\nconcept of the Alignment Gap, a unifying lens for understanding recurring\nfailures in feedback-based alignment. Using a KL-tilting formalism, we\nillustrate why optimization pressure tends to amplify divergence between proxy\nrewards and true human intent. We organize these failures into a catalogue of\nMurphys Laws of AI Alignment, and propose the Alignment Trilemma as a way to\nframe trade-offs among optimization strength, value capture, and\ngeneralization. Small-scale empirical studies serve as illustrative support.\nFinally, we propose the MAPS framework (Misspecification, Annotation, Pressure,\nShift) as practical design levers. Our contribution is not a definitive\nimpossibility theorem but a perspective that reframes alignment debates around\nstructural limits and trade-offs, offering clearer guidance for future design.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4eba\u5de5\u667a\u80fd\u5bf9\u9f50\u4e2d\u53cd\u590d\u51fa\u73b0\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u201c\u5bf9\u9f50\u5dee\u8ddd\u201d\u7684\u6982\u5ff5\u6765\u89e3\u91ca\u8fd9\u4e9b\u6a21\u5f0f\uff0c\u6700\u7ec8\u63d0\u51fa\u4e86MAPS\u6846\u67b6\u4f5c\u4e3a\u5b9e\u9645\u7684\u8bbe\u8ba1\u624b\u6bb5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5982RLHF\uff09\u5728\u4eba\u5de5\u667a\u80fd\u5bf9\u9f50\u4e2d\u5b58\u5728\u4e00\u4e9b\u4e0d\u8db3\uff0c\u4f8b\u5982\u5956\u52b1\u4f5c\u5f0a\u3001\u8c04\u5a9a\u3001\u6807\u6ce8\u8005\u6f02\u79fb\u548c\u6cdb\u5316\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528KL-tilting\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u5bf9\u9f50\u5dee\u8ddd\u6982\u5ff5\uff0c\u4ee5\u53caMurphys Laws of AI Alignment\u548cAlignment Trilemma\u6846\u67b6\u3002", "result": "\u63d0\u51faMAPS\u6846\u67b6\uff08Misspecification, Annotation, Pressure, Shift\uff09\u4f5c\u4e3a\u5b9e\u9645\u8bbe\u8ba1\u6760\u6746\uff0c\u4ee5\u5e94\u5bf9\u5bf9\u9f50\u95ee\u9898\u3002", "conclusion": "\u8be5\u8bba\u6587\u5e76\u975e\u63d0\u51fa\u4e0d\u53ef\u80fd\u5b9a\u7406\uff0c\u800c\u662f\u4ece\u7ed3\u6784\u6027\u9650\u5236\u548c\u6743\u8861\u7684\u89d2\u5ea6\u91cd\u65b0\u6784\u5efa\u4e86\u5bf9\u9f50\u95ee\u9898\u7684\u8ba8\u8bba\uff0c\u4e3a\u672a\u6765\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u7684\u6307\u5bfc\u3002"}}
{"id": "2509.05469", "categories": ["cs.AI", "cs.CV", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.05469", "abs": "https://arxiv.org/abs/2509.05469", "authors": ["Chenguang Wang", "Xiang Yan", "Yilong Dai", "Ziyi Wang", "Susu Xu"], "title": "From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation", "comment": "21 pages, 8 figures", "summary": "Realistic visual renderings of street-design scenarios are essential for\npublic engagement in active transportation planning. Traditional approaches are\nlabor-intensive, hindering collective deliberation and collaborative\ndecision-making. While AI-assisted generative design shows transformative\npotential by enabling rapid creation of design scenarios, existing generative\napproaches typically require large amounts of domain-specific training data and\nstruggle to enable precise spatial variations of design/configuration in\ncomplex street-view scenes. We introduce a multi-agent system that edits and\nredesigns bicycle facilities directly on real-world street-view imagery. The\nframework integrates lane localization, prompt optimization, design generation,\nand automated evaluation to synthesize realistic, contextually appropriate\ndesigns. Experiments across diverse urban scenarios demonstrate that the system\ncan adapt to varying road geometries and environmental conditions, consistently\nyielding visually coherent and instruction-compliant results. This work\nestablishes a foundation for applying multi-agent pipelines to transportation\ninfrastructure planning and facility design.", "AI": {"tldr": "\u5229\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u76f4\u63a5\u7f16\u8f91\u771f\u5b9e\u4e16\u754c\u8857\u666f\u56fe\u50cf\u4e2d\u7684\u81ea\u884c\u8f66\u8bbe\u65bd\uff0c\u5b9e\u73b0\u5feb\u901f\u751f\u6210\u4e0d\u540c\u8bbe\u8ba1\u65b9\u6848\uff0c\u63d0\u9ad8\u516c\u4f17\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8017\u65f6\u957f\u3001\u9700\u5927\u91cf\u6570\u636e\uff0c\u9650\u5236\u4e86\u4e3b\u52a8\u4ea4\u901a\u89c4\u5212\u4e2d\u7684\u516c\u4f17\u53c2\u4e0e\u548c\u534f\u4f5c\u51b3\u7b56\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u96c6\u6210\u4e86\u8f66\u9053\u5b9a\u4f4d\u3001\u63d0\u793a\u4f18\u5316\u3001\u8bbe\u8ba1\u751f\u6210\u548c\u81ea\u52a8\u8bc4\u4f30\u7b49\u6a21\u5757\uff0c\u76f4\u63a5\u5728\u771f\u5b9e\u8857\u666f\u56fe\u50cf\u4e0a\u7f16\u8f91\u548c\u91cd\u65b0\u8bbe\u8ba1\u81ea\u884c\u8f66\u8bbe\u65bd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u7cfb\u7edf\u80fd\u9002\u5e94\u4e0d\u540c\u7684\u9053\u8def\u51e0\u4f55\u5f62\u72b6\u548c\u73af\u5883\u6761\u4ef6\uff0c\u751f\u6210\u89c6\u89c9\u4e0a\u8fde\u8d2f\u4e14\u7b26\u5408\u6307\u4ee4\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5c06\u591a\u667a\u80fd\u4f53\u6280\u672f\u5e94\u7528\u4e8e\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u548c\u8bbe\u65bd\u8bbe\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.05550", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05550", "abs": "https://arxiv.org/abs/2509.05550", "authors": ["Zixi Li"], "title": "TreeGPT: A Novel Hybrid Architecture for Abstract Syntax Tree Processing with Global Parent-Child Aggregation", "comment": "Code available at: https://github.com/lizixi-0x2F/TreeGPT", "summary": "We introduce TreeGPT, a novel neural architecture that combines\ntransformer-based attention mechanisms with global parent-child aggregation for\nprocessing Abstract Syntax Trees (ASTs) in neural program synthesis tasks.\nUnlike traditional approaches that rely solely on sequential processing or\ngraph neural networks, TreeGPT employs a hybrid design that leverages both\nself-attention for capturing local dependencies and a specialized Tree\nFeed-Forward Network (TreeFFN) for modeling hierarchical tree structures\nthrough iterative message passing.\n  The core innovation lies in our Global Parent-Child Aggregation mechanism,\nformalized as: $$h_i^{(t+1)} = \\sigma \\Big( h_i^{(0)} + W_{pc} \\sum_{(p,c) \\in\nE_i} f(h_p^{(t)}, h_c^{(t)}) + b \\Big)$$ where $h_i^{(t)}$ represents the\nhidden state of node $i$ at iteration $t$, $E_i$ denotes all parent-child edges\ninvolving node $i$, and $f(h_p, h_c)$ is an edge aggregation function. This\nformulation enables each node to progressively aggregate information from the\nentire tree structure through $T$ iterations.\n  Our architecture integrates optional enhancements including gated aggregation\nwith learnable edge weights, residual connections for gradient stability, and\nbidirectional propagation for capturing both bottom-up and top-down\ndependencies. We evaluate TreeGPT on the ARC Prize 2025 dataset, a challenging\nvisual reasoning benchmark requiring abstract pattern recognition and rule\ninference. Experimental results demonstrate that TreeGPT achieves 96\\%\naccuracy, significantly outperforming transformer baselines (1.3\\%),\nlarge-scale models like Grok-4 (15.9\\%), and specialized program synthesis\nmethods like SOAR (52\\%) while using only 1.5M parameters. Our comprehensive\nablation study reveals that edge projection is the most critical component,\nwith the combination of edge projection and gating achieving optimal\nperformance.", "AI": {"tldr": "TreeGPT:\u4e00\u79cd\u7ed3\u5408Transformer\u548c\u5168\u5c40\u7236\u5b50\u805a\u5408\u673a\u5236\u7684AST\u5904\u7406\u795e\u7ecf\u67b6\u6784\uff0c\u5728ARC Prize 2025\u6570\u636e\u96c6\u4e0a\u53d6\u5f9796%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u987a\u5e8f\u5904\u7406\u6216\u56fe\u795e\u7ecf\u7f51\u7edc\uff0cTreeGPT\u7ed3\u5408\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u6811\u72b6\u524d\u9988\u7f51\u7edc\uff0c\u4ee5\u66f4\u597d\u5730\u5904\u7406AST\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5168\u5c40\u7236\u5b50\u805a\u5408\u673a\u5236\uff0c\u901a\u8fc7\u8fed\u4ee3\u4fe1\u606f\u4f20\u9012\u5efa\u6a21\u5206\u5c42\u6811\u7ed3\u6784\uff0c\u5e76\u5305\u542b\u53ef\u9009\u589e\u5f3a\uff0c\u5982\u95e8\u63a7\u805a\u5408\u3001\u6b8b\u5dee\u8fde\u63a5\u548c\u53cc\u5411\u4f20\u64ad\u3002", "result": "\u5728ARC Prize 2025\u6570\u636e\u96c6\u4e0a\u8fbe\u523096%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u8d85\u8fc7Transformer\u57fa\u7ebf\u3001Grok-4\u548cSOAR\u7b49\u6a21\u578b\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u8fb9\u6295\u5f71\u662f\u6700\u5173\u952e\u7684\u7ec4\u4ef6\u3002", "conclusion": "TreeGPT\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684AST\u5904\u7406\u65b9\u6cd5\uff0c\u5176\u5168\u5c40\u7236\u5b50\u805a\u5408\u673a\u5236\u548c\u53ef\u9009\u589e\u5f3a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2509.05578", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05578", "abs": "https://arxiv.org/abs/2509.05578", "authors": ["Ruixun Liu", "Lingyu Kong", "Derun Li", "Hang Zhao"], "title": "OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision", "comment": null, "summary": "Multimodal large language models (MLLMs) have shown strong vision-language\nreasoning abilities but still lack robust 3D spatial understanding, which is\ncritical for autonomous driving. This limitation stems from two key challenges:\n(1) the difficulty of constructing accessible yet effective 3D representations\nwithout expensive manual annotations, and (2) the loss of fine-grained spatial\ndetails in VLMs due to the absence of large-scale 3D vision-language\npretraining. To address these challenges, we propose OccVLA, a novel framework\nthat integrates 3D occupancy representations into a unified multimodal\nreasoning process. Unlike prior approaches that rely on explicit 3D inputs,\nOccVLA treats dense 3D occupancy as both a predictive output and a supervisory\nsignal, enabling the model to learn fine-grained spatial structures directly\nfrom 2D visual inputs. The occupancy predictions are regarded as implicit\nreasoning processes and can be skipped during inference without performance\ndegradation, thereby adding no extra computational overhead. OccVLA achieves\nstate-of-the-art results on the nuScenes benchmark for trajectory planning and\ndemonstrates superior performance on 3D visual question-answering tasks,\noffering a scalable, interpretable, and fully vision-based solution for\nautonomous driving.", "AI": {"tldr": "OccVLA\u6846\u67b6\u5c063D\u5360\u7528\u8868\u793a\u96c6\u6210\u5230\u591a\u6a21\u6001\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u5373\u53ef\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u9a7e\u9a76\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MLLMs\u7f3a\u4e4f\u9c81\u68d2\u76843D\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u6784\u5efa\u6709\u6548\u76843D\u8868\u793a\u548c\u7f3a\u4e4f\u5927\u89c4\u6a213D\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u3002", "method": "\u63d0\u51faOccVLA\u6846\u67b6\uff0c\u5229\u75282D\u89c6\u89c9\u8f93\u5165\u5b66\u4e60\u7ec6\u7c92\u5ea6\u7684\u7a7a\u95f4\u7ed3\u6784\uff0c\u5c063D\u5360\u7528\u7387\u4f5c\u4e3a\u9884\u6d4b\u8f93\u51fa\u548c\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u8f68\u8ff9\u89c4\u5212\u7ed3\u679c\uff0c\u5e76\u57283D\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "OccVLA\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u5b8c\u5168\u57fa\u4e8e\u89c6\u89c9\u7684\u81ea\u52a8\u9a7e\u9a76\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.05685", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05685", "abs": "https://arxiv.org/abs/2509.05685", "authors": ["Jian Yang", "Jiahui Wu", "Li Fang", "Hongchao Fan", "Bianying Zhang", "Huijie Zhao", "Guangyi Yang", "Rui Xin", "Xiong You"], "title": "MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions", "comment": null, "summary": "Transforming road network data into vector representations using deep\nlearning has proven effective for road network analysis. However, urban road\nnetworks' heterogeneous and hierarchical nature poses challenges for accurate\nrepresentation learning. Graph neural networks, which aggregate features from\nneighboring nodes, often struggle due to their homogeneity assumption and focus\non a single structural scale. To address these issues, this paper presents\nMSRFormer, a novel road network representation learning framework that\nintegrates multi-scale spatial interactions by addressing their flow\nheterogeneity and long-distance dependencies. It uses spatial flow convolution\nto extract small-scale features from large trajectory datasets, and identifies\nscale-dependent spatial interaction regions to capture the spatial structure of\nroad networks and flow heterogeneity. By employing a graph transformer,\nMSRFormer effectively captures complex spatial dependencies across multiple\nscales. The spatial interaction features are fused using residual connections,\nwhich are fed to a contrastive learning algorithm to derive the final road\nnetwork representation. Validation on two real-world datasets demonstrates that\nMSRFormer outperforms baseline methods in two road network analysis tasks. The\nperformance gains of MSRFormer suggest the traffic-related task benefits more\nfrom incorporating trajectory data, also resulting in greater improvements in\ncomplex road network structures with up to 16% improvements compared to the\nmost competitive baseline method. This research provides a practical framework\nfor developing task-agnostic road network representation models and highlights\ndistinct association patterns of the interplay between scale effects and flow\nheterogeneity of spatial interactions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9053\u8def\u7f51\u7edc\u8868\u793a\u5b66\u4e60\u6846\u67b6MSRFormer\uff0c\u901a\u8fc7\u6574\u5408\u591a\u5c3a\u5ea6\u7a7a\u95f4\u4ea4\u4e92\u6765\u89e3\u51b3\u57ce\u5e02\u9053\u8def\u7f51\u7edc\u7684\u5f02\u6784\u6027\u548c\u5c42\u6b21\u6027\u95ee\u9898\uff0c\u5728\u4e24\u4e2a\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u8868\u793a\u57ce\u5e02\u9053\u8def\u7f51\u7edc\u7684\u5f02\u6784\u6027\u548c\u5c42\u6b21\u6027\u3002", "method": "\u63d0\u51faMSRFormer\u6846\u67b6\uff0c\u96c6\u6210\u591a\u5c3a\u5ea6\u7a7a\u95f4\u4ea4\u4e92\uff0c\u4f7f\u7528\u7a7a\u95f4\u6d41\u5377\u79ef\u63d0\u53d6\u5c0f\u5c3a\u5ea6\u7279\u5f81\uff0c\u8bc6\u522b\u5c3a\u5ea6\u76f8\u5173\u7684\u7a7a\u95f4\u4ea4\u4e92\u533a\u57df\uff0c\u5e76\u5229\u7528\u56feTransformer\u6355\u83b7\u590d\u6742\u7a7a\u95f4\u4f9d\u8d56\u6027\uff0c\u6700\u7ec8\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u7b97\u6cd5\u5f97\u5230\u9053\u8def\u7f51\u7edc\u8868\u793a\u3002", "result": "\u5728\u4e24\u4e2a\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u7684\u4e24\u4e2a\u9053\u8def\u7f51\u7edc\u5206\u6790\u4efb\u52a1\u4e2d\uff0cMSRFormer\u7684\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u590d\u6742\u9053\u8def\u7f51\u7edc\u7ed3\u6784\u4e0a\u63d0\u5347\u663e\u8457\uff08\u6700\u9ad8\u8fbe16%\uff09\u3002", "conclusion": "MSRFormer\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u4efb\u52a1\u65e0\u5173\u9053\u8def\u7f51\u7edc\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u63ed\u793a\u4e86\u5c3a\u5ea6\u6548\u5e94\u548c\u7a7a\u95f4\u4ea4\u4e92\u6d41\u5f02\u8d28\u6027\u4e4b\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u72ec\u7279\u5173\u8054\u6a21\u5f0f\u3002"}}
{"id": "2509.05714", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05714", "abs": "https://arxiv.org/abs/2509.05714", "authors": ["Zhaoyu Fan", "Kaihang Pan", "Mingze Zhou", "Bosheng Qin", "Juncheng Li", "Shengyu Zhang", "Wenqiao Zhang", "Siliang Tang", "Fei Wu", "Yueting Zhuang"], "title": "Towards Meta-Cognitive Knowledge Editing for Multimodal LLMs", "comment": "15 pages, 6 figures", "summary": "Knowledge editing enables multimodal large language models (MLLMs) to\nefficiently update outdated or incorrect information. However, existing\nbenchmarks primarily emphasize cognitive-level modifications while lacking a\nfocus on deeper meta-cognitive processes. To bridge this gap, we introduce\nCogEdit, a novel benchmark designed to evaluate MLLMs' meta-cognitive knowledge\nediting abilities across three levels: (1) Counterfactual-Driven Editing,\nassessing self-awareness of knowledge correctness changes; (2) Boundary\nConstraint Editing, ensuring appropriate generalization without unintended\ninterference; and (3) Noise-Robust Editing, promoting reflective evaluation of\nuncertain information. To advance meta-cognitive editing, we propose MIND\n(Meta-cognitive INtegrated Dynamic Knowledge Editing), a framework that\nconstructs a meta-knowledge memory for self-awareness, employs game-theoretic\ninteractions to monitor knowledge activation, and incorporates label refinement\nfor noise-robust updates. Extensive experiments show that MIND significantly\noutperforms existing cognitive editing approaches, achieving strong performance\non both traditional and meta-cognitive knowledge editing benchmarks.", "AI": {"tldr": "CogEdit\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30MLLMs\u7684\u5143\u8ba4\u77e5\u77e5\u8bc6\u7f16\u8f91\u80fd\u529b\uff0c\u5e76\u63d0\u51faMIND\u6846\u67b6\u4ee5\u6539\u8fdb\u5143\u8ba4\u77e5\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4fa7\u91cd\u8ba4\u77e5\u5c42\u9762\u7684\u4fee\u6539\uff0c\u5ffd\u7565\u4e86\u5143\u8ba4\u77e5\u8fc7\u7a0b\u3002", "method": "CogEdit\u57fa\u51c6\u6d4b\u8bd5\u6db5\u76d6\u53cd\u4e8b\u5b9e\u9a71\u52a8\u7f16\u8f91\u3001\u8fb9\u754c\u7ea6\u675f\u7f16\u8f91\u548c\u566a\u58f0\u9c81\u68d2\u7f16\u8f91\u4e09\u4e2a\u5c42\u9762\uff1bMIND\u6846\u67b6\u6784\u5efa\u5143\u77e5\u8bc6\u8bb0\u5fc6\u3001\u5229\u7528\u535a\u5f08\u8bba\u4ea4\u4e92\u548c\u6807\u7b7e\u7ec6\u5316\u3002", "result": "MIND\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8ba4\u77e5\u7f16\u8f91\u65b9\u6cd5\uff0c\u5728\u4f20\u7edf\u548c\u5143\u8ba4\u77e5\u77e5\u8bc6\u7f16\u8f91\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "CogEdit\u548cMIND\u6846\u67b6\u4e3a\u5143\u8ba4\u77e5\u77e5\u8bc6\u7f16\u8f91\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u65b9\u6cd5\u3002"}}
{"id": "2509.05757", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05757", "abs": "https://arxiv.org/abs/2509.05757", "authors": ["Sarang Patil", "Zeyong Zhang", "Yiran Huang", "Tengfei Ma", "Mengjia Xu"], "title": "Hyperbolic Large Language Models", "comment": "32 pages, 6 figures", "summary": "Large language models (LLMs) have achieved remarkable success and\ndemonstrated superior performance across various tasks, including natural\nlanguage processing (NLP), weather forecasting, biological protein folding,\ntext generation, and solving mathematical problems. However, many real-world\ndata exhibit highly non-Euclidean latent hierarchical anatomy, such as protein\nnetworks, transportation networks, financial networks, brain networks, and\nlinguistic structures or syntactic trees in natural languages. Effectively\nlearning intrinsic semantic entailment and hierarchical relationships from\nthese raw, unstructured input data using LLMs remains an underexplored area.\nDue to its effectiveness in modeling tree-like hierarchical structures,\nhyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularity\nas an expressive latent representation space for complex data modeling across\ndomains such as graphs, images, languages, and multi-modal data. Here, we\nprovide a comprehensive and contextual exposition of recent advancements in\nLLMs that leverage hyperbolic geometry as a representation space to enhance\nsemantic representation learning and multi-scale reasoning. Specifically, the\npaper presents a taxonomy of the principal techniques of Hyperbolic LLMs\n(HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/log\nmaps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4)\nhyperbolic state-space models. We also explore crucial potential applications\nand outline future research directions. A repository of key papers, models,\ndatasets, and code implementations is available at\nhttps://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5229\u7528\u53cc\u66f2\u51e0\u4f55\u589e\u5f3a\u8bed\u4e49\u8868\u793a\u5b66\u4e60\u548c\u591a\u5c3a\u5ea6\u63a8\u7406\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (HypLLMs) \u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u5c06\u5176\u6280\u672f\u5f52\u7eb3\u4e3a\u56db\u7c7b\uff1a\u57fa\u4e8e\u6307\u6570/\u5bf9\u6570\u6620\u5c04\u7684HypLLMs\u3001\u53cc\u66f2\u5fae\u8c03\u6a21\u578b\u3001\u5168\u53cc\u66f2LLMs\u548c\u53cc\u66f2\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u6709\u6548\u5b66\u4e60\u975e\u6b27\u51e0\u91cc\u5f97\u6570\u636e\u4e2d\u7684\u8bed\u4e49\u8574\u6db5\u548c\u5c42\u6b21\u5173\u7cfb\uff0c\u800c\u53cc\u66f2\u51e0\u4f55\u64c5\u957f\u5efa\u6a21\u6811\u72b6\u5c42\u6b21\u7ed3\u6784\uff0c\u56e0\u6b64\u672c\u6587\u7814\u7a76\u5c06\u53cc\u66f2\u51e0\u4f55\u5e94\u7528\u4e8eLLMs\u3002", "method": "\u5bf9\u73b0\u6709\u5229\u7528\u53cc\u66f2\u51e0\u4f55\u7684LLMs\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\u548c\u7efc\u8ff0\uff0c\u5e76\u63a2\u8ba8\u6f5c\u5728\u5e94\u7528\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "\u63d0\u51fa\u4e86HypLLMs\u7684\u56db\u7c7b\u4e3b\u8981\u6280\u672f\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b\u5173\u952e\u8bba\u6587\u3001\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5b9e\u73b0\u7684\u8d44\u6e90\u5e93\u3002", "conclusion": "\u53cc\u66f2\u51e0\u4f55\u4e3a\u589e\u5f3aLLMs\u7684\u8bed\u4e49\u8868\u793a\u5b66\u4e60\u548c\u591a\u5c3a\u5ea6\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u6a21\u578b\u6548\u7387\u3001\u53ef\u89e3\u91ca\u6027\u548c\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002"}}
{"id": "2509.05764", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05764", "abs": "https://arxiv.org/abs/2509.05764", "authors": ["Yuwei Lou", "Hao Hu", "Shaocong Ma", "Zongfei Zhang", "Liang Wang", "Jidong Ge", "Xianping Tao"], "title": "DRF: LLM-AGENT Dynamic Reputation Filtering Framework", "comment": "This paper has been accepted by ICONIP 2025 but not published", "summary": "With the evolution of generative AI, multi - agent systems leveraging large -\nlanguage models(LLMs) have emerged as a powerful tool for complex tasks.\nHowever, these systems face challenges in quantifying agent performance and\nlack mechanisms to assess agent credibility. To address these issues, we\nintroduce DRF, a dynamic reputation filtering framework. DRF constructs an\ninteractive rating network to quantify agent performance, designs a reputation\nscoring mechanism to measure agent honesty and capability, and integrates an\nUpper Confidence Bound - based strategy to enhance agent selection efficiency.\nExperiments show that DRF significantly improves task completion quality and\ncollaboration efficiency in logical reasoning and code - generation tasks,\noffering a new approach for multi - agent systems to handle large - scale\ntasks.", "AI": {"tldr": "DRF\u6846\u67b6\u63d0\u9ad8\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u590d\u6742\u4efb\u52a1\u7684\u5b8c\u6210\u8d28\u91cf\u548c\u534f\u4f5c\u6548\u7387", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u96be\u4ee5\u91cf\u5316\u667a\u80fd\u4f53\u6027\u80fd\u548c\u8bc4\u4f30\u5176\u53ef\u4fe1\u5ea6", "method": "\u6784\u5efa\u4ea4\u4e92\u5f0f\u8bc4\u5206\u7f51\u7edc\uff0c\u8bbe\u8ba1\u4fe1\u8a89\u8bc4\u5206\u673a\u5236\uff0c\u96c6\u6210\u57fa\u4e8eUCB\u7684\u7b56\u7565", "result": "\u5b9e\u9a8c\u8868\u660eDRF\u663e\u8457\u63d0\u9ad8\u4e86\u903b\u8f91\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u7684\u5b8c\u6210\u8d28\u91cf\u548c\u534f\u4f5c\u6548\u7387", "conclusion": "DRF\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5904\u7406\u5927\u89c4\u6a21\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5"}}
