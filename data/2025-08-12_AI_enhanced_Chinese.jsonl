{"id": "2508.06559", "categories": ["cs.AI", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06559", "abs": "https://arxiv.org/abs/2508.06559", "authors": ["Sina Baghal"], "title": "Solving Pasur Using GPU-Accelerated Counterfactual Regret Minimization", "comment": null, "summary": "Pasur is a fishing card game played over six rounds and is played similarly\nto games such as Cassino and Scopa, and Bastra. This paper introduces a\nCUDA-accelerated computational framework for simulating Pasur, emphasizing\nefficient memory management. We use our framework to compute near-Nash\nequilibria via Counterfactual Regret Minimization (CFR), a well-known algorithm\nfor solving large imperfect-information games.\n  Solving Pasur presents unique challenges due to its intricate rules and the\nlarge size of its game tree. We handle rule complexity using PyTorch CUDA\ntensors and to address the memory-intensive nature of the game, we decompose\nthe game tree into two key components: (1) actual game states, and (2)\ninherited scores from previous rounds. We construct the Full Game Tree by\npairing card states with accumulated scores in the Unfolding Process. This\ndesign reduces memory overhead by storing only essential strategy values and\nnode connections. To further manage computational complexity, we apply a\nround-by-round backward training strategy, starting from the final round and\nrecursively propagating average utilities to earlier stages. Our approach\nconstructs the complete game tree, which on average consists of over $10^9$\nnodes. We provide detailed implementation snippets.\n  After computing a near-Nash equilibrium strategy, we train a tree-based model\nto predict these strategies for use during gameplay. We then estimate the fair\nvalue of each deck through large-scale self-play between equilibrium strategies\nby simulating, for instance, 10,000 games per matchup, executed in parallel\nusing GPU acceleration.\n  Similar frameworks can be extended to other reinforcement learning algorithms\nwhere the action tree naturally decomposes into multiple rounds such as\nturn-based strategy games or sequential trading decisions in financial markets.", "AI": {"tldr": "\u5229\u7528CUDA\u52a0\u901f\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7CFR\u7b97\u6cd5\u6c42\u89e3Pasur\u6e38\u620f\u7684\u8fd1\u4f3c\u7eb3\u4ec0\u5747\u8861\u7b56\u7565\uff0c\u5e76\u8bad\u7ec3\u6811\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\uff0c\u6700\u7ec8\u4f30\u7b97\u6bcf\u526f\u724c\u7684\u516c\u5e73\u4ef7\u503c\u3002", "motivation": "Pasur\u6e38\u620f\u89c4\u5219\u590d\u6742\u4e14\u6e38\u620f\u6811\u5de8\u5927\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6c42\u89e3\u3002", "method": "\u4f7f\u7528PyTorch CUDA\u5f20\u91cf\u5904\u7406\u6e38\u620f\u89c4\u5219\u590d\u6742\u6027\uff0c\u5c06\u6e38\u620f\u6811\u5206\u89e3\u4e3a\u6e38\u620f\u72b6\u6001\u548c\u7ee7\u627f\u5206\u6570\u4e24\u90e8\u5206\uff0c\u91c7\u7528\u9010\u8f6e\u53cd\u5411\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u4f7f\u7528\u6811\u6a21\u578b\u9884\u6d4b\u7b56\u7565\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u5305\u542b\u8d85\u8fc710^9\u4e2a\u8282\u70b9\u7684\u6e38\u620f\u6811\uff0c\u8ba1\u7b97\u4e86\u8fd1\u4f3c\u7eb3\u4ec0\u5747\u8861\u7b56\u7565\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u6811\u6a21\u578b\u7528\u4e8e\u6e38\u620f\u7b56\u7565\u9884\u6d4b\uff0c\u4f30\u7b97\u4e86\u6bcf\u526f\u724c\u7684\u516c\u5e73\u4ef7\u503c\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2aCUDA\u52a0\u901f\u7684Pasur\u6e38\u620f\u6a21\u62df\u8ba1\u7b97\u6846\u67b6\uff0c\u5e76\u5229\u7528\u53cd\u4e8b\u5b9e\u540e\u6094\u6700\u5c0f\u5316\u7b97\u6cd5(CFR)\u8ba1\u7b97\u4e86\u8fd1\u4f3c\u7eb3\u4ec0\u5747\u8861\u7b56\u7565\uff0c\u6700\u540e\u8bad\u7ec3\u4e86\u4e00\u4e2a\u6811\u6a21\u578b\u6765\u9884\u6d4b\u8fd9\u4e9b\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u81ea\u535a\u5f08\u4f30\u8ba1\u4e86\u6bcf\u526f\u724c\u7684\u516c\u5e73\u4ef7\u503c\u3002"}}
{"id": "2508.06569", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06569", "abs": "https://arxiv.org/abs/2508.06569", "authors": ["Lance Yao", "Suman Samantray", "Ayana Ghosh", "Kevin Roccapriore", "Libor Kovarik", "Sarah Allec", "Maxim Ziatdinov"], "title": "Operationalizing Serendipity: Multi-Agent AI Workflows for Enhanced Materials Characterization with Theory-in-the-Loop", "comment": null, "summary": "The history of science is punctuated by serendipitous discoveries, where\nunexpected observations, rather than targeted hypotheses, opened new fields of\ninquiry. While modern autonomous laboratories excel at accelerating hypothesis\ntesting, their optimization for efficiency risks overlooking these crucial,\nunplanned findings. To address this gap, we introduce SciLink, an open-source,\nmulti-agent artificial intelligence framework designed to operationalize\nserendipity in materials research by creating a direct, automated link between\nexperimental observation, novelty assessment, and theoretical simulations. The\nframework employs a hybrid AI strategy where specialized machine learning\nmodels perform quantitative analysis of experimental data, while large language\nmodels handle higher-level reasoning. These agents autonomously convert raw\ndata from materials characterization techniques into falsifiable scientific\nclaims, which are then quantitatively scored for novelty against the published\nliterature. We demonstrate the framework's versatility across diverse research\nscenarios, showcasing its application to atomic-resolution and hyperspectral\ndata, its capacity to integrate real-time human expert guidance, and its\nability to close the research loop by proposing targeted follow-up experiments.\nBy systematically analyzing all observations and contextualizing them, SciLink\nprovides a practical framework for AI-driven materials research that not only\nenhances efficiency but also actively cultivates an environment ripe for\nserendipitous discoveries, thereby bridging the gap between automated\nexperimentation and open-ended scientific exploration.", "AI": {"tldr": "SciLink\u6846\u67b6\u81ea\u52a8\u5316\u6750\u6599\u7814\u7a76\u4e2d\u7684\u5076\u7136\u53d1\u73b0\uff0c\u63d0\u9ad8\u6548\u7387\u5e76\u4fc3\u8fdb\u5f00\u653e\u5f0f\u63a2\u7d22\u3002", "motivation": "\u73b0\u4ee3\u81ea\u52a8\u5316\u5b9e\u9a8c\u5ba4\u867d\u7136\u9ad8\u6548\uff0c\u4f46\u53ef\u80fd\u5ffd\u7565\u610f\u5916\u53d1\u73b0\u3002SciLink\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u7f3a\u9677\uff0c\u5c06\u5076\u7136\u6027\u7eb3\u5165\u6750\u6599\u7814\u7a76\u4e2d\u3002", "method": "SciLink\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u667a\u80fd\u4f53\u4eba\u5de5\u667a\u80fd\u6846\u67b6\uff0c\u91c7\u7528\u6df7\u5408AI\u7b56\u7565\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u5b9a\u91cf\u6570\u636e\u5206\u6790\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9ad8\u7ea7\u63a8\u7406\uff0c\u6700\u7ec8\u751f\u6210\u53ef\u8bc1\u4f2a\u7684\u79d1\u5b66\u8bba\u65ad\u5e76\u8bc4\u4f30\u5176\u65b0\u9896\u6027\u3002", "result": "SciLink\u6210\u529f\u5e94\u7528\u4e8e\u539f\u5b50\u5206\u8fa8\u7387\u548c\u9ad8\u5149\u8c31\u6570\u636e\u5206\u6790\uff0c\u80fd\u591f\u6574\u5408\u5b9e\u65f6\u4eba\u5de5\u4e13\u5bb6\u6307\u5bfc\uff0c\u5e76\u63d0\u51fa\u540e\u7eed\u5b9e\u9a8c\u5efa\u8bae\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6750\u6599\u7814\u7a76\u6548\u7387\uff0c\u5e76\u4fc3\u8fdb\u4e86\u5076\u7136\u53d1\u73b0\u3002", "conclusion": "SciLink\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5b9e\u9a8c\u89c2\u5bdf\u3001\u65b0\u9896\u6027\u8bc4\u4f30\u548c\u7406\u8bba\u6a21\u62df\u7684\u81ea\u52a8\u5316\u94fe\u63a5\uff0c\u4ece\u800c\u5728\u6750\u6599\u7814\u7a76\u4e2d\u6709\u6548\u5730\u4fc3\u8fdb\u4e86\u5076\u7136\u53d1\u73b0\u3002"}}
{"id": "2508.06571", "categories": ["cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06571", "abs": "https://arxiv.org/abs/2508.06571", "authors": ["Anqing Jiang", "Yu Gao", "Yiru Wang", "Zhigang Sun", "Shuo Wang", "Yuwen Heng", "Hao Sun", "Shichen Tang", "Lijuan Zhu", "Jinhao Chai", "Jijun Wang", "Zichong Gu", "Hao Jiang", "Li Sun"], "title": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "comment": "9 pagres, 2 figures", "summary": "Vision-Language-Action (VLA) models have demonstrated potential in autonomous\ndriving. However, two critical challenges hinder their development: (1)\nExisting VLA architectures are typically based on imitation learning in\nopen-loop setup which tends to capture the recorded behaviors in the dataset,\nleading to suboptimal and constrained performance, (2) Close-loop training\nrelies heavily on high-fidelity sensor simulation, where domain gaps and\ncomputational inefficiencies pose significant barriers. In this paper, we\nintroduce IRL-VLA, a novel close-loop Reinforcement Learning via\n\\textbf{I}nverse \\textbf{R}einforcement \\textbf{L}earning reward world model\nwith a self-built VLA approach. Our framework proceeds in a three-stage\nparadigm: In the first stage, we propose a VLA architecture and pretrain the\nVLA policy via imitation learning. In the second stage, we construct a\nlightweight reward world model via inverse reinforcement learning to enable\nefficient close-loop reward computation. To further enhance planning\nperformance, finally, we design specialized reward world model guidence\nreinforcement learning via PPO(Proximal Policy Optimization) to effectively\nbalance the safety incidents, comfortable driving, and traffic efficiency. Our\napproach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving\nbenchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that\nour framework will accelerate VLA research in close-loop autonomous driving.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u95ed\u73af\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5IRL-VLA\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684VLA\u67b6\u6784\u901a\u5e38\u57fa\u4e8e\u5f00\u73af\u8bbe\u7f6e\u4e0b\u7684\u6a21\u4eff\u5b66\u4e60\uff0c\u5b58\u5728\u6027\u80fd\u6b21\u4f18\u548c\u53d7\u9650\u7684\u95ee\u9898\uff1b\u95ed\u73af\u8bad\u7ec3\u4f9d\u8d56\u9ad8\u4fdd\u771f\u4f20\u611f\u5668\u6a21\u62df\uff0c\u5b58\u5728\u9886\u57df\u5dee\u5f02\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u95ed\u73af\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5IRL-VLA\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u9006\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u4e16\u754c\u6a21\u578b\u548c\u81ea\u5efaVLA\u65b9\u6cd5\uff0c\u5e76\u91c7\u7528\u4e09\u9636\u6bb5\u8303\u5f0f\u8fdb\u884c\u8bad\u7ec3\uff1a\u6a21\u4eff\u5b66\u4e60\u9884\u8bad\u7ec3\u3001\u9006\u5f3a\u5316\u5b66\u4e60\u6784\u5efa\u8f7b\u91cf\u7ea7\u5956\u52b1\u4e16\u754c\u6a21\u578b\u548c\u57fa\u4e8ePPO\u7684\u5f3a\u5316\u5b66\u4e60\u4ee5\u5e73\u8861\u5b89\u5168\u6027\u3001\u8212\u9002\u6027\u548c\u6548\u7387\u3002", "result": "\u5728NAVSIM v2\u7aef\u5230\u7aef\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728CVPR2025\u81ea\u52a8\u9a7e\u9a76\u5927\u6311\u6218\u4e2d\u83b7\u5f97\u4e9a\u519b\u3002", "conclusion": "IRL-VLA\u6a21\u578b\u5728NAVSIM v2\u7aef\u5230\u7aef\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728CVPR2025\u81ea\u52a8\u9a7e\u9a76\u5927\u6311\u6218\u4e2d\u83b7\u5f97\u4e9a\u519b\u3002"}}
{"id": "2508.06585", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06585", "abs": "https://arxiv.org/abs/2508.06585", "authors": ["Jayant Sravan Tamarapalli", "Rynaa Grover", "Nilay Pande", "Sahiti Yerramilli"], "title": "CountQA: How Well Do MLLMs Count in the Wild?", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in\nunderstanding visual scenes, yet they exhibit a critical lack in a fundamental\ncognitive skill: object counting. This blind spot severely limits their\nreliability in real-world applications. To date, this capability has been\nlargely unevaluated in complex scenarios, as existing benchmarks either feature\nsparse object densities or are confined to specific visual domains, failing to\ntest models under realistic conditions. Addressing this gap, we introduce\nCountQA, a challenging new benchmark designed to probe this deficiency.\nComprising over 1,500 question-answer pairs, CountQA features real-world images\nwith high object density, clutter, and occlusion. We investigate this weakness\nby evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the\ntop-performing model achieves a mere 42.9% accuracy, with performance declining\nas object counts rise. By providing a dedicated benchmark to diagnose and\nrectify this core weakness, CountQA paves the way for a new generation of MLLMs\nthat are not only descriptively fluent but also numerically grounded and\nspatially aware. We will open-source the dataset and code upon paper acceptance\nto foster further research.", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7269\u4f53\u8ba1\u6570\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0cCountQA\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u8fd9\u4e00\u7f3a\u9677\uff0c\u4e3a\u6539\u8fdbMLLM\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u8981\u4e48\u7269\u4f53\u5bc6\u5ea6\u7a00\u758f\uff0c\u8981\u4e48\u5c40\u9650\u4e8e\u7279\u5b9a\u7684\u89c6\u89c9\u9886\u57df\uff0c\u65e0\u6cd5\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u6d4b\u8bd5\u6a21\u578b\uff0c\u56e0\u6b64\u672c\u6587\u521b\u5efaCountQA\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30MLLM\u5728\u7269\u4f53\u8ba1\u6570\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aCountQA\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1500\u591a\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u4f7f\u7528\u5177\u6709\u9ad8\u7269\u4f53\u5bc6\u5ea6\u3001\u6742\u4e71\u548c\u906e\u6321\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u6765\u8bc4\u4f3015\u4e2a\u7a81\u51fa\u7684MLLM\u3002", "result": "\u9876\u7ea7\u6a21\u578b\u5728CountQA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u51c6\u786e\u7387\u4ec5\u4e3a42.9%\uff0c\u5e76\u4e14\u6027\u80fd\u968f\u7740\u7269\u4f53\u6570\u91cf\u7684\u589e\u52a0\u800c\u4e0b\u964d\u3002", "conclusion": "CountQA\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLM)\u5728\u7269\u4f53\u8ba1\u6570\u65b9\u9762\u7684\u5173\u952e\u7f3a\u9677\uff0c\u5373\u4f7f\u662f\u9876\u7ea7\u6a21\u578b\u7684\u51c6\u786e\u7387\u4e5f\u53ea\u670942.9%\uff0c\u5e76\u4e14\u968f\u7740\u7269\u4f53\u6570\u91cf\u7684\u589e\u52a0\u800c\u4e0b\u964d\u3002"}}
{"id": "2508.06668", "categories": ["cs.AI", "cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.06668", "abs": "https://arxiv.org/abs/2508.06668", "authors": ["Jessie Galasso"], "title": "Formal Concept Analysis: a Structural Framework for Variability Extraction and Analysis", "comment": null, "summary": "Formal Concept Analysis (FCA) is a mathematical framework for knowledge\nrepresentation and discovery. It performs a hierarchical clustering over a set\nof objects described by attributes, resulting in conceptual structures in which\nobjects are organized depending on the attributes they share. These conceptual\nstructures naturally highlight commonalities and variabilities among similar\nobjects by categorizing them into groups which are then arranged by similarity,\nmaking it particularly appropriate for variability extraction and analysis.\nDespite the potential of FCA, determining which of its properties can be\nleveraged for variability-related tasks (and how) is not always\nstraightforward, partly due to the mathematical orientation of its foundational\nliterature. This paper attempts to bridge part of this gap by gathering a\nselection of properties of the framework which are essential to variability\nanalysis, and how they can be used to interpret diverse variability information\nwithin the resulting conceptual structures.", "AI": {"tldr": "\u672c\u6587\u9610\u8ff0\u4e86\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\u5728\u53d8\u5f02\u6027\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u603b\u7ed3\u4e86\u5176\u5173\u952e\u5c5e\u6027\u548c\u4f7f\u7528\u65b9\u6cd5\u3002", "motivation": "FCA\u5177\u6709\u63d0\u53d6\u548c\u5206\u6790\u53d8\u5f02\u6027\u7684\u6f5c\u529b\uff0c\u4f46\u5176\u6570\u5b66\u5bfc\u5411\u7684\u6587\u732e\u4f7f\u5f97\u786e\u5b9a\u5176\u54ea\u4e9b\u5c5e\u6027\u53ef\u7528\u4e8e\u53d8\u5f02\u6027\u76f8\u5173\u4efb\u52a1\u5e76\u4e0d\u603b\u662f\u76f4\u622a\u4e86\u5f53\u7684\u3002", "method": "\u672c\u6587\u901a\u8fc7\u6536\u96c6FCA\u6846\u67b6\u4e2d\u4e00\u4e9b\u4e0e\u53d8\u5f02\u6027\u5206\u6790\u76f8\u5173\u7684\u5c5e\u6027\uff0c\u5e76\u89e3\u91ca\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u5c5e\u6027\u6765\u89e3\u91ca\u7ed3\u679c\u6982\u5ff5\u7ed3\u6784\u4e2d\u7684\u5404\u79cd\u53d8\u5f02\u4fe1\u606f\uff0c\u6765\u5f25\u5408FCA\u7684\u6570\u5b66\u7406\u8bba\u548c\u53d8\u5f02\u6027\u5206\u6790\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "result": "\u672c\u6587\u8bc6\u522b\u5e76\u89e3\u91ca\u4e86FCA\u6846\u67b6\u4e2d\u4e00\u4e9b\u5173\u952e\u5c5e\u6027\u5728\u53d8\u5f02\u6027\u5206\u6790\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u5f62\u5f0f\u6982\u5ff5\u5206\u6790(FCA)\u6846\u67b6\u4e2d\u7528\u4e8e\u53d8\u5f02\u6027\u5206\u6790\u7684\u5173\u952e\u5c5e\u6027\u53ca\u5176\u5728\u89e3\u91ca\u4e0d\u540c\u53d8\u5f02\u4fe1\u606f\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2508.06674", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06674", "abs": "https://arxiv.org/abs/2508.06674", "authors": ["Weijie Shi", "Yue Cui", "Hao Chen", "Jiaming Li", "Mengze Li", "Jia Zhu", "Jiajie Xu", "Xiaofang Zhou"], "title": "Zero-Shot Cellular Trajectory Map Matching", "comment": null, "summary": "Cellular Trajectory Map-Matching (CTMM) aims to align cellular location\nsequences to road networks, which is a necessary preprocessing in\nlocation-based services on web platforms like Google Maps, including navigation\nand route optimization. Current approaches mainly rely on ID-based features and\nregion-specific data to learn correlations between cell towers and roads,\nlimiting their adaptability to unexplored areas. To enable high-accuracy CTMM\nwithout additional training in target regions, Zero-shot CTMM requires to\nextract not only region-adaptive features, but also sequential and location\nuncertainty to alleviate positioning errors in cellular data. In this paper, we\npropose a pixel-based trajectory calibration assistant for zero-shot CTMM,\nwhich takes advantage of transferable geospatial knowledge to calibrate\npixelated trajectory, and then guide the path-finding process at the road\nnetwork level. To enhance knowledge sharing across similar regions, a Gaussian\nmixture model is incorporated into VAE, enabling the identification of\nscenario-adaptive experts through soft clustering. To mitigate high positioning\nerrors, a spatial-temporal awareness module is designed to capture sequential\nfeatures and location uncertainty, thereby facilitating the inference of\napproximate user positions. Finally, a constrained path-finding algorithm is\nemployed to reconstruct the road ID sequence, ensuring topological validity\nwithin the road network. This process is guided by the calibrated trajectory\nwhile optimizing for the shortest feasible path, thus minimizing unnecessary\ndetours. Extensive experiments demonstrate that our model outperforms existing\nmethods in zero-shot CTMM by 16.8\\%.", "AI": {"tldr": "\u5229\u7528\u53ef\u8fc1\u79fb\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u548c\u65f6\u7a7a\u611f\u77e5\u6a21\u5757\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u50cf\u7d20\u7684\u8f68\u8ff9\u6821\u51c6\u8f85\u52a9\u5de5\u5177\uff0c\u63d0\u5347\u96f6\u6837\u672cCTMM\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd516.8%\u3002", "motivation": "\u73b0\u6709\u7684CTMM\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u57fa\u4e8eID\u7684\u7279\u5f81\u548c\u7279\u5b9a\u533a\u57df\u7684\u6570\u636e\uff0c\u96be\u4ee5\u9002\u5e94\u672a\u63a2\u7d22\u533a\u57df\u3002\u96f6\u6837\u672cCTMM\u9700\u8981\u63d0\u53d6\u533a\u57df\u81ea\u9002\u5e94\u7279\u5f81\u3001\u5e8f\u5217\u7279\u5f81\u548c\u4f4d\u7f6e\u4e0d\u786e\u5b9a\u6027\u6765\u51cf\u8f7b\u8702\u7a9d\u6570\u636e\u4e2d\u7684\u5b9a\u4f4d\u8bef\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u50cf\u7d20\u7684\u8f68\u8ff9\u6821\u51c6\u8f85\u52a9\u5de5\u5177\uff0c\u8be5\u5de5\u5177\u5229\u7528\u53ef\u8fc1\u79fb\u7684\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\uff0c\u5e76\u7ed3\u5408\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u548c\u65f6\u7a7a\u611f\u77e5\u6a21\u5757\uff0c\u6700\u7ec8\u4f7f\u7528\u7ea6\u675f\u8def\u5f84\u67e5\u627e\u7b97\u6cd5\u91cd\u5efa\u9053\u8defID\u5e8f\u5217\u3002", "result": "\u8be5\u6a21\u578b\u5728\u96f6\u6837\u672cCTMM\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd516.8%\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u50cf\u7d20\u7684\u8f68\u8ff9\u6821\u51c6\u8f85\u52a9\u5de5\u5177\uff0c\u7528\u4e8e\u96f6\u6837\u672cCTMM\uff0c\u8be5\u5de5\u5177\u5229\u7528\u53ef\u8fc1\u79fb\u7684\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u6765\u6821\u51c6\u50cf\u7d20\u5316\u8f68\u8ff9\uff0c\u7136\u540e\u6307\u5bfc\u9053\u8def\u7f51\u7edc\u7ea7\u522b\u7684\u8def\u5f84\u67e5\u627e\u8fc7\u7a0b\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5c06\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u878d\u5165VAE\uff0c\u5b9e\u73b0\u4e86\u8de8\u76f8\u4f3c\u533a\u57df\u7684\u77e5\u8bc6\u5171\u4eab\uff0c\u5e76\u8bbe\u8ba1\u4e86\u65f6\u7a7a\u611f\u77e5\u6a21\u5757\u4ee5\u51cf\u8f7b\u9ad8\u5b9a\u4f4d\u8bef\u5dee\u3002\u6700\u7ec8\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd516.8%\u3002"}}
{"id": "2508.06706", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.06706", "abs": "https://arxiv.org/abs/2508.06706", "authors": ["Jaikrishna Manojkumar Patil", "Nathaniel Lee", "Al Mehdi Saadat Chowdhury", "YooJung Choi", "Paulo Shakarian"], "title": "Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets", "comment": null, "summary": "Rule-based methods for knowledge graph completion provide explainable results\nbut often require a significantly large number of rules to achieve competitive\nperformance. This can hinder explainability due to overwhelmingly large rule\nsets. We discover rule contexts (meaningful subsets of rules that work\ntogether) from training data and use learned probability distribution (i.e.\nprobabilistic circuits) over these rule contexts to more rapidly achieve\nperformance of the full rule set. Our approach achieves a 70-96% reduction in\nnumber of rules used while outperforming baseline by up to 31$\\times$ when\nusing equivalent minimal number of rules and preserves 91% of peak baseline\nperformance even when comparing our minimal rule sets against baseline's full\nrule sets. We show that our framework is grounded in well-known semantics of\nprobabilistic logic, does not require independence assumptions, and that our\ntractable inference procedure provides both approximate lower bounds and exact\nprobability of a given query. The efficacy of our method is validated by\nempirical studies on 8 standard benchmark datasets where we show competitive\nperformance by using only a fraction of the rules required by AnyBURL's\nstandard inference method, the current state-of-the-art for rule-based\nknowledge graph completion. This work may have further implications for general\nprobabilistic reasoning over learned sets of rules.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u89c4\u5219\u4e0a\u4e0b\u6587\u4e0a\u7684\u6982\u7387\u5206\u5e03\uff0c\u51cf\u5c11\u57fa\u4e8e\u89c4\u5219\u7684\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4e2d\u6240\u9700\u89c4\u5219\u7684\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u89c4\u5219\u7684\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u65b9\u6cd5\u867d\u7136\u7ed3\u679c\u53ef\u89e3\u91ca\uff0c\u4f46\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u89c4\u5219\u624d\u80fd\u8fbe\u5230\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u8fd9\u963b\u788d\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u53d1\u73b0\u89c4\u5219\u4e0a\u4e0b\u6587\uff08\u4e00\u8d77\u5de5\u4f5c\u7684\u6709\u610f\u4e49\u7684\u89c4\u5219\u5b50\u96c6\uff09\uff0c\u5e76\u4f7f\u7528\u8fd9\u4e9b\u89c4\u5219\u4e0a\u4e0b\u6587\u4e0a\u7684\u5b66\u4e60\u6982\u7387\u5206\u5e03\uff08\u5373\u6982\u7387\u7535\u8def\uff09\u6765\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06\u89c4\u5219\u6570\u91cf\u51cf\u5c11\u4e8670-96%\uff0c\u540c\u65f6\u5728\u4f7f\u7528\u76f8\u540c\u6700\u5c0f\u6570\u91cf\u89c4\u5219\u65f6\uff0c\u6027\u80fd\u6bd4\u57fa\u7ebf\u9ad8\u51fa31\u500d\uff0c\u5373\u4f7f\u4e0e\u57fa\u7ebf\u7684\u5b8c\u6574\u89c4\u5219\u96c6\u76f8\u6bd4\uff0c\u4e5f\u80fd\u4fdd\u630191%\u7684\u5cf0\u503c\u57fa\u7ebf\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u57288\u4e2a\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u4ec5\u4f7f\u7528\u4e86AnyBURL\u6807\u51c6\u63a8\u7406\u65b9\u6cd5\u6240\u9700\u89c4\u5219\u7684\u4e00\u5c0f\u90e8\u5206\u3002"}}
{"id": "2508.06716", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.06716", "abs": "https://arxiv.org/abs/2508.06716", "authors": ["Blair Johnson", "Clayton Kerce", "Faramarz Fekri"], "title": "GLIDR: Graph-Like Inductive Logic Programming with Differentiable Reasoning", "comment": null, "summary": "Differentiable inductive logic programming (ILP) techniques have proven\neffective at finding approximate rule-based solutions to link prediction and\nnode classification problems on knowledge graphs; however, the common\nassumption of chain-like rule structure can hamper the performance and\ninterpretability of existing approaches. We introduce GLIDR, a differentiable\nrule learning method that models the inference of logic rules with more\nexpressive syntax than previous methods. GLIDR uses a differentiable message\npassing inference algorithm that generalizes previous chain-like rule learning\nmethods to allow rules with features like branches and cycles. GLIDR has a\nsimple and expressive rule search space which is parameterized by a limit on\nthe maximum number of free variables that may be included in a rule. Explicit\nlogic rules can be extracted from the weights of a GLIDR model for use with\nsymbolic solvers. We demonstrate that GLIDR can significantly outperform\nexisting rule learning methods on knowledge graph completion tasks and even\ncompete with embedding methods despite the inherent disadvantage of being a\nstructure-only prediction method. We show that rules extracted from GLIDR\nretain significant predictive performance, and that GLIDR is highly robust to\ntraining data noise. Finally, we demonstrate that GLIDR can be chained with\ndeep neural networks and optimized end-to-end for rule learning on arbitrary\ndata modalities.", "AI": {"tldr": "GLIDR\u662f\u4e00\u79cd\u65b0\u7684\u53ef\u5fae\u5206\u89c4\u5219\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u53ef\u5fae\u5206\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u6280\u672f\u901a\u5e38\u5047\u8bbe\u94fe\u5f0f\u89c4\u5219\u7ed3\u6784\uff0c\u8fd9\u4f1a\u5f71\u54cd\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "GLIDR\u4f7f\u7528\u53ef\u5fae\u5206\u7684\u6d88\u606f\u4f20\u9012\u63a8\u7406\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5c06\u4ee5\u524d\u7684\u94fe\u5f0f\u89c4\u5219\u5b66\u4e60\u65b9\u6cd5\u63a8\u5e7f\u5230\u5141\u8bb8\u5177\u6709\u5206\u652f\u548c\u5faa\u73af\u7b49\u7279\u5f81\u7684\u89c4\u5219\u3002", "result": "GLIDR\u5728\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89c4\u5219\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u4e14\u5177\u6709\u5f88\u5f3a\u7684\u9c81\u68d2\u6027\u3002\u53ef\u4ee5\u4eceGLIDR\u4e2d\u63d0\u53d6\u7684\u89c4\u5219\u4fdd\u7559\u4e86\u663e\u8457\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "GLIDR,\u4e00\u79cd\u65b0\u7684\u53ef\u5fae\u5206\u89c4\u5219\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89c4\u5219\u5b66\u4e60\u65b9\u6cd5\uff0c\u751a\u81f3\u53ef\u4ee5\u4e0e\u5d4c\u5165\u65b9\u6cd5\u7ade\u4e89\u3002GLIDR\u5177\u6709\u7b80\u5355\u7684\u89c4\u5219\u641c\u7d22\u7a7a\u95f4\uff0c\u5e76\u4e14\u53ef\u4ee5\u63d0\u53d6\u663e\u5f0f\u903b\u8f91\u89c4\u5219\u3002"}}
{"id": "2508.06736", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06736", "abs": "https://arxiv.org/abs/2508.06736", "authors": ["Alican Yilmaz", "Junyang Cai", "Serdar Kadioglu", "Bistra Dilkina"], "title": "ParBalans: Parallel Multi-Armed Bandits-based Adaptive Large Neighborhood Search", "comment": null, "summary": "Solving Mixed-Integer Programming (MIP) problems often requires substantial\ncomputational resources due to their combinatorial nature. Parallelization has\nemerged as a critical strategy to accelerate solution times and enhance\nscalability to tackle large, complex instances. This paper investigates the\nparallelization capabilities of Balans, a recently proposed multi-armed\nbandits-based adaptive large neighborhood search for MIPs. While Balans's\nmodular architecture inherently supports parallel exploration of diverse\nparameter configurations, this potential has not been thoroughly examined. To\naddress this gap, we introduce ParBalans, an extension that leverages both\nsolver-level and algorithmic-level parallelism to improve performance on\nchallenging MIP instances. Our experimental results demonstrate that ParBalans\nexhibits competitive performance compared to the state-of-the-art commercial\nsolver Gurobi, particularly on hard optimization benchmarks.", "AI": {"tldr": "ParBalans\u901a\u8fc7\u5e76\u884c\u5316\u6539\u8fdbMIP\u6c42\u89e3\u6027\u80fd\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0eGurobi\u7ade\u4e89\u3002", "motivation": "\u89e3\u51b3MIP\u95ee\u9898\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u5e76\u884c\u5316\u662f\u52a0\u901f\u6c42\u89e3\u65f6\u95f4\u548c\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\u7684\u5173\u952e\u7b56\u7565\u3002", "method": "ParBalans\u5229\u7528\u6c42\u89e3\u5668\u7ea7\u548c\u7b97\u6cd5\u7ea7\u5e76\u884c\u5316\u6765\u6539\u8fdb\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eParBalans\u7684\u6027\u80fd\u4f18\u4e8eGurobi\u3002", "conclusion": "ParBalans\uff0c\u4e00\u79cd\u57fa\u4e8e\u591a\u81c2\u8001\u864e\u673a\u81ea\u9002\u5e94\u5927\u90bb\u57df\u641c\u7d22\u7684MIP\u6c42\u89e3\u5668\u5e76\u884c\u5316\u6269\u5c55\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684MIP\u5b9e\u4f8b\u4e0a\u5c55\u73b0\u51fa\u4e0e\u6700\u5148\u8fdb\u7684\u5546\u4e1a\u6c42\u89e3\u5668Gurobi\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.06746", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06746", "abs": "https://arxiv.org/abs/2508.06746", "authors": ["Xin Tang", "Qian Chen", "Fengshun Li", "Youchun Gong", "Yinqiu Liu", "Wen Tian", "Shaowen Qin", "Xiaohuan Li"], "title": "Topology Generation of UAV Covert Communication Networks: A Graph Diffusion Approach with Incentive Mechanism", "comment": null, "summary": "With the growing demand for Uncrewed Aerial Vehicle (UAV) networks in\nsensitive applications, such as urban monitoring, emergency response, and\nsecure sensing, ensuring reliable connectivity and covert communication has\nbecome increasingly vital. However, dynamic mobility and exposure risks pose\nsignificant challenges. To tackle these challenges, this paper proposes a\nself-organizing UAV network framework combining Graph Diffusion-based Policy\nOptimization (GDPO) with a Stackelberg Game (SG)-based incentive mechanism. The\nGDPO method uses generative AI to dynamically generate sparse but\nwell-connected topologies, enabling flexible adaptation to changing node\ndistributions and Ground User (GU) demands. Meanwhile, the Stackelberg Game\n(SG)-based incentive mechanism guides self-interested UAVs to choose relay\nbehaviors and neighbor links that support cooperation and enhance covert\ncommunication. Extensive experiments are conducted to validate the\neffectiveness of the proposed framework in terms of model convergence, topology\ngeneration quality, and enhancement of covert communication performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eGDPO\u548cSG\u7684\u65e0\u4eba\u673a\u7f51\u7edc\u81ea\u7ec4\u7ec7\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u53ef\u9760\u8fde\u63a5\u548c\u9690\u853d\u901a\u4fe1\u3002", "motivation": "\u65e5\u76ca\u589e\u957f\u7684\u65e0\u4eba\u673a\u7f51\u7edc\u9700\u6c42\u4ee5\u53ca\u5728\u57ce\u5e02\u76d1\u63a7\u3001\u5e94\u6025\u54cd\u5e94\u548c\u5b89\u5168\u4f20\u611f\u7b49\u654f\u611f\u5e94\u7528\u4e2d\u786e\u4fdd\u53ef\u9760\u8fde\u63a5\u548c\u9690\u853d\u901a\u4fe1\u7684\u5fc5\u8981\u6027\u3002", "method": "\u91c7\u7528\u751f\u6210\u5f0fAI\u52a8\u6001\u751f\u6210\u7a00\u758f\u4f46\u8fde\u63a5\u826f\u597d\u7684\u62d3\u6251\u7ed3\u6784\uff0c\u5e76\u5229\u7528\u57fa\u4e8eStackelberg\u535a\u5f08\u7684\u6fc0\u52b1\u673a\u5236\u5f15\u5bfc\u65e0\u4eba\u673a\u9009\u62e9\u652f\u6301\u5408\u4f5c\u5e76\u589e\u5f3a\u9690\u853d\u901a\u4fe1\u7684\u4e2d\u7ee7\u884c\u4e3a\u548c\u76f8\u90bb\u94fe\u8def\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u6a21\u578b\u6536\u655b\u6027\u3001\u62d3\u6251\u751f\u6210\u8d28\u91cf\u548c\u9690\u853d\u901a\u4fe1\u6027\u80fd\u589e\u5f3a\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u6269\u6563\u7b56\u7565\u4f18\u5316(GDPO)\u548c\u57fa\u4e8eStackelberg\u535a\u5f08(SG)\u6fc0\u52b1\u673a\u5236\u7684\u65e0\u4eba\u673a\u7f51\u7edc\u81ea\u7ec4\u7ec7\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u52a8\u6001\u79fb\u52a8\u548c\u66b4\u9732\u98ce\u9669\u5e26\u6765\u7684\u6311\u6218\uff0c\u4ee5\u786e\u4fdd\u53ef\u9760\u7684\u8fde\u63a5\u548c\u9690\u853d\u901a\u4fe1\u3002"}}
{"id": "2508.06753", "categories": ["cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.06753", "abs": "https://arxiv.org/abs/2508.06753", "authors": ["Evangelos Georganas", "Dhiraj Kalamkar", "Alexander Heinecke"], "title": "Pushing the Envelope of LLM Inference on AI-PC", "comment": null, "summary": "The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the\nperplexity and end-task performance of their full-precision counterparts using\nthe same model size, is ushering in a new era of LLM inference for\nresource-constrained environments such as edge devices and AI PCs. While these\nquantization advances promise models that are more cost-effective in terms of\nlatency, memory, throughput, and energy consumption, the computational\nefficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp)\nused to deploy them remains underexplored. In this work, we take a bottom-up\napproach: we first design and implement 1-bit and 2-bit microkernels optimized\nfor modern CPUs, achieving peak computational efficiency across a variety of\nCPU platforms. We integrate these microkernels into a state-of-the-art LLM\ninference framework, namely PyTorch-TPP, and present end-to-end inference\nresults with 2-bit models that outperform the current SOTA runtime bitnet.cpp\nby up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model\ninference. Our optimized runtime advances the state of LLM inference on AI PCs\nand edge devices, paving the way for efficient deployment of ultra-low-bit LLM\nmodels.", "AI": {"tldr": "\u901a\u8fc7\u4f18\u5316\u5fae\u5185\u6838\u548c\u96c6\u6210\u5230PyTorch-TPP\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u8d85\u4f4e\u6bd4\u7279LLM\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\uff0c\u901f\u5ea6\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u63a2\u7d22\u8d85\u4f4e\u6bd4\u7279LLM\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4ee5\u63d0\u9ad8\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0bLLM\u63a8\u7406\u7684\u6548\u7387\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b01\u6bd4\u7279\u548c2\u6bd4\u7279\u5fae\u5185\u6838\uff0c\u96c6\u6210\u5230PyTorch-TPP\u6846\u67b6\u3002", "result": "2\u6bd4\u7279\u6a21\u578b\u7aef\u5230\u7aef\u63a8\u7406\u6027\u80fd\u4f18\u4e8ebitnet.cpp\uff0c\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe2.2\u500d\uff1b\u76f8\u6bd416\u6bd4\u7279\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe7\u500d\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u9488\u5bf9\u73b0\u4ee3CPU\u4f18\u5316\u76841\u6bd4\u7279\u548c2\u6bd4\u7279\u5fae\u5185\u6838\uff0c\u5e76\u5c06\u8fd9\u4e9b\u5fae\u5185\u6838\u96c6\u6210\u5230\u6700\u5148\u8fdb\u7684LLM\u63a8\u7406\u6846\u67b6PyTorch-TPP\u4e2d\uff0c\u6700\u7ec8\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u75282\u6bd4\u7279\u6a21\u578b\u7684\u7aef\u5230\u7aef\u63a8\u7406\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u8fd0\u884c\u65f6bitnet.cpp\uff0c\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe2.2\u500d\uff0c\u76f8\u6bd416\u6bd4\u7279\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe7\u500d\u3002"}}
{"id": "2508.06754", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.06754", "abs": "https://arxiv.org/abs/2508.06754", "authors": ["Vanessa Figueiredo"], "title": "A Fuzzy Logic Prompting Framework for Large Language Models in Adaptive and Uncertain Tasks", "comment": null, "summary": "We introduce a modular prompting framework that supports safer and more\nadaptive use of large language models (LLMs) across dynamic, user-centered\ntasks. Grounded in human learning theory, particularly the Zone of Proximal\nDevelopment (ZPD), our method combines a natural language boundary prompt with\na control schema encoded with fuzzy scaffolding logic and adaptation rules.\nThis architecture enables LLMs to modulate behavior in response to user state\nwithout requiring fine-tuning or external orchestration. In a simulated\nintelligent tutoring setting, the framework improves scaffolding quality,\nadaptivity, and instructional alignment across multiple models, outperforming\nstandard prompting baselines. Evaluation is conducted using rubric-based LLM\ngraders at scale. While initially developed for education, the framework has\nshown promise in other interaction-heavy domains, such as procedural content\ngeneration for games. Designed for safe deployment, it provides a reusable\nmethodology for structuring interpretable, goal-aligned LLM behavior in\nuncertain or evolving contexts.", "AI": {"tldr": "\u6a21\u5757\u5316\u63d0\u793a\u6846\u67b6\u4f7fLLM\u66f4\u5b89\u5168\u3001\u66f4\u81ea\u9002\u5e94\u5730\u5904\u7406\u7528\u6237\u4e2d\u5fc3\u4efb\u52a1\uff0c\u5728\u6a21\u62df\u667a\u80fd\u8f85\u5bfc\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u57fa\u4e8e\u4eba\u7c7b\u5b66\u4e60\u7406\u8bba\uff08\u7279\u522b\u662f\u6700\u8fd1\u53d1\u5c55\u533a\uff09\uff0c\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u5b89\u5168\u3001\u53ef\u89e3\u91ca\u3001\u76ee\u6807\u4e00\u81f4\u7684LLM\u884c\u4e3a\u7ed3\u6784\uff0c\u4ee5\u5e94\u5bf9\u4e0d\u786e\u5b9a\u6216\u4e0d\u65ad\u53d8\u5316\u7684\u73af\u5883\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u8fb9\u754c\u63d0\u793a\u3001\u6a21\u7cca\u811a\u624b\u67b6\u903b\u8f91\u548c\u81ea\u9002\u5e94\u89c4\u5219\u7f16\u7801\u7684\u63a7\u5236\u6a21\u5f0f\uff0c\u4f7fLLM\u80fd\u591f\u6839\u636e\u7528\u6237\u72b6\u6001\u8c03\u8282\u884c\u4e3a\uff0c\u65e0\u9700\u5fae\u8c03\u6216\u5916\u90e8\u534f\u8c03\u3002", "result": "\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u811a\u624b\u67b6\u8d28\u91cf\u3001\u9002\u5e94\u6027\u548c\u6559\u5b66\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u6559\u80b2\u548c\u6e38\u620f\u7a0b\u5e8f\u5185\u5bb9\u751f\u6210\u7b49\u591a\u4e2a\u9886\u57df\u5c55\u73b0\u51fa\u6f5c\u529b\u3002", "conclusion": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684\u63d0\u793a\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u5b89\u5168\u3001\u66f4\u81ea\u9002\u5e94\u5730\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5904\u7406\u52a8\u6001\u7684\u7528\u6237\u4e2d\u5fc3\u4efb\u52a1\uff0c\u5e76\u5728\u6a21\u62df\u7684\u667a\u80fd\u8f85\u5bfc\u73af\u5883\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u4f18\u4e8e\u6807\u51c6\u63d0\u793a\u57fa\u7ebf\u3002"}}
{"id": "2508.06823", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06823", "abs": "https://arxiv.org/abs/2508.06823", "authors": ["Xuan Zhao", "Jun Tao"], "title": "Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation", "comment": "Accepted by IEEE VIS 2025", "summary": "Exploring volumetric data is crucial for interpreting scientific datasets.\nHowever, selecting optimal viewpoints for effective navigation can be\nchallenging, particularly for users without extensive domain expertise or\nfamiliarity with 3D navigation. In this paper, we propose a novel framework\nthat leverages natural language interaction to enhance volumetric data\nexploration. Our approach encodes volumetric blocks to capture and\ndifferentiate underlying structures. It further incorporates a CLIP Score\nmechanism, which provides semantic information to the blocks to guide\nnavigation. The navigation is empowered by a reinforcement learning framework\nthat leverage these semantic cues to efficiently search for and identify\ndesired viewpoints that align with the user's intent. The selected viewpoints\nare evaluated using CLIP Score to ensure that they best reflect the user\nqueries. By automating viewpoint selection, our method improves the efficiency\nof volumetric data navigation and enhances the interpretability of complex\nscientific phenomena.", "AI": {"tldr": "\u5229\u7528\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u81ea\u52a8\u9009\u62e9\u6700\u4f73\u89c6\u89d2\u4ee5\u9ad8\u6548\u63a2\u7d22\u4f53\u6570\u636e\u3002", "motivation": "\u63a2\u7d22\u4f53\u6570\u636e\u5bf9\u4e8e\u89e3\u91ca\u79d1\u5b66\u6570\u636e\u96c6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9009\u62e9\u6700\u4f73\u89c6\u89d2\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u5bf9\u4e8e\u7f3a\u4e4f\u4e13\u4e1a\u77e5\u8bc6\u7684\u7528\u6237\u3002", "method": "\u8be5\u65b9\u6cd5\u7f16\u7801\u4f53\u5757\u4ee5\u6355\u6349\u548c\u533a\u5206\u5e95\u5c42\u7ed3\u6784\uff0c\u7ed3\u5408CLIP\u8bc4\u5206\u673a\u5236\u63d0\u4f9b\u8bed\u4e49\u4fe1\u606f\u6307\u5bfc\u5bfc\u822a\uff0c\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u9ad8\u6548\u641c\u7d22\u548c\u8bc6\u522b\u7b26\u5408\u7528\u6237\u610f\u56fe\u7684\u89c6\u89d2\u3002", "result": "\u8be5\u65b9\u6cd5\u81ea\u52a8\u5316\u89c6\u89d2\u9009\u62e9\uff0c\u63d0\u9ad8\u4e86\u4f53\u6570\u636e\u5bfc\u822a\u6548\u7387\u548c\u590d\u6742\u79d1\u5b66\u73b0\u8c61\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u589e\u5f3a\u4f53\u6570\u636e\u63a2\u7d22\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7f16\u7801\u4f53\u5757\u5e76\u7ed3\u5408CLIP\u8bc4\u5206\u673a\u5236\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u81ea\u52a8\u9009\u62e9\u6700\u4f73\u89c6\u89d2\uff0c\u63d0\u9ad8\u4e86\u4f53\u6570\u636e\u5bfc\u822a\u6548\u7387\u548c\u590d\u6742\u79d1\u5b66\u73b0\u8c61\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.06832", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06832", "abs": "https://arxiv.org/abs/2508.06832", "authors": ["Haifeng Li", "Wang Guo", "Haiyang Wu", "Mengwei Wu", "Jipeng Zhang", "Qing Zhu", "Yu Liu", "Xin Huang", "Chao Tao"], "title": "Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges", "comment": null, "summary": "The mainstream paradigm of remote sensing image interpretation has long been\ndominated by vision-centered models, which rely on visual features for semantic\nunderstanding. However, these models face inherent limitations in handling\nmulti-modal reasoning, semantic abstraction, and interactive decision-making.\nWhile recent advances have introduced Large Language Models (LLMs) into remote\nsensing workflows, existing studies primarily focus on downstream applications,\nlacking a unified theoretical framework that explains the cognitive role of\nlanguage. This review advocates a paradigm shift from vision-centered to\nlanguage-centered remote sensing interpretation. Drawing inspiration from the\nGlobal Workspace Theory (GWT) of human cognition, We propose a\nlanguage-centered framework for remote sensing interpretation that treats LLMs\nas the cognitive central hub integrating perceptual, task, knowledge and action\nspaces to enable unified understanding, reasoning, and decision-making. We\nfirst explore the potential of LLMs as the central cognitive component in\nremote sensing interpretation, and then summarize core technical challenges,\nincluding unified multimodal representation, knowledge association, and\nreasoning and decision-making. Furthermore, we construct a global\nworkspace-driven interpretation mechanism and review how language-centered\nsolutions address each challenge. Finally, we outline future research\ndirections from four perspectives: adaptive alignment of multimodal data, task\nunderstanding under dynamic knowledge constraints, trustworthy reasoning, and\nautonomous interaction. This work aims to provide a conceptual foundation for\nthe next generation of remote sensing interpretation systems and establish a\nroadmap toward cognition-driven intelligent geospatial analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u6838\u5fc3\u7ec4\u4ef6\uff0c\u6784\u5efa\u4e00\u4e2a\u8bed\u8a00\u4e2d\u5fc3\u7684\u9065\u611f\u5f71\u50cf\u89e3\u8bd1\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u73b0\u6709\u89c6\u89c9\u4e2d\u5fc3\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u9065\u611f\u5f71\u50cf\u89e3\u8bd1\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u7279\u5f81\uff0c\u96be\u4ee5\u5904\u7406\u591a\u6a21\u6001\u63a8\u7406\u3001\u8bed\u4e49\u62bd\u8c61\u548c\u4ea4\u4e92\u5f0f\u51b3\u7b56\u3002", "method": "\u8be5\u8bba\u6587\u57fa\u4e8e\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u7406\u8bba (GWT)\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u9a71\u52a8\u7684\u89e3\u8bd1\u673a\u5236\uff0c\u5e76\u4ece\u56db\u4e2a\u89d2\u5ea6\uff08\u591a\u6a21\u6001\u6570\u636e\u7684\u81ea\u9002\u5e94\u5bf9\u9f50\u3001\u52a8\u6001\u77e5\u8bc6\u7ea6\u675f\u4e0b\u7684\u4efb\u52a1\u7406\u89e3\u3001\u53ef\u4fe1\u63a8\u7406\u548c\u81ea\u4e3b\u4ea4\u4e92\uff09\u6982\u8ff0\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u8bed\u8a00\u4e3a\u4e2d\u5fc3\u7684\u9065\u611f\u5f71\u50cf\u89e3\u8bd1\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u6838\u5fc3\u6280\u672f\u6311\u6218\u548c\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u8bba\u6587\u5021\u5bfc\u4ece\u89c6\u89c9\u4e2d\u5fc3\u8f6c\u5411\u8bed\u8a00\u4e2d\u5fc3\u7684\u9065\u611f\u5f71\u50cf\u89e3\u8bd1\u8303\u5f0f\u8f6c\u53d8\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u8bed\u8a00\u4e3a\u4e2d\u5fc3\u7684\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4f5c\u4e3a\u8ba4\u77e5\u4e2d\u5fc3\u67a2\u7ebd\uff0c\u96c6\u6210\u611f\u77e5\u3001\u4efb\u52a1\u3001\u77e5\u8bc6\u548c\u884c\u52a8\u7a7a\u95f4\uff0c\u5b9e\u73b0\u7edf\u4e00\u7684\u7406\u89e3\u3001\u63a8\u7406\u548c\u51b3\u7b56\u3002"}}
{"id": "2508.06836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06836", "abs": "https://arxiv.org/abs/2508.06836", "authors": ["Xutong Zhao", "Yaqi Xie"], "title": "Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning", "comment": "Accepted at AISTATS 2025", "summary": "Cooperative multi-agent reinforcement learning (MARL) aims to coordinate\nmultiple agents to achieve a common goal. A key challenge in MARL is credit\nassignment, which involves assessing each agent's contribution to the shared\nreward. Given the diversity of tasks, agents may perform different types of\ncoordination, with rewards attributed to diverse and often overlapping agent\nsubsets. In this work, we formalize the credit assignment level as the number\nof agents cooperating to obtain a reward, and address scenarios with multiple\ncoexisting levels. We introduce a multi-level advantage formulation that\nperforms explicit counterfactual reasoning to infer credits across distinct\nlevels. Our method, Multi-level Advantage Credit Assignment (MACA), captures\nagent contributions at multiple levels by integrating advantage functions that\nreason about individual, joint, and correlated actions. Utilizing an\nattention-based framework, MACA identifies correlated agent relationships and\nconstructs multi-level advantages to guide policy learning. Comprehensive\nexperiments on challenging Starcraft v1\\&v2 tasks demonstrate MACA's superior\nperformance, underscoring its efficacy in complex credit assignment scenarios.", "AI": {"tldr": "\u9488\u5bf9MARL\u4e2d\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u591a\u5c42\u6b21\u4f18\u52bf\u4fe1\u7528\u5206\u914d(MACA)\u65b9\u6cd5\uff0c\u5728\u661f\u9645\u4e89\u9738\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "MARL\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u7279\u522b\u662f\u5956\u52b1\u5f52\u56e0\u4e8e\u4e0d\u540c\u4e14\u7ecf\u5e38\u91cd\u53e0\u7684\u4ee3\u7406\u5b50\u96c6\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c42\u6b21\u4f18\u52bf\u4fe1\u7528\u5206\u914d(MACA)\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6846\u67b6\u8bc6\u522b\u76f8\u5173\u7684\u4ee3\u7406\u5173\u7cfb\uff0c\u5e76\u6784\u5efa\u591a\u5c42\u6b21\u4f18\u52bf\u6765\u6307\u5bfc\u7b56\u7565\u5b66\u4e60\u3002MACA\u901a\u8fc7\u6574\u5408\u8003\u8651\u4e2a\u4f53\u3001\u8054\u5408\u548c\u76f8\u5173\u884c\u4e3a\u7684\u4f18\u52bf\u51fd\u6570\uff0c\u6355\u83b7\u591a\u4e2a\u5c42\u6b21\u4e0a\u7684\u4ee3\u7406\u8d21\u732e\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u661f\u9645\u4e89\u9738v1&v2\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u7efc\u5408\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eMACA\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "MACA\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u661f\u9645\u4e89\u9738v1&v2\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u4fe1\u7528\u5206\u914d\u573a\u666f\u3002"}}
{"id": "2508.06851", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.06851", "abs": "https://arxiv.org/abs/2508.06851", "authors": ["Pengfei Zhou", "Xiaopeng Peng", "Fanrui Zhang", "Zhaopan Xu", "Jiaxin Ai", "Yansheng Qiu", "Chuanhao Li", "Zhen Li", "Ming Li", "Yukang Feng", "Jianwen Sun", "Haoquan Zhang", "Zizhen Li", "Xiaofeng Mao", "Zekai Li", "Wangbo Zhao", "Kai Wang", "Xiaojun Chang", "Wenqi Shao", "Yang You", "Kaipeng Zhang"], "title": "MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams", "comment": "35 pages, 33 figures", "summary": "Multimodal large language models (MLLMs), which integrate language and visual\ncues for problem-solving, are crucial for advancing artificial general\nintelligence (AGI). However, current benchmarks for measuring the intelligence\nof MLLMs suffer from limited scale, narrow coverage, and unstructured\nknowledge, offering only static and undifferentiated evaluations. To bridge\nthis gap, we introduce MDK12-Bench, a large-scale multidisciplinary benchmark\nbuilt from real-world K-12 exams spanning six disciplines with 141K instances\nand 6,225 knowledge points organized in a six-layer taxonomy. Covering five\nquestion formats with difficulty and year annotations, it enables comprehensive\nevaluation to capture the extent to which MLLMs perform over four dimensions:\n1) difficulty levels, 2) temporal (cross-year) shifts, 3) contextual shifts,\nand 4) knowledge-driven reasoning. We propose a novel dynamic evaluation\nframework that introduces unfamiliar visual, textual, and question form shifts\nto challenge model generalization while improving benchmark objectivity and\nlongevity by mitigating data contamination. We further evaluate knowledge-point\nreference-augmented generation (KP-RAG) to examine the role of knowledge in\nproblem-solving. Key findings reveal limitations in current MLLMs in multiple\naspects and provide guidance for enhancing model robustness, interpretability,\nand AI-assisted education.", "AI": {"tldr": "MDK12-Bench\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u6539\u8fdb\u6a21\u578b\u548c\u4eba\u5de5\u667a\u80fd\u8f85\u52a9\u6559\u80b2\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "motivation": "\u5f53\u524dMLLM\u8bc4\u4f30\u57fa\u51c6\u5b58\u5728\u89c4\u6a21\u6709\u9650\u3001\u8986\u76d6\u9762\u7a84\u548c\u77e5\u8bc6\u975e\u7ed3\u6784\u5316\u7b49\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b141K\u4e2a\u5b9e\u4f8b\u548c6,225\u4e2a\u77e5\u8bc6\u70b9\u7684\u5927\u89c4\u6a21\u591a\u5b66\u79d1\u57fa\u51c6MDK12-Bench\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524dMLLMs\u5728\u591a\u4e2a\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u6539\u8fdb\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4eba\u5de5\u667a\u80fd\u8f85\u52a9\u6559\u80b2\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "MDK12-Bench\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u591a\u5b66\u79d1\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524dMLLM\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4eba\u5de5\u667a\u80fd\u8f85\u52a9\u6559\u80b2\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
