{"id": "2507.07115", "categories": ["cs.AI", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.07115", "abs": "https://arxiv.org/abs/2507.07115", "authors": ["Javal Vyas", "Mehmet Mercangoz"], "title": "Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation", "comment": null, "summary": "The increasing complexity of modern chemical processes, coupled with\nworkforce shortages and intricate fault scenarios, demands novel automation\nparadigms that blend symbolic reasoning with adaptive control. In this work, we\nintroduce a unified agentic framework that leverages large language models\n(LLMs) for both discrete fault-recovery planning and continuous process control\nwithin a single architecture. We adopt Finite State Machines (FSMs) as\ninterpretable operating envelopes: an LLM-driven planning agent proposes\nrecovery sequences through the FSM, a Simulation Agent executes and checks each\ntransition, and a Validator-Reprompting loop iteratively refines invalid plans.\nIn Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25\nstates, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path\nsuccess within five reprompts-outperforming open-source LLMs in both accuracy\nand latency. In Case Study 2, the same framework modulates dual-heater inputs\non a laboratory TCLab platform (and its digital twin) to maintain a target\naverage temperature under persistent asymmetric disturbances. Compared to\nclassical PID control, our LLM-based controller attains similar performance,\nwhile ablation of the prompting loop reveals its critical role in handling\nnonlinear dynamics. We analyze key failure modes-such as instruction following\nlapses and coarse ODE approximations. Our results demonstrate that, with\nstructured feedback and modular agents, LLMs can unify high-level symbolic\nplanningand low-level continuous control, paving the way towards resilient,\nlanguage-driven automation in chemical engineering.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7ed3\u5408\u6709\u9650\u72b6\u6001\u673a (FSM) \u548c\u53cd\u9988\u5faa\u73af\uff0c\u5b9e\u73b0\u5316\u5b66\u8fc7\u7a0b\u7684\u79bb\u6563\u6545\u969c\u6062\u590d\u89c4\u5212\u548c\u8fde\u7eed\u8fc7\u7a0b\u63a7\u5236\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u4ee3\u5316\u5b66\u8fc7\u7a0b\u7684\u590d\u6742\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u4ee5\u53ca\u52b3\u52a8\u529b\u77ed\u7f3a\u548c\u590d\u6742\u7684\u6545\u969c\u573a\u666f\uff0c\u5bf9\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u548c\u81ea\u9002\u5e94\u63a7\u5236\u7684\u65b0\u578b\u81ea\u52a8\u5316\u8303\u5f0f\u63d0\u51fa\u4e86\u9700\u6c42\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u4ee3\u7406\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u89c4\u5212\u4ee3\u7406\u3001\u6a21\u62df\u4ee3\u7406\u548c\u9a8c\u8bc1-\u91cd\u63d0\u793a\u5faa\u73af\u3002\u89c4\u5212\u4ee3\u7406\u5229\u7528 LLM \u89c4\u5212 FSM \u4e2d\u7684\u6062\u590d\u5e8f\u5217\uff0c\u6a21\u62df\u4ee3\u7406\u6267\u884c\u5e76\u68c0\u67e5\u6bcf\u4e2a\u8f6c\u6362\uff0c\u9a8c\u8bc1-\u91cd\u63d0\u793a\u5faa\u73af\u8fed\u4ee3\u5730\u7ec6\u5316\u65e0\u6548\u7684\u8ba1\u5212\u3002", "result": "\u5728\u6848\u4f8b\u7814\u7a76 1 \u4e2d\uff0cGPT-4o \u548c GPT-4o-mini \u5728 180 \u4e2a\u968f\u673a\u751f\u6210\u7684 FSM \u4e2d\u5b9e\u73b0\u4e86 100% \u7684\u6709\u6548\u8def\u5f84\u6210\u529f\u7387\u3002\u5728\u6848\u4f8b\u7814\u7a76 2 \u4e2d\uff0c\u8be5\u6846\u67b6\u5728\u5b9e\u9a8c\u5ba4 TCLab \u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u7ecf\u5178 PID \u63a7\u5236\u7c7b\u4f3c\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5c55\u73b0\u4e86\u63d0\u793a\u5faa\u73af\u5728\u5904\u7406\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7edf\u4e00\u79bb\u6563\u6545\u969c\u6062\u590d\u89c4\u5212\u548c\u8fde\u7eed\u8fc7\u7a0b\u63a7\u5236\u7684\u6846\u67b6\uff0c\u5e76\u5728\u5316\u5b66\u5de5\u7a0b\u81ea\u52a8\u5316\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u6709\u9650\u72b6\u6001\u673a (FSM) \u548c\u53cd\u9988\u5faa\u73af\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2507.07134", "categories": ["cs.AI", "cs.LG", "I.2.10"], "pdf": "https://arxiv.org/pdf/2507.07134", "abs": "https://arxiv.org/abs/2507.07134", "authors": ["Mridula Vijendran", "Shuang Chen", "Jingjing Deng", "Hubert P. H. Shum"], "title": "BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation in Stylistic Convolutional Neural Networks", "comment": "18 pages, 7 figures, 3 tables", "summary": "The pervasive issue of bias in AI presents a significant challenge to\npainting classification, and is getting more serious as these systems become\nincreasingly integrated into tasks like art curation and restoration. Biases,\noften arising from imbalanced datasets where certain artistic styles dominate,\ncompromise the fairness and accuracy of model predictions, i.e., classifiers\nare less accurate on rarely seen paintings. While prior research has made\nstrides in improving classification performance, it has largely overlooked the\ncritical need to address these underlying biases, that is, when dealing with\nout-of-distribution (OOD) data. Our insight highlights the necessity of a more\nrobust approach to bias mitigation in AI models for art classification on\nbiased training data. We propose a novel OOD-informed model bias adaptive\nsampling method called BOOST (Bias-Oriented OOD Sampling and Tuning). It\naddresses these challenges by dynamically adjusting temperature scaling and\nsampling probabilities, thereby promoting a more equitable representation of\nall classes. We evaluate our proposed approach to the KaoKore and PACS\ndatasets, focusing on the model's ability to reduce class-wise bias. We further\npropose a new metric, Same-Dataset OOD Detection Score (SODC), designed to\nassess class-wise separation and per-class bias reduction. Our method\ndemonstrates the ability to balance high performance with fairness, making it a\nrobust solution for unbiasing AI models in the art domain.", "AI": {"tldr": "BOOST \u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u4e86\u827a\u672f\u5206\u7c7b\u4e2d AI \u6a21\u578b\u7684\u504f\u5dee\uff0c\u63d0\u9ad8\u4e86\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684 AI \u6a21\u578b\u5728\u827a\u672f\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b58\u5728\u504f\u5dee\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406 OOD \u6570\u636e\u65f6\uff0c\u8fd9\u4e9b\u504f\u5dee\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a BOOST \u7684 OOD \u611f\u77e5\u6a21\u578b\u504f\u5dee\u81ea\u9002\u5e94\u91c7\u6837\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6e29\u5ea6\u7f29\u653e\u548c\u91c7\u6837\u6982\u7387\u6765\u4fc3\u8fdb\u6240\u6709\u7c7b\u522b\u7684\u66f4\u516c\u5e73\u8868\u793a\u3002\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6307\u6807 SODC \u6765\u8bc4\u4f30\u7c7b\u95f4\u5206\u79bb\u548c\u6bcf\u4e2a\u7c7b\u7684\u504f\u5dee\u51cf\u5c11\u3002", "result": "BOOST \u65b9\u6cd5\u80fd\u591f\u6709\u6548\u964d\u4f4e\u827a\u672f\u5206\u7c7b\u6a21\u578b\u7684\u7c7b\u95f4\u504f\u5dee\uff0c\u5e76\u5728 KaoKore \u548c PACS \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6548\u679c\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u9ad8\u6027\u80fd\u548c\u516c\u5e73\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 OOD \u611f\u77e5\u6a21\u578b\u504f\u5dee\u81ea\u9002\u5e94\u91c7\u6837\u65b9\u6cd5 BOOST\uff0c\u7528\u4e8e\u89e3\u51b3 AI \u6a21\u578b\u5728\u827a\u672f\u5206\u7c7b\u4e2d\u5b58\u5728\u7684\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u5728 KaoKore \u548c PACS \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u9ad8\u6027\u80fd\u548c\u516c\u5e73\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2507.07203", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07203", "abs": "https://arxiv.org/abs/2507.07203", "authors": ["Minkyung Kim", "Junsik Kim", "Hwidong Bae", "Woongcheol Yang", "Sangdon Park", "Sohee Bae"], "title": "State-Inference-Based Prompting for Natural Language Trading with Game NPCs", "comment": "9 pages main content, 4 pages appendix, 3 figures. Accepted to the\n  KDD 2025 Workshop on Prompt Optimization", "summary": "Large Language Models enable dynamic game interactions but struggle with\nrule-governed trading systems. Current implementations suffer from rule\nviolations, such as item hallucinations and calculation errors, that erode\nplayer trust. Here, State-Inference-Based Prompting (SIBP) enables reliable\ntrading through autonomous dialogue state inference and context-specific rule\nadherence. The approach decomposes trading into six states within a unified\nprompt framework, implementing context-aware item referencing and\nplaceholder-based price calculations. Evaluation across 100 trading dialogues\ndemonstrates >97% state compliance, >95% referencing accuracy, and 99.7%\ncalculation precision. SIBP maintains computational efficiency while\noutperforming baseline approaches, establishing a practical foundation for\ntrustworthy NPC interactions in commercial games.", "AI": {"tldr": "SIBP improves large language model reliability in game trading through state inference and context-aware prompting, achieving high accuracy and efficiency.", "motivation": "Current large language models struggle with rule-governed trading systems due to rule violations like item hallucinations and calculation errors, eroding player trust.", "method": "Decomposes trading into six states within a unified prompt framework, implementing context-aware item referencing and placeholder-based price calculations.", "result": "Evaluation across 100 trading dialogues demonstrates >97% state compliance, >95% referencing accuracy, and 99.7% calculation precision. SIBP outperforms baseline approaches while maintaining computational efficiency.", "conclusion": "State-Inference-Based Prompting (SIBP) enables reliable and trustworthy NPC interactions in commercial games by addressing rule violations in large language models during dynamic game interactions, achieving high accuracy in state compliance, referencing, and calculation."}}
{"id": "2507.07217", "categories": ["cs.AI", "cs.LG", "cs.LO", "I.2.4; I.2.7; J.4"], "pdf": "https://arxiv.org/pdf/2507.07217", "abs": "https://arxiv.org/abs/2507.07217", "authors": ["Zili Wang", "Frank Montabon", "Kristin Yvonne Rozier"], "title": "Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains", "comment": null, "summary": "Supply chain networks are complex systems that are challenging to analyze;\nthis problem is exacerbated when there are illicit activities involved in the\nsupply chain, such as counterfeit parts, forced labor, or human trafficking.\nWhile machine learning (ML) can find patterns in complex systems like supply\nchains, traditional ML techniques require large training data sets. However,\nillicit supply chains are characterized by very sparse data, and the data that\nis available is often (purposely) corrupted or unreliable in order to hide the\nnature of the activities. We need to be able to automatically detect new\npatterns that correlate with such illegal activity over complex, even temporal\ndata, without requiring large training data sets. We explore neurosymbolic\nmethods for identifying instances of illicit activity in supply chains and\ncompare the effectiveness of manual and automated feature extraction from news\narticles accurately describing illicit activities uncovered by authorities. We\npropose a question tree approach for querying a large language model (LLM) to\nidentify and quantify the relevance of articles. This enables a systematic\nevaluation of the differences between human and machine classification of news\narticles related to forced labor in supply chains.", "AI": {"tldr": "\u5229\u7528\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u548cLLM\uff0c\u7ed3\u5408\u95ee\u9898\u6811\u65b9\u6cd5\uff0c\u6709\u6548\u8bc6\u522b\u975e\u6cd5\u4f9b\u5e94\u94fe\u6d3b\u52a8\uff0c\u5e76\u6bd4\u8f83\u4eba\u5de5\u4e0e\u673a\u5668\u5206\u7c7b\u7684\u5dee\u5f02\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u800c\u975e\u6cd5\u4f9b\u5e94\u94fe\u6570\u636e\u7a00\u758f\u4e14\u4e0d\u53ef\u9760\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u68c0\u6d4b\u65b0\u6a21\u5f0f\u7684\u65b9\u6cd5\u3002", "method": "\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u3001\u95ee\u9898\u6811\u65b9\u6cd5\u3001LLM\u67e5\u8be2", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u95ee\u9898\u6811\u7684LLM\u67e5\u8be2\u65b9\u6cd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u5730\u8bc4\u4f30\u4eba\u5de5\u548c\u673a\u5668\u5bf9\u65b0\u95fb\u6587\u7ae0\u4e2d\u5f3a\u8feb\u52b3\u52a8\u76f8\u5173\u4fe1\u606f\u7684\u5206\u7c7b\u5dee\u5f02\uff0c\u5e76\u63a2\u7a76\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u8bc6\u522b\u4f9b\u5e94\u94fe\u975e\u6cd5\u6d3b\u52a8\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63a2\u7a76\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u8bc6\u522b\u4f9b\u5e94\u94fe\u975e\u6cd5\u6d3b\u52a8\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u6bd4\u8f83\u4e86\u4eba\u5de5\u548c\u81ea\u52a8\u7279\u5f81\u63d0\u53d6\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u95ee\u9898\u6811\u7684LLM\u67e5\u8be2\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u65b0\u95fb\u6587\u7ae0\u4e2d\u5f3a\u8feb\u52b3\u52a8\u76f8\u5173\u4fe1\u606f\u7684\u5206\u7c7b\u5dee\u5f02\u3002"}}
{"id": "2507.07257", "categories": ["cs.AI", "astro-ph.IM", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.07257", "abs": "https://arxiv.org/abs/2507.07257", "authors": ["Licong Xu", "Milind Sarkar", "Anto I. Lonappan", "\u00cd\u00f1igo Zubeldia", "Pablo Villanueva-Domingo", "Santiago Casas", "Christian Fidler", "Chetana Amancharla", "Ujjwal Tiwari", "Adrian Bayer", "Chadi Ait Ekiou", "Miles Cranmer", "Adrian Dimitrov", "James Fergusson", "Kahaan Gandhi", "Sven Krippendorf", "Andrew Laverick", "Julien Lesgourgues", "Antony Lewis", "Thomas Meier", "Blake Sherwin", "Kristen Surrao", "Francisco Villaescusa-Navarro", "Chi Wang", "Xueqing Xu", "Boris Bolliet"], "title": "Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery", "comment": "Accepted contribution to the ICML 2025 Workshop on Machine Learning\n  for Astrophysics. Code: https://github.com/CMBAgents/cmbagent; Videos:\n  https://www.youtube.com/@cmbagent; HuggingFace:\n  https://huggingface.co/spaces/astropilot-ai/cmbagent; Cloud:\n  https://cmbagent.cloud", "summary": "We present a multi-agent system for automation of scientific research tasks,\ncmbagent. The system is formed by about 30 Large Language Model (LLM) agents\nand implements a Planning & Control strategy to orchestrate the agentic\nworkflow, with no human-in-the-loop at any point. Each agent specializes in a\ndifferent task (performing retrieval on scientific papers and codebases,\nwriting code, interpreting results, critiquing the output of other agents) and\nthe system is able to execute code locally. We successfully apply cmbagent to\ncarry out a PhD level cosmology task (the measurement of cosmological\nparameters using supernova data) and evaluate its performance on two benchmark\nsets, finding superior performance over state-of-the-art LLMs. The source code\nis available on GitHub, demonstration videos are also available, and the system\nis deployed on HuggingFace and will be available on the cloud.", "AI": {"tldr": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edfcmbagent\u81ea\u52a8\u5316\u79d1\u5b66\u7814\u7a76\uff0c\u5728\u5b87\u5b99\u5b66\u53c2\u6570\u6d4b\u91cf\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u81ea\u52a8\u5316\u79d1\u5b66\u7814\u7a76\u4efb\u52a1\uff0c\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u91c7\u7528\u89c4\u5212\u4e0e\u63a7\u5236\u7b56\u7565\u534f\u8c03\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6bcf\u4e2a\u4ee3\u7406\u8d1f\u8d23\u4e0d\u540c\u7684\u4efb\u52a1\uff08\u4f8b\u5982\uff0c\u68c0\u7d22\u79d1\u5b66\u8bba\u6587\u548c\u4ee3\u7801\u5e93\u3001\u7f16\u5199\u4ee3\u7801\u3001\u89e3\u91ca\u7ed3\u679c\u3001\u8bc4\u8bba\u5176\u4ed6\u4ee3\u7406\u7684\u8f93\u51fa\uff09\u3002", "result": "\u5728\u535a\u58eb\u7ea7\u522b\u7684\u5b87\u5b99\u5b66\u4efb\u52a1\uff08\u4f7f\u7528\u8d85\u65b0\u661f\u6570\u636e\u6d4b\u91cf\u5b87\u5b99\u5b66\u53c2\u6570\uff09\u4e2d\u53d6\u5f97\u6210\u529f\uff0c\u5e76\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684LLM\u3002", "conclusion": "cmbagent\uff0c\u4e00\u4e2a\u7531\u7ea630\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u4ee3\u7406\u7ec4\u6210\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u6210\u529f\u5730\u81ea\u52a8\u5316\u4e86\u79d1\u5b66\u7814\u7a76\u4efb\u52a1\uff0c\u5e76\u5728\u5b87\u5b99\u5b66\u53c2\u6570\u6d4b\u91cf\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684LLM\u3002"}}
{"id": "2507.07302", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07302", "abs": "https://arxiv.org/abs/2507.07302", "authors": ["Ashish Kumar"], "title": "Application of LLMs to Multi-Robot Path Planning and Task Allocation", "comment": null, "summary": "Efficient exploration is a well known problem in deep reinforcement learning\nand this problem is exacerbated in multi-agent reinforcement learning due the\nintrinsic complexities of such algorithms. There are several approaches to\nefficiently explore an environment to learn to solve tasks by multi-agent\noperating in that environment, of which, the idea of expert exploration is\ninvestigated in this work. More specifically, this work investigates the\napplication of large-language models as expert planners for efficient\nexploration in planning based tasks for multiple agents.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u63a2\u7d22\u6548\u7387", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u9ad8\u6548\u63a2\u7d22\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u672c\u6587\u7814\u7a76\u5229\u7528\u4e13\u5bb6\u63a2\u7d22\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7528\u4f5c\u4e13\u5bb6\u89c4\u5212\u5668\uff0c\u4ee5\u6539\u8fdb\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u3002", "result": "\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u57fa\u4e8e\u89c4\u5212\u7684\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u7814\u7a76\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4e13\u5bb6\u89c4\u5212\u5668\u6765\u63d0\u9ad8\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u57fa\u4e8e\u89c4\u5212\u7684\u4efb\u52a1\u7684\u63a2\u7d22\u6548\u7387\u3002"}}
{"id": "2507.07306", "categories": ["cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07306", "abs": "https://arxiv.org/abs/2507.07306", "authors": ["Yichen Lu", "Wei Dai", "Jiaen Liu", "Ching Wing Kwok", "Zongheng Wu", "Xudong Xiao", "Ao Sun", "Sheng Fu", "Jianyuan Zhan", "Yian Wang", "Takatomo Saito", "Sicheng Lai"], "title": "ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning", "comment": null, "summary": "LLM-based translation agents have achieved highly human-like translation\nresults and are capable of handling longer and more complex contexts with\ngreater efficiency. However, they are typically limited to text-only inputs. In\nthis paper, we introduce ViDove, a translation agent system designed for\nmultimodal input. Inspired by the workflow of human translators, ViDove\nleverages visual and contextual background information to enhance the\ntranslation process. Additionally, we integrate a multimodal memory system and\nlong-short term memory modules enriched with domain-specific knowledge,\nenabling the agent to perform more accurately and adaptively in real-world\nscenarios. As a result, ViDove achieves significantly higher translation\nquality in both subtitle generation and general translation tasks, with a 28%\nimprovement in BLEU scores and a 15% improvement in SubER compared to previous\nstate-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark\nfor long-form automatic video subtitling and translation, featuring 17 hours of\nhigh-quality, human-annotated data. Our code is available here:\nhttps://github.com/pigeonai-org/ViDove", "AI": {"tldr": "ViDove\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u7ffb\u8bd1\u4ee3\u7406\u7cfb\u7edf\uff0c\u5229\u7528\u89c6\u89c9\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5e76\u521b\u5efa\u4e86\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5DoveBench\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684\u7ffb\u8bd1\u4ee3\u7406\u901a\u5e38\u4ec5\u9650\u4e8e\u6587\u672c\u8f93\u5165\uff0cViDove\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u591a\u6a21\u6001\u8f93\u5165\u3002", "method": "ViDove\u5229\u7528\u89c6\u89c9\u548c\u4e0a\u4e0b\u6587\u80cc\u666f\u4fe1\u606f\u6765\u589e\u5f3a\u7ffb\u8bd1\u8fc7\u7a0b\uff0c\u96c6\u6210\u4e86\u591a\u6a21\u6001\u8bb0\u5fc6\u7cfb\u7edf\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\u6a21\u5757\uff0c\u5e76\u52a0\u5165\u4e86\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u3002", "result": "BLEU\u5206\u6570\u63d0\u9ad828%\uff0cSubER\u63d0\u9ad815%\uff0c\u5e76\u521b\u5efa\u4e86\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5DoveBench\u3002", "conclusion": "ViDove\u7cfb\u7edf\u5728\u5b57\u5e55\u751f\u6210\u548c\u901a\u7528\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u7ffb\u8bd1\u8d28\u91cf\uff0cBLEU\u5206\u6570\u63d0\u9ad8\u4e8628%\uff0cSubER\u63d0\u9ad8\u4e8615%\u3002"}}
{"id": "2507.07341", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07341", "abs": "https://arxiv.org/abs/2507.07341", "authors": ["Sarah Ball", "Greg Gluch", "Shafi Goldwasser", "Frauke Kreuter", "Omer Reingold", "Guy N. Rothblum"], "title": "On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment", "comment": null, "summary": "With the increased deployment of large language models (LLMs), one concern is\ntheir potential misuse for generating harmful content. Our work studies the\nalignment challenge, with a focus on filters to prevent the generation of\nunsafe information. Two natural points of intervention are the filtering of the\ninput prompt before it reaches the model, and filtering the output after\ngeneration. Our main results demonstrate computational challenges in filtering\nboth prompts and outputs. First, we show that there exist LLMs for which there\nare no efficient prompt filters: adversarial prompts that elicit harmful\nbehavior can be easily constructed, which are computationally indistinguishable\nfrom benign prompts for any efficient filter. Our second main result identifies\na natural setting in which output filtering is computationally intractable. All\nof our separation results are under cryptographic hardness assumptions. In\naddition to these core findings, we also formalize and study relaxed mitigation\napproaches, demonstrating further computational barriers. We conclude that\nsafety cannot be achieved by designing filters external to the LLM internals\n(architecture and weights); in particular, black-box access to the LLM will not\nsuffice. Based on our technical results, we argue that an aligned AI system's\nintelligence cannot be separated from its judgment.", "AI": {"tldr": "\u5916\u90e8\u8fc7\u6ee4\u5668\u65e0\u6cd5\u4fdd\u8bc1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\uff0c\u9700\u8981\u8003\u8651\u6a21\u578b\u5185\u90e8\u7684\u5b89\u5168\u673a\u5236\u3002", "motivation": "\u5173\u6ce8LLM\u6f5c\u5728\u7684\u6ee5\u7528\u4ee5\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u3002", "method": "\u7814\u7a76\u4e86\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u8fdb\u884c\u5b89\u5168\u8fc7\u6ee4\u7684\u8ba1\u7b97\u6311\u6218\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u8fc7\u6ee4\u63d0\u793a\u548c\u8f93\u51fa\u65b9\u9762\u90fd\u5b58\u5728\u8ba1\u7b97\u6311\u6218\u3002", "conclusion": "\u5b89\u5168\u65e0\u6cd5\u901a\u8fc7\u8bbe\u8ba1LLM\u5185\u90e8(\u67b6\u6784\u548c\u6743\u91cd)\u5916\u90e8\u7684\u8fc7\u6ee4\u5668\u6765\u5b9e\u73b0;\u7279\u522b\u662f\uff0c\u5bf9LLM\u7684\u9ed1\u76d2\u8bbf\u95ee\u662f\u4e0d\u591f\u7684\u3002"}}
{"id": "2507.07355", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07355", "abs": "https://arxiv.org/abs/2507.07355", "authors": ["Haoyue Bai", "Haoyu Wang", "Nanxu Gong", "Xinyuan Wang", "Wangyang Ying", "Haifeng Chen", "Yanjie Fu"], "title": "Supply Chain Optimization via Generative Simulation and Iterative Decision Policies", "comment": null, "summary": "High responsiveness and economic efficiency are critical objectives in supply\nchain transportation, both of which are influenced by strategic decisions on\nshipping mode. An integrated framework combining an efficient simulator with an\nintelligent decision-making algorithm can provide an observable, low-risk\nenvironment for transportation strategy design. An ideal simulation-decision\nframework must (1) generalize effectively across various settings, (2) reflect\nfine-grained transportation dynamics, (3) integrate historical experience with\npredictive insights, and (4) maintain tight integration between simulation\nfeedback and policy refinement. We propose Sim-to-Dec framework to satisfy\nthese requirements. Specifically, Sim-to-Dec consists of a generative\nsimulation module, which leverages autoregressive modeling to simulate\ncontinuous state changes, reducing dependence on handcrafted domain-specific\nrules and enhancing robustness against data fluctuations; and a history-future\ndual-aware decision model, refined iteratively through end-to-end optimization\nwith simulator interactions. Extensive experiments conducted on three\nreal-world datasets demonstrate that Sim-to-Dec significantly improves timely\ndelivery rates and profit.", "AI": {"tldr": "Sim-to-Dec\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u5f0f\u6a21\u62df\u548c\u667a\u80fd\u51b3\u7b56\u7b97\u6cd5\uff0c\u6709\u6548\u63d0\u9ad8\u4f9b\u5e94\u94fe\u8fd0\u8f93\u7684\u6548\u7387\u548c\u5229\u6da6\u3002", "motivation": "\u63d0\u9ad8\u4f9b\u5e94\u94fe\u8fd0\u8f93\u7684\u54cd\u5e94\u901f\u5ea6\u548c\u7ecf\u6d4e\u6548\u7387\uff0c\u5bfb\u627e\u5408\u9002\u7684\u8fd0\u8f93\u6a21\u5f0f\u3002", "method": "Sim-to-Dec\u6846\u67b6\u7ed3\u5408\u4e86\u751f\u6210\u5f0f\u6a21\u62df\u6a21\u5757\u548c\u5386\u53f2-\u672a\u6765\u53cc\u611f\u77e5\u51b3\u7b56\u6a21\u578b\uff0c\u5229\u7528\u81ea\u56de\u5f52\u5efa\u6a21\u6a21\u62df\u8fde\u7eed\u72b6\u6001\u53d8\u5316\uff0c\u5e76\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u8fed\u4ee3\u7ec6\u5316\u51b3\u7b56\u6a21\u578b\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSim-to-Dec\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u65f6\u9001\u8fbe\u7387\u548c\u5229\u6da6\u3002", "conclusion": "Sim-to-Dec\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u65f6\u9001\u8fbe\u7387\u548c\u5229\u6da6\u3002"}}
{"id": "2507.07426", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07426", "abs": "https://arxiv.org/abs/2507.07426", "authors": ["Zerui Yang", "Yuwei Wan", "Yinqiao Li", "Yudai Matsuda", "Tong Xie", "Linqi Song"], "title": "DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search", "comment": null, "summary": "Recent advances in large language models have demonstrated considerable\npotential in scientific domains such as drug discovery. However, their\neffectiveness remains constrained when reasoning extends beyond the knowledge\nacquired during pretraining. Conventional approaches, such as fine-tuning or\nretrieval-augmented generation, face limitations in either imposing high\ncomputational overhead or failing to fully exploit structured scientific data.\nTo overcome these challenges, we propose DrugMCTS, a novel framework that\nsynergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree\nSearch for drug repurposing. The framework employs five specialized agents\ntasked with retrieving and analyzing molecular and protein information, thereby\nenabling structured and iterative reasoning. Without requiring domain-specific\nfine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by\nover 20\\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate\nthat DrugMCTS achieves substantially higher recall and robustness compared to\nboth general-purpose LLMs and deep learning baselines. Our results highlight\nthe importance of structured reasoning, agent-based collaboration, and\nfeedback-driven search mechanisms in advancing LLM applications for drug\ndiscovery.", "AI": {"tldr": "DrugMCTS\u6846\u67b6\u901a\u8fc7\u6574\u5408RAG\u3001\u591aagent\u534f\u4f5c\u548cMCTS\uff0c\u5728\u836f\u7269 repurposing \u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u6210\u679c\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5982\u5fae\u8c03\u6216\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5728\u5904\u7406\u79d1\u5b66\u9886\u57df\u63a8\u7406\u95ee\u9898\u65f6\uff0c\u9762\u4e34\u8ba1\u7b97\u5f00\u9500\u9ad8\u6216\u65e0\u6cd5\u5145\u5206\u5229\u7528\u7ed3\u6784\u5316\u79d1\u5b66\u6570\u636e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDrugMCTS\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86RAG\u3001\u591aagent\u534f\u4f5c\u548cMonte Carlo Tree Search\uff0c\u5e76\u4f7f\u7528\u4e94\u4e2a\u4e13\u4e1aagent\u68c0\u7d22\u548c\u5206\u6790\u5206\u5b50\u548c\u86cb\u767d\u8d28\u4fe1\u606f\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u548c\u8fed\u4ee3\u63a8\u7406\u3002", "result": "DrugMCTS\u5728DrugBank\u548cKIBA\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u53ec\u56de\u7387\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u8d85\u8fc7\u4e86\u901a\u7528LLM\u548c\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "DrugMCTS\u6846\u67b6\u5728\u836f\u7269 repurposing \u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6bd4Deepseek-R1\u9ad8\u51fa20%\u7684\u6027\u80fd\uff0c\u5728DrugBank\u548cKIBA\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u53ec\u56de\u7387\u548c\u9c81\u68d2\u6027\uff0c\u8bc1\u660e\u4e86\u7ed3\u6784\u5316\u63a8\u7406\u3001\u57fa\u4e8eagent\u7684\u534f\u4f5c\u548c\u53cd\u9988\u9a71\u52a8\u641c\u7d22\u673a\u5236\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.07445", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07445", "abs": "https://arxiv.org/abs/2507.07445", "authors": ["Weihao Tan", "Changjiu Jiang", "Yu Duan", "Mingcong Lei", "Jiageng Li", "Yitian Hong", "Xinrun Wang", "Bo An"], "title": "StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs in Production-Living Simulations with Stardew Valley", "comment": "Project website: https://weihaotan.github.io/StarDojo", "summary": "Autonomous agents navigating human society must master both production\nactivities and social interactions, yet existing benchmarks rarely evaluate\nthese skills simultaneously. To bridge this gap, we introduce StarDojo, a novel\nbenchmark based on Stardew Valley, designed to assess AI agents in open-ended\nproduction-living simulations. In StarDojo, agents are tasked to perform\nessential livelihood activities such as farming and crafting, while\nsimultaneously engaging in social interactions to establish relationships\nwithin a vibrant community. StarDojo features 1,000 meticulously curated tasks\nacross five key domains: farming, crafting, exploration, combat, and social\ninteractions. Additionally, we provide a compact subset of 100 representative\ntasks for efficient model evaluation. The benchmark offers a unified,\nuser-friendly interface that eliminates the need for keyboard and mouse\ncontrol, supports all major operating systems, and enables the parallel\nexecution of multiple environment instances, making it particularly well-suited\nfor evaluating the most capable foundation agents, powered by multimodal large\nlanguage models (MLLMs). Extensive evaluations of state-of-the-art MLLMs agents\ndemonstrate substantial limitations, with the best-performing model, GPT-4.1,\nachieving only a 12.7% success rate, primarily due to challenges in visual\nunderstanding, multimodal reasoning and low-level manipulation. As a\nuser-friendly environment and benchmark, StarDojo aims to facilitate further\nresearch towards robust, open-ended agents in complex production-living\nenvironments.", "AI": {"tldr": "StarDojo \u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86 AI \u4ee3\u7406\u5728\u751f\u4ea7\u751f\u6d3b\u6a21\u62df\u4e2d\u7684\u4e0d\u8db3\uff0cGPT-4.1 \u6210\u529f\u7387\u4ec5 12.7%\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5f88\u5c11\u540c\u65f6\u8bc4\u4f30\u751f\u4ea7\u6d3b\u52a8\u548c\u793e\u4f1a\u4e92\u52a8\uff0cStarDojo\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u57fa\u4e8e\u661f\u9732\u8c37\u7269\u8bed\u7684\u6e38\u620f\u73af\u5883StarDojo\uff0c\u8bc4\u4f30AI\u4ee3\u7406\u5728\u751f\u4ea7\u548c\u751f\u6d3b\u6a21\u62df\u4e2d\u7684\u80fd\u529b\uff0c\u6db5\u76d6\u8015\u4f5c\u3001\u5236\u4f5c\u3001\u63a2\u7d22\u3001\u6218\u6597\u548c\u793e\u4ea4\u4e92\u52a8\u4e94\u5927\u9886\u57df\u3002", "result": "GPT-4.1 \u7b49\u6700\u5148\u8fdb MLLM \u4ee3\u7406\u5728 StarDojo \u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6210\u529f\u7387\u4f4e\uff0c\u51f8\u663e\u4e86\u5728\u590d\u6742\u751f\u4ea7\u751f\u6d3b\u73af\u5883\u4e2d\u6784\u5efa\u9c81\u68d2\u3001\u5f00\u653e\u5f0f\u4ee3\u7406\u7684\u6311\u6218\u3002", "conclusion": "StarDojo \u4f5c\u4e3a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u66b4\u9732\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u4ee3\u7406\u5728\u89c6\u89c9\u7406\u89e3\u3001\u591a\u6a21\u6001\u63a8\u7406\u548c\u4f4e\u7ea7\u64cd\u4f5c\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u6700\u597d\u7684\u6a21\u578b GPT-4.1 \u6210\u529f\u7387\u4ec5\u4e3a 12.7%\u3002"}}
{"id": "2507.07544", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07544", "abs": "https://arxiv.org/abs/2507.07544", "authors": ["Oliver Eberle", "Thomas McGee", "Hamza Giaffar", "Taylor Webb", "Ida Momennejad"], "title": "Position: We Need An Algorithmic Understanding of Generative AI", "comment": "Accepted at ICML 2025 as a Spotlight Position Paper", "summary": "What algorithms do LLMs actually learn and use to solve problems? Studies\naddressing this question are sparse, as research priorities are focused on\nimproving performance through scale, leaving a theoretical and empirical gap in\nunderstanding emergent algorithms. This position paper proposes AlgEval: a\nframework for systematic research into the algorithms that LLMs learn and use.\nAlgEval aims to uncover algorithmic primitives, reflected in latent\nrepresentations, attention, and inference-time compute, and their algorithmic\ncomposition to solve task-specific problems. We highlight potential\nmethodological paths and a case study toward this goal, focusing on emergent\nsearch algorithms. Our case study illustrates both the formation of top-down\nhypotheses about candidate algorithms, and bottom-up tests of these hypotheses\nvia circuit-level analysis of attention patterns and hidden states. The\nrigorous, systematic evaluation of how LLMs actually solve tasks provides an\nalternative to resource-intensive scaling, reorienting the field toward a\nprincipled understanding of underlying computations. Such algorithmic\nexplanations offer a pathway to human-understandable interpretability, enabling\ncomprehension of the model's internal reasoning performance measures. This can\nin turn lead to more sample-efficient methods for training and improving\nperformance, as well as novel architectures for end-to-end and multi-agent\nsystems.", "AI": {"tldr": "AlgEval\u6846\u67b6\u65e8\u5728\u7cfb\u7edf\u7814\u7a76LLM\u7684\u7b97\u6cd5\u673a\u5236\uff0c\u901a\u8fc7\u5206\u6790\u6f5c\u5728\u8868\u793a\u3001\u6ce8\u610f\u529b\u548c\u63a8\u7406\u65f6\u95f4\u8ba1\u7b97\u7b49\uff0c\u7406\u89e3LLM\u5982\u4f55\u89e3\u51b3\u95ee\u9898\uff0c\u5e76\u6700\u7ec8\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u66f4\u5173\u6ce8\u901a\u8fc7\u6269\u5c55\u6a21\u578b\u89c4\u6a21\u6765\u63d0\u9ad8\u6027\u80fd\uff0c\u7f3a\u4e4f\u5bf9LLM\u5e95\u5c42\u7b97\u6cd5\u7684\u7406\u8bba\u548c\u5b9e\u8bc1\u7406\u89e3\u3002", "method": "\u63d0\u51faAlgEval\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u9876\u5411\u4e0b\u5047\u8bbe\u548c\u81ea\u5e95\u5411\u4e0a\u6d4b\u8bd5\uff08\u7535\u8def\u7ea7\u5206\u6790\u6ce8\u610f\u529b\u6a21\u5f0f\u548c\u9690\u85cf\u72b6\u6001\uff09\u5bf9LLM\u7b97\u6cd5\u8fdb\u884c\u7814\u7a76\uff0c\u4ee5\u6848\u4f8b\u7814\u7a76\uff08\u6d8c\u73b0\u641c\u7d22\u7b97\u6cd5\uff09\u4e3a\u4f8b\u3002", "result": "\u63d0\u51fa\u4e86AlgEval\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\uff0c\u4e3a\u7406\u89e3LLM\u5185\u90e8\u8fd0\u4f5c\u673a\u5236\u3001\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u5f00\u53d1\u65b0\u578b\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86AlgEval\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5b66\u4e60\u548c\u4f7f\u7528\u7684\u7b97\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u5206\u6790\u6f5c\u5728\u8868\u793a\u3001\u6ce8\u610f\u529b\u548c\u63a8\u7406\u65f6\u95f4\u8ba1\u7b97\u7b49\u6765\u63ed\u793aLLM\u89e3\u51b3\u7279\u5b9a\u4efb\u52a1\u7684\u7b97\u6cd5\u673a\u5236\u3002"}}
{"id": "2507.07576", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.07576", "abs": "https://arxiv.org/abs/2507.07576", "authors": ["Mohamed Siala", "Jordi Planes", "Joao Marques-Silva"], "title": "On Trustworthy Rule-Based Models and Explanations", "comment": null, "summary": "A task of interest in machine learning (ML) is that of ascribing explanations\nto the predictions made by ML models. Furthermore, in domains deemed high risk,\nthe rigor of explanations is paramount. Indeed, incorrect explanations can and\nwill mislead human decision makers. As a result, and even if interpretability\nis acknowledged as an elusive concept, so-called interpretable models are\nemployed ubiquitously in high-risk uses of ML and data mining (DM). This is the\ncase for rule-based ML models, which encompass decision trees, diagrams, sets\nand lists. This paper relates explanations with well-known undesired facets of\nrule-based ML models, which include negative overlap and several forms of\nredundancy. The paper develops algorithms for the analysis of these undesired\nfacets of rule-based systems, and concludes that well-known and widely used\ntools for learning rule-based ML models will induce rule sets that exhibit one\nor more negative facets.", "AI": {"tldr": "\u57fa\u4e8e\u89c4\u5219\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5b58\u5728\u8d1f\u9762\u65b9\u9762\uff0c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u4f1a\u4ea7\u751f\u6b64\u7c7b\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u5728\u9ad8\u98ce\u9669\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u9519\u8bef\u7684\u89e3\u91ca\u4f1a\u8bef\u5bfc\u4eba\u7c7b\u51b3\u7b56\u8005\u3002\u57fa\u4e8e\u89c4\u5219\u7684\u6a21\u578b\u867d\u7136\u5177\u6709\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u5b58\u5728\u8d1f\u91cd\u53e0\u548c\u5197\u4f59\u7b49\u95ee\u9898\u3002", "method": "\u5206\u6790\u57fa\u4e8e\u89c4\u5219\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8d1f\u9762\u65b9\u9762\uff08\u4f8b\u5982\u8d1f\u91cd\u53e0\u548c\u5197\u4f59\uff09\uff0c\u5e76\u5f00\u53d1\u76f8\u5e94\u7684\u5206\u6790\u7b97\u6cd5\u3002", "result": "\u5f00\u53d1\u4e86\u5206\u6790\u57fa\u4e8e\u89c4\u5219\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u8d1f\u9762\u65b9\u9762\u7684\u7b97\u6cd5\uff0c\u5e76\u5f97\u51fa\u7ed3\u8bba\uff1a\u73b0\u6709\u5e38\u7528\u5de5\u5177\u5b66\u4e60\u5230\u7684\u6a21\u578b\u5b58\u5728\u8fd9\u4e9b\u8d1f\u9762\u65b9\u9762\u3002", "conclusion": "\u73b0\u6709\u5b66\u4e60\u57fa\u4e8e\u89c4\u5219\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5e38\u7528\u5de5\u5177\u4f1a\u4ea7\u751f\u8868\u73b0\u51fa\u4e00\u4e2a\u6216\u591a\u4e2a\u8d1f\u9762\u65b9\u9762\u7684\u89c4\u5219\u96c6\u3002"}}
{"id": "2507.07595", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07595", "abs": "https://arxiv.org/abs/2507.07595", "authors": ["Zhixiang Su", "Di Wang", "Chunyan Miao"], "title": "Context Pooling: Query-specific Graph Pooling for Generic Inductive Link Prediction in Knowledge Graphs", "comment": null, "summary": "Recent investigations on the effectiveness of Graph Neural Network\n(GNN)-based models for link prediction in Knowledge Graphs (KGs) show that\nvanilla aggregation does not significantly impact the model performance. In\nthis paper, we introduce a novel method, named Context Pooling, to enhance\nGNN-based models' efficacy for link predictions in KGs. To our best of\nknowledge, Context Pooling is the first methodology that applies graph pooling\nin KGs. Additionally, Context Pooling is first-of-its-kind to enable the\ngeneration of query-specific graphs for inductive settings, where testing\nentities are unseen during training. Specifically, we devise two metrics,\nnamely neighborhood precision and neighborhood recall, to assess the neighbors'\nlogical relevance regarding the given queries, thereby enabling the subsequent\ncomprehensive identification of only the logically relevant neighbors for link\nprediction. Our method is generic and assessed by being applied to two\nstate-of-the-art (SOTA) models on three public transductive and inductive\ndatasets, achieving SOTA performance in 42 out of 48 settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4e0a\u4e0b\u6587\u6c60\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u77e5\u8bc6\u56fe\u8c31\u94fe\u63a5\u9884\u6d4b\uff0c\u53d6\u5f97\u4e86SOTA\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u77e5\u8bc6\u56fe\u8c31\u94fe\u63a5\u9884\u6d4b\u6a21\u578b\u7684\u7b80\u5355\u805a\u5408\u65b9\u6cd5\u6548\u679c\u5e76\u4e0d\u663e\u8457\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4e0a\u4e0b\u6587\u6c60\u5316\u7684\u56fe\u6c60\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u7279\u5b9a\u4e8e\u67e5\u8be2\u7684\u56fe\uff0c\u5e76\u901a\u8fc7\u90bb\u57df\u7cbe\u5ea6\u548c\u90bb\u57df\u53ec\u56de\u7387\u6307\u6807\u8bc4\u4f30\u90bb\u5c45\u7684\u903b\u8f91\u76f8\u5173\u6027\uff0c\u4ece\u800c\u8bc6\u522b\u51fa\u7528\u4e8e\u94fe\u63a5\u9884\u6d4b\u7684\u903b\u8f91\u76f8\u5173\u90bb\u5c45\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0c\u5e94\u7528\u4e8e\u4e24\u4e2a\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5e76\u572848\u4e2a\u8bbe\u7f6e\u4e2d\u768442\u4e2a\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u6c60\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u77e5\u8bc6\u56fe\u8c31\u94fe\u63a5\u9884\u6d4b\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5e76\u572848\u4e2a\u8bbe\u7f6e\u4e2d\u768442\u4e2a\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.07599", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07599", "abs": "https://arxiv.org/abs/2507.07599", "authors": ["Sedigh Khademi", "Jim Black", "Christopher Palmer", "Muhammad Javed", "Hazel Clothier", "Jim Buttery", "Gerardo Luis Dimaguila"], "title": "Enhancing Vaccine Safety Surveillance: Extracting Vaccine Mentions from Emergency Department Triage Notes Using Fine-Tuned Large Language Models", "comment": "5 pages", "summary": "This study evaluates fine-tuned Llama 3.2 models for extracting\nvaccine-related information from emergency department triage notes to support\nnear real-time vaccine safety surveillance. Prompt engineering was used to\ninitially create a labeled dataset, which was then confirmed by human\nannotators. The performance of prompt-engineered models, fine-tuned models, and\na rule-based approach was compared. The fine-tuned Llama 3 billion parameter\nmodel outperformed other models in its accuracy of extracting vaccine names.\nModel quantization enabled efficient deployment in resource-constrained\nenvironments. Findings demonstrate the potential of large language models in\nautomating data extraction from emergency department notes, supporting\nefficient vaccine safety surveillance and early detection of emerging adverse\nevents following immunization issues.", "AI": {"tldr": "\u5229\u7528\u5fae\u8c03\u7684Llama\u6a21\u578b\u6709\u6548\u63d0\u53d6\u6025\u8bca\u75c5\u5386\u4e2d\u7684\u75ab\u82d7\u4fe1\u606f\uff0c\u63d0\u5347\u75ab\u82d7\u5b89\u5168\u76d1\u6d4b\u6548\u7387", "motivation": "\u8bc4\u4f30\u5fae\u8c03\u7684Llama 3.2\u6a21\u578b\u4ece\u6025\u8bca\u5206\u8bca\u8bb0\u5f55\u4e2d\u63d0\u53d6\u75ab\u82d7\u76f8\u5173\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u4ee5\u652f\u6301\u8fd1\u5b9e\u65f6\u7684\u75ab\u82d7\u5b89\u5168\u76d1\u6d4b\u3002", "method": "\u5fae\u8c03Llama 32\u4ebf\u53c2\u6570\u6a21\u578b\uff0c\u5e76\u4e0e\u63d0\u793a\u5de5\u7a0b\u6a21\u578b\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002\u4f7f\u7528\u4e86\u6a21\u578b\u91cf\u5316\u4ee5\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\u3002", "result": "\u5fae\u8c03\u7684Llama 30\u4ebf\u53c2\u6570\u6a21\u578b\u5728\u63d0\u53d6\u75ab\u82d7\u540d\u79f0\u7684\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002\u6a21\u578b\u91cf\u5316\u4f7f\u5176\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u9ad8\u6548\u90e8\u7f72\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u7528\u4e8e\u81ea\u52a8\u5316\u63d0\u53d6\u6025\u8bca\u5ba4\u75c5\u5386\u4e2d\u7684\u4fe1\u606f\uff0c\u4ece\u800c\u652f\u6301\u75ab\u82d7\u5b89\u5168\u76d1\u6d4b\u548c\u65e9\u671f\u53d1\u73b0\u514d\u75ab\u540e\u4e0d\u826f\u4e8b\u4ef6\u3002"}}
{"id": "2507.07619", "categories": ["cs.AI", "math.PR"], "pdf": "https://arxiv.org/pdf/2507.07619", "abs": "https://arxiv.org/abs/2507.07619", "authors": ["Marco Sangalli", "Thomas Krak", "Cassio de Campos"], "title": "Towards conservative inference in credal networks using belief functions: the case of credal chains", "comment": null, "summary": "This paper explores belief inference in credal networks using Dempster-Shafer\ntheory. By building on previous work, we propose a novel framework for\npropagating uncertainty through a subclass of credal networks, namely chains.\nThe proposed approach efficiently yields conservative intervals through belief\nand plausibility functions, combining computational speed with robust\nuncertainty representation. Key contributions include formalizing belief-based\ninference methods and comparing belief-based inference against classical\nsensitivity analysis. Numerical results highlight the advantages and\nlimitations of applying belief inference within this framework, providing\ninsights into its practical utility for chains and for credal networks in\ngeneral.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5728\u94fe\u5f0fcredal\u7f51\u7edc\u4e2d\u8fdb\u884c\u4fe1\u5ff5\u63a8\u7406\u7684\u6846\u67b6\uff0c\u9ad8\u6548\u3001\u4fdd\u5b88\uff0c\u5e76\u4e0e\u7ecf\u5178\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u63a2\u7d22\u5728credal\u7f51\u7edc\u4e2d\u4f7f\u7528Dempster-Shafer\u7406\u8bba\u8fdb\u884c\u4fe1\u5ff5\u63a8\u7406\u3002", "method": "\u5728\u94fe\u5f0fcredal\u7f51\u7edc\u4e2d\u4f20\u64ad\u4e0d\u786e\u5b9a\u6027\uff0c\u7ed3\u5408\u4e86\u8ba1\u7b97\u901f\u5ea6\u548c\u9c81\u68d2\u7684\u4e0d\u786e\u5b9a\u6027\u8868\u793a\u3002\u5c06\u57fa\u4e8e\u4fe1\u5ff5\u7684\u63a8\u7406\u4e0e\u7ecf\u5178\u7684\u7075\u654f\u5ea6\u5206\u6790\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u6570\u503c\u7ed3\u679c\u7a81\u51fa\u4e86\u8be5\u6846\u67b6\u5185\u5e94\u7528\u4fe1\u5ff5\u63a8\u7406\u7684\u4f18\u70b9\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u4e86\u89e3\u5176\u5728\u94fe\u548ccredal\u7f51\u7edc\u4e2d\u7684\u5b9e\u7528\u6027\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8eDempster-Shafer\u7406\u8bba\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u94fe\u5f0fcredal\u7f51\u7edc\u4e2d\u8fdb\u884c\u4fe1\u5ff5\u63a8\u7406\uff0c\u8be5\u6846\u67b6\u9ad8\u6548\u5730\u4ea7\u751f\u4fdd\u5b88\u533a\u95f4\uff0c\u5e76\u5bf9\u4fe1\u5ff5\u63a8\u7406\u65b9\u6cd5\u8fdb\u884c\u4e86\u5f62\u5f0f\u5316\u3002"}}
{"id": "2507.07644", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07644", "abs": "https://arxiv.org/abs/2507.07644", "authors": ["Fedor Rodionov", "Abdelrahman Eldesokey", "Michael Birsak", "John Femiani", "Bernard Ghanem", "Peter Wonka"], "title": "PlanQA: A Benchmark for Spatial Reasoning in LLMs using Structured Representations", "comment": "25 pages, 18 figures. Diagnostic benchmark for spatial reasoning in\n  LLMs. Project page: https://OldDelorean.github.io/PlanQA/", "summary": "We introduce PlanQA, a diagnostic benchmark for evaluating geometric and\nspatial reasoning in large-language models (LLMs). PlanQA is grounded in\nstructured representations of indoor scenes, such as kitchens, living rooms,\nand bedrooms, encoded in a symbolic format (e.g., JSON, XML layouts). The\nbenchmark includes diverse question types that test not only metric and\ntopological reasoning (e.g., distance, visibility, shortest paths) but also\ninterior design constraints such as affordance, clearance, balance, and\nusability. Our results across a variety of frontier open-source and commercial\nLLMs show that while models may succeed in shallow queries, they often fail to\nsimulate physical constraints, preserve spatial coherence, or generalize under\nlayout perturbation. PlanQA uncovers a clear blind spot in today's LLMs: they\ndo not consistently reason about real-world layouts. We hope that this\nbenchmark inspires new work on language models that can accurately infer and\nmanipulate spatial and geometric properties in practical settings.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u7406\u89e3\u771f\u5b9e\u4e16\u754c\u5e03\u5c40\u7684\u7a7a\u95f4\u548c\u51e0\u4f55\u5c5e\u6027\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u51e0\u4f55\u548c\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u6307\u51fa\u5176\u4e0d\u8db3\u4e4b\u5904\u3002", "method": "\u6784\u5efaPlanQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u51e0\u4f55\u548c\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u57fa\u4e8e\u5ba4\u5185\u573a\u666f\u7684\u7ed3\u6784\u5316\u8868\u793a\uff08\u4f8b\u5982JSON\u6216XML\u5e03\u5c40\uff09\uff0c\u5305\u542b\u591a\u79cd\u95ee\u9898\u7c7b\u578b\uff0c\u6db5\u76d6\u8ddd\u79bb\u3001\u53ef\u89c1\u6027\u3001\u6700\u77ed\u8def\u5f84\u7b49\u5ea6\u91cf\u548c\u62d3\u6251\u63a8\u7406\uff0c\u4ee5\u53ca\u9002\u7528\u6027\u3001\u51c0\u7a7a\u3001\u5e73\u8861\u6027\u548c\u53ef\u7528\u6027\u7b49\u5ba4\u5185\u8bbe\u8ba1\u7ea6\u675f\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u6a21\u578b\u5728\u6d45\u5c42\u67e5\u8be2\u4e2d\u53ef\u80fd\u6210\u529f\uff0c\u4f46\u5728\u6a21\u62df\u7269\u7406\u7ea6\u675f\u3001\u4fdd\u6301\u7a7a\u95f4\u8fde\u8d2f\u6027\u6216\u5728\u5e03\u5c40\u6270\u52a8\u4e0b\u8fdb\u884c\u6cdb\u5316\u65b9\u9762\u5e38\u5e38\u5931\u8d25\u3002", "conclusion": "PlanQA\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5b83\u4eec\u96be\u4ee5\u6a21\u62df\u7269\u7406\u7ea6\u675f\u3001\u4fdd\u6301\u7a7a\u95f4\u8fde\u8d2f\u6027\u6216\u5728\u5e03\u5c40\u6270\u52a8\u4e0b\u8fdb\u884c\u6cdb\u5316\u3002"}}
