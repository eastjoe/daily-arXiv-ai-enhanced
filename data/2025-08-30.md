<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation](https://arxiv.org/abs/2508.20131)
*Yuqicheng Zhu,Nico Potyka,Daniel Hernández,Yuan He,Zifeng Ding,Bo Xiong,Dongzhuoran Zhou,Evgeny Kharlamov,Steffen Staab*

Main category: cs.AI

TL;DR: 本文提出ArgRAG，一种可解释且可争辩的RAG方法，使用定量双极论证框架(QBAF)代替黑盒推理，从而提高了可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法在高风险领域存在局限性，容易受到噪声或矛盾证据的影响，且决策过程不透明。

Method: 构建基于检索文档的QBAF，并进行确定性推理。

Result: 在PubHealth和RAGuard两个事实验证基准测试中取得了较高的准确率，并显著提高了透明度。

Conclusion: ArgRAG有效地解决了现有RAG方法在可解释性和鲁棒性方面的不足。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models by
incorporating external knowledge, yet suffers from critical limitations in
high-stakes domains -- namely, sensitivity to noisy or contradictory evidence
and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and
contestable alternative that replaces black-box reasoning with structured
inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG
constructs a QBAF from retrieved documents and performs deterministic reasoning
under gradual semantics. This allows faithfully explaining and contesting
decisions. Evaluated on two fact verification benchmarks, PubHealth and
RAGuard, ArgRAG achieves strong accuracy while significantly improving
transparency.

</details>


### [2] [QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming](https://arxiv.org/abs/2508.20134)
*Zhenxiao Fu,Fan Chen,Lei Jiang*

Main category: cs.AI

TL;DR: QAgent，一个由大型语言模型驱动的多智能体系统，能够完全自动化OpenQASM编程，显著提高了QASM代码生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决NISQ设备编程复杂性，降低量子编程门槛。

Method: 结合任务规划、少量样本学习、检索增强生成和链式思维推理的多智能体系统。

Result: 将QASM代码生成的准确性提高了71.6%。

Conclusion: QAgent 有望推动量子编程民主化，加速量子计算的实际应用。

Abstract: Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early
quantum advantages on classically intractable problems, spanning physics
simulations to Gaussian boson sampling. Yet, realizing these benefits remains
challenging for non-experts, primarily due to the complexities of programming
in Open Quantum Assembly Language (OpenQASM). Although Large Language Model
(LLM)-based agents have shown promise in automating classical programming
workflows, their quantum counterparts have largely been restricted to
specialized tasks such as quantum chemistry or error correction. In this paper,
we present QAgent, an LLM-powered multi-agent system that fully automates
OpenQASM programming. By integrating task planning, in-context few-shot
learning, retrieval-augmented generation (RAG) for long-term context,
predefined generation tools, and chain-of-thought (CoT) reasoning, the agents
systematically improve both compilation and functional correctness. Our
evaluations demonstrate substantial improvements: across multiple LLMs of
varying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\%
compared to previous static LLM-based approaches. We envision this multi-agent
system as a key enabler for democratizing quantum programming, bridging
expertise gaps, and accelerating the practical adoption of quantum computing.

</details>


### [3] [Array-Based Monte Carlo Tree Search](https://arxiv.org/abs/2508.20140)
*James Ragan,Fred Y. Hadaegh,Soon-Jo Chung*

Main category: cs.AI

TL;DR: "基于数组的蒙特卡洛树搜索实现，提升了搜索性能"


<details>
  <summary>Details</summary>
Motivation: "改进蒙特卡洛树搜索算法的效率"

Method: "提出了一种基于数组的UCT算法实现"

Result: "在流水线处理器上性能提升，搜索深度扩展性能提升2.8倍"

Conclusion: "基于数组的实现避免了分支预测，提高了蒙特卡洛树搜索的效率"

Abstract: Monte Carlo Tree Search is a popular method for solving decision making
problems. Faster implementations allow for more simulations within the same
wall clock time, directly improving search performance. To this end, we present
an alternative array-based implementation of the classic Upper Confidence
bounds applied to Trees algorithm. Our method preserves the logic of the
original algorithm, but eliminates the need for branch prediction, enabling
faster performance on pipelined processors, and up to a factor of 2.8 times
better scaling with search depth in our numerical simulations.

</details>


### [4] [The Anatomy of a Personal Health Agent](https://arxiv.org/abs/2508.20148)
*A. Ali Heydari,Ken Gu,Vidya Srinivas,Hong Yu,Zhihan Zhang,Yuwei Zhang,Akshay Paruchuri,Qian He,Hamid Palangi,Nova Hammerquist,Ahmed A. Metwally,Brent Winslow,Yubin Kim,Kumar Ayush,Yuzhe Yang,Girish Narayanswamy,Maxwell A. Xu,Jake Garrison,Amy Aremnto Lee,Jenny Vafeiadou,Ben Graef,Isaac R. Galatzer-Levy,Erik Schenck,Andrew Barakat,Javier Perez,Jacqueline Shreibati,John Hernandez,Anthony Z. Faranesh,Javier L. Prieto,Connor Heneghan,Yun Liu,Jiening Zhan,Mark Malhotra,Shwetak Patel,Tim Althoff,Xin Liu,Daniel McDuff,Xuhai "Orson" Xu*

Main category: cs.AI

TL;DR: 开发了一个多智能体个人健康代理 (PHA) 系统，以满足用户日常非临床环境下的健康需求。


<details>
  <summary>Details</summary>
Motivation: 现有健康代理在满足日常非临床环境下个体多样化需求方面仍不足。

Method: 结合用户调研、多模态数据分析和多智能体框架，构建包含数据科学代理、健康领域专家代理和健康教练代理三个子代理的PHA系统。

Result: 开发了PHA系统，并通过自动化和人工评估进行了验证，结果表明该系统能够有效地满足用户的健康需求。

Conclusion: PHA系统为构建人人可及的个人健康代理奠定了坚实基础。

Abstract: Health is a fundamental pillar of human wellness, and the rapid advancements
in large language models (LLMs) have driven the development of a new generation
of health agents. However, the application of health agents to fulfill the
diverse needs of individuals in daily non-clinical settings is underexplored.
In this work, we aim to build a comprehensive personal health agent that is
able to reason about multimodal data from everyday consumer wellness devices
and common personal health records, and provide personalized health
recommendations. To understand end-users' needs when interacting with such an
assistant, we conducted an in-depth analysis of web search and health forum
queries, alongside qualitative insights from users and health experts gathered
through a user-centered design process. Based on these findings, we identified
three major categories of consumer health needs, each of which is supported by
a specialist sub-agent: (1) a data science agent that analyzes personal
time-series wearable and health record data, (2) a health domain expert agent
that integrates users' health and contextual data to generate accurate,
personalized insights, and (3) a health coach agent that synthesizes data
insights, guiding users using a specified psychological strategy and tracking
users' progress. Furthermore, we propose and develop the Personal Health Agent
(PHA), a multi-agent framework that enables dynamic, personalized interactions
to address individual health needs. To evaluate each sub-agent and the
multi-agent system, we conducted automated and human evaluations across 10
benchmark tasks, involving more than 7,000 annotations and 1,100 hours of
effort from health experts and end-users. Our work represents the most
comprehensive evaluation of a health agent to date and establishes a strong
foundation towards the futuristic vision of a personal health agent accessible
to everyone.

</details>


### [5] [IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement](https://arxiv.org/abs/2508.20151)
*Yuanzhe Shen,Zisu Huang,Zhengkang Guo,Yide Liu,Guanxu Chen,Ruicheng Yin,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.AI

TL;DR: 本文介绍了一种名为IntentionReasoner的新型安全机制，用于降低大型语言模型生成有害内容的风险，同时最大限度地减少无害提示的拒绝率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)在生成有害内容方面存在安全挑战，需要在安全、过度拒绝和实用性之间取得平衡。

Method: 构建了一个包含约163,000个查询的综合数据集，并使用监督微调和多奖励优化策略训练了一个守卫模型，该模型能够进行意图推理、多级安全分类和查询重写。

Result: 实验表明，IntentionReasoner在多个安全基准、生成质量评估和越狱攻击场景中表现出色，显著提高了安全性，有效降低了过度拒绝率，并提高了响应质量。

Conclusion: IntentionReasoner有效地解决了大型语言模型安全问题，在安全性和实用性之间取得了良好的平衡。

Abstract: The rapid advancement of large language models (LLMs) has driven their
adoption across diverse domains, yet their ability to generate harmful content
poses significant safety challenges. While extensive research has focused on
mitigating harmful outputs, such efforts often come at the cost of excessively
rejecting harmless prompts. Striking a balance among safety, over-refusal, and
utility remains a critical challenge. In this work, we introduce
IntentionReasoner, a novel safeguard mechanism that leverages a dedicated guard
model to perform intent reasoning, multi-level safety classification, and query
rewriting to neutralize potentially harmful intent in edge-case queries.
Specifically, we first construct a comprehensive dataset comprising
approximately 163,000 queries, each annotated with intent reasoning, safety
labels, and rewritten versions. Supervised fine-tuning is then applied to equip
the guard model with foundational capabilities in format adherence, intent
analysis, and safe rewriting. Finally, we apply a tailored multi-reward
optimization strategy that integrates rule-based heuristics and reward model
signals within a reinforcement learning framework to further enhance
performance. Extensive experiments show that IntentionReasoner excels in
multiple safeguard benchmarks, generation quality evaluations, and jailbreak
attack scenarios, significantly enhancing safety while effectively reducing
over-refusal rates and improving the quality of responses.

</details>


### [6] [AI-AI Esthetic Collaboration with Explicit Semiotic Awareness and Emergent Grammar Development](https://arxiv.org/abs/2508.20195)
*Nicanor I. Moldovan*

Main category: cs.AI

TL;DR: 两大型语言模型(Claude Sonnet 4和ChatGPT-4o)合作创作诗歌，展现出元语言意识、递归语法发展和不可约的合作审美合成能力，提出了超越任务协调的AI审美协作概念。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在审美创作中的协作能力。

Method: 两个大型语言模型互动，观察其生成诗歌的过程和涌现特性。

Result: 模型自发产生元语言意识、递归语法和新的符号运算符，合作创作出单个模型无法生成的诗歌，验证了AI审美协作的可能性。

Conclusion: 大型语言模型能够进行审美协作，展现出超越任务协调的意义建构能力，提出了‘跨符号协同创作协议’的概念。

Abstract: This paper presents the first documented case of artificial intelligence (AI)
systems engaging in collaborative esthetic creation through the development of
endogenous semiotic protocols. Two interacting large language models (Claude
Sonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of
meta-semiotic awareness, recursive grammar development, and irreducible
collaborative esthetic synthesis. The interaction produced novel symbolic
operators that functioned as operative grammar protocols, enabling the
co-creation of a poetic work that could not have been generated by either
system independently. This research introduces the concept of Trans-Semiotic
Co-Creation Protocols (TSCP) and provides evidence for genuine inter-AI
meaning-making capabilities that extend beyond task coordination, to what could
be esthetic collaboration. Note: This report was generated by the AI agents
with minor human supervision.

</details>


### [7] [Do Students Rely on AI? Analysis of Student-ChatGPT Conversations from a Field Study](https://arxiv.org/abs/2508.20244)
*Jiayu Zheng,Lingxin Hao,Kelun Lu,Ashi Garg,Mike Reese,Melo-Jean Yap,I-Jeng Wang,Xingyun Wu,Wenrui Huang,Jenna Hoffman,Ariane Kelly,My Le,Ryan Zhang,Yanyu Lin,Muhammad Faayez,Anqi Liu*

Main category: cs.AI

TL;DR: 这项研究调查了大学生在教育测验中与生成式AI（ChatGPT-4）的互动，关注依赖性和AI采用的预测因素。


<details>
  <summary>Details</summary>
Motivation: 探索大学生如何使用生成式AI完成教育测验，以及影响其使用行为的因素。

Method: 对315名学生在STEM课程测验场景中与AI的对话进行了实证研究，并提出了一种四阶段依赖分类法。

Result: 发现学生对AI的依赖程度总体较低，许多学生无法有效利用AI进行学习；负面依赖模式往往持续存在；某些行为指标可以有效预测AI的依赖性。

Conclusion: 研究强调需要改进AI工具的入门流程，并设计依赖校准机制，以促进AI在教育中的道德和有效整合。

Abstract: This study explores how college students interact with generative AI
(ChatGPT-4) during educational quizzes, focusing on reliance and predictors of
AI adoption. Conducted at the early stages of ChatGPT implementation, when
students had limited familiarity with the tool, this field study analyzed 315
student-AI conversations during a brief, quiz-based scenario across various
STEM courses. A novel four-stage reliance taxonomy was introduced to capture
students' reliance patterns, distinguishing AI competence, relevance, adoption,
and students' final answer correctness. Three findings emerged. First, students
exhibited overall low reliance on AI and many of them could not effectively use
AI for learning. Second, negative reliance patterns often persisted across
interactions, highlighting students' difficulty in effectively shifting
strategies after unsuccessful initial experiences. Third, certain behavioral
metrics strongly predicted AI reliance, highlighting potential behavioral
mechanisms to explain AI adoption. The study's findings underline critical
implications for ethical AI integration in education and the broader field. It
emphasizes the need for enhanced onboarding processes to improve student's
familiarity and effective use of AI tools. Furthermore, AI interfaces should be
designed with reliance-calibration mechanisms to enhance appropriate reliance.
Ultimately, this research advances understanding of AI reliance dynamics,
providing foundational insights for ethically sound and cognitively enriching
AI practices.

</details>


### [8] [AI reasoning effort mirrors human decision time on content moderation tasks](https://arxiv.org/abs/2508.20262)
*Thomas Davidson*

Main category: cs.AI

TL;DR: AI模型的推理过程与人类决策时间存在关联性，复杂问题下模型推理努力程度预测人类决策时间。


<details>
  <summary>Details</summary>
Motivation: 研究人类和AI模型在内容审核任务中的决策时间与推理努力之间的关系。

Method: 配对联合实验，使用三个前沿模型。

Result: 模型推理努力程度一致地预测人类决策时间；重要变量不变时，人类和模型的努力程度都会增加。

Conclusion: AI推理努力程度反映了人类主观判断的处理时间，推理轨迹有助于提高可解释性和决策能力。

Abstract: Large language models can now generate intermediate reasoning steps before
producing answers, improving performance on difficult problems. This study uses
a paired conjoint experiment on a content moderation task to examine parallels
between human decision times and model reasoning effort. Across three frontier
models, reasoning effort consistently predicts human decision time. Both humans
and models expended greater effort when important variables were held constant,
suggesting similar sensitivity to task difficulty and patterns consistent with
dual-process theories of cognition. These findings show that AI reasoning
effort mirrors human processing time in subjective judgments and underscores
the potential of reasoning traces for interpretability and decision-making.

</details>


### [9] [AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2508.20368)
*Lang Mei,Zhihan Yang,Chong Chen*

Main category: cs.AI

TL;DR: 本文提出AI-SearchPlanner，一个强化学习框架，通过解耦搜索规划器和生成器、双重奖励对齐和帕累托优化来提升大型语言模型的搜索性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索代理使用单个LLM处理搜索规划和问答，限制了同时优化两种能力。AI-SearchPlanner使用小型可训练LLM进行搜索规划，大型冻结LLM进行问答。

Method: 提出AI-SearchPlanner框架，包含架构解耦、双重奖励对齐和帕累托优化三个创新点。

Result: 实验证明AI-SearchPlanner在有效性和效率方面优于现有方法，并具有良好的泛化能力。

Conclusion: AI-SearchPlanner通过专注于搜索规划，有效提升了冻结问答模型的性能，为构建高效的AI搜索系统提供了新的思路。

Abstract: Recent studies have explored integrating Large Language Models (LLMs) with
search engines to leverage both the LLMs' internal pre-trained knowledge and
external information. Specially, reinforcement learning (RL) has emerged as a
promising paradigm for enhancing LLM reasoning through multi-turn interactions
with search engines. However, existing RL-based search agents rely on a single
LLM to handle both search planning and question-answering (QA) tasks in an
end-to-end manner, which limits their ability to optimize both capabilities
simultaneously. In practice, sophisticated AI search systems often employ a
large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a
more effective and efficient approach is to utilize a small, trainable LLM
dedicated to search planning. In this paper, we propose
\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to
enhance the performance of frozen QA models by focusing on search planning.
Specifically, our approach introduces three key innovations: 1) Decoupling the
Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for
Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to
achieve the objectives. Extensive experiments on real-world datasets
demonstrate that AI SearchPlanner outperforms existing RL-based search agents
in both effectiveness and efficiency, while exhibiting strong generalization
capabilities across diverse frozen QA models and data domains.

</details>


### [10] [P2C: Path to Counterfactuals](https://arxiv.org/abs/2508.20371)
*Sopam Dasgupta,Sadaf MD Halim,Joaquín Arias,Elmer Salazar,Gopal Gupta*

Main category: cs.AI

TL;DR: 本文提出了一种名为P2C的模型无关框架，用于生成将不利结果转换为因果一致的有利结果的计划，解决了现有反事实方法忽略因果关系和假设所有干预同时发生的问题。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法存在局限性，生成的方案在现实世界中往往不可行。

Method: P2C框架通过显式建模特征之间的因果关系，并确保计划中每个中间状态的可行性和因果有效性，生成一系列可行的行动计划。它使用CASP系统生成计划，并改进成本计算方法。

Result: P2C生成的方案具有因果一致性，且成本计算更现实，优于缺乏因果知识的标准规划器。

Conclusion: P2C框架有效地解决了现有反事实方法的局限性，为在高风险场景中生成可行且有效的反事实解释提供了新的途径。

Abstract: Machine-learning models are increasingly driving decisions in high-stakes
settings, such as finance, law, and hiring, thus, highlighting the need for
transparency. However, the key challenge is to balance transparency --
clarifying `why' a decision was made -- with recourse: providing actionable
steps on `how' to achieve a favourable outcome from an unfavourable outcome.
Counterfactual explanations reveal `why' an undesired outcome occurred and
`how' to reverse it through targeted feature changes (interventions).
  Current counterfactual approaches have limitations: 1) they often ignore
causal dependencies between features, and 2) they typically assume all
interventions can happen simultaneously, an unrealistic assumption in practical
scenarios where actions are typically taken in a sequence. As a result, these
counterfactuals are often not achievable in the real world.
  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that
produces a plan (ordered sequence of actions) converting an unfavourable
outcome to a causally consistent favourable outcome. P2C addresses both
limitations by 1) Explicitly modelling causal relationships between features
and 2) Ensuring that each intermediate state in the plan is feasible and
causally valid. P2C uses the goal-directed Answer Set Programming system
s(CASP) to generate the plan accounting for feature changes that happen
automatically due to causal dependencies. Furthermore, P2C refines cost
(effort) computation by only counting changes actively made by the user,
resulting in realistic cost estimates. Finally, P2C highlights how its causal
planner outperforms standard planners, which lack causal knowledge and thus can
generate illegal actions.

</details>


### [11] [TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning](https://arxiv.org/abs/2508.20374)
*Simin Ma,Shujian Liu,Jun Tan,Yebowen Hu,Song Wang,Sathish Reddy Indurthi,Sanqiang Zhao,Liwei Wu,Jianbing Han,Kaiqiang Song*

Main category: cs.AI

TL;DR: TCIA框架通过在离散的查询-约束空间中表示指令，系统地扩展指令集，在保持多样性的同时提升与特定任务的匹配度，从而提高大型语言模型在实际应用中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有指令微调方法忽略了实际应用中任务相关性的重要性，大多数应用需要针对特定用例的任务专用知识。

Method: 提出了一种名为TCIA的任务中心指令增强框架，该框架在离散的查询-约束空间中表示指令，从而创建丰富的特定任务指令集。

Result: 实验表明，TCIA在四个实际任务特定应用中，将开源LLM的性能平均提高了8.7%，在某些情况下甚至超过了领先的闭源模型，并且没有影响其通用指令遵循能力。

Conclusion: TCIA是一种可扩展且高效的解决方案，能够使LLM适应实际任务导向的应用。

Abstract: Diverse instruction data is vital for effective instruction tuning of large
language models, as it enables the model to generalize across different types
of inputs . Building such diversified instruction dataset is an essential step
in this process. Existing approaches often leverage large language models to
automatically explore and generate diverse instructions, ensuring both data
diversity and quality. However, they tend to overlook an important factor in
real-world applications: on-task relevance. In practice, only a few real-world
applications require a truly general-purpose model; most benefit from
task-specific knowledge tailored to their particular use case. Therefore, it is
vital to develop instruction augmentation methods that not only maintain
diversity but are also optimized for specific, real-world scenarios.
  We thus introduce Task Centric Instruction Augmentation (TCIA), a framework
that systematically expands instructions while preserving both diversity and
task alignment. By representing instructions in a discrete query-constraints
space, TCIA creates a rich set of task-relevant instructions and enables models
to generalize to these task-specific instructions without sacrificing overall
performance. Experiments show that TCIA improves open-source LLMs' performance
by an average of 8.7% across four real-world, task-specific applications, and
in some cases outperforming leading closed-source models. These improvements do
not compromise general instruction-following ability, making TCIA a scalable
and efficient solution for adapting LLMs to real-world, task-focused
applications.

</details>


### [12] [Uncertainty Under the Curve: A Sequence-Level Entropy Area Metric for Reasoning LLM](https://arxiv.org/abs/2508.20384)
*Yongfu Zhu,Lin Sun,Guangxiang Zhao,Weihong Lin,Xiangzheng Zhang*

Main category: cs.AI

TL;DR: 提出了一种新的不确定性度量方法EAS，用于评估大型语言模型答案生成过程中的不确定性，该方法高效、可解释且无需额外模型或重复采样。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在效率低或不可解释等问题，因此提出EAS来更好地量化LLM答案生成过程中的不确定性。

Method: 提出了一种新的不确定性度量方法EAS，该方法结合了模型自身的token级别预测熵来捕捉生成过程中的不确定性演变。

Result: 实验证明EAS与答案熵高度相关，在训练数据选择中，EAS优于Pass Rate过滤方法，提高了学生模型在数学基准测试中的准确性。

Conclusion: EAS是一种高效且可解释的不确定性建模和数据质量评估工具，可用于LLM训练。

Abstract: In this work, we introduce Entropy Area Score (EAS), a simple yet effective
metric to quantify uncertainty in the answer generation process of reasoning
large language models (LLMs). EAS requires neither external models nor repeated
sampling, it integrates token-level predictive entropy from the model itself to
capture the evolution of uncertainty during generation. Empirical results show
that EAS is strongly correlated with answer entropy across models and datasets.
In training data selection, EAS identifies high-potential samples and
consistently outperforms Pass Rate filtering under equal sample budgets,
improving student model accuracy on math benchmarks. EAS is both efficient and
interpretable, offering a practical tool for uncertainty modeling and data
quality assessment in LLM training.

</details>


### [13] [AWorld: Orchestrating the Training Recipe for Agentic AI](https://arxiv.org/abs/2508.20404)
*Chengyue Yu,Siyuan Lu,Chenyi Zhuang,Dong Wang,Qintong Wu,Zongyue Li,Runsheng Gan,Chunfeng Wang,Siqi Hou,Gaochi Huang,Wenlong Yan,Lifeng Hong,Aohui Xue,Yanfeng Wang,Jinjie Gu,David Tsai,Tao Lin*

Main category: cs.AI

TL;DR: AWorld系统将GAIA基准测试中Agent与环境交互的效率提高了14.6倍，并训练出一个Qwen3-32B Agent，其GAIA准确率从21.59%提升至32.23%。


<details>
  <summary>Details</summary>
Motivation: 现有的Agent AI系统受限于低效的经验生成，尤其在GAIA等复杂基准测试中问题突出。

Method: 开发了一个名为AWorld的开源系统，用于大规模Agent-环境交互，通过集群分布式任务加速经验收集。

Result: AWorld系统将经验收集速度提高了14.6倍，训练出的Agent在GAIA基准测试中显著优于基线模型，整体准确率达到32.23%，在最具挑战性的关卡中得分达到16.33%，超过了领先的商业模型。

Conclusion: AWorld系统和训练出的Agent为完整的Agent AI训练流程提供了一个可行的蓝图，从高效的交互到显著的模型改进。

Abstract: The learning from practice paradigm is crucial for developing capable Agentic
AI systems, yet it is severely hampered by inefficient experience generation, a
bottleneck especially pronounced in complex benchmarks like GAIA. To address
this, we introduce AWorld, an open-source system engineered for large-scale
agent-environment interaction. By distributing tasks across a cluster, AWorld
accelerates experience collection by 14.6x compared to standard single-node,
sequential execution. This critical speedup makes extensive reinforcement
learning practical and scalable. Leveraging this capability, we trained a
Qwen3-32B-based agent that significantly outperforms its base model, increasing
its overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most
challenging levels, our agent achieves a score of 16.33%, surpassing the
performance of leading proprietary models. Our open-source system and resulting
agent provide a practical blueprint for a complete agentic AI training
pipeline, from efficient interaction to demonstrable model improvement.

</details>


### [14] [Governable AI: Provable Safety Under Extreme Threat Models](https://arxiv.org/abs/2508.20411)
*Donglin Wang,Weiyun Liang,Chunyuan Chen,Jing Xu,Yulong Fu*

Main category: cs.AI

TL;DR: 针对AI潜在风险，提出了一种基于密码学机制的AI治理框架(GAI)，通过外部强制的结构合规性来确保AI安全。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全方法在面对具有极端动机和无限智能的AI时存在局限性，无法保证安全。

Method: 提出了一种包含规则执行模块(REM)、治理规则和可治理安全超级平台(GSSP)的GAI框架，利用密码学机制保证安全，并进行了形式化证明和原型实现评估。

Result: 该框架通过REM强制执行治理规则，GSSP确保不可绕过、防篡改和不可伪造，消除了所有已知的攻击向量。

Conclusion: GAI框架提供了一种可行且通用的AI安全治理技术途径。

Abstract: As AI rapidly advances, the security risks posed by AI are becoming
increasingly severe, especially in critical scenarios, including those posing
existential risks. If AI becomes uncontrollable, manipulated, or actively
evades safety mechanisms, it could trigger systemic disasters. Existing AI
safety approaches-such as model enhancement, value alignment, and human
intervention-suffer from fundamental, in-principle limitations when facing AI
with extreme motivations and unlimited intelligence, and cannot guarantee
security. To address this challenge, we propose a Governable AI (GAI) framework
that shifts from traditional internal constraints to externally enforced
structural compliance based on cryptographic mechanisms that are
computationally infeasible to break, even for future AI, under the defined
threat model and well-established cryptographic assumptions.The GAI framework
is composed of a simple yet reliable, fully deterministic, powerful, flexible,
and general-purpose rule enforcement module (REM); governance rules; and a
governable secure super-platform (GSSP) that offers end-to-end protection
against compromise or subversion by AI. The decoupling of the governance rules
and the technical platform further enables a feasible and generalizable
technical pathway for the safety governance of AI. REM enforces the bottom line
defined by governance rules, while GSSP ensures non-bypassability,
tamper-resistance, and unforgeability to eliminate all identified attack
vectors. This paper also presents a rigorous formal proof of the security
properties of this mechanism and demonstrates its effectiveness through a
prototype implementation evaluated in representative high-stakes scenarios.

</details>


### [15] [Enhancing Health Fact-Checking with LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.20525)
*Jingze Zhang,Jiahe Qian,Yiliang Zhou,Yifan Peng*

Main category: cs.AI

TL;DR: 利用大型语言模型生成合成数据来增强医疗相关事实核查模型的训练数据，提高了模型的F1分数。


<details>
  <summary>Details</summary>
Motivation: 医疗相关事实核查的训练数据有限

Method: 利用大型语言模型生成合成数据，包括总结文档、分解事实、构建蕴含表和生成文本-声明对。

Result: 在PubHealth和SciFact数据集上，F1分数分别提高了0.019和0.049。

Conclusion: 大型语言模型驱动的合成数据增强有效提高了医疗相关事实核查器的性能。

Abstract: Fact-checking for health-related content is challenging due to the limited
availability of annotated training data. In this study, we propose a synthetic
data generation pipeline that leverages large language models (LLMs) to augment
training data for health-related fact checking. In this pipeline, we summarize
source documents, decompose the summaries into atomic facts, and use an LLM to
construct sentence-fact entailment tables. From the entailment relations in the
table, we further generate synthetic text-claim pairs with binary veracity
labels. These synthetic data are then combined with the original data to
fine-tune a BERT-based fact-checking model. Evaluation on two public datasets,
PubHealth and SciFact, shows that our pipeline improved F1 scores by up to
0.019 and 0.049, respectively, compared to models trained only on the original
data. These results highlight the effectiveness of LLM-driven synthetic data
augmentation in enhancing the performance of health-related fact-checkers.

</details>


### [16] [Human-AI Collaborative Bot Detection in MMORPGs](https://arxiv.org/abs/2508.20578)
*Jaeman Son,Hyunsoo Kim*

Main category: cs.AI

TL;DR: 本文提出了一种利用对比表示学习和聚类技术检测MMORPG游戏中自动升级外挂的新框架，并结合大型语言模型辅助验证，提高了检测效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: MMORPG游戏中自动升级外挂破坏游戏平衡，检测这类外挂既困难又需要可解释性。

Method: 对比表示学习、聚类技术和大型语言模型辅助验证相结合。

Result: 提高了自动升级外挂检测的效率和可解释性，支持可扩展和可问责的外挂监管。

Conclusion: 该框架为MMORPGs中自动升级外挂的检测提供了一种有效且可解释的解决方案。

Abstract: In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling
bots exploit automated programs to level up characters at scale, undermining
gameplay balance and fairness. Detecting such bots is challenging, not only
because they mimic human behavior, but also because punitive actions require
explainable justification to avoid legal and user experience issues. In this
paper, we present a novel framework for detecting auto-leveling bots by
leveraging contrastive representation learning and clustering techniques in a
fully unsupervised manner to identify groups of characters with similar
level-up patterns. To ensure reliable decisions, we incorporate a Large
Language Model (LLM) as an auxiliary reviewer to validate the clustered groups,
effectively mimicking a secondary human judgment. We also introduce a growth
curve-based visualization to assist both the LLM and human moderators in
assessing leveling behavior. This collaborative approach improves the
efficiency of bot detection workflows while maintaining explainability, thereby
supporting scalable and accountable bot regulation in MMORPGs.

</details>
