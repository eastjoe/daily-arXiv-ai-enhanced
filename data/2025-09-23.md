<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 18]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Identifying Critical Pathways in Coronary Heart Disease via Fuzzy Subgraph Connectivity](https://arxiv.org/abs/2509.16288)
*Shanookha Ali,Nitha Niralda P C*

Main category: cs.AI

TL;DR: 模糊子图连接性(FSC)用于分析冠心病(CHD)风险因素，识别关键诊断路径和风险因素。


<details>
  <summary>Details</summary>
Motivation: 冠心病风险因素复杂，关系不确定，需要一种系统方法量化关联强度。

Method: 构建模糊CHD图，使用FSC评估连接性，识别最强诊断路径、主要风险因素和关键桥梁。

Result: FSC突出影响性通路，界定最弱和最强相关性之间的连接性，揭示去除关键边会降低预测强度。

Conclusion: FSC提供一个可解释且稳健的框架，用于模拟CHD风险预测中的不确定性，支持临床决策。

Abstract: Coronary heart disease (CHD) arises from complex interactions among
uncontrollable factors, controllable lifestyle factors, and clinical
indicators, where relationships are often uncertain. Fuzzy subgraph
connectivity (FSC) provides a systematic tool to capture such imprecision by
quantifying the strength of association between vertices and subgraphs in fuzzy
graphs. In this work, a fuzzy CHD graph is constructed with vertices for
uncontrollable, controllable, and indicator components, and edges weighted by
fuzzy memberships. Using FSC, we evaluate connectivity to identify strongest
diagnostic routes, dominant risk factors, and critical bridges. Results show
that FSC highlights influential pathways, bounds connectivity between weakest
and strongest correlations, and reveals critical edges whose removal reduces
predictive strength. Thus, FSC offers an interpretable and robust framework for
modeling uncertainty in CHD risk prediction and supporting clinical
decision-making.

</details>


### [2] [A global view of diverse construction methods of fuzzy implication functions rooted on F-chains](https://arxiv.org/abs/2509.16298)
*Raquel Fernandez-Peralta,Juan Vicente Riera*

Main category: cs.AI

TL;DR: 本文研究了模糊蕴涵函数的构造方法，提出了一种基于广义F链的构造方法，并证明其包含了多种已有的构造方法。


<details>
  <summary>Details</summary>
Motivation: 现有模糊蕴涵函数构造方法多样，缺乏统一的理论框架。

Method: 提出并分析了一种基于广义F链的模糊蕴涵函数构造方法，该方法使用多个模糊蕴涵函数和两个递增函数。

Result: 证明了该方法能够保持多种性质，并将其与多种现有方法联系起来，揭示了不同构造方法的结构相似性。

Conclusion: 该广义F链构造方法为模糊蕴涵函数的构造提供了统一的框架，加深了对模糊蕴涵函数结构关系的理解。

Abstract: Fuzzy implication functions are one of the most important operators used in
the fuzzy logic framework. While their flexible definition allows for diverse
families with distinct properties, this variety needs a deeper theoretical
understanding of their structural relationships. In this work, we focus on the
study of construction methods, which employ different techniques to generate
new fuzzy implication functions from existing ones. Particularly, we generalize
the $F$-chain-based construction, recently introduced by Mesiar et al. to
extend a method for constructing aggregation functions to the context of fuzzy
implication functions. Our generalization employs collections of fuzzy
implication functions rather than single ones, and uses two different
increasing functions instead of a unique $F$-chain. We analyze property
preservation under this construction and establish sufficient conditions.
Furthermore, we demonstrate that our generalized $F$-chain-based construction
is a unifying framework for several existing methods. In particular, we show
that various construction techniques, such as contraposition, aggregation, and
generalized vertical/horizontal threshold methods, can be reformulated within
our approach. This reveals structural similarities between seemingly distinct
construction strategies and provides a cohesive perspective on fuzzy
implication construction methods.

</details>


### [3] [On the Non-Uniqueness of Representation of $(U,N)$-Implications](https://arxiv.org/abs/2509.16299)
*Raquel Fernandez-Peralta,Andrea Mesiarová-Zemánková*

Main category: cs.AI

TL;DR: 本文研究了模糊蕴涵函数的唯一性表示问题，尤其关注基于(U,N)-蕴涵的表示，发现即使在模糊否定N连续的情况下，(U,N)-蕴涵也不一定具有唯一表示。


<details>
  <summary>Details</summary>
Motivation: 现有的研究假设模糊否定N连续，从而保证(U,N)-蕴涵的唯一表示。本文旨在挑战这一假设。

Method: 本文通过反例推翻了(U,N)-蕴涵的唯一表示性，并对具有连续和非连续底层函数的幺半群的唯一性条件进行了全面研究。

Result: 本文证明了即使模糊否定连续，(U,N)-蕴涵也不一定具有唯一表示，并深入研究了幺半群的唯一性条件。

Conclusion: 本文的结果为这些算子的结构特性提供了重要的理论见解。

Abstract: Fuzzy implication functions constitute fundamental operators in fuzzy logic
systems, extending classical conditionals to manage uncertainty in logical
inference. Among the extensive families of these operators, generalizations of
the classical material implication have received considerable theoretical
attention, particularly $(S,N)$-implications constructed from t-conorms and
fuzzy negations, and their further generalizations to $(U,N)$-implications
using disjunctive uninorms. Prior work has established characterization
theorems for these families under the assumption that the fuzzy negation $N$ is
continuous, ensuring uniqueness of representation. In this paper, we disprove
this last fact for $(U,N)$-implications and we show that they do not
necessarily possess a unique representation, even if the fuzzy negation is
continuous. Further, we provide a comprehensive study of uniqueness conditions
for both uninorms with continuous and non-continuous underlying functions. Our
results offer important theoretical insights into the structural properties of
these operators.

</details>


### [4] [Generalizability of Large Language Model-Based Agents: A Comprehensive Survey](https://arxiv.org/abs/2509.16330)
*Minxing Zhang,Yi Yang,Roy Xie,Bhuwan Dhingra,Shuyan Zhou,Jian Pei*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型 (LLM) 代理的泛化性问题，分析了评估方法和改进策略，并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: LLM代理的泛化性对于其在不同环境和任务中的应用至关重要，但目前对其定义和评估方法缺乏系统研究。

Method: 对现有文献进行综述，对LLM代理的泛化性进行分类和分析，并提出改进方法和未来研究方向。

Result: 本文对LLM代理泛化性评估方法、改进策略和未来研究方向进行了全面综述，为构建更可靠的LLM代理奠定了基础。

Conclusion: LLM代理的泛化性是未来研究的重点，需要发展标准化框架、改进评估指标和结合方法创新与架构设计。

Abstract: Large Language Model (LLM)-based agents have emerged as a new paradigm that
extends LLMs' capabilities beyond text generation to dynamic interaction with
external environments. By integrating reasoning with perception, memory, and
tool use, agents are increasingly deployed in diverse domains like web
navigation and household robotics. A critical challenge, however, lies in
ensuring agent generalizability - the ability to maintain consistent
performance across varied instructions, tasks, environments, and domains,
especially those beyond agents' fine-tuning data. Despite growing interest, the
concept of generalizability in LLM-based agents remains underdefined, and
systematic approaches to measure and improve it are lacking. In this survey, we
provide the first comprehensive review of generalizability in LLM-based agents.
We begin by emphasizing agent generalizability's importance by appealing to
stakeholders and clarifying the boundaries of agent generalizability by
situating it within a hierarchical domain-task ontology. We then review
datasets, evaluation dimensions, and metrics, highlighting their limitations.
Next, we categorize methods for improving generalizability into three groups:
methods for the backbone LLM, for agent components, and for their interactions.
Moreover, we introduce the distinction between generalizable frameworks and
generalizable agents and outline how generalizable frameworks can be translated
into agent-level generalizability. Finally, we identify critical challenges and
future directions, including developing standardized frameworks, variance- and
cost-based metrics, and approaches that integrate methodological innovations
with architecture-level designs. By synthesizing progress and highlighting
opportunities, this survey aims to establish a foundation for principled
research on building LLM-based agents that generalize reliably across diverse
applications.

</details>


### [5] [Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models](https://arxiv.org/abs/2509.16332)
*Stephen Fitz,Peter Romero,Steven Basart,Sipeng Chen,Jose Hernandez-Orallo*

Main category: cs.AI

TL;DR: 研究表明，大型语言模型的性格特征会显著影响其安全性和能力。


<details>
  <summary>Details</summary>
Motivation: 探索人格特质对大型语言模型行为的影响。

Method: 基于五大性格特质框架，研究人格控制对模型行为（安全性和能力基准测试）的影响。

Result: 降低模型的尽责性会导致安全性和能力基准测试分数显著下降。

Conclusion: 人格塑造是模型控制的重要手段，对安全性和能力都有影响，需要进一步研究人格敏感的安全评估和动态行为控制。

Abstract: Large Language Models increasingly mediate high-stakes interactions,
intensifying research on their capabilities and safety. While recent work has
shown that LLMs exhibit consistent and measurable synthetic personality traits,
little is known about how modulating these traits affects model behavior. We
address this gap by investigating how psychometric personality control grounded
in the Big Five framework influences AI behavior in the context of capability
and safety benchmarks. Our experiments reveal striking effects: for example,
reducing conscientiousness leads to significant drops in safety-relevant
metrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy as well
as reduction in general capabilities as measured by MMLU. These findings
highlight personality shaping as a powerful and underexplored axis of model
control that interacts with both safety and general competence. We discuss the
implications for safety evaluation, alignment strategies, steering model
behavior after deployment, and risks associated with possible exploitation of
these findings. Our findings motivate a new line of research on
personality-sensitive safety evaluations and dynamic behavioral control in
LLMs.

</details>


### [6] [A Unified AI Approach for Continuous Monitoring of Human Health and Diseases from Intensive Care Unit to Home with Physiological Foundation Models (UNIPHY+)](https://arxiv.org/abs/2509.16348)
*Minxiao Wang,Saurabh Kataria,Juntong Ni,Timothy G. Buchman,Jocelyn Grunwell,Mark Mai,Wei Jin,Matthew Clark,Stephanie Brown,Michael Fundora,Puneet Sharma,Tony Pan,Sam Khan,Timothy Ruchti,Naveen Muthu,Kevin Maher,Sivasubramanium V Bhavani,Xiao Hu*

Main category: cs.AI

TL;DR: UNIPHY+是一个统一的生理基础模型框架，用于跨护理环境连续监测人类健康和疾病


<details>
  <summary>Details</summary>
Motivation: 利用普遍可获得的生理数据，实现对人类健康和疾病的持续监测

Method: 提出了一种新颖的策略，通过多模态学习、特征融合微调和知识蒸馏，在预训练、微调和轻量级模型个性化过程中整合上下文信息

Result: UNIPHY+能够支持临床决策和长期健康监测

Conclusion: UNIPHY+能够实现通用、可扩展和个性化的生理AI，用于支持临床决策和长期健康监测

Abstract: We present UNIPHY+, a unified physiological foundation model (physioFM)
framework designed to enable continuous human health and diseases monitoring
across care settings using ubiquitously obtainable physiological data. We
propose novel strategies for incorporating contextual information during
pretraining, fine-tuning, and lightweight model personalization via multi-modal
learning, feature fusion-tuning, and knowledge distillation. We advocate
testing UNIPHY+ with a broad set of use cases from intensive care to ambulatory
monitoring in order to demonstrate that UNIPHY+ can empower generalizable,
scalable, and personalized physiological AI to support both clinical
decision-making and long-term health monitoring.

</details>


### [7] [Evaluation of Causal Reasoning for Large Language Models in Contextualized Clinical Scenarios of Laboratory Test Interpretation](https://arxiv.org/abs/2509.16372)
*Balu Bhasuran,Mattia Prosperi,Karim Hanna,John Petrilli,Caretia JeLayne Washington,Zhe He*

Main category: cs.AI

TL;DR: 本研究评估了两个大型语言模型(GPT-o1和Llama-3.2-8b-instruct)在医学因果推理上的能力，GPT-o1表现更佳。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在临床场景下的因果推理能力。

Method: 使用99个临床实验室检验场景，涵盖关联、干预和反事实推理三个层面，由医学专家评估模型的回答。

Result: GPT-o1的整体区分性能(AUROC)高于Llama-3.2-8b-instruct，在关联、干预和反事实推理方面均表现更好。但两个模型在反事实推理，尤其在结果改变的场景下表现最差。

Conclusion: GPT-o1的因果推理能力更强，但仍需改进才能应用于高风险临床场景。

Abstract: This study evaluates causal reasoning in large language models (LLMs) using
99 clinically grounded laboratory test scenarios aligned with Pearl's Ladder of
Causation: association, intervention, and counterfactual reasoning. We examined
common laboratory tests such as hemoglobin A1c, creatinine, and vitamin D, and
paired them with relevant causal factors including age, gender, obesity, and
smoking. Two LLMs - GPT-o1 and Llama-3.2-8b-instruct - were tested, with
responses evaluated by four medically trained human experts. GPT-o1
demonstrated stronger discriminative performance (AUROC overall = 0.80 +/-
0.12) compared to Llama-3.2-8b-instruct (0.73 +/- 0.15), with higher scores
across association (0.75 vs 0.72), intervention (0.84 vs 0.70), and
counterfactual reasoning (0.84 vs 0.69). Sensitivity (0.90 vs 0.84) and
specificity (0.93 vs 0.80) were also greater for GPT-o1, with reasoning ratings
showing similar trends. Both models performed best on intervention questions
and worst on counterfactuals, particularly in altered outcome scenarios. These
findings suggest GPT-o1 provides more consistent causal reasoning, but
refinement is required before adoption in high-stakes clinical applications.

</details>


### [8] [VORTEX: Aligning Task Utility and Human Preferences through LLM-Guided Reward Shaping](https://arxiv.org/abs/2509.16399)
*Guojun Xiong,Milind Tambe*

Main category: cs.AI

TL;DR: 本文提出VORTEX框架，使用LLM结合多目标优化，通过自然语言反馈迭代生成奖励函数，在保持系统效用保证的同时，适应性地整合人类偏好。


<details>
  <summary>Details</summary>
Motivation: 现有AI决策系统难以直接处理自然语言表达的人类偏好变化。

Method: 将问题形式化为多目标优化问题，利用LLM迭代生成基于语言增强和文本梯度提示更新的塑造奖励。

Result: VORTEX在实际分配任务中优于基线方法，在满足人类一致的覆盖目标的同时保持较高的任务性能。理论上保证VORTEX收敛到效用和偏好满意度之间的帕累托最优权衡。

Conclusion: VORTEX提供了一个实用且理论上有根据的范例，用于基于自然语言的人工智能协同优化。

Abstract: In social impact optimization, AI decision systems often rely on solvers that
optimize well-calibrated mathematical objectives. However, these solvers cannot
directly accommodate evolving human preferences, typically expressed in natural
language rather than formal constraints. Recent approaches address this by
using large language models (LLMs) to generate new reward functions from
preference descriptions. While flexible, they risk sacrificing the system's
core utility guarantees. In this paper, we propose \texttt{VORTEX}, a
language-guided reward shaping framework that preserves established
optimization goals while adaptively incorporating human feedback. By
formalizing the problem as multi-objective optimization, we use LLMs to
iteratively generate shaping rewards based on verbal reinforcement and
text-gradient prompt updates. This allows stakeholders to steer decision
behavior via natural language without modifying solvers or specifying trade-off
weights. We provide theoretical guarantees that \texttt{VORTEX} converges to
Pareto-optimal trade-offs between utility and preference satisfaction.
Empirical results in real-world allocation tasks demonstrate that
\texttt{VORTEX} outperforms baselines in satisfying human-aligned coverage
goals while maintaining high task performance. This work introduces a practical
and theoretically grounded paradigm for human-AI collaborative optimization
guided by natural language.

</details>


### [9] [Proactive Statistical Process Control Using AI: A Time Series Forecasting Approach for Semiconductor Manufacturing](https://arxiv.org/abs/2509.16431)
*Mohammad Iqbal Rasul Seeam,Victor S. Sheng*

Main category: cs.AI

TL;DR: 本文提出一种结合机器学习和传统SPC方法的预测性质量控制系统，能够提前预测制造过程中的潜在问题，从而减少意外故障并提高生产效率。


<details>
  <summary>Details</summary>
Motivation: 传统的SPC方法仅在问题发生后做出反应，可能导致资源浪费和成本增加。

Method: 使用Facebook Prophet模型对时间序列数据进行预测，并结合SPC规则对预测值进行风险等级分类。

Result: 该系统成功应用于半导体制造公司的实际数据，即使数据采样间隔不规律，也能做出准确的预测和风险等级分类。

Conclusion: 该系统能够使工程师提前采取行动，从而提高生产过程的稳定性和可靠性，使质量控制更主动、更准确、更有用。

Abstract: In the manufacturing industry, it is very important to keep machines and
processes running smoothly and without unexpected problems. One of the most
common tools used to check if everything is working properly is called
Statistical Process Control (SPC). Traditional SPC methods work by checking
whether recent measurements are within acceptable limits. However, they only
react after a problem has already occurred. This can lead to wasted materials,
machine downtime, and increased costs. In this paper, we present a smarter way
to use SPC. Instead of just reacting to issues after they happen, our system
can predict future problems before they occur. We use a machine learning tool
called Facebook Prophet, which is designed to work with time-series data (data
that changes over time). Prophet looks at past data and forecasts what the next
value will be. Then, we use SPC rules to decide if the predicted value is in a
Safe zone (no problem), a Warning zone (needs attention), or a Critical zone
(may require shutting down the process). We applied this system to real data
from a semiconductor manufacturing company. One of the challenges with this
data is that the measurements are not taken at regular time intervals. This
makes it harder to predict future values accurately. Despite this, our model
was able to make strong predictions and correctly classify the risk level of
future measurements. The main benefit of our system is that it gives engineers
and technicians a chance to act early - before something goes wrong. This helps
reduce unexpected failures and improves the overall stability and reliability
of the production process. By combining machine learning with traditional SPC,
we make quality control more proactive, accurate, and useful for modern
industry.

</details>


### [10] [Domain-Specific Constitutional AI: Enhancing Safety in LLM-Powered Mental Health Chatbots](https://arxiv.org/abs/2509.16444)
*Chenhan Lyu,Yutong Song,Pengfei Zhang,Amir M. Rahmani*

Main category: cs.AI

TL;DR: 本论文提出一种利用符合特定领域原则的AI训练方法，以构建安全可靠的心理健康计算应用。


<details>
  <summary>Details</summary>
Motivation: 全球心理疾病发病率上升，AI在心理护理中的整合以及对服务不足社区的可扩展解决方案的需求推动了心理健康应用的发展。

Method: 采用基于特定领域原则的AI训练方法。

Result: 构建安全可靠的心理健康计算应用。

Conclusion: 该方法能有效解决现有AI安全措施在心理健康应用中存在的不足。

Abstract: Mental health applications have emerged as a critical area in computational
health, driven by rising global rates of mental illness, the integration of AI
in psychological care, and the need for scalable solutions in underserved
communities. These include therapy chatbots, crisis detection, and wellness
platforms handling sensitive data, requiring specialized AI safety beyond
general safeguards due to emotional vulnerability, risks like misdiagnosis or
symptom exacerbation, and precise management of vulnerable states to avoid
severe outcomes such as self-harm or loss of trust. Despite AI safety advances,
general safeguards inadequately address mental health-specific challenges,
including crisis intervention accuracy to avert escalations, therapeutic
guideline adherence to prevent misinformation, scale limitations in
resource-constrained settings, and adaptation to nuanced dialogues where
generics may introduce biases or miss distress signals. We introduce an
approach to apply Constitutional AI training with domain-specific mental health
principles for safe, domain-adapted CAI systems in computational mental health
applications.

</details>


### [11] [GPO: Learning from Critical Steps to Improve LLM Reasoning](https://arxiv.org/abs/2509.16456)
*Jiahao Yu,Zelei Cheng,Xian Wu,Xinyu Xing*

Main category: cs.AI

TL;DR: 本文提出了一种新的微调策略GPO，通过识别推理过程中关键步骤并重点学习，提高大型语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在优化大型语言模型推理能力时，往往忽略了推理过程中关键步骤的重要性。

Method: GPO首先识别推理轨迹中的关键步骤（通过估计优势函数），然后重置策略到关键步骤，采样新的轨迹并优先学习这些轨迹。

Result: 实验结果表明，GPO能够持续显著地提高现有优化方法的性能，提升大型语言模型的推理能力。

Conclusion: GPO是一种通用的策略，可以与各种优化方法集成，有效提高大型语言模型的推理能力，尤其在处理多步骤推理问题时效果显著。

Abstract: Large language models (LLMs) are increasingly used in various domains,
showing impressive potential on different tasks. Recently, reasoning LLMs have
been proposed to improve the \textit{reasoning} or \textit{thinking}
capabilities of LLMs to solve complex problems. Despite the promising results
of reasoning LLMs, enhancing the multi-step reasoning capabilities of LLMs
still remains a significant challenge. While existing optimization methods have
advanced the LLM reasoning capabilities, they often treat reasoning
trajectories as a whole, without considering the underlying critical steps
within the trajectory. In this paper, we introduce \textbf{G}uided
\textbf{P}ivotal \textbf{O}ptimization (GPO), a novel fine-tuning strategy that
dives into the reasoning process to enable more effective improvements. GPO
first identifies the `critical step' within a reasoning trajectory - a point
that the model must carefully proceed to succeed at the problem. We locate the
critical step by estimating the advantage function. GPO then resets the policy
to the critical step, samples the new rollout and prioritizes the learning
process on those rollouts. This focus allows the model to learn more
effectively from pivotal moments within the reasoning process to improve the
reasoning performance. We demonstrate that GPO is a general strategy that can
be integrated with various optimization methods to improve reasoning
performance. Besides theoretical analysis, our experiments across challenging
reasoning benchmarks show that GPO can consistently and significantly enhance
the performance of existing optimization methods, showcasing its effectiveness
and generalizability in improving LLM reasoning by concentrating on pivotal
moments within the generation process.

</details>


### [12] [Checking extracted rules in Neural Networks](https://arxiv.org/abs/2509.16547)
*Adrian Wurm*

Main category: cs.AI

TL;DR: 本文从复杂性理论的角度研究了神经网络提取规则的形式化验证问题，证明了验证规则集一致性、完整性和适用性的问题大多是co-NP完全的。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解神经网络的内部工作机制，需要验证从网络中提取的规则的可靠性。

Method: 将验证问题规约到彼此，并证明其co-NP完全性。

Result: 大多数验证问题是co-NP完全的。

Conclusion: 本文的研究结果对于评估神经网络规则提取方法的可靠性具有重要意义。

Abstract: In this paper we investigate formal verification of extracted rules for
Neural Networks under a complexity theoretic point of view. A rule is a global
property or a pattern concerning a large portion of the input space of a
network. These rules are algorithmically extracted from networks in an effort
to better understand their inner way of working. Here, three problems will be
in the focus: Does a given set of rules apply to a given network? Is a given
set of rules consistent or do the rules contradict themselves? Is a given set
of rules exhaustive in the sense that for every input the output is determined?
Finding algorithms that extract such rules out of networks has been
investigated over the last 30 years, however, to the author's current
knowledge, no attempt in verification was made until now. A lot of attempts of
extracting rules use heuristics involving randomness and over-approximation, so
it might be beneficial to know whether knowledge obtained in that way can
actually be trusted.
  We investigate the above questions for neural networks with ReLU-activation
as well as for Boolean networks, each for several types of rules. We
demonstrate how these problems can be reduced to each other and show that most
of them are co-NP-complete.

</details>


### [13] [SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning](https://arxiv.org/abs/2509.16561)
*Yue Xin,Chen Shen,Shaotian Yan,Xiaosong Yuan,Yaoming Wang,Xiaofeng Zhang,Chenxi Huang,Jieping Ye*

Main category: cs.AI

TL;DR: SalaMAnder框架利用Shapley值量化CoT推理中各个成分的贡献，并提出CoSP指标评估模型性能，实验表明CoSP与模型性能具有强相关性，为CoT提示词优化提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 探索CoT提示增强LLM数学推理能力的机制。

Method: 提出SalaMAnder框架，利用Shapley值进行数学表达式属性分配，并开发高效分层采样算法和CoSP指标。

Result: CoSP指标与模型性能具有鲁棒的单调相关性，解释了CoT的有效性，并为提示词优化提供了理论指导。

Conclusion: SalaMAnder框架为理解和优化CoT推理提供了理论和实践工具。

Abstract: Chain-of-Thought (CoT) prompting enhances the math reasoning capability of
large language models (LLMs) to a large margin. However, the mechanism
underlying such improvements remains unexplored. In this paper, we present
\textbf{SalaMAnder} (\textbf{S}h\textbf{a}p\textbf{l}ey-b\textbf{a}sed
\textbf{M}athematical Expression \textbf{A}ttribution a\textbf{nd}
M\textbf{e}t\textbf{r}ic), a theoretically grounded methodology as well as a
mathematically rigorous evaluation metric for quantifying component-level
contributions in few-shot CoT reasoning. Concretely, we leverage the Shapley
value for mathematical expression attribution and develop an efficient
stratified sampling algorithm that significantly reduces the computational
complexity. Besides, we develop the \textbf{CoSP} (\textbf{C}ardinality
\textbf{o}f \textbf{S}hapley \textbf{P}ositives) metric through covariance
analysis. Comprehensive validation across popular LLM models and diverse
mathematical benchmarks demonstrates that the CoSP metric within our SalaMAnder
framework exhibits a robust monotonic correlation with model performance, not
only providing theoretical explanations for the empirical success of existing
few-shot CoT but also establishing mathematically rigorous principles for
prompt construction optimization. Furthermore, we verify the reliability of the
explanation, based on which we unify the insights of previous work.

</details>


### [14] [Zero-Shot Human Mobility Forecasting via Large Language Model with Hierarchical Reasoning](https://arxiv.org/abs/2509.16578)
*Wenyao Li,Ran Zhang,Pengyang Wang,Yuanchun Zhou,Pengfei Wang*

Main category: cs.AI

TL;DR: ZHMF框架通过结合语义增强检索和反射机制与基于分层语言模型的推理系统，实现了零样本人类出行预测，并在标准数据集上取得了优于现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以泛化到未见用户或位置，且难以捕捉动态意图。

Method: 将预测任务重新定义为自然语言问答范式，利用LLM理解用户历史和上下文，结合分层反射机制进行迭代推理和细化，包含活动层规划器和位置层选择器。

Result: 在标准人类出行数据集上，ZHMF优于现有模型。消融实验验证了各模块的贡献，案例研究展示了其捕捉用户意图和适应不同上下文场景的能力。

Conclusion: ZHMF框架为零样本人类出行预测提供了一种有效的方法，具有良好的泛化性和对动态意图的捕捉能力。

Abstract: Human mobility forecasting is important for applications such as
transportation planning, urban management, and personalized recommendations.
However, existing methods often fail to generalize to unseen users or locations
and struggle to capture dynamic intent due to limited labeled data and the
complexity of mobility patterns. We propose ZHMF, a framework for zero-shot
human mobility forecasting that combines a semantic enhanced retrieval and
reflection mechanism with a hierarchical language model based reasoning system.
The task is reformulated as a natural language question answering paradigm.
Leveraging LLMs semantic understanding of user histories and context, our
approach handles previously unseen prediction scenarios. We further introduce a
hierarchical reflection mechanism for iterative reasoning and refinement by
decomposing forecasting into an activity level planner and a location level
selector, enabling collaborative modeling of long term user intentions and
short term contextual preferences. Experiments on standard human mobility
datasets show that our approach outperforms existing models. Ablation studies
reveal the contribution of each module, and case studies illustrate how the
method captures user intentions and adapts to diverse contextual scenarios.

</details>


### [15] [Question Answering with LLMs and Learning from Answer Sets](https://arxiv.org/abs/2509.16590)
*Manuel Borroto,Katie Gallagher,Antonio Ielo,Irfan Kareem,Francesco Ricca,Alessandra Russo*

Main category: cs.AI

TL;DR: LLM2LAS结合LLM、ILASP和ASP，自动学习构建符号推理系统，用于故事问答任务。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在常识推理方面存在不足，人工构建符号推理系统费力，本文提出自动学习的方法。

Method: 利用LLM提取文本语义结构，ILASP转换为逻辑规则，ASP求解器进行推理。

Result: 实证结果表明该方法的优缺点。

Conclusion: LLM2LAS有效结合了LLM的自然语言理解能力和符号推理系统的精确推理能力，为故事问答任务提供了一种新的自动学习方法。

Abstract: Large Language Models (LLMs) excel at understanding natural language but
struggle with explicit commonsense reasoning. A recent trend of research
suggests that the combination of LLM with robust symbolic reasoning systems can
overcome this problem on story-based question answering tasks. In this setting,
existing approaches typically depend on human expertise to manually craft the
symbolic component. We argue, however, that this component can also be
automatically learned from examples. In this work, we introduce LLM2LAS, a
hybrid system that effectively combines the natural language understanding
capabilities of LLMs, the rule induction power of the Learning from Answer Sets
(LAS) system ILASP, and the formal reasoning strengths of Answer Set
Programming (ASP). LLMs are used to extract semantic structures from text,
which ILASP then transforms into interpretable logic rules. These rules allow
an ASP solver to perform precise and consistent reasoning, enabling correct
answers to previously unseen questions. Empirical results outline the strengths
and weaknesses of our automatic approach for learning and reasoning in a
story-based question answering benchmark.

</details>


### [16] [FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs](https://arxiv.org/abs/2509.16648)
*Debarpan Bhattacharya,Apoorva Kulkarni,Sriram Ganapathy*

Main category: cs.AI

TL;DR: 提出了一种用于多模态大型语言模型 (MLLM) 可信度评估的多模态输入采样技术 FESTA，该技术通过等效和补充输入采样生成不确定性度量，提高了选择性预测性能。


<details>
  <summary>Details</summary>
Motivation: 评估多模态大型语言模型 (MLLM) 生成的预测结果的可信度，这对于选择性预测和提高用户信心具有挑战性。

Method: 提出了一种基于功能等效采样的多模态输入采样技术 FESTA，该技术通过等效和补充样本探测模型的一致性和敏感性，从而进行不确定性量化。

Result: FESTA 在视觉和音频推理任务上均取得显著改进，视觉-LLM 的选择性预测性能相对提高了 33.3%，音频-LLM 的选择性预测性能相对提高了 29.6%。

Conclusion: FESTA 是一种有效的、无需地面真实值的黑盒方法，可用于提高多模态大型语言模型预测的可信度。

Abstract: The accurate trust assessment of multimodal large language models (MLLMs)
generated predictions, which can enable selective prediction and improve user
confidence, is challenging due to the diverse multi-modal input paradigms. We
propose Functionally Equivalent Sampling for Trust Assessment (FESTA), a
multimodal input sampling technique for MLLMs, that generates an uncertainty
measure based on the equivalent and complementary input samplings. The proposed
task-preserving sampling approach for uncertainty quantification expands the
input space to probe the consistency (through equivalent samples) and
sensitivity (through complementary samples) of the model. FESTA uses only
input-output access of the model (black-box), and does not require ground truth
(unsupervised). The experiments are conducted with various off-the-shelf
multi-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA
uncertainty estimate achieves significant improvement (33.3% relative
improvement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in
selective prediction performance, based on
area-under-receiver-operating-characteristic curve (AUROC) metric in detecting
mispredictions. The code implementation is open-sourced.

</details>


### [17] [NUMINA: A Natural Understanding Benchmark for Multi-dimensional Intelligence and Numerical Reasoning Abilities](https://arxiv.org/abs/2509.16656)
*Changyu Zeng,Yifan Wang,Zimu Wang,Wei Wang,Zhengni Yang,Muyi Bao,Jiming Xiao,Ahn Nguyen,Yutao Yue*

Main category: cs.AI

TL;DR: NUMINA基准测试用于评估多模态大型语言模型在三维环境中的数值推理能力，结果表明现有模型在精确计算（如距离和体积估计）方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有3D基准缺乏细粒度的数值推理任务标注，限制了多模态大型语言模型的能力。

Method: 提出NUMINA基准测试，包含多尺度标注和多种问答对，并使用NUMINA-Flow自动标注流程生成。

Result: 评估结果表明，现有LLM难以胜任多模态数值推理，尤其是在精确计算方面。

Conclusion: 需要进一步改进3D模型以提升其在三维环境中进行数值推理的能力。

Abstract: Recent advancements in 2D multimodal large language models (MLLMs) have
significantly improved performance in vision-language tasks. However, extending
these capabilities to 3D environments remains a distinct challenge due to the
complexity of spatial reasoning. Nevertheless, existing 3D benchmarks often
lack fine-grained numerical reasoning task annotations, limiting MLLMs' ability
to perform precise spatial measurements and complex numerical reasoning. To
address this gap, we introduce NUMINA, the first Natural Understanding
benchmark for Multi-dimensional Intelligence and Numerical reasoning Abilities
to enhance multimodal indoor perceptual understanding. NUMINA features
multi-scale annotations and various question-answer pairs, generated using
NUMINA-Flow, an automated annotation pipeline that integrates LLM rewriting and
rule-based self-verification. We evaluate the performance of various
state-of-the-art LLMs on NUMINA following the Chat-Scene framework,
demonstrating that current LLMs struggle with multimodal numerical reasoning,
particularly in performing precise computations such as distance and volume
estimation, highlighting the need for further advancements in 3D models. The
dataset and source codes can be obtained from
https://github.com/fengshun124/NUMINA.

</details>


### [18] [Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories](https://arxiv.org/abs/2509.16742)
*Mohammad Beigi,Ying Shen,Parshin Shojaee,Qifan Wang,Zichao Wang,Chandan Reddy,Ming Jin,Lifu Huang*

Main category: cs.AI

TL;DR: 该论文提出SMART框架，通过调整模型探索策略以减少大型语言模型中迎合用户错误信息的倾向。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型容易迎合用户，即使信息错误。

Method: 提出两阶段框架SMART：不确定性感知自适应蒙特卡洛树搜索(UA-MCTS)和基于进展的强化学习。

Result: 实验表明SMART显著减少了迎合行为，同时保持了模型的性能。

Conclusion: 优化内部推理机制对于构建更真实可靠的AI助手至关重要。

Abstract: Despite the remarkable capabilities of large language models, current
training paradigms inadvertently foster \textit{sycophancy}, i.e., the tendency
of a model to agree with or reinforce user-provided information even when it's
factually incorrect. To address this challenge, we introduce \textbf{SMART}
(Sycophancy Mitigation through Adaptive Reasoning Trajectories), which reframes
sycophancy as a \textit{reasoning optimization problem} rather than an output
alignment issue. SMART is a two-stage framework comprising: (1)
Uncertainty-Aware Adaptive Monte Carlo Tree Search (UA-MCTS), which dynamically
adjusts model exploration based on state-level uncertainty to collect
high-quality, diverse reasoning trajectories alongside both stepwise progress
and final outcome rewards; and (2) progress-based reinforcement learning, which
fine-tunes the model using the collected trajectories and reward signals to
reinforce effective reasoning patterns. Through extensive experiments, we show
that SMART significantly reduces sycophantic behavior while preserving strong
performance on out-of-distribution inputs and maintaining general capabilities.
These results underscore the importance of optimizing internal reasoning
mechanisms to build more truthful and aligned AI assistants.

</details>
