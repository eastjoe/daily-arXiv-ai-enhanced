{"id": "2509.16288", "categories": ["cs.AI", "05C22, 05C90, 68R10 05C22, 05C90, 68R10 05C22, 05C90, 68R10"], "pdf": "https://arxiv.org/pdf/2509.16288", "abs": "https://arxiv.org/abs/2509.16288", "authors": ["Shanookha Ali", "Nitha Niralda P C"], "title": "Identifying Critical Pathways in Coronary Heart Disease via Fuzzy Subgraph Connectivity", "comment": null, "summary": "Coronary heart disease (CHD) arises from complex interactions among\nuncontrollable factors, controllable lifestyle factors, and clinical\nindicators, where relationships are often uncertain. Fuzzy subgraph\nconnectivity (FSC) provides a systematic tool to capture such imprecision by\nquantifying the strength of association between vertices and subgraphs in fuzzy\ngraphs. In this work, a fuzzy CHD graph is constructed with vertices for\nuncontrollable, controllable, and indicator components, and edges weighted by\nfuzzy memberships. Using FSC, we evaluate connectivity to identify strongest\ndiagnostic routes, dominant risk factors, and critical bridges. Results show\nthat FSC highlights influential pathways, bounds connectivity between weakest\nand strongest correlations, and reveals critical edges whose removal reduces\npredictive strength. Thus, FSC offers an interpretable and robust framework for\nmodeling uncertainty in CHD risk prediction and supporting clinical\ndecision-making.", "AI": {"tldr": "\u6a21\u7cca\u5b50\u56fe\u8fde\u63a5\u6027(FSC)\u7528\u4e8e\u5206\u6790\u51a0\u5fc3\u75c5(CHD)\u98ce\u9669\u56e0\u7d20\uff0c\u8bc6\u522b\u5173\u952e\u8bca\u65ad\u8def\u5f84\u548c\u98ce\u9669\u56e0\u7d20\u3002", "motivation": "\u51a0\u5fc3\u75c5\u98ce\u9669\u56e0\u7d20\u590d\u6742\uff0c\u5173\u7cfb\u4e0d\u786e\u5b9a\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\u91cf\u5316\u5173\u8054\u5f3a\u5ea6\u3002", "method": "\u6784\u5efa\u6a21\u7ccaCHD\u56fe\uff0c\u4f7f\u7528FSC\u8bc4\u4f30\u8fde\u63a5\u6027\uff0c\u8bc6\u522b\u6700\u5f3a\u8bca\u65ad\u8def\u5f84\u3001\u4e3b\u8981\u98ce\u9669\u56e0\u7d20\u548c\u5173\u952e\u6865\u6881\u3002", "result": "FSC\u7a81\u51fa\u5f71\u54cd\u6027\u901a\u8def\uff0c\u754c\u5b9a\u6700\u5f31\u548c\u6700\u5f3a\u76f8\u5173\u6027\u4e4b\u95f4\u7684\u8fde\u63a5\u6027\uff0c\u63ed\u793a\u53bb\u9664\u5173\u952e\u8fb9\u4f1a\u964d\u4f4e\u9884\u6d4b\u5f3a\u5ea6\u3002", "conclusion": "FSC\u63d0\u4f9b\u4e00\u4e2a\u53ef\u89e3\u91ca\u4e14\u7a33\u5065\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62dfCHD\u98ce\u9669\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2509.16298", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16298", "abs": "https://arxiv.org/abs/2509.16298", "authors": ["Raquel Fernandez-Peralta", "Juan Vicente Riera"], "title": "A global view of diverse construction methods of fuzzy implication functions rooted on F-chains", "comment": null, "summary": "Fuzzy implication functions are one of the most important operators used in\nthe fuzzy logic framework. While their flexible definition allows for diverse\nfamilies with distinct properties, this variety needs a deeper theoretical\nunderstanding of their structural relationships. In this work, we focus on the\nstudy of construction methods, which employ different techniques to generate\nnew fuzzy implication functions from existing ones. Particularly, we generalize\nthe $F$-chain-based construction, recently introduced by Mesiar et al. to\nextend a method for constructing aggregation functions to the context of fuzzy\nimplication functions. Our generalization employs collections of fuzzy\nimplication functions rather than single ones, and uses two different\nincreasing functions instead of a unique $F$-chain. We analyze property\npreservation under this construction and establish sufficient conditions.\nFurthermore, we demonstrate that our generalized $F$-chain-based construction\nis a unifying framework for several existing methods. In particular, we show\nthat various construction techniques, such as contraposition, aggregation, and\ngeneralized vertical/horizontal threshold methods, can be reformulated within\nour approach. This reveals structural similarities between seemingly distinct\nconstruction strategies and provides a cohesive perspective on fuzzy\nimplication construction methods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6a21\u7cca\u8574\u6db5\u51fd\u6570\u7684\u6784\u9020\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49F\u94fe\u7684\u6784\u9020\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u5176\u5305\u542b\u4e86\u591a\u79cd\u5df2\u6709\u7684\u6784\u9020\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6a21\u7cca\u8574\u6db5\u51fd\u6570\u6784\u9020\u65b9\u6cd5\u591a\u6837\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u5e76\u5206\u6790\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49F\u94fe\u7684\u6a21\u7cca\u8574\u6db5\u51fd\u6570\u6784\u9020\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u591a\u4e2a\u6a21\u7cca\u8574\u6db5\u51fd\u6570\u548c\u4e24\u4e2a\u9012\u589e\u51fd\u6570\u3002", "result": "\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u80fd\u591f\u4fdd\u6301\u591a\u79cd\u6027\u8d28\uff0c\u5e76\u5c06\u5176\u4e0e\u591a\u79cd\u73b0\u6709\u65b9\u6cd5\u8054\u7cfb\u8d77\u6765\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6784\u9020\u65b9\u6cd5\u7684\u7ed3\u6784\u76f8\u4f3c\u6027\u3002", "conclusion": "\u8be5\u5e7f\u4e49F\u94fe\u6784\u9020\u65b9\u6cd5\u4e3a\u6a21\u7cca\u8574\u6db5\u51fd\u6570\u7684\u6784\u9020\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u52a0\u6df1\u4e86\u5bf9\u6a21\u7cca\u8574\u6db5\u51fd\u6570\u7ed3\u6784\u5173\u7cfb\u7684\u7406\u89e3\u3002"}}
{"id": "2509.16299", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16299", "abs": "https://arxiv.org/abs/2509.16299", "authors": ["Raquel Fernandez-Peralta", "Andrea Mesiarov\u00e1-Zem\u00e1nkov\u00e1"], "title": "On the Non-Uniqueness of Representation of $(U,N)$-Implications", "comment": null, "summary": "Fuzzy implication functions constitute fundamental operators in fuzzy logic\nsystems, extending classical conditionals to manage uncertainty in logical\ninference. Among the extensive families of these operators, generalizations of\nthe classical material implication have received considerable theoretical\nattention, particularly $(S,N)$-implications constructed from t-conorms and\nfuzzy negations, and their further generalizations to $(U,N)$-implications\nusing disjunctive uninorms. Prior work has established characterization\ntheorems for these families under the assumption that the fuzzy negation $N$ is\ncontinuous, ensuring uniqueness of representation. In this paper, we disprove\nthis last fact for $(U,N)$-implications and we show that they do not\nnecessarily possess a unique representation, even if the fuzzy negation is\ncontinuous. Further, we provide a comprehensive study of uniqueness conditions\nfor both uninorms with continuous and non-continuous underlying functions. Our\nresults offer important theoretical insights into the structural properties of\nthese operators.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6a21\u7cca\u8574\u6db5\u51fd\u6570\u7684\u552f\u4e00\u6027\u8868\u793a\u95ee\u9898\uff0c\u5c24\u5176\u5173\u6ce8\u57fa\u4e8e(U,N)-\u8574\u6db5\u7684\u8868\u793a\uff0c\u53d1\u73b0\u5373\u4f7f\u5728\u6a21\u7cca\u5426\u5b9aN\u8fde\u7eed\u7684\u60c5\u51b5\u4e0b\uff0c(U,N)-\u8574\u6db5\u4e5f\u4e0d\u4e00\u5b9a\u5177\u6709\u552f\u4e00\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u5047\u8bbe\u6a21\u7cca\u5426\u5b9aN\u8fde\u7eed\uff0c\u4ece\u800c\u4fdd\u8bc1(U,N)-\u8574\u6db5\u7684\u552f\u4e00\u8868\u793a\u3002\u672c\u6587\u65e8\u5728\u6311\u6218\u8fd9\u4e00\u5047\u8bbe\u3002", "method": "\u672c\u6587\u901a\u8fc7\u53cd\u4f8b\u63a8\u7ffb\u4e86(U,N)-\u8574\u6db5\u7684\u552f\u4e00\u8868\u793a\u6027\uff0c\u5e76\u5bf9\u5177\u6709\u8fde\u7eed\u548c\u975e\u8fde\u7eed\u5e95\u5c42\u51fd\u6570\u7684\u5e7a\u534a\u7fa4\u7684\u552f\u4e00\u6027\u6761\u4ef6\u8fdb\u884c\u4e86\u5168\u9762\u7814\u7a76\u3002", "result": "\u672c\u6587\u8bc1\u660e\u4e86\u5373\u4f7f\u6a21\u7cca\u5426\u5b9a\u8fde\u7eed\uff0c(U,N)-\u8574\u6db5\u4e5f\u4e0d\u4e00\u5b9a\u5177\u6709\u552f\u4e00\u8868\u793a\uff0c\u5e76\u6df1\u5165\u7814\u7a76\u4e86\u5e7a\u534a\u7fa4\u7684\u552f\u4e00\u6027\u6761\u4ef6\u3002", "conclusion": "\u672c\u6587\u7684\u7ed3\u679c\u4e3a\u8fd9\u4e9b\u7b97\u5b50\u7684\u7ed3\u6784\u7279\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u7406\u8bba\u89c1\u89e3\u3002"}}
{"id": "2509.16330", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16330", "abs": "https://arxiv.org/abs/2509.16330", "authors": ["Minxing Zhang", "Yi Yang", "Roy Xie", "Bhuwan Dhingra", "Shuyan Zhou", "Jian Pei"], "title": "Generalizability of Large Language Model-Based Agents: A Comprehensive Survey", "comment": null, "summary": "Large Language Model (LLM)-based agents have emerged as a new paradigm that\nextends LLMs' capabilities beyond text generation to dynamic interaction with\nexternal environments. By integrating reasoning with perception, memory, and\ntool use, agents are increasingly deployed in diverse domains like web\nnavigation and household robotics. A critical challenge, however, lies in\nensuring agent generalizability - the ability to maintain consistent\nperformance across varied instructions, tasks, environments, and domains,\nespecially those beyond agents' fine-tuning data. Despite growing interest, the\nconcept of generalizability in LLM-based agents remains underdefined, and\nsystematic approaches to measure and improve it are lacking. In this survey, we\nprovide the first comprehensive review of generalizability in LLM-based agents.\nWe begin by emphasizing agent generalizability's importance by appealing to\nstakeholders and clarifying the boundaries of agent generalizability by\nsituating it within a hierarchical domain-task ontology. We then review\ndatasets, evaluation dimensions, and metrics, highlighting their limitations.\nNext, we categorize methods for improving generalizability into three groups:\nmethods for the backbone LLM, for agent components, and for their interactions.\nMoreover, we introduce the distinction between generalizable frameworks and\ngeneralizable agents and outline how generalizable frameworks can be translated\ninto agent-level generalizability. Finally, we identify critical challenges and\nfuture directions, including developing standardized frameworks, variance- and\ncost-based metrics, and approaches that integrate methodological innovations\nwith architecture-level designs. By synthesizing progress and highlighting\nopportunities, this survey aims to establish a foundation for principled\nresearch on building LLM-based agents that generalize reliably across diverse\napplications.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4ee3\u7406\u7684\u6cdb\u5316\u6027\u95ee\u9898\uff0c\u5206\u6790\u4e86\u8bc4\u4f30\u65b9\u6cd5\u548c\u6539\u8fdb\u7b56\u7565\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "LLM\u4ee3\u7406\u7684\u6cdb\u5316\u6027\u5bf9\u4e8e\u5176\u5728\u4e0d\u540c\u73af\u5883\u548c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u5bf9\u5176\u5b9a\u4e49\u548c\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u5bf9\u73b0\u6709\u6587\u732e\u8fdb\u884c\u7efc\u8ff0\uff0c\u5bf9LLM\u4ee3\u7406\u7684\u6cdb\u5316\u6027\u8fdb\u884c\u5206\u7c7b\u548c\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "\u672c\u6587\u5bf9LLM\u4ee3\u7406\u6cdb\u5316\u6027\u8bc4\u4f30\u65b9\u6cd5\u3001\u6539\u8fdb\u7b56\u7565\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684LLM\u4ee3\u7406\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "LLM\u4ee3\u7406\u7684\u6cdb\u5316\u6027\u662f\u672a\u6765\u7814\u7a76\u7684\u91cd\u70b9\uff0c\u9700\u8981\u53d1\u5c55\u6807\u51c6\u5316\u6846\u67b6\u3001\u6539\u8fdb\u8bc4\u4f30\u6307\u6807\u548c\u7ed3\u5408\u65b9\u6cd5\u521b\u65b0\u4e0e\u67b6\u6784\u8bbe\u8ba1\u3002"}}
{"id": "2509.16332", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16332", "abs": "https://arxiv.org/abs/2509.16332", "authors": ["Stephen Fitz", "Peter Romero", "Steven Basart", "Sipeng Chen", "Jose Hernandez-Orallo"], "title": "Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models", "comment": null, "summary": "Large Language Models increasingly mediate high-stakes interactions,\nintensifying research on their capabilities and safety. While recent work has\nshown that LLMs exhibit consistent and measurable synthetic personality traits,\nlittle is known about how modulating these traits affects model behavior. We\naddress this gap by investigating how psychometric personality control grounded\nin the Big Five framework influences AI behavior in the context of capability\nand safety benchmarks. Our experiments reveal striking effects: for example,\nreducing conscientiousness leads to significant drops in safety-relevant\nmetrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy as well\nas reduction in general capabilities as measured by MMLU. These findings\nhighlight personality shaping as a powerful and underexplored axis of model\ncontrol that interacts with both safety and general competence. We discuss the\nimplications for safety evaluation, alignment strategies, steering model\nbehavior after deployment, and risks associated with possible exploitation of\nthese findings. Our findings motivate a new line of research on\npersonality-sensitive safety evaluations and dynamic behavioral control in\nLLMs.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u683c\u7279\u5f81\u4f1a\u663e\u8457\u5f71\u54cd\u5176\u5b89\u5168\u6027\u548c\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u4eba\u683c\u7279\u8d28\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "method": "\u57fa\u4e8e\u4e94\u5927\u6027\u683c\u7279\u8d28\u6846\u67b6\uff0c\u7814\u7a76\u4eba\u683c\u63a7\u5236\u5bf9\u6a21\u578b\u884c\u4e3a\uff08\u5b89\u5168\u6027\u548c\u80fd\u529b\u57fa\u51c6\u6d4b\u8bd5\uff09\u7684\u5f71\u54cd\u3002", "result": "\u964d\u4f4e\u6a21\u578b\u7684\u5c3d\u8d23\u6027\u4f1a\u5bfc\u81f4\u5b89\u5168\u6027\u548c\u80fd\u529b\u57fa\u51c6\u6d4b\u8bd5\u5206\u6570\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u4eba\u683c\u5851\u9020\u662f\u6a21\u578b\u63a7\u5236\u7684\u91cd\u8981\u624b\u6bb5\uff0c\u5bf9\u5b89\u5168\u6027\u548c\u80fd\u529b\u90fd\u6709\u5f71\u54cd\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4eba\u683c\u654f\u611f\u7684\u5b89\u5168\u8bc4\u4f30\u548c\u52a8\u6001\u884c\u4e3a\u63a7\u5236\u3002"}}
{"id": "2509.16348", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16348", "abs": "https://arxiv.org/abs/2509.16348", "authors": ["Minxiao Wang", "Saurabh Kataria", "Juntong Ni", "Timothy G. Buchman", "Jocelyn Grunwell", "Mark Mai", "Wei Jin", "Matthew Clark", "Stephanie Brown", "Michael Fundora", "Puneet Sharma", "Tony Pan", "Sam Khan", "Timothy Ruchti", "Naveen Muthu", "Kevin Maher", "Sivasubramanium V Bhavani", "Xiao Hu"], "title": "A Unified AI Approach for Continuous Monitoring of Human Health and Diseases from Intensive Care Unit to Home with Physiological Foundation Models (UNIPHY+)", "comment": null, "summary": "We present UNIPHY+, a unified physiological foundation model (physioFM)\nframework designed to enable continuous human health and diseases monitoring\nacross care settings using ubiquitously obtainable physiological data. We\npropose novel strategies for incorporating contextual information during\npretraining, fine-tuning, and lightweight model personalization via multi-modal\nlearning, feature fusion-tuning, and knowledge distillation. We advocate\ntesting UNIPHY+ with a broad set of use cases from intensive care to ambulatory\nmonitoring in order to demonstrate that UNIPHY+ can empower generalizable,\nscalable, and personalized physiological AI to support both clinical\ndecision-making and long-term health monitoring.", "AI": {"tldr": "UNIPHY+\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u751f\u7406\u57fa\u7840\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u62a4\u7406\u73af\u5883\u8fde\u7eed\u76d1\u6d4b\u4eba\u7c7b\u5065\u5eb7\u548c\u75be\u75c5", "motivation": "\u5229\u7528\u666e\u904d\u53ef\u83b7\u5f97\u7684\u751f\u7406\u6570\u636e\uff0c\u5b9e\u73b0\u5bf9\u4eba\u7c7b\u5065\u5eb7\u548c\u75be\u75c5\u7684\u6301\u7eed\u76d1\u6d4b", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5b66\u4e60\u3001\u7279\u5f81\u878d\u5408\u5fae\u8c03\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u5728\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u548c\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e2a\u6027\u5316\u8fc7\u7a0b\u4e2d\u6574\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f", "result": "UNIPHY+\u80fd\u591f\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u548c\u957f\u671f\u5065\u5eb7\u76d1\u6d4b", "conclusion": "UNIPHY+\u80fd\u591f\u5b9e\u73b0\u901a\u7528\u3001\u53ef\u6269\u5c55\u548c\u4e2a\u6027\u5316\u7684\u751f\u7406AI\uff0c\u7528\u4e8e\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u548c\u957f\u671f\u5065\u5eb7\u76d1\u6d4b"}}
{"id": "2509.16372", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16372", "abs": "https://arxiv.org/abs/2509.16372", "authors": ["Balu Bhasuran", "Mattia Prosperi", "Karim Hanna", "John Petrilli", "Caretia JeLayne Washington", "Zhe He"], "title": "Evaluation of Causal Reasoning for Large Language Models in Contextualized Clinical Scenarios of Laboratory Test Interpretation", "comment": null, "summary": "This study evaluates causal reasoning in large language models (LLMs) using\n99 clinically grounded laboratory test scenarios aligned with Pearl's Ladder of\nCausation: association, intervention, and counterfactual reasoning. We examined\ncommon laboratory tests such as hemoglobin A1c, creatinine, and vitamin D, and\npaired them with relevant causal factors including age, gender, obesity, and\nsmoking. Two LLMs - GPT-o1 and Llama-3.2-8b-instruct - were tested, with\nresponses evaluated by four medically trained human experts. GPT-o1\ndemonstrated stronger discriminative performance (AUROC overall = 0.80 +/-\n0.12) compared to Llama-3.2-8b-instruct (0.73 +/- 0.15), with higher scores\nacross association (0.75 vs 0.72), intervention (0.84 vs 0.70), and\ncounterfactual reasoning (0.84 vs 0.69). Sensitivity (0.90 vs 0.84) and\nspecificity (0.93 vs 0.80) were also greater for GPT-o1, with reasoning ratings\nshowing similar trends. Both models performed best on intervention questions\nand worst on counterfactuals, particularly in altered outcome scenarios. These\nfindings suggest GPT-o1 provides more consistent causal reasoning, but\nrefinement is required before adoption in high-stakes clinical applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e24\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b(GPT-o1\u548cLlama-3.2-8b-instruct)\u5728\u533b\u5b66\u56e0\u679c\u63a8\u7406\u4e0a\u7684\u80fd\u529b\uff0cGPT-o1\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u573a\u666f\u4e0b\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f7f\u752899\u4e2a\u4e34\u5e8a\u5b9e\u9a8c\u5ba4\u68c0\u9a8c\u573a\u666f\uff0c\u6db5\u76d6\u5173\u8054\u3001\u5e72\u9884\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u4e09\u4e2a\u5c42\u9762\uff0c\u7531\u533b\u5b66\u4e13\u5bb6\u8bc4\u4f30\u6a21\u578b\u7684\u56de\u7b54\u3002", "result": "GPT-o1\u7684\u6574\u4f53\u533a\u5206\u6027\u80fd(AUROC)\u9ad8\u4e8eLlama-3.2-8b-instruct\uff0c\u5728\u5173\u8054\u3001\u5e72\u9884\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u65b9\u9762\u5747\u8868\u73b0\u66f4\u597d\u3002\u4f46\u4e24\u4e2a\u6a21\u578b\u5728\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u5c24\u5176\u5728\u7ed3\u679c\u6539\u53d8\u7684\u573a\u666f\u4e0b\u8868\u73b0\u6700\u5dee\u3002", "conclusion": "GPT-o1\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u66f4\u5f3a\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u624d\u80fd\u5e94\u7528\u4e8e\u9ad8\u98ce\u9669\u4e34\u5e8a\u573a\u666f\u3002"}}
{"id": "2509.16399", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16399", "abs": "https://arxiv.org/abs/2509.16399", "authors": ["Guojun Xiong", "Milind Tambe"], "title": "VORTEX: Aligning Task Utility and Human Preferences through LLM-Guided Reward Shaping", "comment": "28pages, 19figures", "summary": "In social impact optimization, AI decision systems often rely on solvers that\noptimize well-calibrated mathematical objectives. However, these solvers cannot\ndirectly accommodate evolving human preferences, typically expressed in natural\nlanguage rather than formal constraints. Recent approaches address this by\nusing large language models (LLMs) to generate new reward functions from\npreference descriptions. While flexible, they risk sacrificing the system's\ncore utility guarantees. In this paper, we propose \\texttt{VORTEX}, a\nlanguage-guided reward shaping framework that preserves established\noptimization goals while adaptively incorporating human feedback. By\nformalizing the problem as multi-objective optimization, we use LLMs to\niteratively generate shaping rewards based on verbal reinforcement and\ntext-gradient prompt updates. This allows stakeholders to steer decision\nbehavior via natural language without modifying solvers or specifying trade-off\nweights. We provide theoretical guarantees that \\texttt{VORTEX} converges to\nPareto-optimal trade-offs between utility and preference satisfaction.\nEmpirical results in real-world allocation tasks demonstrate that\n\\texttt{VORTEX} outperforms baselines in satisfying human-aligned coverage\ngoals while maintaining high task performance. This work introduces a practical\nand theoretically grounded paradigm for human-AI collaborative optimization\nguided by natural language.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVORTEX\u6846\u67b6\uff0c\u4f7f\u7528LLM\u7ed3\u5408\u591a\u76ee\u6807\u4f18\u5316\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u8fed\u4ee3\u751f\u6210\u5956\u52b1\u51fd\u6570\uff0c\u5728\u4fdd\u6301\u7cfb\u7edf\u6548\u7528\u4fdd\u8bc1\u7684\u540c\u65f6\uff0c\u9002\u5e94\u6027\u5730\u6574\u5408\u4eba\u7c7b\u504f\u597d\u3002", "motivation": "\u73b0\u6709AI\u51b3\u7b56\u7cfb\u7edf\u96be\u4ee5\u76f4\u63a5\u5904\u7406\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u7684\u4eba\u7c7b\u504f\u597d\u53d8\u5316\u3002", "method": "\u5c06\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528LLM\u8fed\u4ee3\u751f\u6210\u57fa\u4e8e\u8bed\u8a00\u589e\u5f3a\u548c\u6587\u672c\u68af\u5ea6\u63d0\u793a\u66f4\u65b0\u7684\u5851\u9020\u5956\u52b1\u3002", "result": "VORTEX\u5728\u5b9e\u9645\u5206\u914d\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6ee1\u8db3\u4eba\u7c7b\u4e00\u81f4\u7684\u8986\u76d6\u76ee\u6807\u7684\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u4efb\u52a1\u6027\u80fd\u3002\u7406\u8bba\u4e0a\u4fdd\u8bc1VORTEX\u6536\u655b\u5230\u6548\u7528\u548c\u504f\u597d\u6ee1\u610f\u5ea6\u4e4b\u95f4\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u6743\u8861\u3002", "conclusion": "VORTEX\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u7406\u8bba\u4e0a\u6709\u6839\u636e\u7684\u8303\u4f8b\uff0c\u7528\u4e8e\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u4eba\u5de5\u667a\u80fd\u534f\u540c\u4f18\u5316\u3002"}}
{"id": "2509.16431", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16431", "abs": "https://arxiv.org/abs/2509.16431", "authors": ["Mohammad Iqbal Rasul Seeam", "Victor S. Sheng"], "title": "Proactive Statistical Process Control Using AI: A Time Series Forecasting Approach for Semiconductor Manufacturing", "comment": "7 pages, 3 figures, no .bbl file needed because bibliography already\n  in main.tex file", "summary": "In the manufacturing industry, it is very important to keep machines and\nprocesses running smoothly and without unexpected problems. One of the most\ncommon tools used to check if everything is working properly is called\nStatistical Process Control (SPC). Traditional SPC methods work by checking\nwhether recent measurements are within acceptable limits. However, they only\nreact after a problem has already occurred. This can lead to wasted materials,\nmachine downtime, and increased costs. In this paper, we present a smarter way\nto use SPC. Instead of just reacting to issues after they happen, our system\ncan predict future problems before they occur. We use a machine learning tool\ncalled Facebook Prophet, which is designed to work with time-series data (data\nthat changes over time). Prophet looks at past data and forecasts what the next\nvalue will be. Then, we use SPC rules to decide if the predicted value is in a\nSafe zone (no problem), a Warning zone (needs attention), or a Critical zone\n(may require shutting down the process). We applied this system to real data\nfrom a semiconductor manufacturing company. One of the challenges with this\ndata is that the measurements are not taken at regular time intervals. This\nmakes it harder to predict future values accurately. Despite this, our model\nwas able to make strong predictions and correctly classify the risk level of\nfuture measurements. The main benefit of our system is that it gives engineers\nand technicians a chance to act early - before something goes wrong. This helps\nreduce unexpected failures and improves the overall stability and reliability\nof the production process. By combining machine learning with traditional SPC,\nwe make quality control more proactive, accurate, and useful for modern\nindustry.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u4f20\u7edfSPC\u65b9\u6cd5\u7684\u9884\u6d4b\u6027\u8d28\u91cf\u63a7\u5236\u7cfb\u7edf\uff0c\u80fd\u591f\u63d0\u524d\u9884\u6d4b\u5236\u9020\u8fc7\u7a0b\u4e2d\u7684\u6f5c\u5728\u95ee\u9898\uff0c\u4ece\u800c\u51cf\u5c11\u610f\u5916\u6545\u969c\u5e76\u63d0\u9ad8\u751f\u4ea7\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684SPC\u65b9\u6cd5\u4ec5\u5728\u95ee\u9898\u53d1\u751f\u540e\u505a\u51fa\u53cd\u5e94\uff0c\u53ef\u80fd\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\u548c\u6210\u672c\u589e\u52a0\u3002", "method": "\u4f7f\u7528Facebook Prophet\u6a21\u578b\u5bf9\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u7ed3\u5408SPC\u89c4\u5219\u5bf9\u9884\u6d4b\u503c\u8fdb\u884c\u98ce\u9669\u7b49\u7ea7\u5206\u7c7b\u3002", "result": "\u8be5\u7cfb\u7edf\u6210\u529f\u5e94\u7528\u4e8e\u534a\u5bfc\u4f53\u5236\u9020\u516c\u53f8\u7684\u5b9e\u9645\u6570\u636e\uff0c\u5373\u4f7f\u6570\u636e\u91c7\u6837\u95f4\u9694\u4e0d\u89c4\u5f8b\uff0c\u4e5f\u80fd\u505a\u51fa\u51c6\u786e\u7684\u9884\u6d4b\u548c\u98ce\u9669\u7b49\u7ea7\u5206\u7c7b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u4f7f\u5de5\u7a0b\u5e08\u63d0\u524d\u91c7\u53d6\u884c\u52a8\uff0c\u4ece\u800c\u63d0\u9ad8\u751f\u4ea7\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u9760\u6027\uff0c\u4f7f\u8d28\u91cf\u63a7\u5236\u66f4\u4e3b\u52a8\u3001\u66f4\u51c6\u786e\u3001\u66f4\u6709\u7528\u3002"}}
{"id": "2509.16444", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16444", "abs": "https://arxiv.org/abs/2509.16444", "authors": ["Chenhan Lyu", "Yutong Song", "Pengfei Zhang", "Amir M. Rahmani"], "title": "Domain-Specific Constitutional AI: Enhancing Safety in LLM-Powered Mental Health Chatbots", "comment": null, "summary": "Mental health applications have emerged as a critical area in computational\nhealth, driven by rising global rates of mental illness, the integration of AI\nin psychological care, and the need for scalable solutions in underserved\ncommunities. These include therapy chatbots, crisis detection, and wellness\nplatforms handling sensitive data, requiring specialized AI safety beyond\ngeneral safeguards due to emotional vulnerability, risks like misdiagnosis or\nsymptom exacerbation, and precise management of vulnerable states to avoid\nsevere outcomes such as self-harm or loss of trust. Despite AI safety advances,\ngeneral safeguards inadequately address mental health-specific challenges,\nincluding crisis intervention accuracy to avert escalations, therapeutic\nguideline adherence to prevent misinformation, scale limitations in\nresource-constrained settings, and adaptation to nuanced dialogues where\ngenerics may introduce biases or miss distress signals. We introduce an\napproach to apply Constitutional AI training with domain-specific mental health\nprinciples for safe, domain-adapted CAI systems in computational mental health\napplications.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u7b26\u5408\u7279\u5b9a\u9886\u57df\u539f\u5219\u7684AI\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u6784\u5efa\u5b89\u5168\u53ef\u9760\u7684\u5fc3\u7406\u5065\u5eb7\u8ba1\u7b97\u5e94\u7528\u3002", "motivation": "\u5168\u7403\u5fc3\u7406\u75be\u75c5\u53d1\u75c5\u7387\u4e0a\u5347\uff0cAI\u5728\u5fc3\u7406\u62a4\u7406\u4e2d\u7684\u6574\u5408\u4ee5\u53ca\u5bf9\u670d\u52a1\u4e0d\u8db3\u793e\u533a\u7684\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u7684\u9700\u6c42\u63a8\u52a8\u4e86\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7279\u5b9a\u9886\u57df\u539f\u5219\u7684AI\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u6784\u5efa\u5b89\u5168\u53ef\u9760\u7684\u5fc3\u7406\u5065\u5eb7\u8ba1\u7b97\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u73b0\u6709AI\u5b89\u5168\u63aa\u65bd\u5728\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u4e2d\u5b58\u5728\u7684\u4e0d\u8db3\u3002"}}
{"id": "2509.16456", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16456", "abs": "https://arxiv.org/abs/2509.16456", "authors": ["Jiahao Yu", "Zelei Cheng", "Xian Wu", "Xinyu Xing"], "title": "GPO: Learning from Critical Steps to Improve LLM Reasoning", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "Large language models (LLMs) are increasingly used in various domains,\nshowing impressive potential on different tasks. Recently, reasoning LLMs have\nbeen proposed to improve the \\textit{reasoning} or \\textit{thinking}\ncapabilities of LLMs to solve complex problems. Despite the promising results\nof reasoning LLMs, enhancing the multi-step reasoning capabilities of LLMs\nstill remains a significant challenge. While existing optimization methods have\nadvanced the LLM reasoning capabilities, they often treat reasoning\ntrajectories as a whole, without considering the underlying critical steps\nwithin the trajectory. In this paper, we introduce \\textbf{G}uided\n\\textbf{P}ivotal \\textbf{O}ptimization (GPO), a novel fine-tuning strategy that\ndives into the reasoning process to enable more effective improvements. GPO\nfirst identifies the `critical step' within a reasoning trajectory - a point\nthat the model must carefully proceed to succeed at the problem. We locate the\ncritical step by estimating the advantage function. GPO then resets the policy\nto the critical step, samples the new rollout and prioritizes the learning\nprocess on those rollouts. This focus allows the model to learn more\neffectively from pivotal moments within the reasoning process to improve the\nreasoning performance. We demonstrate that GPO is a general strategy that can\nbe integrated with various optimization methods to improve reasoning\nperformance. Besides theoretical analysis, our experiments across challenging\nreasoning benchmarks show that GPO can consistently and significantly enhance\nthe performance of existing optimization methods, showcasing its effectiveness\nand generalizability in improving LLM reasoning by concentrating on pivotal\nmoments within the generation process.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5fae\u8c03\u7b56\u7565GPO\uff0c\u901a\u8fc7\u8bc6\u522b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5173\u952e\u6b65\u9aa4\u5e76\u91cd\u70b9\u5b66\u4e60\uff0c\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65f6\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5173\u952e\u6b65\u9aa4\u7684\u91cd\u8981\u6027\u3002", "method": "GPO\u9996\u5148\u8bc6\u522b\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u5173\u952e\u6b65\u9aa4\uff08\u901a\u8fc7\u4f30\u8ba1\u4f18\u52bf\u51fd\u6570\uff09\uff0c\u7136\u540e\u91cd\u7f6e\u7b56\u7565\u5230\u5173\u952e\u6b65\u9aa4\uff0c\u91c7\u6837\u65b0\u7684\u8f68\u8ff9\u5e76\u4f18\u5148\u5b66\u4e60\u8fd9\u4e9b\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGPO\u80fd\u591f\u6301\u7eed\u663e\u8457\u5730\u63d0\u9ad8\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "GPO\u662f\u4e00\u79cd\u901a\u7528\u7684\u7b56\u7565\uff0c\u53ef\u4ee5\u4e0e\u5404\u79cd\u4f18\u5316\u65b9\u6cd5\u96c6\u6210\uff0c\u6709\u6548\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u5728\u5904\u7406\u591a\u6b65\u9aa4\u63a8\u7406\u95ee\u9898\u65f6\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2509.16547", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16547", "abs": "https://arxiv.org/abs/2509.16547", "authors": ["Adrian Wurm"], "title": "Checking extracted rules in Neural Networks", "comment": "7 pages, one figure", "summary": "In this paper we investigate formal verification of extracted rules for\nNeural Networks under a complexity theoretic point of view. A rule is a global\nproperty or a pattern concerning a large portion of the input space of a\nnetwork. These rules are algorithmically extracted from networks in an effort\nto better understand their inner way of working. Here, three problems will be\nin the focus: Does a given set of rules apply to a given network? Is a given\nset of rules consistent or do the rules contradict themselves? Is a given set\nof rules exhaustive in the sense that for every input the output is determined?\nFinding algorithms that extract such rules out of networks has been\ninvestigated over the last 30 years, however, to the author's current\nknowledge, no attempt in verification was made until now. A lot of attempts of\nextracting rules use heuristics involving randomness and over-approximation, so\nit might be beneficial to know whether knowledge obtained in that way can\nactually be trusted.\n  We investigate the above questions for neural networks with ReLU-activation\nas well as for Boolean networks, each for several types of rules. We\ndemonstrate how these problems can be reduced to each other and show that most\nof them are co-NP-complete.", "AI": {"tldr": "\u672c\u6587\u4ece\u590d\u6742\u6027\u7406\u8bba\u7684\u89d2\u5ea6\u7814\u7a76\u4e86\u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u89c4\u5219\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u9a8c\u8bc1\u89c4\u5219\u96c6\u4e00\u81f4\u6027\u3001\u5b8c\u6574\u6027\u548c\u9002\u7528\u6027\u7684\u95ee\u9898\u5927\u591a\u662fco-NP\u5b8c\u5168\u7684\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\uff0c\u9700\u8981\u9a8c\u8bc1\u4ece\u7f51\u7edc\u4e2d\u63d0\u53d6\u7684\u89c4\u5219\u7684\u53ef\u9760\u6027\u3002", "method": "\u5c06\u9a8c\u8bc1\u95ee\u9898\u89c4\u7ea6\u5230\u5f7c\u6b64\uff0c\u5e76\u8bc1\u660e\u5176co-NP\u5b8c\u5168\u6027\u3002", "result": "\u5927\u591a\u6570\u9a8c\u8bc1\u95ee\u9898\u662fco-NP\u5b8c\u5168\u7684\u3002", "conclusion": "\u672c\u6587\u7684\u7814\u7a76\u7ed3\u679c\u5bf9\u4e8e\u8bc4\u4f30\u795e\u7ecf\u7f51\u7edc\u89c4\u5219\u63d0\u53d6\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.16561", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16561", "abs": "https://arxiv.org/abs/2509.16561", "authors": ["Yue Xin", "Chen Shen", "Shaotian Yan", "Xiaosong Yuan", "Yaoming Wang", "Xiaofeng Zhang", "Chenxi Huang", "Jieping Ye"], "title": "SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning", "comment": "accpeted by EMNLP 2025", "summary": "Chain-of-Thought (CoT) prompting enhances the math reasoning capability of\nlarge language models (LLMs) to a large margin. However, the mechanism\nunderlying such improvements remains unexplored. In this paper, we present\n\\textbf{SalaMAnder} (\\textbf{S}h\\textbf{a}p\\textbf{l}ey-b\\textbf{a}sed\n\\textbf{M}athematical Expression \\textbf{A}ttribution a\\textbf{nd}\nM\\textbf{e}t\\textbf{r}ic), a theoretically grounded methodology as well as a\nmathematically rigorous evaluation metric for quantifying component-level\ncontributions in few-shot CoT reasoning. Concretely, we leverage the Shapley\nvalue for mathematical expression attribution and develop an efficient\nstratified sampling algorithm that significantly reduces the computational\ncomplexity. Besides, we develop the \\textbf{CoSP} (\\textbf{C}ardinality\n\\textbf{o}f \\textbf{S}hapley \\textbf{P}ositives) metric through covariance\nanalysis. Comprehensive validation across popular LLM models and diverse\nmathematical benchmarks demonstrates that the CoSP metric within our SalaMAnder\nframework exhibits a robust monotonic correlation with model performance, not\nonly providing theoretical explanations for the empirical success of existing\nfew-shot CoT but also establishing mathematically rigorous principles for\nprompt construction optimization. Furthermore, we verify the reliability of the\nexplanation, based on which we unify the insights of previous work.", "AI": {"tldr": "SalaMAnder\u6846\u67b6\u5229\u7528Shapley\u503c\u91cf\u5316CoT\u63a8\u7406\u4e2d\u5404\u4e2a\u6210\u5206\u7684\u8d21\u732e\uff0c\u5e76\u63d0\u51faCoSP\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u9a8c\u8868\u660eCoSP\u4e0e\u6a21\u578b\u6027\u80fd\u5177\u6709\u5f3a\u76f8\u5173\u6027\uff0c\u4e3aCoT\u63d0\u793a\u8bcd\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "motivation": "\u63a2\u7d22CoT\u63d0\u793a\u589e\u5f3aLLM\u6570\u5b66\u63a8\u7406\u80fd\u529b\u7684\u673a\u5236\u3002", "method": "\u63d0\u51faSalaMAnder\u6846\u67b6\uff0c\u5229\u7528Shapley\u503c\u8fdb\u884c\u6570\u5b66\u8868\u8fbe\u5f0f\u5c5e\u6027\u5206\u914d\uff0c\u5e76\u5f00\u53d1\u9ad8\u6548\u5206\u5c42\u91c7\u6837\u7b97\u6cd5\u548cCoSP\u6307\u6807\u3002", "result": "CoSP\u6307\u6807\u4e0e\u6a21\u578b\u6027\u80fd\u5177\u6709\u9c81\u68d2\u7684\u5355\u8c03\u76f8\u5173\u6027\uff0c\u89e3\u91ca\u4e86CoT\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u63d0\u793a\u8bcd\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002", "conclusion": "SalaMAnder\u6846\u67b6\u4e3a\u7406\u89e3\u548c\u4f18\u5316CoT\u63a8\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u5de5\u5177\u3002"}}
{"id": "2509.16578", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.16578", "abs": "https://arxiv.org/abs/2509.16578", "authors": ["Wenyao Li", "Ran Zhang", "Pengyang Wang", "Yuanchun Zhou", "Pengfei Wang"], "title": "Zero-Shot Human Mobility Forecasting via Large Language Model with Hierarchical Reasoning", "comment": null, "summary": "Human mobility forecasting is important for applications such as\ntransportation planning, urban management, and personalized recommendations.\nHowever, existing methods often fail to generalize to unseen users or locations\nand struggle to capture dynamic intent due to limited labeled data and the\ncomplexity of mobility patterns. We propose ZHMF, a framework for zero-shot\nhuman mobility forecasting that combines a semantic enhanced retrieval and\nreflection mechanism with a hierarchical language model based reasoning system.\nThe task is reformulated as a natural language question answering paradigm.\nLeveraging LLMs semantic understanding of user histories and context, our\napproach handles previously unseen prediction scenarios. We further introduce a\nhierarchical reflection mechanism for iterative reasoning and refinement by\ndecomposing forecasting into an activity level planner and a location level\nselector, enabling collaborative modeling of long term user intentions and\nshort term contextual preferences. Experiments on standard human mobility\ndatasets show that our approach outperforms existing models. Ablation studies\nreveal the contribution of each module, and case studies illustrate how the\nmethod captures user intentions and adapts to diverse contextual scenarios.", "AI": {"tldr": "ZHMF\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u589e\u5f3a\u68c0\u7d22\u548c\u53cd\u5c04\u673a\u5236\u4e0e\u57fa\u4e8e\u5206\u5c42\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u4eba\u7c7b\u51fa\u884c\u9884\u6d4b\uff0c\u5e76\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u672a\u89c1\u7528\u6237\u6216\u4f4d\u7f6e\uff0c\u4e14\u96be\u4ee5\u6355\u6349\u52a8\u6001\u610f\u56fe\u3002", "method": "\u5c06\u9884\u6d4b\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u81ea\u7136\u8bed\u8a00\u95ee\u7b54\u8303\u5f0f\uff0c\u5229\u7528LLM\u7406\u89e3\u7528\u6237\u5386\u53f2\u548c\u4e0a\u4e0b\u6587\uff0c\u7ed3\u5408\u5206\u5c42\u53cd\u5c04\u673a\u5236\u8fdb\u884c\u8fed\u4ee3\u63a8\u7406\u548c\u7ec6\u5316\uff0c\u5305\u542b\u6d3b\u52a8\u5c42\u89c4\u5212\u5668\u548c\u4f4d\u7f6e\u5c42\u9009\u62e9\u5668\u3002", "result": "\u5728\u6807\u51c6\u4eba\u7c7b\u51fa\u884c\u6570\u636e\u96c6\u4e0a\uff0cZHMF\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u6a21\u5757\u7684\u8d21\u732e\uff0c\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u6355\u6349\u7528\u6237\u610f\u56fe\u548c\u9002\u5e94\u4e0d\u540c\u4e0a\u4e0b\u6587\u573a\u666f\u7684\u80fd\u529b\u3002", "conclusion": "ZHMF\u6846\u67b6\u4e3a\u96f6\u6837\u672c\u4eba\u7c7b\u51fa\u884c\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u548c\u5bf9\u52a8\u6001\u610f\u56fe\u7684\u6355\u6349\u80fd\u529b\u3002"}}
{"id": "2509.16590", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.16590", "abs": "https://arxiv.org/abs/2509.16590", "authors": ["Manuel Borroto", "Katie Gallagher", "Antonio Ielo", "Irfan Kareem", "Francesco Ricca", "Alessandra Russo"], "title": "Question Answering with LLMs and Learning from Answer Sets", "comment": "Under consideration for TPLP journal", "summary": "Large Language Models (LLMs) excel at understanding natural language but\nstruggle with explicit commonsense reasoning. A recent trend of research\nsuggests that the combination of LLM with robust symbolic reasoning systems can\novercome this problem on story-based question answering tasks. In this setting,\nexisting approaches typically depend on human expertise to manually craft the\nsymbolic component. We argue, however, that this component can also be\nautomatically learned from examples. In this work, we introduce LLM2LAS, a\nhybrid system that effectively combines the natural language understanding\ncapabilities of LLMs, the rule induction power of the Learning from Answer Sets\n(LAS) system ILASP, and the formal reasoning strengths of Answer Set\nProgramming (ASP). LLMs are used to extract semantic structures from text,\nwhich ILASP then transforms into interpretable logic rules. These rules allow\nan ASP solver to perform precise and consistent reasoning, enabling correct\nanswers to previously unseen questions. Empirical results outline the strengths\nand weaknesses of our automatic approach for learning and reasoning in a\nstory-based question answering benchmark.", "AI": {"tldr": "LLM2LAS\u7ed3\u5408LLM\u3001ILASP\u548cASP\uff0c\u81ea\u52a8\u5b66\u4e60\u6784\u5efa\u7b26\u53f7\u63a8\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u6545\u4e8b\u95ee\u7b54\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709LLM\u5728\u5e38\u8bc6\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4eba\u5de5\u6784\u5efa\u7b26\u53f7\u63a8\u7406\u7cfb\u7edf\u8d39\u529b\uff0c\u672c\u6587\u63d0\u51fa\u81ea\u52a8\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528LLM\u63d0\u53d6\u6587\u672c\u8bed\u4e49\u7ed3\u6784\uff0cILASP\u8f6c\u6362\u4e3a\u903b\u8f91\u89c4\u5219\uff0cASP\u6c42\u89e3\u5668\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "LLM2LAS\u6709\u6548\u7ed3\u5408\u4e86LLM\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u548c\u7b26\u53f7\u63a8\u7406\u7cfb\u7edf\u7684\u7cbe\u786e\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u6545\u4e8b\u95ee\u7b54\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2509.16648", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16648", "abs": "https://arxiv.org/abs/2509.16648", "authors": ["Debarpan Bhattacharya", "Apoorva Kulkarni", "Sriram Ganapathy"], "title": "FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs", "comment": "Accepted in the Findings of EMNLP, 2025", "summary": "The accurate trust assessment of multimodal large language models (MLLMs)\ngenerated predictions, which can enable selective prediction and improve user\nconfidence, is challenging due to the diverse multi-modal input paradigms. We\npropose Functionally Equivalent Sampling for Trust Assessment (FESTA), a\nmultimodal input sampling technique for MLLMs, that generates an uncertainty\nmeasure based on the equivalent and complementary input samplings. The proposed\ntask-preserving sampling approach for uncertainty quantification expands the\ninput space to probe the consistency (through equivalent samples) and\nsensitivity (through complementary samples) of the model. FESTA uses only\ninput-output access of the model (black-box), and does not require ground truth\n(unsupervised). The experiments are conducted with various off-the-shelf\nmulti-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA\nuncertainty estimate achieves significant improvement (33.3% relative\nimprovement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in\nselective prediction performance, based on\narea-under-receiver-operating-characteristic curve (AUROC) metric in detecting\nmispredictions. The code implementation is open-sourced.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u53ef\u4fe1\u5ea6\u8bc4\u4f30\u7684\u591a\u6a21\u6001\u8f93\u5165\u91c7\u6837\u6280\u672f FESTA\uff0c\u8be5\u6280\u672f\u901a\u8fc7\u7b49\u6548\u548c\u8865\u5145\u8f93\u5165\u91c7\u6837\u751f\u6210\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\uff0c\u63d0\u9ad8\u4e86\u9009\u62e9\u6027\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u751f\u6210\u7684\u9884\u6d4b\u7ed3\u679c\u7684\u53ef\u4fe1\u5ea6\uff0c\u8fd9\u5bf9\u4e8e\u9009\u62e9\u6027\u9884\u6d4b\u548c\u63d0\u9ad8\u7528\u6237\u4fe1\u5fc3\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u529f\u80fd\u7b49\u6548\u91c7\u6837\u7684\u591a\u6a21\u6001\u8f93\u5165\u91c7\u6837\u6280\u672f FESTA\uff0c\u8be5\u6280\u672f\u901a\u8fc7\u7b49\u6548\u548c\u8865\u5145\u6837\u672c\u63a2\u6d4b\u6a21\u578b\u7684\u4e00\u81f4\u6027\u548c\u654f\u611f\u6027\uff0c\u4ece\u800c\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "FESTA \u5728\u89c6\u89c9\u548c\u97f3\u9891\u63a8\u7406\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u89c6\u89c9-LLM \u7684\u9009\u62e9\u6027\u9884\u6d4b\u6027\u80fd\u76f8\u5bf9\u63d0\u9ad8\u4e86 33.3%\uff0c\u97f3\u9891-LLM \u7684\u9009\u62e9\u6027\u9884\u6d4b\u6027\u80fd\u76f8\u5bf9\u63d0\u9ad8\u4e86 29.6%\u3002", "conclusion": "FESTA \u662f\u4e00\u79cd\u6709\u6548\u7684\u3001\u65e0\u9700\u5730\u9762\u771f\u5b9e\u503c\u7684\u9ed1\u76d2\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u63d0\u9ad8\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2509.16656", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16656", "abs": "https://arxiv.org/abs/2509.16656", "authors": ["Changyu Zeng", "Yifan Wang", "Zimu Wang", "Wei Wang", "Zhengni Yang", "Muyi Bao", "Jiming Xiao", "Ahn Nguyen", "Yutao Yue"], "title": "NUMINA: A Natural Understanding Benchmark for Multi-dimensional Intelligence and Numerical Reasoning Abilities", "comment": null, "summary": "Recent advancements in 2D multimodal large language models (MLLMs) have\nsignificantly improved performance in vision-language tasks. However, extending\nthese capabilities to 3D environments remains a distinct challenge due to the\ncomplexity of spatial reasoning. Nevertheless, existing 3D benchmarks often\nlack fine-grained numerical reasoning task annotations, limiting MLLMs' ability\nto perform precise spatial measurements and complex numerical reasoning. To\naddress this gap, we introduce NUMINA, the first Natural Understanding\nbenchmark for Multi-dimensional Intelligence and Numerical reasoning Abilities\nto enhance multimodal indoor perceptual understanding. NUMINA features\nmulti-scale annotations and various question-answer pairs, generated using\nNUMINA-Flow, an automated annotation pipeline that integrates LLM rewriting and\nrule-based self-verification. We evaluate the performance of various\nstate-of-the-art LLMs on NUMINA following the Chat-Scene framework,\ndemonstrating that current LLMs struggle with multimodal numerical reasoning,\nparticularly in performing precise computations such as distance and volume\nestimation, highlighting the need for further advancements in 3D models. The\ndataset and source codes can be obtained from\nhttps://github.com/fengshun124/NUMINA.", "AI": {"tldr": "NUMINA\u57fa\u51c6\u6d4b\u8bd5\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e09\u7ef4\u73af\u5883\u4e2d\u7684\u6570\u503c\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u679c\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u7cbe\u786e\u8ba1\u7b97\uff08\u5982\u8ddd\u79bb\u548c\u4f53\u79ef\u4f30\u8ba1\uff09\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u73b0\u67093D\u57fa\u51c6\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u6570\u503c\u63a8\u7406\u4efb\u52a1\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faNUMINA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u591a\u5c3a\u5ea6\u6807\u6ce8\u548c\u591a\u79cd\u95ee\u7b54\u5bf9\uff0c\u5e76\u4f7f\u7528NUMINA-Flow\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b\u751f\u6210\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709LLM\u96be\u4ee5\u80dc\u4efb\u591a\u6a21\u6001\u6570\u503c\u63a8\u7406\uff0c\u5c24\u5176\u662f\u5728\u7cbe\u786e\u8ba1\u7b97\u65b9\u9762\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb3D\u6a21\u578b\u4ee5\u63d0\u5347\u5176\u5728\u4e09\u7ef4\u73af\u5883\u4e2d\u8fdb\u884c\u6570\u503c\u63a8\u7406\u7684\u80fd\u529b\u3002"}}
{"id": "2509.16742", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16742", "abs": "https://arxiv.org/abs/2509.16742", "authors": ["Mohammad Beigi", "Ying Shen", "Parshin Shojaee", "Qifan Wang", "Zichao Wang", "Chandan Reddy", "Ming Jin", "Lifu Huang"], "title": "Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories", "comment": null, "summary": "Despite the remarkable capabilities of large language models, current\ntraining paradigms inadvertently foster \\textit{sycophancy}, i.e., the tendency\nof a model to agree with or reinforce user-provided information even when it's\nfactually incorrect. To address this challenge, we introduce \\textbf{SMART}\n(Sycophancy Mitigation through Adaptive Reasoning Trajectories), which reframes\nsycophancy as a \\textit{reasoning optimization problem} rather than an output\nalignment issue. SMART is a two-stage framework comprising: (1)\nUncertainty-Aware Adaptive Monte Carlo Tree Search (UA-MCTS), which dynamically\nadjusts model exploration based on state-level uncertainty to collect\nhigh-quality, diverse reasoning trajectories alongside both stepwise progress\nand final outcome rewards; and (2) progress-based reinforcement learning, which\nfine-tunes the model using the collected trajectories and reward signals to\nreinforce effective reasoning patterns. Through extensive experiments, we show\nthat SMART significantly reduces sycophantic behavior while preserving strong\nperformance on out-of-distribution inputs and maintaining general capabilities.\nThese results underscore the importance of optimizing internal reasoning\nmechanisms to build more truthful and aligned AI assistants.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faSMART\u6846\u67b6\uff0c\u901a\u8fc7\u8c03\u6574\u6a21\u578b\u63a2\u7d22\u7b56\u7565\u4ee5\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u8fce\u5408\u7528\u6237\u9519\u8bef\u4fe1\u606f\u7684\u503e\u5411\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u8fce\u5408\u7528\u6237\uff0c\u5373\u4f7f\u4fe1\u606f\u9519\u8bef\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6SMART\uff1a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u81ea\u9002\u5e94\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22(UA-MCTS)\u548c\u57fa\u4e8e\u8fdb\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSMART\u663e\u8457\u51cf\u5c11\u4e86\u8fce\u5408\u884c\u4e3a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u4f18\u5316\u5185\u90e8\u63a8\u7406\u673a\u5236\u5bf9\u4e8e\u6784\u5efa\u66f4\u771f\u5b9e\u53ef\u9760\u7684AI\u52a9\u624b\u81f3\u5173\u91cd\u8981\u3002"}}
