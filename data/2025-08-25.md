<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [T-ILR: a Neurosymbolic Integration for LTLf](https://arxiv.org/abs/2508.15943)
*Riccardo Andreoni,Andrei Buliga,Alessandro Daniele,Chiara Ghidini,Marco Montali,Massimiliano Ronzani*

Main category: cs.AI

TL;DR: T-ILR框架有效地将线性时序逻辑(LTLf)规范集成到深度学习中，提升了时序神经符号模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理时序逻辑规范，特别是基于有限状态自动机的显式表示方法效率低下。

Method: 扩展了迭代局部细化(ILR)神经符号算法，利用模糊LTLf解释。

Result: 在图像序列分类基准测试中，T-ILR方法比现有技术实现了更高的准确性和计算效率。

Conclusion: 提出了一种名为T-ILR的新的神经符号框架，用于将线性时序逻辑规范直接融入深度学习架构，并在图像序列分类任务中取得了比现有技术更高的准确率和计算效率。

Abstract: State-of-the-art approaches for integrating symbolic knowledge with deep
learning architectures have demonstrated promising results in static domains.
However, methods to handle temporal logic specifications remain underexplored.
The only existing approach relies on an explicit representation of a
finite-state automaton corresponding to the temporal specification. Instead, we
aim at proposing a neurosymbolic framework designed to incorporate temporal
logic specifications, expressed in Linear Temporal Logic over finite traces
(LTLf), directly into deep learning architectures for sequence-based tasks. We
extend the Iterative Local Refinement (ILR) neurosymbolic algorithm, leveraging
the recent introduction of fuzzy LTLf interpretations. We name this proposed
method Temporal Iterative Local Refinement (T-ILR). We assess T-ILR on an
existing benchmark for temporal neurosymbolic architectures, consisting of the
classification of image sequences in the presence of temporal knowledge. The
results demonstrate improved accuracy and computational efficiency compared to
the state-of-the-art method.

</details>


### [2] [CoFE: A Framework Generating Counterfactual ECG for Explainable Cardiac AI-Diagnostics](https://arxiv.org/abs/2508.16033)
*Jong-Hwan Jang,Junho Song,Yong-Yeon Jo*

Main category: cs.AI

TL;DR: 该研究提出一个框架CoFE，通过生成反事实ECG来解释AI-ECG模型的预测，提高模型可解释性，支持更有效的临床决策。


<details>
  <summary>Details</summary>
Motivation: 为了使基于AI的ECG预测模型能够成功地整合到临床实践中，需要可解释的AI (XAI) 方法。

Method: 提出了一种生成反事实ECG (CoFE) 的框架，用于解释AI-ECG模型的预测结果。

Result: CoFE框架揭示了ECG信号中与既定临床知识相符的特征变化，阐明了有效特征在ECG中的位置以及它们如何影响模型的预测。

Conclusion: 提出了一种名为CoFE的框架，通过生成反事实ECG来解释AI-ECG模型的预测结果，并在心房颤动分类和钾水平回归模型两个案例研究中验证了其有效性，有助于提高AI-ECG模型的可解释性，支持更有效的临床决策。

Abstract: Recognizing the need for explainable AI (XAI) approaches to enable the
successful integration of AI-based ECG prediction models (AI-ECG) into clinical
practice, we introduce a framework generating \textbf{Co}unter\textbf{F}actual
\textbf{E}CGs (i,e., named CoFE) to illustrate how specific features, such as
amplitudes and intervals, influence the model's predictive decisions. To
demonstrate the applicability of the CoFE, we present two case studies: atrial
fibrillation classification and potassium level regression models. The CoFE
reveals feature changes in ECG signals that align with the established clinical
knowledge. By clarifying both \textbf{where valid features appear} in the ECG
and \textbf{how they influence the model's predictions}, we anticipate that our
framework will enhance the interpretability of AI-ECG models and support more
effective clinical decision-making. Our demonstration video is available at:
https://www.youtube.com/watch?v=YoW0bNBPglQ.

</details>


### [3] [MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs](https://arxiv.org/abs/2508.16051)
*Yiheng Hu,Xiaoyang Wang,Qing Liu,Xiwei Xu,Qian Fu,Wenjie Zhang,Liming Zhu*

Main category: cs.AI

TL;DR: 提出一种无训练的多模态多跳问答框架，通过自适应规划图实现动态推理路径探索，性能优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于顺序检索和推理的单路径范式，容易受到中间步骤错误的影响，且开发多模态模型计算成本高。

Method: 提出了一种基于自适应规划图的无训练框架，该框架包含规划、检索和推理三个模块。规划模块分析当前状态，确定下一步操作和扩展图的位置；检索模块采用特定于模态的策略，动态适应不同数据类型；推理模块基于检索结果进行推理。

Result: 在MultimodalQA和WebQA数据集上，该方法的性能与现有模型相当或更好。

Conclusion: 提出了一种基于自适应规划图的无训练框架，用于解决多模态多跳问答问题，该框架通过规划、检索和推理模块实现动态灵活的推理路径探索，并在MultimodalQA和WebQA数据集上取得了与现有模型相当或更好的结果。

Abstract: Multimodal Multi-hop question answering requires integrating information from
diverse sources, such as images and texts, to derive answers. Existing methods
typically rely on sequential retrieval and reasoning, where each step builds on
the previous output. However, this single-path paradigm makes them vulnerable
to errors due to misleading intermediate steps. Moreover, developing multimodal
models can be computationally expensive, often requiring extensive training. To
address these limitations, we propose a training-free framework guided by an
Adaptive Planning Graph, which consists of planning, retrieval and reasoning
modules. The planning module analyzes the current state of the Adaptive
Planning Graph, determines the next action and where to expand the graph, which
enables dynamic and flexible exploration of reasoning paths. To handle
retrieval of text to unspecified target modalities, we devise modality-specific
strategies that dynamically adapt to distinct data types. Our approach
preserves the characteristics of multimodal information without costly
task-specific training, enabling seamless integration with up-to-date models.
Finally, the experiments on MultimodalQA and WebQA show that our approach
matches or outperforms existing models that rely on training.

</details>


### [4] [Generative Foundation Model for Structured and Unstructured Electronic Health Records](https://arxiv.org/abs/2508.16054)
*Sonish Sivarajkumar,Hang Zhang,Yuelyu Ji,Maneesh Bilalpur,Xizhi Wu,Chenyu Li,Min Gu Kwak,Shyam Visweswaran,Yanshan Wang*

Main category: cs.AI

TL;DR: GDP，一种多模态基础模型，能够预测临床可操作事件并生成高质量的临床叙事，在MIMIC-IV数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 充分利用电子健康记录（EHR）中结构化和非结构化数据的异质性，以改善患者预后。

Method: GDP模型采用CNN-Transformer编码器对结构化EHR时间序列进行编码，并通过跨模态注意力将其与非结构化EHR融合到基于LLaMA的解码器中。模型训练分为两个阶段：生成式预训练和多任务微调。

Result: 在MIMIC-IV数据集上，GDP模型在心脏衰竭、2型糖尿病和30天再入院预测方面取得了优异的AUROC值（分别为0.923、0.817和0.627）。在叙事生成方面，ROUGE-L和BERTScore-F1值也较高。

Conclusion: Generative Deep Patient (GDP)模型在临床预测和临床叙事生成方面均表现出色，并且其灵活的架构可以扩展到其他模态。

Abstract: Electronic health records (EHRs) are rich clinical data sources but complex
repositories of patient data, spanning structured elements (demographics,
vitals, lab results, codes), unstructured clinical notes and other modalities
of data. Harnessing this heterogeneity is critical for improving patient
outcomes. Recent advances in large language models (LLMs) have enabled
foundation models that can learn from multiple data modalities and support
clinical tasks. However, most current approaches simply serialize numeric EHR
data into text, which risks losing temporal and quantitative detail. We
introduce Generative Deep Patient (GDP), a multimodal foundation model that
natively encodes structured EHR time-series via a CNN-Transformer encoder and
fuses it with unstructured EHRs through cross-modal attention into a
LLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining,
where it learns to produce clinical narratives from raw patient timelines while
also performing masked feature prediction (MFP) and next time-step prediction
(NTP) to capture temporal dynamics; and (2) multi-task fine-tuning for
clinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-day
readmission). In clinical prediction, GDP demonstrated superior performance on
MIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and
30-day readmission AUROC = 0.627. For narrative generation, GDP achieved
ROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation,
GDP-Instruct scored highest on faithfulness, fluency, and overall clinical
utility, suggesting reduced hospital documentation workload without sacrificing
accuracy. Our results demonstrate that a single multimodal foundation model can
both predict clinically actionable events and generate high-quality clinical
narratives. Furthermore, GDP's flexible architecture can be extended to
additional modalities.

</details>


### [5] [Urban Comfort Assessment in the Era of Digital Planning: A Multidimensional, Data-driven, and AI-assisted Framework](https://arxiv.org/abs/2508.16057)
*Sijie Yang,Binyu Lei,Filip Biljecki*

Main category: cs.AI

TL;DR: 缺乏清晰的城市舒适性定义和评估框架，本研究提出一个基于多维度分析、数据支持和AI辅助的新框架。


<details>
  <summary>Details</summary>
Motivation: 确保宜居性和舒适性是城市规划的基本目标，但目前缺乏清晰的城市舒适性定义和综合评估框架。

Method: 探索性研究，对现有文献进行综述和分析，并提出新的评估框架。

Result: 提出了一个包含多维度分析、数据支持和AI辅助的城市舒适性评估框架。

Conclusion: 该研究探讨了数字规划中评估城市舒适性的理论解释和方法，强调多维度分析、数据支持和AI辅助三个关键维度。

Abstract: Ensuring liveability and comfort is one of the fundamental objectives of
urban planning. Numerous studies have employed computational methods to assess
and quantify factors related to urban comfort such as greenery coverage,
thermal comfort, and walkability. However, a clear definition of urban comfort
and its comprehensive evaluation framework remain elusive. Our research
explores the theoretical interpretations and methodologies for assessing urban
comfort within digital planning, emphasising three key dimensions:
multidimensional analysis, data support, and AI assistance.

</details>


### [6] [Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting](https://arxiv.org/abs/2508.16059)
*Zhuomin Chen,Dan Li,Jiahui Zhou,Shunyu Wu,Haozheng Ye,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: MSEF框架通过多层融合时间序列信息，有效提升了LLM的时间序列预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法将时间序列信息浅层整合到LLM中，导致时间序列信息在深层逐渐消失，影响预测效果。

Method: 提出了一种名为多层可控嵌入融合(MSEF)的新框架，该框架利用现成的时序基础模型提取语义丰富的嵌入，并通过特定层的转向向量将其与LLM层的中间文本表示融合。

Result: 在七个基准测试中，MSEF与基线相比取得了显著的性能提升，平均MSE降低了31.8%。

Conclusion: MSEF框架通过在LLM所有层级直接访问时间序列模式，显著提升了LLM的时间序列预测性能，平均MSE降低了31.8%。

Abstract: Time series (TS) data are ubiquitous across various application areas,
rendering time series forecasting (TSF) a fundamental task. With the astounding
advances in large language models (LLMs), a variety of methods have been
developed to adapt LLMs for time series forecasting. Despite unlocking the
potential of LLMs in comprehending TS data, existing methods are inherently
constrained by their shallow integration of TS information, wherein LLMs
typically access TS representations at shallow layers, primarily at the input
layer. This causes the influence of TS representations to progressively fade in
deeper layers and eventually leads to ineffective adaptation between textual
embeddings and TS representations. In this paper, we propose the Multi-layer
Steerable Embedding Fusion (MSEF), a novel framework that enables LLMs to
directly access time series patterns at all depths, thereby mitigating the
progressive loss of TS information in deeper layers. Specifically, MSEF
leverages off-the-shelf time series foundation models to extract semantically
rich embeddings, which are fused with intermediate text representations across
LLM layers via layer-specific steering vectors. These steering vectors are
designed to continuously optimize the alignment between time series and textual
modalities and facilitate a layer-specific adaptation mechanism that ensures
efficient few-shot learning capabilities. Experimental results on seven
benchmarks demonstrate significant performance improvements by MSEF compared
with baselines, with an average reduction of 31.8% in terms of MSE. The code is
available at https://github.com/One1sAll/MSEF.

</details>


### [7] [InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles](https://arxiv.org/abs/2508.16072)
*Zizhen Li,Chuanhao Li,Yibin Wang,Qi Chen,Diping Song,Yukang Feng,Jianwen Sun,Jiaxin Ai,Fanrui Zhang,Mingzhu Sun,Kaipeng Zhang*

Main category: cs.AI

TL;DR: InMind框架评估LLM在社交推理游戏中的个性化推理能力，发现通用LLM存在局限性，而增强推理能力的LLM表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估忽略了个性化推理风格对社交环境中行为的影响，社交推理游戏是评估个性化推理的理想环境。

Method: 提出InMind评估框架，结合游戏数据、策略追踪和赛后反思，评估LLM在社交推理游戏（以Avalon为例）中的表现。

Result: 通用LLM依赖词汇线索，难以适应动态策略；增强推理能力的LLM（如DeepSeek-R1）表现出对风格敏感的推理能力。

Conclusion: 当前LLM在个性化、适应性推理方面存在局限性，InMind框架有助于评估LLM在社交推理游戏中的认知能力。

Abstract: LLMs have shown strong performance on human-centric reasoning tasks. While
previous evaluations have explored whether LLMs can infer intentions or detect
deception, they often overlook the individualized reasoning styles that
influence how people interpret and act in social contexts. Social deduction
games (SDGs) provide a natural testbed for evaluating individualized reasoning
styles, where different players may adopt diverse but contextually valid
reasoning strategies under identical conditions. To address this, we introduce
InMind, a cognitively grounded evaluation framework designed to assess whether
LLMs can capture and apply personalized reasoning styles in SDGs. InMind
enhances structured gameplay data with round-level strategy traces and
post-game reflections, collected under both Observer and Participant modes. It
supports four cognitively motivated tasks that jointly evaluate both static
alignment and dynamic adaptation. As a case study, we apply InMind to the game
Avalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o
frequently rely on lexical cues, struggling to anchor reflections in temporal
gameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs
like DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These
findings reveal key limitations in current LLMs' capacity for individualized,
adaptive reasoning, and position InMind as a step toward cognitively aligned
human-AI interaction.

</details>


### [8] [IR-Agent: Expert-Inspired LLM Agents for Structure Elucidation from Infrared Spectra](https://arxiv.org/abs/2508.16112)
*Heewoong Noh,Namkyeong Lee,Gyoung S. Na,Kibum Kim,Chanyoung Park*

Main category: cs.AI

TL;DR: IR-Agent:一种新的多Agent框架，用于从红外光谱中阐明分子结构，提高了准确性并具有适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能反映专家分析流程，缺乏灵活性。

Method: 多Agent框架，模拟专家驱动的红外分析流程。

Result: IR-Agent提高了实验红外光谱的基线性能，并对各种形式的化学信息表现出强大的适应性。

Conclusion: IR-Agent框架提高了基于红外光谱的分子结构解析的准确性，并展现出对各种化学信息的强大适应性。

Abstract: Spectral analysis provides crucial clues for the elucidation of unknown
materials. Among various techniques, infrared spectroscopy (IR) plays an
important role in laboratory settings due to its high accessibility and low
cost. However, existing approaches often fail to reflect expert analytical
processes and lack flexibility in incorporating diverse types of chemical
knowledge, which is essential in real-world analytical scenarios. In this
paper, we propose IR-Agent, a novel multi-agent framework for molecular
structure elucidation from IR spectra. The framework is designed to emulate
expert-driven IR analysis procedures and is inherently extensible. Each agent
specializes in a specific aspect of IR interpretation, and their complementary
roles enable integrated reasoning, thereby improving the overall accuracy of
structure elucidation. Through extensive experiments, we demonstrate that
IR-Agent not only improves baseline performance on experimental IR spectra but
also shows strong adaptability to various forms of chemical information.

</details>


### [9] [Extending FKG.in: Towards a Food Claim Traceability Network](https://arxiv.org/abs/2508.16117)
*Saransh Kumar Gupta,Rizwan Gulzar Mir,Lipika Dey,Partha Pratim Das,Anirban Sen,Ramesh Jain*

Main category: cs.AI

TL;DR: 构建食品声明可追溯性网络（FCN），解决食品信息真伪难辨的问题。


<details>
  <summary>Details</summary>
Motivation: 全球食品领域存在大量真假难辨的食品声明，缺乏有效的追踪和验证机制。

Method: 基于FKG.in知识图谱，利用Reddit数据和大型语言模型，构建FCN，整合数据输入、结构化模式和溯源感知流程。

Result: 开发了一个可追溯食品声明的原型系统，该系统具有应用无关性，可扩展到其他地区和场景。

Conclusion: 提出了一种食品声明可追溯性网络（FCN）用于追踪和验证食品相关的声明，并构建了一个基于印度食物知识图谱的原型系统。

Abstract: The global food landscape is rife with scientific, cultural, and commercial
claims about what foods are, what they do, what they should not do, or should
not do. These range from rigorously studied health benefits (probiotics improve
gut health) and misrepresentations (soaked almonds make one smarter) to vague
promises (superfoods boost immunity) and culturally rooted beliefs (cold foods
cause coughs). Despite their widespread influence, the infrastructure for
tracing, verifying, and contextualizing these claims remains fragmented and
underdeveloped. In this paper, we propose a Food Claim-Traceability Network
(FCN) as an extension of FKG.in, a knowledge graph of Indian food that we have
been incrementally building. We also present the ontology design and the
semi-automated knowledge curation workflow that we used to develop a proof of
concept of FKG.in-FCN using Reddit data and Large Language Models. FCN
integrates curated data inputs, structured schemas, and provenance-aware
pipelines for food-related claim extraction and validation. While directly
linked to the Indian food knowledge graph as an application, our methodology
remains application-agnostic and adaptable to other geographic, culinary, or
regulatory settings. By modeling food claims and their traceability in a
structured, verifiable, and explainable way, we aim to contribute to more
transparent and accountable food knowledge ecosystems, supporting researchers,
policymakers, and most importantly, everyday consumers in navigating a world
saturated with dietary assertions.

</details>


### [10] [Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning](https://arxiv.org/abs/2508.16129)
*Ruiqi Wu,Yuang Yao,Tengfei Ma,Chenran Zhang,Na Su,Tao Zhou,Geng Chen,Wen Fan,Yi Zhou*

Main category: cs.AI

TL;DR: 开发了首个眼科多模态推理模型OphthaReason，并通过UADT方法提升了其在基础和复杂推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的医学多模态推理模型大多集中于基于视觉特征匹配的浅层推理（基础推理），无法胜任需要整合异构临床信息（如主诉和病史）和多模态医学影像数据的复杂临床诊断任务。

Method: 提出了一种新的不确定性感知动态思维(UADT)方法，该方法通过熵估计样本级不确定性，并使用形状优势机制动态调节模型的探索深度。构建了MM-Retinal-Reason数据集，包含基础推理和复杂推理任务。

Result: 在基础和复杂推理任务上，OphthaReason模型均取得了最先进的性能，显著优于其他模型。

Conclusion: OphthaReason模型在基础和复杂推理任务上都取得了最先进的性能，优于通用MLLM、医学MLLM、基于RL的医学MLLM和眼科MLLM。

Abstract: Multimodal large language models (MLLMs) have recently demonstrated
remarkable reasoning abilities with reinforcement learning paradigm. Although
several multimodal reasoning models have been explored in the medical domain,
most of them focus exclusively on basic reasoning, which refers to shallow
inference based on visual feature matching. However, real-world clinical
diagnosis extends beyond basic reasoning, demanding reasoning processes that
integrate heterogeneous clinical information (such as chief complaints and
medical history) with multimodal medical imaging data. To bridge this gap, we
introduce MM-Retinal-Reason, the first ophthalmic multimodal dataset with the
full spectrum of perception and reasoning. It encompasses both basic reasoning
tasks and complex reasoning tasks, aiming to enhance visual-centric fundamental
reasoning capabilities and emulate realistic clinical thinking patterns.
Building upon MM-Retinal-Reason, we propose OphthaReason, the first
ophthalmology-specific multimodal reasoning model with step-by-step reasoning
traces. To enable flexible adaptation to both basic and complex reasoning
tasks, we specifically design a novel method called Uncertainty-Aware Dynamic
Thinking (UADT), which estimates sample-level uncertainty via entropy and
dynamically modulates the model's exploration depth using a shaped advantage
mechanism. Comprehensive experiments demonstrate that our model achieves
state-of-the-art performance on both basic and complex reasoning tasks,
outperforming general-purpose MLLMs, medical MLLMs, RL-based medical MLLMs, and
ophthalmic MLLMs by at least 24.92\%, 15.00\%, 21.20\%, and 17.66\%. Project
Page: \href{https://github.com/lxirich/OphthaReason}{link}.

</details>


### [11] [Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain](https://arxiv.org/abs/2508.16172)
*Kai Hu,Parfait Atchade-Adelomou,Carlo Adornetto,Adrian Mora-Carrero,Luis Alonso-Pastor,Ariel Noyman,Yubo Liu,Kent Larson*

Main category: cs.AI

TL;DR: 利用RAG和LLM改进人类行为模拟，在Replica数据集上效果优于标准LLM，但存在速度慢和幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 模拟城市环境中的人类行为，尤其是在新建区域缺乏数据的情况下。

Method: 结合图检索增强生成(RAG)与LLM的Preference Chain方法。

Result: Preference Chain在Replica数据集上优于标准LLM，在交通模式选择方面更符合现实世界。Mobility Agent的开发突出了该方法在城市交通建模、个性化出行行为分析和动态交通预测中的应用潜力。

Conclusion: Preference Chain方法在Replica数据集上优于标准LLM，为数据稀缺环境下模拟复杂人类行为提供了一个有前景的框架，尽管存在推理速度慢和幻觉风险等局限性。

Abstract: Understanding human behavior in urban environments is a crucial field within
city sciences. However, collecting accurate behavioral data, particularly in
newly developed areas, poses significant challenges. Recent advances in
generative agents, powered by Large Language Models (LLMs), have shown promise
in simulating human behaviors without relying on extensive datasets.
Nevertheless, these methods often struggle with generating consistent,
context-sensitive, and realistic behavioral outputs. To address these
limitations, this paper introduces the Preference Chain, a novel method that
integrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance
context-aware simulation of human behavior in transportation systems.
Experiments conducted on the Replica dataset demonstrate that the Preference
Chain outperforms standard LLM in aligning with real-world transportation mode
choices. The development of the Mobility Agent highlights potential
applications of proposed method in urban mobility modeling for emerging cities,
personalized travel behavior analysis, and dynamic traffic forecasting. Despite
limitations such as slow inference and the risk of hallucination, the method
offers a promising framework for simulating complex human behavior in
data-scarce environments, where traditional data-driven models struggle due to
limited data availability.

</details>


### [12] [Competition and Attraction Improve Model Fusion](https://arxiv.org/abs/2508.16204)
*João Abrantes,Robert Tjarko Lange,Yujin Tang*

Main category: cs.AI

TL;DR: M2N2是一种新型模型合并进化算法，能够从零开始进化模型，并在多个任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的模型合并方法需要手动划分模型参数，限制了潜在组合的探索和性能。

Method: 提出了一种名为M2N2的进化算法，该算法具有动态调整合并边界、多样性保持机制和启发式吸引度量三个关键特性。

Result: M2N2在MNIST分类任务中取得了与CMA-ES相当的性能，同时计算效率更高；在语言和图像生成模型的合并中取得了最先进的性能，并保持了关键的模型能力。

Conclusion: M2N2算法能够从零开始进化模型，并在MNIST分类和语言/图像生成模型合并中取得了最先进的性能，同时保持了模型的关键能力。

Abstract: Model merging is a powerful technique for integrating the specialized
knowledge of multiple machine learning models into a single model. However,
existing methods require manually partitioning model parameters into fixed
groups for merging, which restricts the exploration of potential combinations
and limits performance. To overcome these limitations, we propose Model Merging
of Natural Niches (M2N2), an evolutionary algorithm with three key features:
(1) dynamic adjustment of merging boundaries to progressively explore a broader
range of parameter combinations; (2) a diversity preservation mechanism
inspired by the competition for resources in nature, to maintain a population
of diverse, high-performing models that are particularly well-suited for
merging; and (3) a heuristicbased attraction metric to identify the most
promising pairs of models for fusion. Our experimental results demonstrate, for
the first time, that model merging can be used to evolve models entirely from
scratch. Specifically, we apply M2N2 to evolve MNIST classifiers from scratch
and achieve performance comparable to CMA-ES, while being computationally more
efficient. Furthermore, M2N2 scales to merge specialized language and image
generation models, achieving state-of-the-art performance. Notably, it
preserves crucial model capabilities beyond those explicitly optimized by the
fitness function, highlighting its robustness and versatility. Our code is
available at https://github.com/SakanaAI/natural_niches

</details>


### [13] [The next question after Turing's question: Introducing the Grow-AI test](https://arxiv.org/abs/2508.16277)
*Alexandru Tugui*

Main category: cs.AI

TL;DR: GROW-AI框架通过六个标准和四个领域的“游戏”评估AI的成长成熟度，结果表明该方法有效且可复制。


<details>
  <summary>Details</summary>
Motivation: 探索如何评估AI的“成长”，旨在回答“机器能否成长？”这个问题，超越图灵测试。

Method: GROW-AI框架基于六个主要标准（C1-C6），每个标准通过特定的“游戏”进行评估，分为四个领域，探索人类维度及其在AI中的转换。评估使用专家先验方法确定初始权重，最终计算“成长指数”。

Result: 该方法允许对AI实体的“成长”水平进行连贯且可比较的评估，多游戏结构能突出其优缺点，统一的日志保证了评估的可追溯性和可复制性。

Conclusion: GROW-AI框架能有效评估AI的“成长”水平，无论AI实体的类型如何，都能进行连贯且可比较的评估。多游戏结构突出了AI的优势和劣势，统一的AI日志保证了评估的可追溯性和可复制性。

Abstract: This study aims to extend the framework for assessing artificial
intelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom),
designed to answer the question "Can machines grow up?" -- a natural successor
to the Turing Test. The methodology applied is based on a system of six primary
criteria (C1-C6), each assessed through a specific "game", divided into four
arenas that explore both the human dimension and its transposition into AI. All
decisions and actions of the entity are recorded in a standardized AI Journal,
the primary source for calculating composite scores. The assessment uses the
prior expert method to establish initial weights, and the global score -- Grow
Up Index -- is calculated as the arithmetic mean of the six scores, with
interpretation on maturity thresholds. The results show that the methodology
allows for a coherent and comparable assessment of the level of "growth" of AI
entities, regardless of their type (robots, software agents, LLMs). The
multi-game structure highlights strengths and vulnerable areas, and the use of
a unified journal guarantees traceability and replicability in the evaluation.
The originality of the work lies in the conceptual transposition of the process
of "growing" from the human world to that of artificial intelligence, in an
integrated testing format that combines perspectives from psychology, robotics,
computer science, and ethics. Through this approach, GROW-AI not only measures
performance but also captures the evolutionary path of an AI entity towards
maturity.

</details>


### [14] [AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications](https://arxiv.org/abs/2508.16279)
*Dawei Gao,Zitao Li,Yuexiang Xie,Weirui Kuang,Liuyi Yao,Bingchen Qian,Zhijian Ma,Yue Cui,Haohao Luo,Shen Li,Lu Yi,Yi Yu,Shiqi He,Zhiling Luo,Wenmeng Zhou,Zhicheng Zhang,Xuguang He,Ziqian Chen,Weikai Liao,Farruh Isakulovich Kushnazarov,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: AgentScope 1.0 是一个改进的工具型智能体框架，支持灵活高效的工具使用，并提供开发者友好的工程支持。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，智能体能够结合内在知识和动态工具使用，AgentScope旨在全面支持灵活高效的基于工具的代理环境交互。

Method: AgentScope 1.0 抽象了代理应用程序的基本组件，提供统一接口和可扩展模块，支持最新的模型和MCP，并基于异步设计构建了高级代理基础设施。

Result: AgentScope 1.0 提供了可扩展的评估模块、可视化Studio界面、运行时沙箱等，方便开发者构建可扩展、自适应且有效的代理应用程序。

Conclusion: AgentScope 1.0 显著改进了基于工具的智能体与环境交互，为构建代理应用程序提供了实用基础。

Abstract: Driven by rapid advancements of Large Language Models (LLMs), agents are
empowered to combine intrinsic knowledge with dynamic tool use, greatly
enhancing their capacity to address real-world tasks. In line with such an
evolution, AgentScope introduces major improvements in a new version (1.0),
towards comprehensively supporting flexible and efficient tool-based
agent-environment interactions for building agentic applications. Specifically,
we abstract foundational components essential for agentic applications and
provide unified interfaces and extensible modules, enabling developers to
easily leverage the latest progress, such as new models and MCPs. Furthermore,
we ground agent behaviors in the ReAct paradigm and offer advanced agent-level
infrastructure based on a systematic asynchronous design, which enriches both
human-agent and agent-agent interaction patterns while improving execution
efficiency. Building on this foundation, we integrate several built-in agents
tailored to specific practical scenarios. AgentScope also includes robust
engineering support for developer-friendly experiences. We provide a scalable
evaluation module with a visual studio interface, making the development of
long-trajectory agentic applications more manageable and easier to trace. In
addition, AgentScope offers a runtime sandbox to ensure safe agent execution
and facilitates rapid deployment in production environments. With these
enhancements, AgentScope provides a practical foundation for building scalable,
adaptive, and effective agentic applications.

</details>


### [15] [Do What? Teaching Vision-Language-Action Models to Reject the Impossible](https://arxiv.org/abs/2508.16292)
*Wen-Han Hsieh,Elvis Hsieh,Dantong Niu,Trevor Darrell,Roei Herzig,David M. Chan*

Main category: cs.AI

TL;DR: 该论文提出了一种名为IVA的框架，用于改进机器人对错误前提指令的处理，实验结果表明该框架显著提高了准确性和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-行动(VLA)模型在处理错误前提指令时表现不佳，本文旨在研究VLA模型如何识别、解释和响应这类指令。

Method: 提出了一种统一的指令-验证-行动(IVA)框架，该框架能够检测、解释和响应错误前提指令，并利用上下文增强、半合成数据集进行训练。

Result: 实验结果表明，IVA框架在错误前提指令检测和成功响应方面均取得了显著改进。

Conclusion: IVA框架显著提高了对错误前提指令的检测准确率（97.56%）和成功响应率（50.78%），有效处理了机器人任务中语言指令的错误前提问题。

Abstract: Recently, Vision-Language-Action (VLA) models have demonstrated strong
performance on a range of robotic tasks. These models rely on multimodal
inputs, with language instructions playing a crucial role -- not only in
predicting actions, but also in robustly interpreting user intent, even when
the requests are impossible to fulfill. In this work, we investigate how VLAs
can recognize, interpret, and respond to false-premise instructions: natural
language commands that reference objects or conditions absent from the
environment. We propose Instruct-Verify-and-Act (IVA), a unified framework that
(i) detects when an instruction cannot be executed due to a false premise, (ii)
engages in language-based clarification or correction, and (iii) grounds
plausible alternatives in perception and action. Towards this end, we construct
a large-scale instruction tuning setup with structured language prompts and
train a VLA model capable of handling both accurate and erroneous requests. Our
approach leverages a contextually augmented, semi-synthetic dataset containing
paired positive and false-premise instructions, enabling robust detection and
natural language correction. Our experiments show that IVA improves false
premise detection accuracy by 97.56% over baselines, while increasing
successful responses in false-premise scenarios by 50.78%.

</details>


### [16] [Causal Beam Selection for Reliable Initial Access in AI-driven Beam Management](https://arxiv.org/abs/2508.16352)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.AI

TL;DR: 利用因果发现技术改进毫米波MIMO系统波束对准，显著降低时间和开销。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的波束对准方法往往忽略输入和输出之间的因果关系，导致可解释性差、泛化能力差以及不必要的波束扫描开销。

Method: 提出了一种两阶段因果波束选择算法，首先使用因果发现学习贝叶斯图以捕获接收功率输入和最佳波束之间的依赖关系，然后该图指导基于深度学习的分类器的因果特征选择。

Result: 仿真结果表明，该因果波束选择方法的性能与传统方法相当，同时通过仅关注因果相关的特征，将输入选择时间减少了 94.4%，将波束扫描开销减少了 59.4%。

Conclusion: 提出了一种因果感知的深度学习框架，该框架将因果发现集成到波束管理管道中，通过因果特征选择减少输入选择时间和波束扫描开销，在性能与现有方法相当的情况下，显著提高了效率。

Abstract: Efficient and reliable beam alignment is a critical requirement for mmWave
multiple-input multiple-output (MIMO) systems, especially in 6G and beyond,
where communication must be fast, adaptive, and resilient to real-world
uncertainties. Existing deep learning (DL)-based beam alignment methods often
neglect the underlying causal relationships between inputs and outputs, leading
to limited interpretability, poor generalization, and unnecessary beam sweeping
overhead. In this work, we propose a causally-aware DL framework that
integrates causal discovery into beam management pipeline. Particularly, we
propose a novel two-stage causal beam selection algorithm to identify a minimal
set of relevant inputs for beam prediction. First, causal discovery learns a
Bayesian graph capturing dependencies between received power inputs and the
optimal beam. Then, this graph guides causal feature selection for the DL-based
classifier. Simulation results reveal that the proposed causal beam selection
matches the performance of conventional methods while drastically reducing
input selection time by 94.4% and beam sweeping overhead by 59.4% by focusing
only on causally relevant features.

</details>
