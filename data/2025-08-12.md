<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Solving Pasur Using GPU-Accelerated Counterfactual Regret Minimization](https://arxiv.org/abs/2508.06559)
*Sina Baghal*

Main category: cs.AI

TL;DR: 利用CUDA加速的计算框架，通过CFR算法求解Pasur游戏的近似纳什均衡策略，并训练树模型进行预测，最终估算每副牌的公平价值。


<details>
  <summary>Details</summary>
Motivation: Pasur游戏规则复杂且游戏树巨大，现有方法难以有效求解。

Method: 使用PyTorch CUDA张量处理游戏规则复杂性，将游戏树分解为游戏状态和继承分数两部分，采用逐轮反向训练策略，并使用树模型预测策略。

Result: 构建了一个完整的包含超过10^9个节点的游戏树，计算了近似纳什均衡策略，并训练了一个树模型用于游戏策略预测，估算了每副牌的公平价值。

Conclusion: 本文介绍了一个CUDA加速的Pasur游戏模拟计算框架，并利用反事实后悔最小化算法(CFR)计算了近似纳什均衡策略，最后训练了一个树模型来预测这些策略，并通过大规模自博弈估计了每副牌的公平价值。

Abstract: Pasur is a fishing card game played over six rounds and is played similarly
to games such as Cassino and Scopa, and Bastra. This paper introduces a
CUDA-accelerated computational framework for simulating Pasur, emphasizing
efficient memory management. We use our framework to compute near-Nash
equilibria via Counterfactual Regret Minimization (CFR), a well-known algorithm
for solving large imperfect-information games.
  Solving Pasur presents unique challenges due to its intricate rules and the
large size of its game tree. We handle rule complexity using PyTorch CUDA
tensors and to address the memory-intensive nature of the game, we decompose
the game tree into two key components: (1) actual game states, and (2)
inherited scores from previous rounds. We construct the Full Game Tree by
pairing card states with accumulated scores in the Unfolding Process. This
design reduces memory overhead by storing only essential strategy values and
node connections. To further manage computational complexity, we apply a
round-by-round backward training strategy, starting from the final round and
recursively propagating average utilities to earlier stages. Our approach
constructs the complete game tree, which on average consists of over $10^9$
nodes. We provide detailed implementation snippets.
  After computing a near-Nash equilibrium strategy, we train a tree-based model
to predict these strategies for use during gameplay. We then estimate the fair
value of each deck through large-scale self-play between equilibrium strategies
by simulating, for instance, 10,000 games per matchup, executed in parallel
using GPU acceleration.
  Similar frameworks can be extended to other reinforcement learning algorithms
where the action tree naturally decomposes into multiple rounds such as
turn-based strategy games or sequential trading decisions in financial markets.

</details>


### [2] [Operationalizing Serendipity: Multi-Agent AI Workflows for Enhanced Materials Characterization with Theory-in-the-Loop](https://arxiv.org/abs/2508.06569)
*Lance Yao,Suman Samantray,Ayana Ghosh,Kevin Roccapriore,Libor Kovarik,Sarah Allec,Maxim Ziatdinov*

Main category: cs.AI

TL;DR: SciLink框架自动化材料研究中的偶然发现，提高效率并促进开放式探索。


<details>
  <summary>Details</summary>
Motivation: 现代自动化实验室虽然高效，但可能忽略意外发现。SciLink旨在弥补这一缺陷，将偶然性纳入材料研究中。

Method: SciLink是一个开源的多智能体人工智能框架，采用混合AI策略，利用机器学习模型进行定量数据分析，利用大型语言模型进行高级推理，最终生成可证伪的科学论断并评估其新颖性。

Result: SciLink成功应用于原子分辨率和高光谱数据分析，能够整合实时人工专家指导，并提出后续实验建议，有效提升了材料研究效率，并促进了偶然发现。

Conclusion: SciLink框架通过结合机器学习和大型语言模型，实现了实验观察、新颖性评估和理论模拟的自动化链接，从而在材料研究中有效地促进了偶然发现。

Abstract: The history of science is punctuated by serendipitous discoveries, where
unexpected observations, rather than targeted hypotheses, opened new fields of
inquiry. While modern autonomous laboratories excel at accelerating hypothesis
testing, their optimization for efficiency risks overlooking these crucial,
unplanned findings. To address this gap, we introduce SciLink, an open-source,
multi-agent artificial intelligence framework designed to operationalize
serendipity in materials research by creating a direct, automated link between
experimental observation, novelty assessment, and theoretical simulations. The
framework employs a hybrid AI strategy where specialized machine learning
models perform quantitative analysis of experimental data, while large language
models handle higher-level reasoning. These agents autonomously convert raw
data from materials characterization techniques into falsifiable scientific
claims, which are then quantitatively scored for novelty against the published
literature. We demonstrate the framework's versatility across diverse research
scenarios, showcasing its application to atomic-resolution and hyperspectral
data, its capacity to integrate real-time human expert guidance, and its
ability to close the research loop by proposing targeted follow-up experiments.
By systematically analyzing all observations and contextualizing them, SciLink
provides a practical framework for AI-driven materials research that not only
enhances efficiency but also actively cultivates an environment ripe for
serendipitous discoveries, thereby bridging the gap between automated
experimentation and open-ended scientific exploration.

</details>


### [3] [IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model](https://arxiv.org/abs/2508.06571)
*Anqing Jiang,Yu Gao,Yiru Wang,Zhigang Sun,Shuo Wang,Yuwen Heng,Hao Sun,Shichen Tang,Lijuan Zhu,Jinhao Chai,Jijun Wang,Zichong Gu,Hao Jiang,Li Sun*

Main category: cs.AI

TL;DR: 提出一种新的闭环强化学习方法IRL-VLA，在自动驾驶基准测试中取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA架构通常基于开环设置下的模仿学习，存在性能次优和受限的问题；闭环训练依赖高保真传感器模拟，存在领域差异和计算效率低下的问题。

Method: 提出了一种新的闭环强化学习方法IRL-VLA，该方法结合了逆强化学习奖励世界模型和自建VLA方法，并采用三阶段范式进行训练：模仿学习预训练、逆强化学习构建轻量级奖励世界模型和基于PPO的强化学习以平衡安全性、舒适性和效率。

Result: 在NAVSIM v2端到端驾驶基准测试中取得了最先进的性能，并在CVPR2025自动驾驶大挑战中获得亚军。

Conclusion: IRL-VLA模型在NAVSIM v2端到端驾驶基准测试中取得了最先进的性能，并在CVPR2025自动驾驶大挑战中获得亚军。

Abstract: Vision-Language-Action (VLA) models have demonstrated potential in autonomous
driving. However, two critical challenges hinder their development: (1)
Existing VLA architectures are typically based on imitation learning in
open-loop setup which tends to capture the recorded behaviors in the dataset,
leading to suboptimal and constrained performance, (2) Close-loop training
relies heavily on high-fidelity sensor simulation, where domain gaps and
computational inefficiencies pose significant barriers. In this paper, we
introduce IRL-VLA, a novel close-loop Reinforcement Learning via
\textbf{I}nverse \textbf{R}einforcement \textbf{L}earning reward world model
with a self-built VLA approach. Our framework proceeds in a three-stage
paradigm: In the first stage, we propose a VLA architecture and pretrain the
VLA policy via imitation learning. In the second stage, we construct a
lightweight reward world model via inverse reinforcement learning to enable
efficient close-loop reward computation. To further enhance planning
performance, finally, we design specialized reward world model guidence
reinforcement learning via PPO(Proximal Policy Optimization) to effectively
balance the safety incidents, comfortable driving, and traffic efficiency. Our
approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving
benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that
our framework will accelerate VLA research in close-loop autonomous driving.

</details>


### [4] [CountQA: How Well Do MLLMs Count in the Wild?](https://arxiv.org/abs/2508.06585)
*Jayant Sravan Tamarapalli,Rynaa Grover,Nilay Pande,Sahiti Yerramilli*

Main category: cs.AI

TL;DR: 多模态大型语言模型在物体计数方面表现不佳，CountQA基准测试揭示了这一缺陷，为改进MLLM提供了方向。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试要么物体密度稀疏，要么局限于特定的视觉领域，无法在现实条件下测试模型，因此本文创建CountQA基准测试来评估MLLM在物体计数方面的能力。

Method: 创建了一个名为CountQA的基准测试，包含1500多个问题-答案对，使用具有高物体密度、杂乱和遮挡的真实世界图像来评估15个突出的MLLM。

Result: 顶级模型在CountQA基准测试上的准确率仅为42.9%，并且性能随着物体数量的增加而下降。

Conclusion: CountQA基准测试揭示了多模态大型语言模型(MLLM)在物体计数方面的关键缺陷，即使是顶级模型的准确率也只有42.9%，并且随着物体数量的增加而下降。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in
understanding visual scenes, yet they exhibit a critical lack in a fundamental
cognitive skill: object counting. This blind spot severely limits their
reliability in real-world applications. To date, this capability has been
largely unevaluated in complex scenarios, as existing benchmarks either feature
sparse object densities or are confined to specific visual domains, failing to
test models under realistic conditions. Addressing this gap, we introduce
CountQA, a challenging new benchmark designed to probe this deficiency.
Comprising over 1,500 question-answer pairs, CountQA features real-world images
with high object density, clutter, and occlusion. We investigate this weakness
by evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the
top-performing model achieves a mere 42.9% accuracy, with performance declining
as object counts rise. By providing a dedicated benchmark to diagnose and
rectify this core weakness, CountQA paves the way for a new generation of MLLMs
that are not only descriptively fluent but also numerically grounded and
spatially aware. We will open-source the dataset and code upon paper acceptance
to foster further research.

</details>


### [5] [Formal Concept Analysis: a Structural Framework for Variability Extraction and Analysis](https://arxiv.org/abs/2508.06668)
*Jessie Galasso*

Main category: cs.AI

TL;DR: 本文阐述了形式概念分析在变异性分析中的应用，总结了其关键属性和使用方法。


<details>
  <summary>Details</summary>
Motivation: FCA具有提取和分析变异性的潜力，但其数学导向的文献使得确定其哪些属性可用于变异性相关任务并不总是直截了当的。

Method: 本文通过收集FCA框架中一些与变异性分析相关的属性，并解释如何利用这些属性来解释结果概念结构中的各种变异信息，来弥合FCA的数学理论和变异性分析应用之间的差距。

Result: 本文识别并解释了FCA框架中一些关键属性在变异性分析中的作用。

Conclusion: 本文总结了形式概念分析(FCA)框架中用于变异性分析的关键属性及其在解释不同变异信息中的应用。

Abstract: Formal Concept Analysis (FCA) is a mathematical framework for knowledge
representation and discovery. It performs a hierarchical clustering over a set
of objects described by attributes, resulting in conceptual structures in which
objects are organized depending on the attributes they share. These conceptual
structures naturally highlight commonalities and variabilities among similar
objects by categorizing them into groups which are then arranged by similarity,
making it particularly appropriate for variability extraction and analysis.
Despite the potential of FCA, determining which of its properties can be
leveraged for variability-related tasks (and how) is not always
straightforward, partly due to the mathematical orientation of its foundational
literature. This paper attempts to bridge part of this gap by gathering a
selection of properties of the framework which are essential to variability
analysis, and how they can be used to interpret diverse variability information
within the resulting conceptual structures.

</details>


### [6] [Zero-Shot Cellular Trajectory Map Matching](https://arxiv.org/abs/2508.06674)
*Weijie Shi,Yue Cui,Hao Chen,Jiaming Li,Mengze Li,Jia Zhu,Jiajie Xu,Xiaofang Zhou*

Main category: cs.AI

TL;DR: 利用可迁移地理空间知识和时空感知模块，提出一种基于像素的轨迹校准辅助工具，提升零样本CTMM精度，优于现有方法16.8%。


<details>
  <summary>Details</summary>
Motivation: 现有的CTMM方法主要依赖于基于ID的特征和特定区域的数据，难以适应未探索区域。零样本CTMM需要提取区域自适应特征、序列特征和位置不确定性来减轻蜂窝数据中的定位误差。

Method: 提出了一种基于像素的轨迹校准辅助工具，该工具利用可迁移的地理空间知识，并结合高斯混合模型和时空感知模块，最终使用约束路径查找算法重建道路ID序列。

Result: 该模型在零样本CTMM方面优于现有方法16.8%。

Conclusion: 提出了一种基于像素的轨迹校准辅助工具，用于零样本CTMM，该工具利用可迁移的地理空间知识来校准像素化轨迹，然后指导道路网络级别的路径查找过程。该模型通过将高斯混合模型融入VAE，实现了跨相似区域的知识共享，并设计了时空感知模块以减轻高定位误差。最终，该方法优于现有方法16.8%。

Abstract: Cellular Trajectory Map-Matching (CTMM) aims to align cellular location
sequences to road networks, which is a necessary preprocessing in
location-based services on web platforms like Google Maps, including navigation
and route optimization. Current approaches mainly rely on ID-based features and
region-specific data to learn correlations between cell towers and roads,
limiting their adaptability to unexplored areas. To enable high-accuracy CTMM
without additional training in target regions, Zero-shot CTMM requires to
extract not only region-adaptive features, but also sequential and location
uncertainty to alleviate positioning errors in cellular data. In this paper, we
propose a pixel-based trajectory calibration assistant for zero-shot CTMM,
which takes advantage of transferable geospatial knowledge to calibrate
pixelated trajectory, and then guide the path-finding process at the road
network level. To enhance knowledge sharing across similar regions, a Gaussian
mixture model is incorporated into VAE, enabling the identification of
scenario-adaptive experts through soft clustering. To mitigate high positioning
errors, a spatial-temporal awareness module is designed to capture sequential
features and location uncertainty, thereby facilitating the inference of
approximate user positions. Finally, a constrained path-finding algorithm is
employed to reconstruct the road ID sequence, ensuring topological validity
within the road network. This process is guided by the calibrated trajectory
while optimizing for the shortest feasible path, thus minimizing unnecessary
detours. Extensive experiments demonstrate that our model outperforms existing
methods in zero-shot CTMM by 16.8\%.

</details>


### [7] [Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets](https://arxiv.org/abs/2508.06706)
*Jaikrishna Manojkumar Patil,Nathaniel Lee,Al Mehdi Saadat Chowdhury,YooJung Choi,Paulo Shakarian*

Main category: cs.AI

TL;DR: 该研究提出一种新方法，通过学习规则上下文上的概率分布，减少基于规则的知识图谱补全中所需规则的数量，同时保持或提高性能。


<details>
  <summary>Details</summary>
Motivation: 基于规则的知识图谱补全方法虽然结果可解释，但通常需要大量的规则才能达到具有竞争力的性能，这阻碍了可解释性。

Method: 该方法通过从训练数据中发现规则上下文（一起工作的有意义的规则子集），并使用这些规则上下文上的学习概率分布（即概率电路）来实现这一目标。

Result: 该方法将规则数量减少了70-96%，同时在使用相同最小数量规则时，性能比基线高出31倍，即使与基线的完整规则集相比，也能保持91%的峰值基线性能。

Conclusion: 该方法在8个标准基准数据集上取得了具有竞争力的性能，仅使用了AnyBURL标准推理方法所需规则的一小部分。

Abstract: Rule-based methods for knowledge graph completion provide explainable results
but often require a significantly large number of rules to achieve competitive
performance. This can hinder explainability due to overwhelmingly large rule
sets. We discover rule contexts (meaningful subsets of rules that work
together) from training data and use learned probability distribution (i.e.
probabilistic circuits) over these rule contexts to more rapidly achieve
performance of the full rule set. Our approach achieves a 70-96% reduction in
number of rules used while outperforming baseline by up to 31$\times$ when
using equivalent minimal number of rules and preserves 91% of peak baseline
performance even when comparing our minimal rule sets against baseline's full
rule sets. We show that our framework is grounded in well-known semantics of
probabilistic logic, does not require independence assumptions, and that our
tractable inference procedure provides both approximate lower bounds and exact
probability of a given query. The efficacy of our method is validated by
empirical studies on 8 standard benchmark datasets where we show competitive
performance by using only a fraction of the rules required by AnyBURL's
standard inference method, the current state-of-the-art for rule-based
knowledge graph completion. This work may have further implications for general
probabilistic reasoning over learned sets of rules.

</details>


### [8] [GLIDR: Graph-Like Inductive Logic Programming with Differentiable Reasoning](https://arxiv.org/abs/2508.06716)
*Blair Johnson,Clayton Kerce,Faramarz Fekri*

Main category: cs.AI

TL;DR: GLIDR是一种新的可微分规则学习方法，在知识图谱补全任务中表现优异，并具有良好的可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的可微分归纳逻辑编程技术通常假设链式规则结构，这会影响性能和可解释性。

Method: GLIDR使用可微分的消息传递推理算法，该算法将以前的链式规则学习方法推广到允许具有分支和循环等特征的规则。

Result: GLIDR在知识图谱补全任务上显著优于现有规则学习方法，并且具有很强的鲁棒性。可以从GLIDR中提取的规则保留了显著的预测性能。

Conclusion: GLIDR,一种新的可微分规则学习方法，在知识图谱补全任务中显著优于现有规则学习方法，甚至可以与嵌入方法竞争。GLIDR具有简单的规则搜索空间，并且可以提取显式逻辑规则。

Abstract: Differentiable inductive logic programming (ILP) techniques have proven
effective at finding approximate rule-based solutions to link prediction and
node classification problems on knowledge graphs; however, the common
assumption of chain-like rule structure can hamper the performance and
interpretability of existing approaches. We introduce GLIDR, a differentiable
rule learning method that models the inference of logic rules with more
expressive syntax than previous methods. GLIDR uses a differentiable message
passing inference algorithm that generalizes previous chain-like rule learning
methods to allow rules with features like branches and cycles. GLIDR has a
simple and expressive rule search space which is parameterized by a limit on
the maximum number of free variables that may be included in a rule. Explicit
logic rules can be extracted from the weights of a GLIDR model for use with
symbolic solvers. We demonstrate that GLIDR can significantly outperform
existing rule learning methods on knowledge graph completion tasks and even
compete with embedding methods despite the inherent disadvantage of being a
structure-only prediction method. We show that rules extracted from GLIDR
retain significant predictive performance, and that GLIDR is highly robust to
training data noise. Finally, we demonstrate that GLIDR can be chained with
deep neural networks and optimized end-to-end for rule learning on arbitrary
data modalities.

</details>


### [9] [ParBalans: Parallel Multi-Armed Bandits-based Adaptive Large Neighborhood Search](https://arxiv.org/abs/2508.06736)
*Alican Yilmaz,Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: ParBalans通过并行化改进MIP求解性能，在基准测试中与Gurobi竞争。


<details>
  <summary>Details</summary>
Motivation: 解决MIP问题通常需要大量的计算资源，并行化是加速求解时间和提高可扩展性的关键策略。

Method: ParBalans利用求解器级和算法级并行化来改进性能。

Result: 实验结果表明ParBalans的性能优于Gurobi。

Conclusion: ParBalans，一种基于多臂老虎机自适应大邻域搜索的MIP求解器并行化扩展，在具有挑战性的MIP实例上展现出与最先进的商业求解器Gurobi相比具有竞争力的性能。

Abstract: Solving Mixed-Integer Programming (MIP) problems often requires substantial
computational resources due to their combinatorial nature. Parallelization has
emerged as a critical strategy to accelerate solution times and enhance
scalability to tackle large, complex instances. This paper investigates the
parallelization capabilities of Balans, a recently proposed multi-armed
bandits-based adaptive large neighborhood search for MIPs. While Balans's
modular architecture inherently supports parallel exploration of diverse
parameter configurations, this potential has not been thoroughly examined. To
address this gap, we introduce ParBalans, an extension that leverages both
solver-level and algorithmic-level parallelism to improve performance on
challenging MIP instances. Our experimental results demonstrate that ParBalans
exhibits competitive performance compared to the state-of-the-art commercial
solver Gurobi, particularly on hard optimization benchmarks.

</details>


### [10] [Topology Generation of UAV Covert Communication Networks: A Graph Diffusion Approach with Incentive Mechanism](https://arxiv.org/abs/2508.06746)
*Xin Tang,Qian Chen,Fengshun Li,Youchun Gong,Yinqiu Liu,Wen Tian,Shaowen Qin,Xiaohuan Li*

Main category: cs.AI

TL;DR: 提出一种基于GDPO和SG的无人机网络自组织框架，以增强可靠连接和隐蔽通信。


<details>
  <summary>Details</summary>
Motivation: 日益增长的无人机网络需求以及在城市监控、应急响应和安全传感等敏感应用中确保可靠连接和隐蔽通信的必要性。

Method: 采用生成式AI动态生成稀疏但连接良好的拓扑结构，并利用基于Stackelberg博弈的激励机制引导无人机选择支持合作并增强隐蔽通信的中继行为和相邻链路。

Result: 实验验证了该框架在模型收敛性、拓扑生成质量和隐蔽通信性能增强方面的有效性。

Conclusion: 提出了一种结合图扩散策略优化(GDPO)和基于Stackelberg博弈(SG)激励机制的无人机网络自组织框架，用于解决动态移动和暴露风险带来的挑战，以确保可靠的连接和隐蔽通信。

Abstract: With the growing demand for Uncrewed Aerial Vehicle (UAV) networks in
sensitive applications, such as urban monitoring, emergency response, and
secure sensing, ensuring reliable connectivity and covert communication has
become increasingly vital. However, dynamic mobility and exposure risks pose
significant challenges. To tackle these challenges, this paper proposes a
self-organizing UAV network framework combining Graph Diffusion-based Policy
Optimization (GDPO) with a Stackelberg Game (SG)-based incentive mechanism. The
GDPO method uses generative AI to dynamically generate sparse but
well-connected topologies, enabling flexible adaptation to changing node
distributions and Ground User (GU) demands. Meanwhile, the Stackelberg Game
(SG)-based incentive mechanism guides self-interested UAVs to choose relay
behaviors and neighbor links that support cooperation and enhance covert
communication. Extensive experiments are conducted to validate the
effectiveness of the proposed framework in terms of model convergence, topology
generation quality, and enhancement of covert communication performance.

</details>


### [11] [Pushing the Envelope of LLM Inference on AI-PC](https://arxiv.org/abs/2508.06753)
*Evangelos Georganas,Dhiraj Kalamkar,Alexander Heinecke*

Main category: cs.AI

TL;DR: 通过优化微内核和集成到PyTorch-TPP框架，实现了超低比特LLM模型的高效推理，速度显著提升。


<details>
  <summary>Details</summary>
Motivation: 探索超低比特LLM模型的计算效率，以提高资源受限环境下LLM推理的效率。

Method: 设计并实现1比特和2比特微内核，集成到PyTorch-TPP框架。

Result: 2比特模型端到端推理性能优于bitnet.cpp，速度提升高达2.2倍；相比16比特模型推理速度提升高达7倍。

Conclusion: 这项工作设计并实现了针对现代CPU优化的1比特和2比特微内核，并将这些微内核集成到最先进的LLM推理框架PyTorch-TPP中，最终结果表明，使用2比特模型的端到端推理性能优于当前最先进的运行时bitnet.cpp，速度提升高达2.2倍，相比16比特模型推理速度提升高达7倍。

Abstract: The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the
perplexity and end-task performance of their full-precision counterparts using
the same model size, is ushering in a new era of LLM inference for
resource-constrained environments such as edge devices and AI PCs. While these
quantization advances promise models that are more cost-effective in terms of
latency, memory, throughput, and energy consumption, the computational
efficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp)
used to deploy them remains underexplored. In this work, we take a bottom-up
approach: we first design and implement 1-bit and 2-bit microkernels optimized
for modern CPUs, achieving peak computational efficiency across a variety of
CPU platforms. We integrate these microkernels into a state-of-the-art LLM
inference framework, namely PyTorch-TPP, and present end-to-end inference
results with 2-bit models that outperform the current SOTA runtime bitnet.cpp
by up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model
inference. Our optimized runtime advances the state of LLM inference on AI PCs
and edge devices, paving the way for efficient deployment of ultra-low-bit LLM
models.

</details>


### [12] [A Fuzzy Logic Prompting Framework for Large Language Models in Adaptive and Uncertain Tasks](https://arxiv.org/abs/2508.06754)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: 模块化提示框架使LLM更安全、更自适应地处理用户中心任务，在模拟智能辅导环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 基于人类学习理论（特别是最近发展区），旨在构建一个安全、可解释、目标一致的LLM行为结构，以应对不确定或不断变化的环境。

Method: 该方法结合自然语言边界提示、模糊脚手架逻辑和自适应规则编码的控制模式，使LLM能够根据用户状态调节行为，无需微调或外部协调。

Result: 该框架提高了脚手架质量、适应性和教学一致性，并在教育和游戏程序内容生成等多个领域展现出潜力。

Conclusion: 该论文介绍了一种模块化的提示框架，用于更安全、更自适应地使用大型语言模型 (LLM) 处理动态的用户中心任务，并在模拟的智能辅导环境中得到验证，优于标准提示基线。

Abstract: We introduce a modular prompting framework that supports safer and more
adaptive use of large language models (LLMs) across dynamic, user-centered
tasks. Grounded in human learning theory, particularly the Zone of Proximal
Development (ZPD), our method combines a natural language boundary prompt with
a control schema encoded with fuzzy scaffolding logic and adaptation rules.
This architecture enables LLMs to modulate behavior in response to user state
without requiring fine-tuning or external orchestration. In a simulated
intelligent tutoring setting, the framework improves scaffolding quality,
adaptivity, and instructional alignment across multiple models, outperforming
standard prompting baselines. Evaluation is conducted using rubric-based LLM
graders at scale. While initially developed for education, the framework has
shown promise in other interaction-heavy domains, such as procedural content
generation for games. Designed for safe deployment, it provides a reusable
methodology for structuring interpretable, goal-aligned LLM behavior in
uncertain or evolving contexts.

</details>


### [13] [Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation](https://arxiv.org/abs/2508.06823)
*Xuan Zhao,Jun Tao*

Main category: cs.AI

TL;DR: 利用自然语言交互和强化学习，自动选择最佳视角以高效探索体数据。


<details>
  <summary>Details</summary>
Motivation: 探索体数据对于解释科学数据集至关重要，但选择最佳视角具有挑战性，尤其对于缺乏专业知识的用户。

Method: 该方法编码体块以捕捉和区分底层结构，结合CLIP评分机制提供语义信息指导导航，并利用强化学习框架高效搜索和识别符合用户意图的视角。

Result: 该方法自动化视角选择，提高了体数据导航效率和复杂科学现象的可解释性。

Conclusion: 该论文提出了一种利用自然语言交互增强体数据探索的新框架，通过编码体块并结合CLIP评分机制和强化学习，自动选择最佳视角，提高了体数据导航效率和复杂科学现象的可解释性。

Abstract: Exploring volumetric data is crucial for interpreting scientific datasets.
However, selecting optimal viewpoints for effective navigation can be
challenging, particularly for users without extensive domain expertise or
familiarity with 3D navigation. In this paper, we propose a novel framework
that leverages natural language interaction to enhance volumetric data
exploration. Our approach encodes volumetric blocks to capture and
differentiate underlying structures. It further incorporates a CLIP Score
mechanism, which provides semantic information to the blocks to guide
navigation. The navigation is empowered by a reinforcement learning framework
that leverage these semantic cues to efficiently search for and identify
desired viewpoints that align with the user's intent. The selected viewpoints
are evaluated using CLIP Score to ensure that they best reflect the user
queries. By automating viewpoint selection, our method improves the efficiency
of volumetric data navigation and enhances the interpretability of complex
scientific phenomena.

</details>


### [14] [Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges](https://arxiv.org/abs/2508.06832)
*Haifeng Li,Wang Guo,Haiyang Wu,Mengwei Wu,Jipeng Zhang,Qing Zhu,Yu Liu,Xin Huang,Chao Tao*

Main category: cs.AI

TL;DR: 该论文提出将大型语言模型作为核心组件，构建一个语言中心的遥感影像解译框架，以应对现有视觉中心模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的遥感影像解译模型主要依赖视觉特征，难以处理多模态推理、语义抽象和交互式决策。

Method: 该论文基于全局工作空间理论 (GWT)，构建了一个全局工作空间驱动的解译机制，并从四个角度（多模态数据的自适应对齐、动态知识约束下的任务理解、可信推理和自主交互）概述了未来的研究方向。

Result: 提出了一种以语言为中心的遥感影像解译框架，并指出了核心技术挑战和未来的研究方向。

Conclusion: 该论文倡导从视觉中心转向语言中心的遥感影像解译范式转变，提出了一种以语言为中心的框架，利用大型语言模型 (LLM) 作为认知中心枢纽，集成感知、任务、知识和行动空间，实现统一的理解、推理和决策。

Abstract: The mainstream paradigm of remote sensing image interpretation has long been
dominated by vision-centered models, which rely on visual features for semantic
understanding. However, these models face inherent limitations in handling
multi-modal reasoning, semantic abstraction, and interactive decision-making.
While recent advances have introduced Large Language Models (LLMs) into remote
sensing workflows, existing studies primarily focus on downstream applications,
lacking a unified theoretical framework that explains the cognitive role of
language. This review advocates a paradigm shift from vision-centered to
language-centered remote sensing interpretation. Drawing inspiration from the
Global Workspace Theory (GWT) of human cognition, We propose a
language-centered framework for remote sensing interpretation that treats LLMs
as the cognitive central hub integrating perceptual, task, knowledge and action
spaces to enable unified understanding, reasoning, and decision-making. We
first explore the potential of LLMs as the central cognitive component in
remote sensing interpretation, and then summarize core technical challenges,
including unified multimodal representation, knowledge association, and
reasoning and decision-making. Furthermore, we construct a global
workspace-driven interpretation mechanism and review how language-centered
solutions address each challenge. Finally, we outline future research
directions from four perspectives: adaptive alignment of multimodal data, task
understanding under dynamic knowledge constraints, trustworthy reasoning, and
autonomous interaction. This work aims to provide a conceptual foundation for
the next generation of remote sensing interpretation systems and establish a
roadmap toward cognition-driven intelligent geospatial analysis.

</details>


### [15] [Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.06836)
*Xutong Zhao,Yaqi Xie*

Main category: cs.AI

TL;DR: 针对MARL中信用分配问题，提出一种多层次优势信用分配(MACA)方法，在星际争霸任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: MARL中的信用分配问题，特别是奖励归因于不同且经常重叠的代理子集的情况。

Method: 提出了一种多层次优势信用分配(MACA)方法，该方法利用基于注意力的框架识别相关的代理关系，并构建多层次优势来指导策略学习。MACA通过整合考虑个体、联合和相关行为的优势函数，捕获多个层次上的代理贡献。

Result: 在具有挑战性的星际争霸v1&v2任务上进行了综合实验，结果表明MACA方法的有效性。

Conclusion: MACA方法在具有挑战性的星际争霸v1&v2任务中表现出优越的性能，有效解决了复杂信用分配场景。

Abstract: Cooperative multi-agent reinforcement learning (MARL) aims to coordinate
multiple agents to achieve a common goal. A key challenge in MARL is credit
assignment, which involves assessing each agent's contribution to the shared
reward. Given the diversity of tasks, agents may perform different types of
coordination, with rewards attributed to diverse and often overlapping agent
subsets. In this work, we formalize the credit assignment level as the number
of agents cooperating to obtain a reward, and address scenarios with multiple
coexisting levels. We introduce a multi-level advantage formulation that
performs explicit counterfactual reasoning to infer credits across distinct
levels. Our method, Multi-level Advantage Credit Assignment (MACA), captures
agent contributions at multiple levels by integrating advantage functions that
reason about individual, joint, and correlated actions. Utilizing an
attention-based framework, MACA identifies correlated agent relationships and
constructs multi-level advantages to guide policy learning. Comprehensive
experiments on challenging Starcraft v1\&v2 tasks demonstrate MACA's superior
performance, underscoring its efficacy in complex credit assignment scenarios.

</details>


### [16] [MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams](https://arxiv.org/abs/2508.06851)
*Pengfei Zhou,Xiaopeng Peng,Fanrui Zhang,Zhaopan Xu,Jiaxin Ai,Yansheng Qiu,Chuanhao Li,Zhen Li,Ming Li,Yukang Feng,Jianwen Sun,Haoquan Zhang,Zizhen Li,Xiaofeng Mao,Zekai Li,Wangbo Zhao,Kai Wang,Xiaojun Chang,Wenqi Shao,Yang You,Kaipeng Zhang*

Main category: cs.AI

TL;DR: MDK12-Bench基准测试揭示了多模态大型语言模型的局限性，并为改进模型和人工智能辅助教育提供了方向。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM评估基准存在规模有限、覆盖面窄和知识非结构化等问题。

Method: 构建了一个包含141K个实例和6,225个知识点的大规模多学科基准MDK12-Bench，并提出了一种动态评估框架。

Result: 评估结果揭示了当前MLLMs在多个方面的局限性，并为改进模型的鲁棒性、可解释性和人工智能辅助教育提供了指导。

Conclusion: MDK12-Bench，一个大规模的多学科基准，用于评估多模态大型语言模型 (MLLM) 的能力，揭示了当前MLLM的局限性，并为增强模型鲁棒性、可解释性和人工智能辅助教育提供了指导。

Abstract: Multimodal large language models (MLLMs), which integrate language and visual
cues for problem-solving, are crucial for advancing artificial general
intelligence (AGI). However, current benchmarks for measuring the intelligence
of MLLMs suffer from limited scale, narrow coverage, and unstructured
knowledge, offering only static and undifferentiated evaluations. To bridge
this gap, we introduce MDK12-Bench, a large-scale multidisciplinary benchmark
built from real-world K-12 exams spanning six disciplines with 141K instances
and 6,225 knowledge points organized in a six-layer taxonomy. Covering five
question formats with difficulty and year annotations, it enables comprehensive
evaluation to capture the extent to which MLLMs perform over four dimensions:
1) difficulty levels, 2) temporal (cross-year) shifts, 3) contextual shifts,
and 4) knowledge-driven reasoning. We propose a novel dynamic evaluation
framework that introduces unfamiliar visual, textual, and question form shifts
to challenge model generalization while improving benchmark objectivity and
longevity by mitigating data contamination. We further evaluate knowledge-point
reference-augmented generation (KP-RAG) to examine the role of knowledge in
problem-solving. Key findings reveal limitations in current MLLMs in multiple
aspects and provide guidance for enhancing model robustness, interpretability,
and AI-assisted education.

</details>
