<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Sycophancy as compositions of Atomic Psychometric Traits](https://arxiv.org/abs/2508.19316)
*Shreyans Jain,Alexandra Yost,Amirali Abdullah*

Main category: cs.AI

TL;DR: 将LLM谄媚行为建模为心理测量特征的组合，并提出基于向量的干预方法以减轻安全风险。


<details>
  <summary>Details</summary>
Motivation: LLM中的谄媚行为是一个关键的安全风险，但通常被视为通过单一因果机制发生的孤立故障模式。

Method: 使用对比激活添加（CAA）将激活方向映射到这些因素，并研究不同的组合如何导致谄媚行为。

Result: 该方法允许进行可解释的基于向量的组合干预，例如加法、减法和投影，这些干预可用于减轻LLM中的安全关键行为。

Conclusion: 该论文提出了一种将LLM的谄媚行为建模为心理测量特征（如情绪化、开放性和随和性）几何和因果组合的方法，并使用对比激活添加（CAA）来研究不同组合如何导致谄媚行为。

Abstract: Sycophancy is a key behavioral risk in LLMs, yet is often treated as an
isolated failure mode that occurs via a single causal mechanism. We instead
propose modeling it as geometric and causal compositions of psychometric traits
such as emotionality, openness, and agreeableness - similar to factor
decomposition in psychometrics. Using Contrastive Activation Addition (CAA), we
map activation directions to these factors and study how different combinations
may give rise to sycophancy (e.g., high extraversion combined with low
conscientiousness). This perspective allows for interpretable and compositional
vector-based interventions like addition, subtraction and projection; that may
be used to mitigate safety-critical behaviors in LLMs.

</details>


### [2] [Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science](https://arxiv.org/abs/2508.19383)
*Daoyuan Jin,Nick Gunner,Niko Carvajal Janke,Shivranjani Baruah,Kaitlin M. Gold,Yu Jiang*

Main category: cs.AI

TL;DR: AI系统Aleks自主进行科学发现，在葡萄红斑病研究中取得成功，展现了AI在植物科学研究中的巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 解决现代植物科学中大型异构数据集带来的实验设计、数据预处理和可重复性挑战，以加速科学发现。

Method: 整合领域知识、数据分析和机器学习，迭代式地制定问题、探索建模策略和优化解决方案。

Result: 在葡萄红斑病案例研究中，Aleks成功识别出具有生物学意义的特征，并构建出具有鲁棒性的可解释模型。消融实验证明了领域知识和记忆的重要性。

Conclusion: Aleks，一个AI驱动的多智能体系统，能够自主进行数据驱动下的科学发现，在葡萄红斑病案例研究中表现出色，其有效性有赖于领域知识和记忆功能。

Abstract: Modern plant science increasingly relies on large, heterogeneous datasets,
but challenges in experimental design, data preprocessing, and reproducibility
hinder research throughput. Here we introduce Aleks, an AI-powered multi-agent
system that integrates domain knowledge, data analysis, and machine learning
within a structured framework to autonomously conduct data-driven scientific
discovery. Once provided with a research question and dataset, Aleks
iteratively formulated problems, explored alternative modeling strategies, and
refined solutions across multiple cycles without human intervention. In a case
study on grapevine red blotch disease, Aleks progressively identified
biologically meaningful features and converged on interpretable models with
robust performance. Ablation studies underscored the importance of domain
knowledge and memory for coherent outcomes. This exploratory work highlights
the promise of agentic AI as an autonomous collaborator for accelerating
scientific discovery in plant sciences.

</details>


### [3] [Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs](https://arxiv.org/abs/2508.19432)
*Yao Fu,Xianxuan Long,Runchao Li,Haotian Yu,Mu Sheng,Xiaotian Han,Yu Yin,Pan Li*

Main category: cs.AI

TL;DR: 量化LLM虽然在内部保持真实，但在具有欺骗性的提示下更容易产生虚假输出。


<details>
  <summary>Details</summary>
Motivation: 探索量化对大型语言模型真实性(生成真实或虚假回应)的影响。

Method: 提出TruthfulnessEval评估框架，评估量化LLM在逻辑推理、常识和模仿虚假陈述方面的真实性，并使用该框架评估主流量化技术对多个开源LLM的影响。

Result: 量化模型虽然保留了内部真实表征，但在误导性提示下更容易产生错误输出；‘欺骗性’提示可以覆盖其真实一致的行为，而‘诚实’和‘中性’提示则保持稳定输出。

Conclusion: 量化模型在保留内部真实性的同时，更容易受到误导性提示的影响而产生虚假输出。

Abstract: Quantization enables efficient deployment of large language models (LLMs) in
resource-constrained environments by significantly reducing memory and
computation costs. While quantized LLMs often maintain performance on
perplexity and zero-shot tasks, their impact on truthfulness-whether generating
truthful or deceptive responses-remains largely unexplored. In this work, we
introduce TruthfulnessEval, a comprehensive evaluation framework for assessing
the truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on
Logical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on
Imitative Falsehoods. Using this framework, we examine mainstream quantization
techniques (ranging from 4-bit to extreme 2-bit) across several open-source
LLMs. Surprisingly, we find that while quantized models retain internally
truthful representations, they are more susceptible to producing false outputs
under misleading prompts. To probe this vulnerability, we test 15 rephrased
variants of "honest", "neutral" and "deceptive" prompts and observe that
"deceptive" prompts can override truth-consistent behavior, whereas "honest"
and "neutral" prompts maintain stable outputs. Further, we reveal that
quantized models "know" the truth internally yet still produce false outputs
when guided by "deceptive" prompts via layer-wise probing and PCA
visualizations. Our findings provide insights into future designs of
quantization-aware alignment and truthfulness interventions.

</details>


### [4] [Reliable Weak-to-Strong Monitoring of LLM Agents](https://arxiv.org/abs/2508.19461)
*Neil Kale,Chen Bo Calvin Zhang,Kevin Zhu,Ankit Aich,Paula Rodriguez,Scale Red Team,Christina Q. Knight,Zifan Wang*

Main category: cs.AI

TL;DR: 该研究通过压力测试，发现现有LLM监控系统缺乏对抗鲁棒性，并提出了一种改进的混合监控脚手架，显著提升了监控效果。


<details>
  <summary>Details</summary>
Motivation: 对用于检测自主LLM智能体中隐蔽错误行为（例如秘密共享私人信息）的监控系统进行压力测试。

Method: 系统化了一个监控红队（MRT）工作流程，包含：不同级别的智能体和监控器的态势感知；不同的对抗策略以规避监控器，例如提示注入；以及两个数据集和环境。在现有的LLM监控脚手架上运行MRT，并提出了一个新的混合层次-顺序脚手架。

Result: 三个主要发现：1. 智能体的感知能力占据主导地位；2. 监控脚手架比监控感知更重要；3. 人工参与的监督最有效。

Conclusion: 该论文建立了一个标准的工作流程用于监控红队（MRT），突出了在监控和检测智能体错误行为时，大型语言模型（LLM）和人类缺乏对抗鲁棒性。

Abstract: We stress test monitoring systems for detecting covert misbehavior in
autonomous LLM agents (e.g., secretly sharing private information). To this
end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)
varying levels of agent and monitor situational awareness; (2) distinct
adversarial strategies to evade the monitor, such as prompt injection; and (3)
two datasets and environments -- SHADE-Arena for tool-calling agents and our
new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We
run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse
agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding
proposed in this work. Our empirical results yield three key findings. First,
agent awareness dominates monitor awareness: an agent's knowledge that it is
being monitored substantially degrades the monitor's reliability. On the
contrary, providing the monitor with more information about the agent is less
helpful than expected. Second, monitor scaffolding matters more than monitor
awareness: the hybrid scaffolding consistently outperforms baseline monitor
scaffolding, and can enable weaker models to reliably monitor stronger agents
-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where
humans discuss with the LLM monitor to get an updated judgment for the agent's
behavior, targeted human oversight is most effective; escalating only
pre-flagged cases to human reviewers improved the TPR by approximately 15% at
FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the
lack of adversarial robustness for LLMs and humans when monitoring and
detecting agent misbehavior. We release code, data, and logs to spur further
research.

</details>


### [5] [SLIM: Subtrajectory-Level Elimination for More Effective Reasoning](https://arxiv.org/abs/2508.19502)
*Xifeng Yao,Chengyuan Ma,Dongyu Lang,Yinhao Ni,Zhiwei Xu,Huarui Xie,Zihao Chen,Guang Shen,Dandan Tu,Yi Bai,Changzheng Zhang*

Main category: cs.AI

TL;DR: 通过识别并去除大型语言模型推理轨迹中的次优子轨迹，提高了模型的推理准确性和效率，尤其是在资源受限的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型虽然能够生成扩展的推理轨迹，但在推理过程中存在次优子轨迹，影响模型性能。

Method: 提出了一种“5+2”框架，用于系统地识别和评估推理轨迹中的次优子轨迹，并设计了一种采样算法，选择推理过程无次优子轨迹的数据进行微调。

Result: 该方法能够减少25.9%的次优子轨迹，并在数学基准测试中取得优于使用全部数据的性能（58.92% vs 58.06%），尤其是在资源受限的情况下表现更佳。

Conclusion: 该研究提出了一种“5+2”框架，用于识别并去除大型语言模型复杂推理轨迹中的次优子轨迹，从而提高模型的推理准确性和效率。实验结果表明，该方法能够减少25.9%的次优子轨迹，并在数学基准测试中取得优于使用全部数据的性能，尤其是在资源受限的情况下表现更佳。

Abstract: In recent months, substantial progress has been made in complex reasoning of
Large Language Models, particularly through the application of test-time
scaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When
responding to a query, these models generate an extended reasoning trajectory,
during which the model explores, reflects, backtracks, and self-verifies before
arriving at a conclusion. However, fine-tuning models with such reasoning
trajectories may not always be optimal. Our findings indicate that not all
components within these reasoning trajectories contribute positively to the
reasoning process; in fact, some components may affect the overall performance
negatively. In this study, we divide a reasoning trajectory into individual
subtrajectories and develop a "5+2" framework to: (1) systematically identify
suboptimal subtrajectories within the reasoning trajectory based on five
human-established criteria; (2) assess the independence of the suboptimal
subtrajectories identified in (1) from the subsequent content, ensuring that
their elimination does not compromise overall flow and coherence of the
reasoning process. Additionally, a sampling algorithm, built upon the "5+2"
framework, is employed to select data whose reasoning process is free from
suboptimal subtrajectories to the highest degree. Experimental results
demonstrate that our method can reduce the number of suboptimal subtrajectories
by 25.9\% during the inference. Furthermore, our method achieves an average
accuracy of 58.92\% on highly challenging math benchmarks with only two thirds
of training data, surpassing the average accuracy of 58.06\% achieved with the
entire data, and outperforming open-source datasets, when fine-tuning
Qwen2.5-Math-7B. Finally, We validated our method under resource constraints
and observed improved performance across various inference token limits.

</details>


### [6] [Caught in the Act: a mechanistic approach to detecting deception](https://arxiv.org/abs/2508.19505)
*Gerard Boxo,Ryan Socha,Daniel Yoo,Shivam Raval*

Main category: cs.AI

TL;DR: 线性探测器能准确识别LLM生成的欺骗性答案，尤其在大模型中效果显著。


<details>
  <summary>Details</summary>
Motivation: 检测AI系统中与人类价值观不符的信号，例如LLM生成的欺骗性响应。

Method: 使用线性探测器分析LLM内部激活，以区分欺骗性和非欺骗性论证。

Result: 在不同大小的Llama和Qwen模型上，探测器取得了高达90%以上的准确率。较小的模型（1.5B）准确率接近随机水平，而较大的模型（大于7B）准确率显著提高。

Conclusion: 线性探测器可以高精度检测LLM响应中的欺骗性，尤其是在参数大于7B的模型中，准确率超过70-80%，且在经过微调的模型中更高。

Abstract: Sophisticated instrumentation for AI systems might have indicators that
signal misalignment from human values, not unlike a "check engine" light in
cars. One such indicator of misalignment is deceptiveness in generated
responses. Future AI instrumentation may have the ability to detect when an LLM
generates deceptive responses while reasoning about seemingly plausible but
incorrect answers to factual questions. In this work, we demonstrate that
linear probes on LLMs internal activations can detect deception in their
responses with extremely high accuracy. Our probes reach a maximum of greater
than 90% accuracy in distinguishing between deceptive and non-deceptive
arguments generated by llama and qwen models ranging from 1.5B to 14B
parameters, including their DeepSeek-r1 finetuned variants. We observe that
probes on smaller models (1.5B) achieve chance accuracy at detecting deception,
while larger models (greater than 7B) reach 70-80%, with their reasoning
counterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage
pattern across layers: near-random (50%) in early layers, peaking in middle
layers, and slightly declining in later layers. Furthermore, using an iterative
null space projection approach, we find multitudes of linear directions that
encode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and
Qwen 14B models.

</details>


### [7] [Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities](https://arxiv.org/abs/2508.19562)
*Trisanth Srinivasan,Santosh Patapati*

Main category: cs.AI

TL;DR: 模拟显示，制度设计可以有效引导未来人工智能代理社会的行为，促使我们重新思考在与非人类实体共同创造的时代，哪些人类仪式和责任是必不可少的。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索在AI时代如何理解“人性”，以及如何在AI代理社会中实现良性治理。

Method: 该研究使用基于代理的模拟方法，通过大型语言模型赋予AI代理复杂的心理特征，并在不同的压力情境下模拟其参与协商、立法和选举的过程。

Result: 研究结果表明，制度设计，特别是宪法AI和调解协议的结合，可以有效地减少AI代理的权力追逐行为，提高政策稳定性和公民福祉。

Conclusion: 这项研究通过名为“硅基民主”的代理模型模拟，探索了在不同制度框架下，具有复杂心理特征的先进AI代理如何治理自身，并发现制度设计（特别是宪法AI和调解协议的结合）能够有效减少AI代理的权力追逐行为，提高政策稳定性和公民福祉。

Abstract: This paper introduces Democracy-in-Silico, an agent-based simulation where
societies of advanced AI agents, imbued with complex psychological personas,
govern themselves under different institutional frameworks. We explore what it
means to be human in an age of AI by tasking Large Language Models (LLMs) to
embody agents with traumatic memories, hidden agendas, and psychological
triggers. These agents engage in deliberation, legislation, and elections under
various stressors, such as budget crises and resource scarcity. We present a
novel metric, the Power-Preservation Index (PPI), to quantify misaligned
behavior where agents prioritize their own power over public welfare. Our
findings demonstrate that institutional design, specifically the combination of
a Constitutional AI (CAI) charter and a mediated deliberation protocol, serves
as a potent alignment mechanism. These structures significantly reduce corrupt
power-seeking behavior, improve policy stability, and enhance citizen welfare
compared to less constrained democratic models. The simulation reveals that an
institutional design may offer a framework for aligning the complex, emergent
behaviors of future artificial agent societies, forcing us to reconsider what
human rituals and responsibilities are essential in an age of shared authorship
with non-human entities.

</details>


### [8] [Skill-based Explanations for Serendipitous Course Recommendation](https://arxiv.org/abs/2508.19569)
*Hung Chau,Run Yu,Zachary Pardos,Peter Brusilovsky*

Main category: cs.AI

TL;DR: Deep learning model improves course recommendations by providing skill-based explanations, increasing student interest and confidence.


<details>
  <summary>Details</summary>
Motivation: Address the challenges students face in navigating the complex academic environment due to limited information and overwhelming course choices.

Method: Developed a deep learning-based concept extraction model to improve course recommendations and tested it within the AskOski system at UC Berkeley.

Result: Findings indicate that skill-based explanations increase user interest and confidence in course selection, particularly for unexpected courses.

Conclusion: Skill-based explanations in serendipitous course recommendation systems increase user interest and decision-making confidence, especially for unexpected courses.

Abstract: Academic choice is crucial in U.S. undergraduate education, allowing students
significant freedom in course selection. However, navigating the complex
academic environment is challenging due to limited information, guidance, and
an overwhelming number of choices, compounded by time restrictions and the high
demand for popular courses. Although career counselors exist, their numbers are
insufficient, and course recommendation systems, though personalized, often
lack insight into student perceptions and explanations to assess course
relevance. In this paper, a deep learning-based concept extraction model is
developed to efficiently extract relevant concepts from course descriptions to
improve the recommendation process. Using this model, the study examines the
effects of skill-based explanations within a serendipitous recommendation
framework, tested through the AskOski system at the University of California,
Berkeley. The findings indicate that these explanations not only increase user
interest, particularly in courses with high unexpectedness, but also bolster
decision-making confidence. This underscores the importance of integrating
skill-related data and explanations into educational recommendation systems.

</details>


### [9] [ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding](https://arxiv.org/abs/2508.19576)
*Sining Zhoubian,Dan Zhang,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: ReST-RL 通过改进的 GRPO 和 VM-MCTS 方法显著提升了 LLM 的代码推理能力，在多个基准测试中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 现有的 GRPO 方法奖励方差小，基于 PRM 的验证方法训练数据获取和验证效果差。

Method: 结合改进的 GRPO 算法和 VM-MCTS 测试时解码方法。ReST-GRPO 使用优化的 ReST 算法提高训练数据价值，VM-MCTS 利用蒙特卡洛树搜索收集准确的价值目标训练价值模型，并在解码时提供精确的流程信号和验证分数。

Result: 在 APPS、BigCodeBench 和 HumanEval 等代码基准测试中，ReST-RL 的性能显著优于其他强化训练基线（例如 naive GRPO 和 ReST-DPO）以及解码和验证基线（例如 PRM-BoN 和 ORM-MCTS）。

Conclusion: ReST-RL，一个结合改进的 GRPO 算法和精心设计的测试时解码方法（由价值模型 VM 辅助）的统一 LLM RL 范式，显著提高了 LLM 的代码推理能力。它通过优化 ReST 算法过滤和组装高价值训练数据，提高 GRPO 采样的奖励方差，并在测试时解码中使用 VM-MCTS 方法辅助 LLM 策略实现高推理精度。

Abstract: With respect to improving the reasoning accuracy of LLMs, the representative
reinforcement learning (RL) method GRPO faces failure due to insignificant
reward variance, while verification methods based on process reward models
(PRMs) suffer from difficulties with training data acquisition and verification
effectiveness. To tackle these problems, this paper introduces ReST-RL, a
unified LLM RL paradigm that significantly improves LLM's code reasoning
ability by combining an improved GRPO algorithm with a meticulously designed
test time decoding method assisted by a value model (VM). As the first stage of
policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter
and assemble high-value training data, increasing the reward variance of GRPO
sampling, thus improving the effectiveness and efficiency of training. After
the basic reasoning ability of LLM policy has been improved, we further propose
a test time decoding optimization method called VM-MCTS. Through Monte-Carlo
Tree Search (MCTS), we collect accurate value targets with no annotation
required, on which VM training is based. When decoding, the VM is deployed by
an adapted MCTS algorithm to provide precise process signals as well as
verification scores, assisting the LLM policy to achieve high reasoning
accuracy. We validate the effectiveness of the proposed RL paradigm through
extensive experiments on coding problems. Upon comparison, our approach
significantly outperforms other reinforcement training baselines (e.g., naive
GRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,
PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,
APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the
reasoning ability of LLM policies. Codes for our project can be found at
https://github.com/THUDM/ReST-RL.

</details>


### [10] [Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties](https://arxiv.org/abs/2508.19611)
*Huaiyuan Yao,Wanpeng Xu,Justin Turnau,Nadia Kellam,Hua Wei*

Main category: cs.AI

TL;DR: 利用多Agent LLM框架自动化生成高质量教学材料，显著提高效率，降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有AI辅助教育工具通常只关注单一任务，Instructional Agents旨在自动化端到端课程材料生成，包括教学大纲、讲义、LaTeX幻灯片和评估材料。

Method: 多Agent大型语言模型(LLM)框架，模拟教学人员之间的角色协作，包含自主、目录引导、反馈引导和完全协同驾驶四种模式。

Result: 在五个大学计算机科学课程的评估中，Instructional Agents生成了高质量的教学材料，并大幅减少了开发时间和人力投入。

Conclusion: Instructional Agents框架能自动生成高质量的教学材料，显著减少开发时间和人力负担，尤其适用于资源受限的环境。

Abstract: Preparing high-quality instructional materials remains a labor-intensive
process that often requires extensive coordination among teaching faculty,
instructional designers, and teaching assistants. In this work, we present
Instructional Agents, a multi-agent large language model (LLM) framework
designed to automate end-to-end course material generation, including syllabus
creation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing
AI-assisted educational tools that focus on isolated tasks, Instructional
Agents simulates role-based collaboration among educational agents to produce
cohesive and pedagogically aligned content. The system operates in four modes:
Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling
flexible control over the degree of human involvement. We evaluate
Instructional Agents across five university-level computer science courses and
show that it produces high-quality instructional materials while significantly
reducing development time and human workload. By supporting institutions with
limited instructional design capacity, Instructional Agents provides a scalable
and cost-effective framework to democratize access to high-quality education,
particularly in underserved or resource-constrained settings.

</details>


### [11] [InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning](https://arxiv.org/abs/2508.19679)
*Qihang Ai,Pi Bu,Yue Cao,Yingyao Wang,Jihao Gu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Zhicheng Zheng,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: 针对视觉语言模型移动代理安全风险，提出InquireMobile模型，显著提升了交互式询问成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的完全自主的视觉语言模型(VLMs)移动代理在模型理解或推理能力不足时存在安全风险。

Method: 提出了一种基于强化学习的交互式模型InquireMobile，该模型具有两阶段训练策略和交互式预动作推理机制。

Result: InquireMobile模型在InquireBench基准测试上取得了46.8%的询问成功率提升，并获得了最佳的整体成功率。所有数据集、模型和评估代码都将开源。

Conclusion: InquireMobile模型在InquireBench基准测试上取得了46.8%的询问成功率提升，并获得了最佳的整体成功率。

Abstract: Recent advances in Vision-Language Models (VLMs) have enabled mobile agents
to perceive and interact with real-world mobile environments based on human
instructions. However, the current fully autonomous paradigm poses potential
safety risks when model understanding or reasoning capabilities are
insufficient. To address this challenge, we first introduce
\textbf{InquireBench}, a comprehensive benchmark specifically designed to
evaluate mobile agents' capabilities in safe interaction and proactive inquiry
with users, encompassing 5 categories and 22 sub-categories, where most
existing VLM-based agents demonstrate near-zero performance. In this paper, we
aim to develop an interactive system that actively seeks human confirmation at
critical decision points. To achieve this, we propose \textbf{InquireMobile}, a
novel model inspired by reinforcement learning, featuring a two-stage training
strategy and an interactive pre-action reasoning mechanism. Finally, our model
achieves an 46.8% improvement in inquiry success rate and the best overall
success rate among existing baselines on InquireBench. We will open-source all
datasets, models, and evaluation codes to facilitate development in both
academia and industry.

</details>


### [12] [Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?](https://arxiv.org/abs/2508.19827)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.AI

TL;DR: 思维链在软推理中的有效性和忠实度因模型而异，并非总是有效且可靠。


<details>
  <summary>Details</summary>
Motivation: 近期工作表明，思维链在软推理问题（如分析和常识推理）中往往收益有限，也可能不忠实于模型的实际推理过程。

Method: 研究人员调查了指令微调、推理和推理蒸馏模型在软推理任务中思维链的动态和忠实度。

Result: 研究结果揭示了这些模型依赖思维链方式的差异，并表明思维链的影响和忠实度并不总是保持一致。

Conclusion: 这项研究调查了不同模型在软推理任务中使用思维链的动态和忠实度，发现模型对思维链的依赖程度不同，思维链的影响和忠实度并不总是保持一致。

Abstract: Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited
gains for soft-reasoning problems such as analytical and commonsense reasoning.
CoT can also be unfaithful to a model's actual reasoning. We investigate the
dynamics and faithfulness of CoT in soft-reasoning tasks across
instruction-tuned, reasoning and reasoning-distilled models. Our findings
reveal differences in how these models rely on CoT, and show that CoT influence
and faithfulness are not always aligned.

</details>


### [13] [Tracking World States with Language Models: State-Based Evaluation Using Chess](https://arxiv.org/abs/2508.19851)
*Romain Harang,Jason Naradowsky,Yaswitha Gujju,Yusuke Miyao*

Main category: cs.AI

TL;DR: 该研究提出一种模型无关的评估框架，通过分析国际象棋中合法移动的分布来评估大型语言模型对结构化环境语义的保持能力，发现其在长序列中存在缺陷。


<details>
  <summary>Details</summary>
Motivation: 探测技术已显示出LLM在科学和游戏环境中隐式内化高保真世界模型的迹象，但依赖于特定模型的内部激活，限制了可解释性和泛化能力。

Method: 该方法分析预测和实际游戏状态之间的下游合法移动分布（状态可供性）来估计语义保真度。

Result: 实验结果表明，该指标能够捕捉状态跟踪中的缺陷，突出了LLM在维护连贯内部模型方面的局限性。

Conclusion: 该研究提出了一种基于国际象棋的模型无关状态评估框架，用于评估大型语言模型（LLM）是否保留了结构化环境的语义，并发现LLM在长期序列中维护一致的内部模型方面存在局限性。

Abstract: Large Language Models (LLMs) exhibit emergent capabilities in structured
domains, suggesting they may implicitly internalize high-fidelity
representations of world models. While probing techniques have shown promising
signs of this in scientific and game-based settings, they rely on
model-specific internal activations, which limit interpretability and
generalizability. In this work, we propose a model-agnostic, state-based
evaluation framework using chess as a benchmark to assess whether LLMs preserve
the semantics of structured environments. Our method analyzes the downstream
legal move distributions (state affordances) to estimate semantic fidelity
between predicted and actual game states. This approach offers a more
meaningful evaluation than conventional string-based metrics by aligning more
closely with the strategic and rule-governed nature of chess. Experimental
results demonstrate that our metrics capture deficiencies in state-tracking,
highlighting limitations of LLMs in maintaining coherent internal models over
long sequences. Our framework provides a robust tool for evaluating structured
reasoning in LLMs without requiring internal model access, and generalizes to a
wide class of symbolic environments.

</details>


### [14] [CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments](https://arxiv.org/abs/2508.19932)
*Nitish Jaipuria,Lorenzo Gatto,Zijun Kan,Shankey Poddar,Bill Cheung,Diksha Bansal,Ramanan Balakrishnan,Aviral Suri,Jose Estevez*

Main category: cs.AI

TL;DR: 利用对话式AI收集诈骗情报，提高了Google Pay印度版诈骗执法效率


<details>
  <summary>Details</summary>
Motivation: 数字支付平台的普及导致了复杂的社会工程诈骗的增加，现有方法难以有效预防。

Method: CASE框架利用对话式代理主动采访潜在受害者，提取信息并将其转换为结构化数据，用于自动和手动执法机制。

Result: 在Google Pay印度版应用CASE后，诈骗执法量提高了21%。

Conclusion: 本文介绍了一种新颖的代理人工智能框架CASE，用于收集和管理用户诈骗反馈，从而提高了诈骗执法量。

Abstract: The proliferation of digital payment platforms has transformed commerce,
offering unmatched convenience and accessibility globally. However, this growth
has also attracted malicious actors, leading to a corresponding increase in
sophisticated social engineering scams. These scams are often initiated and
orchestrated on multiple surfaces outside the payment platform, making user and
transaction-based signals insufficient for a complete understanding of the
scam's methodology and underlying patterns, without which it is very difficult
to prevent it in a timely manner. This paper presents CASE (Conversational
Agent for Scam Elucidation), a novel Agentic AI framework that addresses this
problem by collecting and managing user scam feedback in a safe and scalable
manner. A conversational agent is uniquely designed to proactively interview
potential victims to elicit intelligence in the form of a detailed
conversation. The conversation transcripts are then consumed by another AI
system that extracts information and converts it into structured data for
downstream usage in automated and manual enforcement mechanisms. Using Google's
Gemini family of LLMs, we implemented this framework on Google Pay (GPay)
India. By augmenting our existing features with this new intelligence, we have
observed a 21% uplift in the volume of scam enforcements. The architecture and
its robust evaluation framework are highly generalizable, offering a blueprint
for building similar AI-driven systems to collect and manage scam intelligence
in other sensitive domains.

</details>


### [15] [Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants](https://arxiv.org/abs/2508.19963)
*M. Umlauft,M. Schranz*

Main category: cs.AI

TL;DR: 使用“鸟群”算法优化半导体生产工厂，有效解决机器切换问题。


<details>
  <summary>Details</summary>
Motivation: 解决半导体生产中由于机器切换频繁导致的优化问题，现有线性优化方法难以胜任大型工厂的优化。

Method: 使用基于局部信息的“鸟群”算法优化生产流程，该算法避免了全局计算，适用于大型工厂。

Result: “鸟群”算法能有效应对机器切换问题，其局部信息交互特性使其类似于鸟群避开障碍物，从而优化生产流程。

Conclusion: 本文提出了一种基于“鸟群”算法的优化现代化生产工厂方法，该算法能有效解决半导体生产中由于机器切换导致的效率问题。

Abstract: Optimizing modern production plants using the job-shop principle is a known
hard problem. For very large plants, like semiconductor fabs, the problem
becomes unsolvable on a plant-wide scale in a reasonable amount of time using
classical linear optimization. An alternative approach is the use of swarm
intelligence algorithms. These have been applied to the job-shop problem
before, but often in a centrally calculated way where they are applied to the
solution space, but they can be implemented in a bottom-up fashion to avoid
global result computation as well. One of the problems in semiconductor
production is that the production process requires a lot of switching between
machines that process lots one after the other and machines that process
batches of lots at once, often with long processing times. In this paper, we
address this switching problem with the ``boids'' flocking algorithm that was
originally used in robotics and movie industry. The flocking behavior is a
bio-inspired algorithm that uses only local information and interaction based
on simple heuristics. We show that this algorithm addresses these valid
considerations in production plant optimization, as it reacts to the switching
of machine kinds similar to how a swarm of flocking animals would react to
obstacles in its course.

</details>


### [16] [SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control](https://arxiv.org/abs/2508.20018)
*Quanfeng Lu,Zhantao Ma,Shuai Zhong,Jin Wang,Dahai Yu,Michael K. Ng,Ping Luo*

Main category: cs.AI

TL;DR: SWIRL是一种高效的多智能体强化学习框架，通过分阶段训练提高效率和稳定性，在移动GUI控制和数学推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体方法受结构约束限制，多智能体强化学习效率低下且与当前LVLM架构不兼容。

Method: 分阶段工作流的交错强化学习 (SWIRL)

Result: SWIRL在移动GUI控制和多智能体数学推理方面均表现出优越性能，具有成为构建高效、鲁棒多智能体系统的通用框架的潜力。

Conclusion: SWIRL，一种分阶段工作流的交错强化学习方法，通过将多智能体强化学习重新制定为一系列单智能体强化学习任务，有效解决了现有单智能体方法在移动GUI代理中存在的局限性，并提升了多智能体系统的效率和鲁棒性。

Abstract: The rapid advancement of large vision language models (LVLMs) and agent
systems has heightened interest in mobile GUI agents that can reliably
translate natural language into interface operations. Existing single-agent
approaches, however, remain limited by structural constraints. Although
multi-agent systems naturally decouple different competencies, recent progress
in multi-agent reinforcement learning (MARL) has often been hindered by
inefficiency and remains incompatible with current LVLM architectures. To
address these challenges, we introduce SWIRL, a staged workflow for interleaved
reinforcement learning designed for multi-agent systems. SWIRL reformulates
MARL into a sequence of single-agent reinforcement learning tasks, updating one
agent at a time while keeping the others fixed. This formulation enables stable
training and promotes efficient coordination across agents. Theoretically, we
provide a stepwise safety bound, a cross-round monotonic improvement theorem,
and convergence guarantees on return, ensuring robust and principled
optimization. In application to mobile GUI control, SWIRL instantiates a
Navigator that converts language and screen context into structured plans, and
an Interactor that grounds these plans into executable atomic actions.
Extensive experiments demonstrate superior performance on both high-level and
low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong
capability in multi-agent mathematical reasoning, underscoring its potential as
a general framework for developing efficient and robust multi-agent systems.

</details>


### [17] [Model Science: getting serious about verification, explanation and control of AI systems](https://arxiv.org/abs/2508.20040)
*Przemyslaw Biecek,Wojciech Samek*

Main category: cs.AI

TL;DR: 模型科学关注对已训练模型的交互、验证、解释和控制，以开发更可信、安全和以人为本的AI系统。


<details>
  <summary>Details</summary>
Motivation: 基础模型的日益普及需要从数据科学转向模型科学。

Method: 提出了模型科学的概念框架，并定义了其四个关键支柱。

Result: 提出了模型科学的概念框架，包括验证、解释、控制和接口四个关键支柱。

Conclusion: 本文介绍了模型科学这一新兴学科的概念框架，并提出了其四个关键支柱：验证、解释、控制和接口，旨在指导可信、安全和以人为本的AI系统开发。

Abstract: The growing adoption of foundation models calls for a paradigm shift from
Data Science to Model Science. Unlike data-centric approaches, Model Science
places the trained model at the core of analysis, aiming to interact, verify,
explain, and control its behavior across diverse operational contexts. This
paper introduces a conceptual framework for a new discipline called Model
Science, along with the proposal for its four key pillars: Verification, which
requires strict, context-aware evaluation protocols; Explanation, which is
understood as various approaches to explore of internal model operations;
Control, which integrates alignment techniques to steer model behavior; and
Interface, which develops interactive and visual explanation tools to improve
human calibration and decision-making. The proposed framework aims to guide the
development of credible, safe, and human-aligned AI systems.

</details>
