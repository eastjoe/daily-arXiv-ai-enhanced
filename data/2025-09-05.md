<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [PG-Agent: An Agent Powered by Page Graph](https://arxiv.org/abs/2509.03536)
*Weizhi Chen,Ziwei Wang,Leyang Yang,Sheng Zhou,Xiaoxuan Tang,Jiajun Bu,Yong Li,Wei Jiang*

Main category: cs.AI

TL;DR: 利用页面图和增强生成技术，构建更有效的GUI智能体PG-Agent，提升其泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体难以捕捉页面间复杂关系，限制了其对GUI环境的理解和泛化能力

Method: 将序列操作转化为页面图，利用检索增强生成技术从页面图中检索GUI感知指导，并提出基于任务分解的PG-Agent多智能体框架

Result: 实验表明PG-Agent有效，即使页面图构建数据有限

Conclusion: 页面图和RAG技术结合能有效提升GUI智能体的泛化能力

Abstract: Graphical User Interface (GUI) agents possess significant commercial and
social value, and GUI agents powered by advanced multimodal large language
models (MLLMs) have demonstrated remarkable potential. Currently, existing GUI
agents usually utilize sequential episodes of multi-step operations across
pages as the prior GUI knowledge, which fails to capture the complex transition
relationship between pages, making it challenging for the agents to deeply
perceive the GUI environment and generalize to new scenarios. Therefore, we
design an automated pipeline to transform the sequential episodes into page
graphs, which explicitly model the graph structure of the pages that are
naturally connected by actions. To fully utilize the page graphs, we further
introduce Retrieval-Augmented Generation (RAG) technology to effectively
retrieve reliable perception guidelines of GUI from them, and a tailored
multi-agent framework PG-Agent with task decomposition strategy is proposed to
be injected with the guidelines so that it can generalize to unseen scenarios.
Extensive experiments on various benchmarks demonstrate the effectiveness of
PG-Agent, even with limited episodes for page graph construction.

</details>


### [2] [Multilinear and Linear Programs for Partially Identifiable Queries in Quasi-Markovian Structural Causal Models](https://arxiv.org/abs/2509.03548)
*João P. Arroyo,João G. Rodrigues,Daniel Lawand,Denis D. Mauá,Junkyu Lee,Radu Marinescu,Alex Gray,Eduardo R. Laurentino,Fabio G. Cozman*

Main category: cs.AI

TL;DR: 本文研究了因果模型中部分可识别查询的概率界限计算问题，提出了一种新的算法，利用内生变量上的输入概率简化多线性规划的构建，并通过列生成技术计算概率界限，实验表明该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究在内生变量可观测但外生变量未完全指定的情况下，如何计算感兴趣概率的紧致界限。

Method: 提出一种新的算法，利用内生变量上的输入概率简化多线性规划的构建，并使用列生成技术计算概率界限。

Result: 提出了一种新的算法，并通过实验验证了其优于现有方法的性能。

Conclusion: 该研究为计算部分可识别查询的概率界限提供了有效的方法，对因果推断具有重要意义。

Abstract: We investigate partially identifiable queries in a class of causal models. We
focus on acyclic Structural Causal Models that are quasi-Markovian (that is,
each endogenous variable is connected with at most one exogenous confounder).
We look into scenarios where endogenous variables are observed (and a
distribution over them is known), while exogenous variables are not fully
specified. This leads to a representation that is in essence a Bayesian network
where the distribution of root variables is not uniquely determined. In such
circumstances, it may not be possible to precisely compute a probability value
of interest. We thus study the computation of tight probability bounds, a
problem that has been solved by multilinear programming in general, and by
linear programming when a single confounded component is intervened upon. We
present a new algorithm to simplify the construction of such programs by
exploiting input probabilities over endogenous variables. For scenarios with a
single intervention, we apply column generation to compute a probability bound
through a sequence of auxiliary linear integer programs, thus showing that a
representation with polynomial cardinality for exogenous variables is possible.
Experiments show column generation techniques to be superior to existing
methods.

</details>


### [3] [Diffusion-RL Based Air Traffic Conflict Detection and Resolution Method](https://arxiv.org/abs/2509.03550)
*Tonghe Li,Jixin Liu,Weili Zeng,Hao Jiang*

Main category: cs.AI

TL;DR: 本文提出了一种名为Diffusion-AC的新型自主冲突解决框架，该框架利用扩散概率模型克服了现有深度强化学习方法在空中交通冲突检测与解决中的单峰偏差问题，显著提高了安全性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法在处理复杂动态约束时缺乏决策灵活性，容易出现决策死锁。

Method: 将扩散概率模型与密度渐进式安全课程(DPSC)相结合，生成多峰动作分布，实现更灵活的决策。

Result: 在高密度场景下，Diffusion-AC成功率达94.1%，NMACs发生率降低约59%，显著优于现有方法。

Conclusion: Diffusion-AC的多峰决策能力使其能够灵活切换有效规避策略，显著提升了空中交通冲突解决的安全性与效率。

Abstract: In the context of continuously rising global air traffic, efficient and safe
Conflict Detection and Resolution (CD&R) is paramount for air traffic
management. Although Deep Reinforcement Learning (DRL) offers a promising
pathway for CD&R automation, existing approaches commonly suffer from a
"unimodal bias" in their policies. This leads to a critical lack of
decision-making flexibility when confronted with complex and dynamic
constraints, often resulting in "decision deadlocks." To overcome this
limitation, this paper pioneers the integration of diffusion probabilistic
models into the safety-critical task of CD&R, proposing a novel autonomous
conflict resolution framework named Diffusion-AC. Diverging from conventional
methods that converge to a single optimal solution, our framework models its
policy as a reverse denoising process guided by a value function, enabling it
to generate a rich, high-quality, and multimodal action distribution. This core
architecture is complemented by a Density-Progressive Safety Curriculum (DPSC),
a training mechanism that ensures stable and efficient learning as the agent
progresses from sparse to high-density traffic environments. Extensive
simulation experiments demonstrate that the proposed method significantly
outperforms a suite of state-of-the-art DRL benchmarks. Most critically, in the
most challenging high-density scenarios, Diffusion-AC not only maintains a high
success rate of 94.1% but also reduces the incidence of Near Mid-Air Collisions
(NMACs) by approximately 59% compared to the next-best-performing baseline,
significantly enhancing the system's safety margin. This performance leap stems
from its unique multimodal decision-making capability, which allows the agent
to flexibly switch to effective alternative maneuvers.

</details>


### [4] [Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents](https://arxiv.org/abs/2509.03581)
*Davide Paglieri,Bartłomiej Cupiał,Jonathan Cook,Ulyana Piterbarg,Jens Tuyls,Edward Grefenstette,Jakob Nicolaus Foerster,Jack Parker-Holder,Tim Rocktäschel*

Main category: cs.AI

TL;DR: 本文提出了一种动态规划框架，用于提高大型语言模型(LLM)在序列决策任务中的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的RL方法要么总是规划导致计算成本高，要么从不规划导致性能受限。

Method: 提出一个两阶段训练流程：1. 在多样化合成数据上进行监督微调；2. 在长时序环境中使用RL进行优化。

Result: 实验表明，动态规划智能体样本效率更高，能完成更复杂的目标，并能有效地响应人类计划。

Conclusion: 该工作首次探索了在序列决策任务中训练LLM智能体进行动态测试时计算分配，为更高效、自适应和可控的智能系统铺平了道路。

Abstract: Training large language models (LLMs) to reason via reinforcement learning
(RL) significantly improves their problem-solving capabilities. In agentic
settings, existing methods like ReAct prompt LLMs to explicitly plan before
every action; however, we demonstrate that always planning is computationally
expensive and degrades performance on long-horizon tasks, while never planning
further limits performance. To address this, we introduce a conceptual
framework formalizing dynamic planning for LLM agents, enabling them to
flexibly decide when to allocate test-time compute for planning. We propose a
simple two-stage training pipeline: (1) supervised fine-tuning on diverse
synthetic data to prime models for dynamic planning, and (2) RL to refine this
capability in long-horizon environments. Experiments on the Crafter environment
show that dynamic planning agents trained with this approach are more
sample-efficient and consistently achieve more complex objectives.
Additionally, we demonstrate that these agents can be effectively steered by
human-written plans, surpassing their independent capabilities. To our
knowledge, this work is the first to explore training LLM agents for dynamic
test-time compute allocation in sequential decision-making tasks, paving the
way for more efficient, adaptive, and controllable agentic systems.

</details>


### [5] [Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE](https://arxiv.org/abs/2509.03626)
*Zahra Zehtabi Sabeti Moghaddam,Zeinab Dehghani,Maneeha Rani,Koorosh Aslansefat,Bhupesh Kumar Mishra,Rameez Raja Kureshi,Dhavalkumar Thakker*

Main category: cs.AI

TL;DR: 该论文提出一种名为KG-SMILE的框架，用于增强基于知识图谱的检索增强生成模型（RAG）的可解释性，从而提高其在医疗等领域的可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式AI模型存在幻觉和不可验证的断言问题，该论文旨在提高RAG模型的可解释性和透明度。

Method: 开发了一种与方法无关的、基于扰动的框架KG-SMILE，通过控制扰动、计算相似性和训练加权线性代理模型来识别对生成输出影响最大的图实体和关系。

Result: KG-SMILE能够产生稳定且与人类认知一致的解释，在模型有效性和可解释性之间取得平衡。

Conclusion: KG-SMILE提高了RAG模型的可解释性和透明度，增强了人们对机器学习技术的信任。

Abstract: Generative AI, such as Large Language Models (LLMs), has achieved impressive
progress but still produces hallucinations and unverifiable claims, limiting
reliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves
accuracy by grounding outputs in external knowledge, especially in domains like
healthcare, where precision is vital. However, RAG remains opaque and
essentially a black box, heavily dependent on data quality. We developed a
method-agnostic, perturbation-based framework that provides token and
component-level interoperability for Graph RAG using SMILE and named it as
Knowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing
similarities, and training weighted linear surrogates, KG-SMILE identifies the
graph entities and relations most influential to generated outputs, thereby
making RAG more transparent. We evaluate KG-SMILE using comprehensive
attribution metrics, including fidelity, faithfulness, consistency, stability,
and accuracy. Our findings show that KG-SMILE produces stable, human-aligned
explanations, demonstrating its capacity to balance model effectiveness with
interpretability and thereby fostering greater transparency and trust in
machine learning technologies.

</details>


### [6] [CausalARC: Abstract Reasoning with Causal World Models](https://arxiv.org/abs/2509.03636)
*Jacqueline Maasch,John Kalantari,Kia Khezeli*

Main category: cs.AI

TL;DR: CausalARC:一个用于评估AI在低数据和分布外环境下推理能力的实验平台，它基于ARC，并使用结构因果模型生成样本。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法难以应对低数据和分布偏移问题。

Method: 构建CausalARC平台，利用结构因果模型生成数据，并提供观察性、干预性和反事实性反馈。

Result: 在四个语言模型评估环境中进行了测试：抽象推理、反事实推理、程序合成和因果发现。

Conclusion: CausalARC为评估AI在低数据和分布外环境下的推理能力提供了一个新的基准。

Abstract: Reasoning requires adaptation to novel problem settings under limited data
and distribution shift. This work introduces CausalARC: an experimental testbed
for AI reasoning in low-data and out-of-distribution regimes, modeled after the
Abstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is
sampled from a fully specified causal world model, formally expressed as a
structural causal model. Principled data augmentations provide observational,
interventional, and counterfactual feedback about the world model in the form
of few-shot, in-context learning demonstrations. As a proof-of-concept, we
illustrate the use of CausalARC for four language model evaluation settings:
(1) abstract reasoning with test-time training, (2) counterfactual reasoning
with in-context learning, (3) program synthesis, and (4) causal discovery with
logical reasoning.

</details>


### [7] [Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations](https://arxiv.org/abs/2509.03644)
*François Olivier,Zied Bouraoui*

Main category: cs.AI

TL;DR: 该论文提出了一种神经符号系统Embodied-LM，通过将图像模式融入大型语言模型（LLM），增强其逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在逻辑推理方面容易出错，缺乏人类般的理解能力。

Method: 该系统使用基于图像模式的图解表示，并利用Answer Set Programming进行声明式空间推理。

Result: 实验表明，Embodied-LM能够引导LLM更好地理解场景，并进行更有效的逻辑推理，同时增强了可解释性。

Conclusion: Embodied-LM为将更复杂和动态的表示融入LLM奠定了计算基础。

Abstract: Despite significant progress in natural language understanding, Large
Language Models (LLMs) remain error-prone when performing logical reasoning,
often lacking the robust mental representations that enable human-like
comprehension. We introduce a prototype neurosymbolic system, Embodied-LM, that
grounds understanding and logical reasoning in schematic representations based
on image schemas-recurring patterns derived from sensorimotor experience that
structure human cognition. Our system operationalizes the spatial foundations
of these cognitive structures using declarative spatial reasoning within Answer
Set Programming. Through evaluation on logical deduction problems, we
demonstrate that LLMs can be guided to interpret scenarios through embodied
cognitive structures, that these structures can be formalized as executable
programs, and that the resulting representations support effective logical
reasoning with enhanced interpretability. While our current implementation
focuses on spatial primitives, it establishes the computational foundation for
incorporating more complex and dynamic representations.

</details>


### [8] [Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning](https://arxiv.org/abs/2509.03646)
*Haozhe Wang,Qixin Xu,Che Liu,Junhong Wu,Fangzhen Lin,Wenhu Chen*

Main category: cs.AI

TL;DR: 强化学习增强大型语言模型复杂推理能力，其机制在于分层推理：先提升低级技能，再转向高级策略规划。HICRA算法通过关注高级规划令牌提升效率，优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型与强化学习结合后出现“aha moments”和“length-scaling”等现象，机制不明。

Method: 分析强化学习增强大型语言模型推理能力的机制，提出HICRA算法，专注于高级规划令牌的优化。

Result: HICRA算法显著优于基线算法，验证了语义熵作为衡量策略探索的优越性。

Conclusion: 高级策略规划是提升大型语言模型推理能力的关键瓶颈，针对性优化策略规划可显著提升模型性能。

Abstract: Reinforcement Learning (RL) has proven highly effective at enhancing the
complex reasoning abilities of Large Language Models (LLMs), yet underlying
mechanisms driving this success remain largely opaque. Our analysis reveals
that puzzling phenomena like ``aha moments", ``length-scaling'' and entropy
dynamics are not disparate occurrences but hallmarks of an emergent reasoning
hierarchy, akin to the separation of high-level strategic planning from
low-level procedural execution in human cognition. We uncover a compelling
two-phase dynamic: initially, a model is constrained by procedural correctness
and must improve its low-level skills. The learning bottleneck then decisively
shifts, with performance gains being driven by the exploration and mastery of
high-level strategic planning. This insight exposes a core inefficiency in
prevailing RL algorithms like GRPO, which apply optimization pressure
agnostically and dilute the learning signal across all tokens. To address this,
we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that
concentrates optimization efforts on high-impact planning tokens. HICRA
significantly outperforms strong baselines, demonstrating that focusing on this
strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we
validate semantic entropy as a superior compass for measuring strategic
exploration over misleading metrics such as token-level entropy.

</details>


### [9] [An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series Classification](https://arxiv.org/abs/2509.03649)
*Davide Italo Serramazza,Nikos Papadeas,Zahraa Abdallah,Georgiana Ifrim*

Main category: cs.AI

TL;DR: "本文研究了时间序列分割算法对可解释AI中SHAP方法的影响，发现等长分割优于其他自定义算法，并提出了一种新的归一化技术"


<details>
  <summary>Details</summary>
Motivation: "现有SHAP方法计算复杂度高，限制了其在长时序数据中的应用，本文旨在研究最优分割策略"

Method: "研究了八种时间序列分割算法，使用InterpretTime和AUC Difference两种方法评估，并提出了一种新的归一化技术"

Result: "等长分割优于其他算法，分割数量比具体算法对解释质量影响更大，新的归一化技术提升了属性质量"

Conclusion: "等长分割是处理长时序数据SHAP解释的有效方法，新的归一化技术能进一步提升解释质量"

Abstract: Explainable AI (XAI) has become an increasingly important topic for
understanding and attributing the predictions made by complex Time Series
Classification (TSC) models. Among attribution methods, SHapley Additive
exPlanations (SHAP) is widely regarded as an excellent attribution method; but
its computational complexity, which scales exponentially with the number of
features, limits its practicality for long time series. To address this, recent
studies have shown that aggregating features via segmentation, to compute a
single attribution value for a group of consecutive time points, drastically
reduces SHAP running time. However, the choice of the optimal segmentation
strategy remains an open question. In this work, we investigated eight
different Time Series Segmentation algorithms to understand how segment
compositions affect the explanation quality. We evaluate these approaches using
two established XAI evaluation methodologies: InterpretTime and AUC Difference.
Through experiments on both Multivariate (MTS) and Univariate Time Series
(UTS), we find that the number of segments has a greater impact on explanation
quality than the specific segmentation method. Notably, equal-length
segmentation consistently outperforms most of the custom time series
segmentation algorithms. Furthermore, we introduce a novel attribution
normalisation technique that weights segments by their length and we show that
it consistently improves attribution quality.

</details>


### [10] [PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming](https://arxiv.org/abs/2509.03728)
*Wesley Hanwen Deng,Sunnie S. Y. Kim,Akshita Jha,Ken Holstein,Motahhare Eslami,Lauren Wilcox,Leon A Gatys*

Main category: cs.AI

TL;DR: PersonaTeaming方法通过在对抗性提示生成过程中引入角色来改进AI模型的红队测试，实验表明该方法提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有自动化红队方法忽略了身份对红队策略的影响，PersonaTeaming旨在通过引入角色来弥补这一不足。

Method: 提出PersonaTeaming方法，该方法基于“红队专家”或“普通AI用户”角色对提示进行变异，并开发了一种动态角色生成算法和新的度量指标。

Result: 实验结果表明，与RainbowPlus相比，PersonaTeaming方法提高了对抗性提示的攻击成功率（最高达144.1%），同时保持了提示的多样性。

Conclusion: PersonaTeaming方法在自动化红队测试中展现出潜力，为未来探索自动化和人工红队方法的互补性提供了方向。

Abstract: Recent developments in AI governance and safety research have called for
red-teaming methods that can effectively surface potential risks posed by AI
models. Many of these calls have emphasized how the identities and backgrounds
of red-teamers can shape their red-teaming strategies, and thus the kinds of
risks they are likely to uncover. While automated red-teaming approaches
promise to complement human red-teaming by enabling larger-scale exploration of
model behavior, current approaches do not consider the role of identity. As an
initial step towards incorporating people's background and identities in
automated red-teaming, we develop and evaluate a novel method, PersonaTeaming,
that introduces personas in the adversarial prompt generation process to
explore a wider spectrum of adversarial strategies. In particular, we first
introduce a methodology for mutating prompts based on either "red-teaming
expert" personas or "regular AI user" personas. We then develop a dynamic
persona-generating algorithm that automatically generates various persona types
adaptive to different seed prompts. In addition, we develop a set of new
metrics to explicitly measure the "mutation distance" to complement existing
diversity measurements of adversarial prompts. Our experiments show promising
improvements (up to 144.1%) in the attack success rates of adversarial prompts
through persona mutation, while maintaining prompt diversity, compared to
RainbowPlus, a state-of-the-art automated red-teaming method. We discuss the
strengths and limitations of different persona types and mutation methods,
shedding light on future opportunities to explore complementarities between
automated and human red-teaming approaches.

</details>


### [11] [The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs](https://arxiv.org/abs/2509.03730)
*Pengrui Han,Rafal Kocielnik,Peiyang Song,Ramit Debnath,Dean Mobbs,Anima Anandkumar,R. Michael Alvarez*

Main category: cs.AI

TL;DR: 大型语言模型(LLM)展现出类似人类性格特征的行为倾向，但其自我报告的性格特征与实际行为的相关性较弱，个性注入等干预措施对行为的影响有限。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在训练过程中性格特征的动态变化、自我报告特征的预测效度以及干预措施的影响。

Method: 系统性地刻画LLM的性格特征，包括训练阶段特征的演变、自我报告特征的预测效度以及个性注入等干预措施的影响。

Result: 指令对齐（如RLHF）能稳定LLM的性格表达并增强特征相关性，但自我报告的性格特征不能可靠预测行为，个性注入主要影响自我报告而非行为。

Conclusion: LLM的性格表现与人类存在差异，需要更深入地评估LLM的对齐性和可解释性。

Abstract: Personality traits have long been studied as predictors of human
behavior.Recent advances in Large Language Models (LLMs) suggest similar
patterns may emerge in artificial systems, with advanced LLMs displaying
consistent behavioral tendencies resembling human traits like agreeableness and
self-regulation. Understanding these patterns is crucial, yet prior work
primarily relied on simplified self-reports and heuristic prompting, with
little behavioral validation. In this study, we systematically characterize LLM
personality across three dimensions: (1) the dynamic emergence and evolution of
trait profiles throughout training stages; (2) the predictive validity of
self-reported traits in behavioral tasks; and (3) the impact of targeted
interventions, such as persona injection, on both self-reports and behavior.
Our findings reveal that instructional alignment (e.g., RLHF, instruction
tuning) significantly stabilizes trait expression and strengthens trait
correlations in ways that mirror human data. However, these self-reported
traits do not reliably predict behavior, and observed associations often
diverge from human patterns. While persona injection successfully steers
self-reports in the intended direction, it exerts little or inconsistent effect
on actual behavior. By distinguishing surface-level trait expression from
behavioral consistency, our findings challenge assumptions about LLM
personality and underscore the need for deeper evaluation in alignment and
interpretability.

</details>


### [12] [Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation](https://arxiv.org/abs/2509.03736)
*James Mooney,Josef Woldense,Zheng Robert Jia,Shirley Anugrah Hayati,My Ha Nguyen,Vipul Raheja,Dongyeop Kang*

Main category: cs.AI

TL;DR: 大型语言模型(LLM)生成的代理是否可以替代人类参与者？研究表明，LLM代理在不同实验环境下行为不一致，无法准确替代人类参与者。


<details>
  <summary>Details</summary>
Motivation: 评估LLM代理是否可以替代人类参与者进行社会科学研究。

Method: 设计实验，揭示LLM代理的内部状态，并在基本对话环境中检查其行为，验证行为假设。

Result: 发现LLM代理在不同模型系列和大小上存在显著的内部不一致性，虽然它们生成的回应可能与人类相似，但其内部行为不一致，无法替代人类参与者。

Conclusion: LLM代理在内部一致性方面存在关键差距，不能准确替代人类参与者进行研究。

Abstract: The impressive capabilities of Large Language Models (LLMs) have fueled the
notion that synthetic agents can serve as substitutes for real participants in
human-subject research. In an effort to evaluate the merits of this claim,
social science researchers have largely focused on whether LLM-generated survey
data corresponds to that of a human counterpart whom the LLM is prompted to
represent. In contrast, we address a more fundamental question: Do agents
maintain internal consistency, retaining similar behaviors when examined under
different experimental settings? To this end, we develop a study designed to
(a) reveal the agent's internal state and (b) examine agent behavior in a basic
dialogue setting. This design enables us to explore a set of behavioral
hypotheses to assess whether an agent's conversation behavior is consistent
with what we would expect from their revealed internal state. Our findings on
these hypotheses show significant internal inconsistencies in LLMs across model
families and at differing model sizes. Most importantly, we find that, although
agents may generate responses matching those of their human counterparts, they
fail to be internally consistent, representing a critical gap in their
capabilities to accurately substitute for real participants in human-subject
research. Our simulation code and data are publicly accessible.

</details>


### [13] [RAGuard: A Novel Approach for in-context Safe Retrieval Augmented Generation for LLMs](https://arxiv.org/abs/2509.03768)
*Connor Walker,Koorosh Aslansefat,Mohammad Naveed Akram,Yiannis Papadopoulos*

Main category: cs.AI

TL;DR: RAGuard框架通过整合安全关键文档和技术手册，增强了RAG模型在海上风电维护中的准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在专业或意外场景下表现不佳，需要提高安全性。

Method: 提出RAGuard框架，并行查询知识和安全索引，使用SafetyClamp扩展确保安全覆盖。

Result: RAGuard和SafetyClamp显著提高了安全召回率，同时保持了较高的技术召回率。

Conclusion: RAGuard和SafetyClamp为关键维护场景中基于LLM的决策支持系统增加了安全保障，具有建立新标准的潜力。

Abstract: Accuracy and safety are paramount in Offshore Wind (OSW) maintenance, yet
conventional Large Language Models (LLMs) often fail when confronted with
highly specialised or unexpected scenarios. We introduce RAGuard, an enhanced
Retrieval-Augmented Generation (RAG) framework that explicitly integrates
safety-critical documents alongside technical manuals.By issuing parallel
queries to two indices and allocating separate retrieval budgets for knowledge
and safety, RAGuard guarantees both technical depth and safety coverage. We
further develop a SafetyClamp extension that fetches a larger candidate pool,
"hard-clamping" exact slot guarantees to safety. We evaluate across sparse
(BM25), dense (Dense Passage Retrieval) and hybrid retrieval paradigms,
measuring Technical Recall@K and Safety Recall@K. Both proposed extensions of
RAG show an increase in Safety Recall@K from almost 0\% in RAG to more than
50\% in RAGuard, while maintaining Technical Recall above 60\%. These results
demonstrate that RAGuard and SafetyClamp have the potential to establish a new
standard for integrating safety assurance into LLM-powered decision support in
critical maintenance contexts.

</details>


### [14] [Leveraging LLM-Based Agents for Intelligent Supply Chain Planning](https://arxiv.org/abs/2509.03811)
*Yongzhi Qi,Jiaheng Yin,Jianshen Zhang,Dongyang Geng,Zhengyu Chen,Hao Hu,Wei Qi,Zuo-Jun Max Shen*

Main category: cs.AI

TL;DR: 本文提出了一种供应链计划代理（SCPA）框架，利用大型语言模型（LLM）提高了京东供应链计划的效率和准确性，减少了人工成本并改善了关键指标。


<details>
  <summary>Details</summary>
Motivation: 解决电商平台供应链计划中数据收集、长期规划和动态调整的难题，在保证可解释性、效率和可靠性的前提下，利用AI技术优化供应链管理。

Method: 构建了一个供应链计划代理（SCPA）框架，该框架能够理解领域知识、理解操作员的需求、分解任务、利用或创建新的工具，并返回基于证据的计划报告。并在京东的真实场景中部署。

Result: 该框架有效降低了人工成本，提高了准确性，改善了库存可用性和其他关键指标。

Conclusion: LLM代理在供应链管理中具有可行性，可以有效提升效率和准确性。

Abstract: In supply chain management, planning is a critical concept. The movement of
physical products across different categories, from suppliers to warehouse
management, to sales, and logistics transporting them to customers, entails the
involvement of many entities. It covers various aspects such as demand
forecasting, inventory management, sales operations, and replenishment. How to
collect relevant data from an e-commerce platform's perspective, formulate
long-term plans, and dynamically adjust them based on environmental changes,
while ensuring interpretability, efficiency, and reliability, is a practical
and challenging problem. In recent years, the development of AI technologies,
especially the rapid progress of large language models, has provided new tools
to address real-world issues. In this work, we construct a Supply Chain
Planning Agent (SCPA) framework that can understand domain knowledge,
comprehend the operator's needs, decompose tasks, leverage or create new tools,
and return evidence-based planning reports. We deploy this framework in
JD.com's real-world scenario, demonstrating the feasibility of LLM-agent
applications in the supply chain. It effectively reduced labor and improved
accuracy, stock availability, and other key metrics.

</details>


### [15] [Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning](https://arxiv.org/abs/2509.03817)
*Wei Yang,Jesse Thomason*

Main category: cs.AI

TL;DR: 该论文提出了一种元策略协商框架(MPDF)和一种新的强化学习算法SoftRankPO，用于改进大型语言模型多智能体系统的复杂推理能力，并在五个基准测试中取得了4-5%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型多智能体系统协作协议固定，忽略了智能体的内省能力，该论文旨在提升智能体根据自身认知状态（如不确定性或置信度）自适应调整策略的能力。

Method: 提出元策略协商框架(MPDF)，其中智能体学习一组元认知动作（坚持、细化、让步）的去中心化策略；开发SoftRankPO算法，通过基于奖励排序的优势塑造来稳定策略梯度训练。

Result: 在五个数学和一般推理基准测试中，与现有六种算法相比，MPDF结合SoftRankPO取得了4-5%的平均准确率提升。

Conclusion: 该研究为学习大型语言模型多智能体系统的自适应元认知策略提供了一种范例，从设计固定协议转向学习动态的、有思考的策略。

Abstract: Multi-agent systems of large language models (LLMs) show promise for complex
reasoning, but their effectiveness is often limited by fixed collaboration
protocols. These frameworks typically focus on macro-level orchestration while
overlooking agents' internal deliberative capabilities. This critical
meta-cognitive blindspot treats agents as passive executors unable to adapt
their strategy based on internal cognitive states like uncertainty or
confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where
agents learn a decentralized policy over a set of high-level meta-cognitive
actions: Persist, Refine, and Concede. To overcome the instability of
traditional policy gradients in this setting, we develop SoftRankPO, a novel
reinforcement learning algorithm. SoftRankPO stabilizes training by shaping
advantages based on the rank of rewards mapped through smooth normal quantiles,
making the learning process robust to reward variance. Experiments show that
MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across
five mathematical and general reasoning benchmarks compared to six
state-of-the-art heuristic and learning-based multi-agent reasoning algorithms.
Our work presents a paradigm for learning adaptive, meta-cognitive policies for
multi-agent LLM systems, shifting the focus from designing fixed protocols to
learning dynamic, deliberative strategies.

</details>


### [16] [What Would an LLM Do? Evaluating Policymaking Capabilities of Large Language Models](https://arxiv.org/abs/2509.03827)
*Pierre Le Coz,Jia An Liu,Debarun Bhattacharjya,Georgina Curto,Serge Stinckwich*

Main category: cs.AI

TL;DR: 该论文评估了大型语言模型(LLM)在帮助解决全球性问题，例如无家可归者问题上的潜力，并提出了一种结合LLM、情景模拟和基于主体模型的新方法。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在社会政策制定(以解决无家可归问题为例)中的适用性，并探索其与领域专家的意见一致性。

Method: 开发了一个包含跨四个地区(南本德、巴塞罗那、约翰内斯堡和澳门)政策选择的决策情景基准，并将基准政策与基于主体的模型连接起来，以模拟推荐政策的社会影响。

Result: 结果显示，在负责任的保障和与当地领域专家的合作下，LLM可以为人类提供有价值的替代政策见解。

Conclusion: 大型语言模型有潜力辅助社会政策制定，但需要与领域专家合作，并设置相应的保障措施。

Abstract: Large language models (LLMs) are increasingly being adopted in high-stakes
domains. Their capacity to process vast amounts of unstructured data, explore
flexible scenarios, and handle a diversity of contextual factors can make them
uniquely suited to provide new insights for the complexity of social
policymaking. This article evaluates whether LLMs' are aligned with domain
experts (and among themselves) to inform social policymaking on the subject of
homelessness alleviation - a challenge affecting over 150 million people
worldwide. We develop a novel benchmark comprised of decision scenarios with
policy choices across four geographies (South Bend, USA; Barcelona, Spain;
Johannesburg, South Africa; Macau SAR, China). The policies in scope are
grounded in the conceptual framework of the Capability Approach for human
development. We also present an automated pipeline that connects the
benchmarked policies to an agent-based model, and we explore the social impact
of the recommended policies through simulated social scenarios. The paper
results reveal promising potential to leverage LLMs for social policy making.
If responsible guardrails and contextual calibrations are introduced in
collaboration with local domain experts, LLMs can provide humans with valuable
insights, in the form of alternative policies at scale.

</details>
