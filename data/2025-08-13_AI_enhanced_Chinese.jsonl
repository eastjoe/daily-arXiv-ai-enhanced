{"id": "2508.08293", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08293", "abs": "https://arxiv.org/abs/2508.08293", "authors": ["Sridhar Mahadevan"], "title": "Topos Theory for Generative AI and LLMs", "comment": "30 pages", "summary": "We propose the design of novel categorical generative AI architectures\n(GAIAs) using topos theory, a type of category that is ``set-like\": a topos has\nall (co)limits, is Cartesian closed, and has a subobject classifier. Previous\ntheoretical results on the Transformer model have shown that it is a universal\nsequence-to-sequence function approximator, and dense in the space of all\ncontinuous functions with compact support on the Euclidean space of embeddings\nof tokens. Building on this theoretical result, we explore novel architectures\nfor LLMs that exploit the property that the category of LLMs, viewed as\nfunctions, forms a topos. Previous studies of large language models (LLMs) have\nfocused on daisy-chained linear architectures or mixture-of-experts. In this\npaper, we use universal constructions in category theory to construct novel LLM\narchitectures based on new types of compositional structures. In particular,\nthese new compositional structures are derived from universal properties of LLM\ncategories, and include pullback, pushout, (co) equalizers, exponential\nobjects, and subobject classifiers. We theoretically validate these new\ncompositional structures by showing that the category of LLMs is (co)complete,\nmeaning that all diagrams have solutions in the form of (co)limits. Building on\nthis completeness result, we then show that the category of LLMs forms a topos,\na ``set-like\" category, which requires showing the existence of exponential\nobjects as well as subobject classifiers. We use a functorial characterization\nof backpropagation to define a potential implementation of an LLM topos\narchitecture.", "AI": {"tldr": "\u57fa\u4e8e\u62d3\u6251\u7406\u8bba\uff0c\u63d0\u51fa\u65b0\u578bLLM\u67b6\u6784\uff0c\u5e76\u7406\u8bba\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u8868\u660eTransformer\u662f\u901a\u7528\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u51fd\u6570\u903c\u8fd1\u5668\uff0c\u800c\u672c\u6587\u5219\u63a2\u7d22\u5229\u7528LLM\u8303\u7574\u6784\u6210\u62d3\u6251\u7684\u7279\u6027\u8bbe\u8ba1\u65b0\u578b\u67b6\u6784\u3002", "method": "\u4f7f\u7528\u8303\u7574\u8bba\u4e2d\u7684\u6cdb\u6784\u9020\u6784\u5efa\u65b0\u578bLLM\u67b6\u6784\uff0c\u5305\u62ec\u62c9\u56de\u3001\u63a8\u524d\u3001\uff08\u4f59\uff09\u5747\u8861\u5b50\u3001\u6307\u6570\u5bf9\u8c61\u548c\u5b50\u5bf9\u8c61\u5206\u7c7b\u5668\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86LLM\u8303\u7574\u662f\uff08\u4f59\uff09\u5b8c\u5168\u7684\uff0c\u5e76\u6784\u6210\u4e00\u4e2a\u62d3\u6251\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51fd\u5b50\u63cf\u8ff0\u7684\u53cd\u5411\u4f20\u64ad\u7684\u6f5c\u5728\u5b9e\u73b0\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u62d3\u6251\u7406\u8bba\u8bbe\u8ba1\u65b0\u578b\u5206\u7c7b\u751f\u6210\u5f0fAI\u67b6\u6784\u7684\u65b9\u6cd5\uff0c\u5e76\u7406\u8bba\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u67b6\u6784\u7684\u6709\u6548\u6027\uff0c\u5305\u62ecLLM\u7684\u8303\u7574\u6784\u6210\u4e00\u4e2a\u62d3\u6251\u8fd9\u4e00\u7ed3\u8bba\u3002"}}
{"id": "2508.08295", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08295", "abs": "https://arxiv.org/abs/2508.08295", "authors": ["Sridhar Mahadevan"], "title": "Topos Causal Models", "comment": "31 pages", "summary": "We propose topos causal models (TCMs), a novel class of causal models that\nexploit the key properties of a topos category: they are (co)complete, meaning\nall (co)limits exist, they admit a subobject classifier, and allow exponential\nobjects. The main goal of this paper is to show that these properties are\ncentral to many applications in causal inference. For example, subobject\nclassifiers allow a categorical formulation of causal intervention, which\ncreates sub-models. Limits and colimits allow causal diagrams of arbitrary\ncomplexity to be ``solved\", using a novel interpretation of causal\napproximation. Exponential objects enable reasoning about equivalence classes\nof operations on causal models, such as covered edge reversal and causal\nhomotopy. Analogous to structural causal models (SCMs), TCMs are defined by a\ncollection of functions, each defining a ``local autonomous\" causal mechanism\nthat assemble to induce a unique global function from exogenous to endogenous\nvariables. Since the category of TCMs is (co)complete, which we prove in this\npaper, every causal diagram has a ``solution\" in the form of a (co)limit: this\nimplies that any arbitrary causal model can be ``approximated\" by some global\nfunction with respect to the morphisms going into or out of the diagram.\nNatural transformations are crucial in measuring the quality of approximation.\nIn addition, we show that causal interventions are modeled by subobject\nclassifiers: any sub-model is defined by a monic arrow into its parent model.\nExponential objects permit reasoning about entire classes of causal\nequivalences and interventions. Finally, as TCMs form a topos, they admit an\ninternal logic defined as a Mitchell-Benabou language with an associated\nKripke-Joyal semantics. We show how to reason about causal models in TCMs using\nthis internal logic.", "AI": {"tldr": "\u63d0\u51fa\u62d3\u6251\u56e0\u679c\u6a21\u578b (TCM)\uff0c\u5229\u7528\u62d3\u6251\u8303\u7574\u7406\u8bba\u89e3\u51b3\u590d\u6742\u56e0\u679c\u5173\u7cfb\u5efa\u6a21\u548c\u5e72\u9884\u95ee\u9898\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u8868\u8fbe\u80fd\u529b\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u56e0\u679c\u5173\u7cfb\u548c\u5e72\u9884\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u62d3\u6251\u8303\u7574\u7684\u7406\u8bba\u6784\u5efa\u56e0\u679c\u6a21\u578b\uff0c\u5229\u7528\u5176\u5b8c\u5907\u6027\u3001\u5bf9\u5076\u5b8c\u5907\u6027\u3001\u5b50\u5bf9\u8c61\u5206\u7c7b\u5668\u548c\u6307\u6570\u5bf9\u8c61\u7b49\u5c5e\u6027\u89e3\u51b3\u56e0\u679c\u63a8\u7406\u95ee\u9898\u3002", "result": "\u6784\u5efa\u4e86\u62d3\u6251\u56e0\u679c\u6a21\u578b (TCM)\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u5904\u7406\u4efb\u610f\u590d\u6742\u7684\u56e0\u679c\u56fe\uff0c\u5bf9\u56e0\u679c\u5e72\u9884\u8fdb\u884c\u5206\u7c7b\u8868\u793a\uff0c\u5e76\u5141\u8bb8\u5bf9\u56e0\u679c\u6a21\u578b\u4e0a\u7684\u8fd0\u7b97\u7b49\u4ef7\u7c7b\u8fdb\u884c\u63a8\u7406\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u56e0\u679c\u6a21\u578b\uff1a\u62d3\u6251\u56e0\u679c\u6a21\u578b (TCM)\uff0c\u5229\u7528\u4e86\u62d3\u6251\u8303\u7574\u7684\u5173\u952e\u5c5e\u6027\uff08\u5b8c\u5907\u6027\u3001\u5bf9\u5076\u5b8c\u5907\u6027\u3001\u5b50\u5bf9\u8c61\u5206\u7c7b\u5668\u548c\u6307\u6570\u5bf9\u8c61\uff09\uff0c\u89e3\u51b3\u4e86\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u591a\u4e2a\u95ee\u9898\uff0c\u4f8b\u5982\uff1a\u5b50\u5bf9\u8c61\u5206\u7c7b\u5668\u5141\u8bb8\u5bf9\u56e0\u679c\u5e72\u9884\u8fdb\u884c\u5206\u7c7b\u8868\u793a\uff0c\u6781\u9650\u548c\u4e0a\u6781\u9650\u5141\u8bb8\u6c42\u89e3\u4efb\u610f\u590d\u6742\u7684\u56e0\u679c\u56fe\uff0c\u6307\u6570\u5bf9\u8c61\u5141\u8bb8\u5bf9\u56e0\u679c\u6a21\u578b\u4e0a\u7684\u8fd0\u7b97\u7b49\u4ef7\u7c7b\u8fdb\u884c\u63a8\u7406\u3002"}}
{"id": "2508.08297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08297", "abs": "https://arxiv.org/abs/2508.08297", "authors": ["Rodrigo Lankaites Pinheiro", "Dario Landa-Silva", "Wasakorn Laesanklang", "Ademir Aparecido Constantino"], "title": "An Efficient Application of Goal Programming to Tackle Multiobjective Problems with Recurring Fitness Landscapes", "comment": null, "summary": "Many real-world applications require decision-makers to assess the quality of\nsolutions while considering multiple conflicting objectives. Obtaining good\napproximation sets for highly constrained many-objective problems is often a\ndifficult task even for modern multiobjective algorithms. In some cases,\nmultiple instances of the problem scenario present similarities in their\nfitness landscapes. That is, there are recurring features in the fitness\nlandscapes when searching for solutions to different problem instances. We\npropose a methodology to exploit this characteristic by solving one instance of\na given problem scenario using computationally expensive multiobjective\nalgorithms to obtain a good approximation set and then using Goal Programming\nwith efficient single-objective algorithms to solve other instances of the same\nproblem scenario. We use three goal-based objective functions and show that on\nbenchmark instances of the multiobjective vehicle routing problem with time\nwindows, the methodology is able to produce good results in short computation\ntime. The methodology allows to combine the effectiveness of state-of-the-art\nmultiobjective algorithms with the efficiency of goal programming to find good\ncompromise solutions in problem scenarios where instances have similar fitness\nlandscapes.", "AI": {"tldr": "\u9488\u5bf9\u5177\u6709\u76f8\u4f3c\u9002\u5e94\u5ea6\u666f\u89c2\u7684\u591a\u76ee\u6807\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u76ee\u6807\u89c4\u5212\u548c\u9ad8\u6548\u5355\u76ee\u6807\u7b97\u6cd5\uff0c\u7ed3\u5408\u5148\u8fdb\u591a\u76ee\u6807\u7b97\u6cd5\u7684\u4f18\u52bf\uff0c\u5feb\u901f\u6709\u6548\u5730\u627e\u5230\u9ad8\u8d28\u91cf\u89e3\u3002", "motivation": "\u8bb8\u591a\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u9700\u8981\u51b3\u7b56\u8005\u5728\u8003\u8651\u591a\u4e2a\u76f8\u4e92\u51b2\u7a81\u7684\u76ee\u6807\u65f6\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848\u7684\u8d28\u91cf\u3002\u5373\u4f7f\u5bf9\u4e8e\u73b0\u4ee3\u591a\u76ee\u6807\u7b97\u6cd5\uff0c\u83b7\u5f97\u9ad8\u5ea6\u7ea6\u675f\u7684\u591a\u76ee\u6807\u95ee\u9898\u7684\u826f\u597d\u8fd1\u4f3c\u96c6\u901a\u5e38\u4e5f\u662f\u4e00\u9879\u8270\u5de8\u7684\u4efb\u52a1\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u95ee\u9898\u573a\u666f\u7684\u591a\u4e2a\u5b9e\u4f8b\u5728\u5176\u9002\u5e94\u5ea6\u666f\u89c2\u4e2d\u5b58\u5728\u76f8\u4f3c\u4e4b\u5904\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u4f7f\u7528\u8ba1\u7b97\u4ee3\u4ef7\u9ad8\u7684\u591a\u76ee\u6807\u7b97\u6cd5\u6c42\u89e3\u4e00\u4e2a\u7ed9\u5b9a\u95ee\u9898\u573a\u666f\u7684\u5b9e\u4f8b\uff0c\u5f97\u5230\u4e00\u4e2a\u597d\u7684\u8fd1\u4f3c\u96c6\uff1b\u7136\u540e\u4f7f\u7528\u76ee\u6807\u89c4\u5212\u548c\u9ad8\u6548\u7684\u5355\u76ee\u6807\u7b97\u6cd5\u6c42\u89e3\u540c\u4e00\u95ee\u9898\u573a\u666f\u7684\u5176\u4ed6\u5b9e\u4f8b\u3002\u4f7f\u7528\u4e86\u4e09\u4e2a\u57fa\u4e8e\u76ee\u6807\u7684\u5ba2\u89c2\u51fd\u6570\u3002", "result": "\u5728\u5e26\u65f6\u95f4\u7a97\u7684\u591a\u76ee\u6807\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u7684\u57fa\u51c6\u5b9e\u4f8b\u4e0a\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u8f83\u77ed\u7684\u8ba1\u7b97\u65f6\u95f4\u5185\u4ea7\u751f\u826f\u597d\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u6700\u5148\u8fdb\u7684\u591a\u76ee\u6807\u7b97\u6cd5\u7684\u6709\u6548\u6027\u548c\u76ee\u6807\u89c4\u5212\u7684\u6548\u7387\uff0c\u5728\u95ee\u9898\u5b9e\u4f8b\u5177\u6709\u76f8\u4f3c\u9002\u5e94\u5ea6\u666f\u89c2\u7684\u573a\u666f\u4e2d\u627e\u5230\u4e86\u826f\u597d\u7684\u6298\u8877\u65b9\u6848\u3002"}}
{"id": "2508.08300", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08300", "abs": "https://arxiv.org/abs/2508.08300", "authors": ["Yongchao Huang"], "title": "LLM-BI: Towards Fully Automated Bayesian Inference with Large Language Models", "comment": "6 pages", "summary": "A significant barrier to the widespread adoption of Bayesian inference is the\nspecification of prior distributions and likelihoods, which often requires\nspecialized statistical expertise. This paper investigates the feasibility of\nusing a Large Language Model (LLM) to automate this process. We introduce\nLLM-BI (Large Language Model-driven Bayesian Inference), a conceptual pipeline\nfor automating Bayesian workflows. As a proof-of-concept, we present two\nexperiments focused on Bayesian linear regression. In Experiment I, we\ndemonstrate that an LLM can successfully elicit prior distributions from\nnatural language. In Experiment II, we show that an LLM can specify the entire\nmodel structure, including both priors and the likelihood, from a single\nhigh-level problem description. Our results validate the potential of LLMs to\nautomate key steps in Bayesian modeling, enabling the possibility of an\nautomated inference pipeline for probabilistic programming.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u81ea\u52a8\u5316\u8d1d\u53f6\u65af\u63a8\u7406\u6d41\u7a0b\uff0c\u51cf\u5c11\u5bf9\u4e13\u4e1a\u7edf\u8ba1\u77e5\u8bc6\u7684\u9700\u6c42", "motivation": "\u8d1d\u53f6\u65af\u63a8\u7406\u7684\u5e7f\u6cdb\u5e94\u7528\u53d7\u5230\u5148\u9a8c\u5206\u5e03\u548c\u4f3c\u7136\u51fd\u6570\u89c4\u8303\u7684\u963b\u788d\uff0c\u8fd9\u901a\u5e38\u9700\u8981\u4e13\u4e1a\u7684\u7edf\u8ba1\u77e5\u8bc6\u3002", "method": "\u63d0\u51faLLM-BI\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8d1d\u53f6\u65af\u63a8\u7406\u6d41\u7a0b\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u8d1d\u53f6\u65af\u7ebf\u6027\u56de\u5f52\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002\u5b9e\u9a8c\u4e00\u9a8c\u8bc1\u4e86LLM\u53ef\u4ee5\u4ece\u81ea\u7136\u8bed\u8a00\u4e2d\u6210\u529f\u63d0\u53d6\u5148\u9a8c\u5206\u5e03\uff1b\u5b9e\u9a8c\u4e8c\u9a8c\u8bc1\u4e86LLM\u53ef\u4ee5\u4ece\u5355\u4e2a\u9ad8\u7ea7\u95ee\u9898\u63cf\u8ff0\u4e2d\u6307\u5b9a\u6574\u4e2a\u6a21\u578b\u7ed3\u6784\uff0c\u5305\u62ec\u5148\u9a8c\u548c\u4f3c\u7136\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86LLM\u5728\u81ea\u52a8\u5316\u8d1d\u53f6\u65af\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "LLM\u53ef\u4ee5\u81ea\u52a8\u5316\u8d1d\u53f6\u65af\u5efa\u6a21\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u5b9e\u73b0\u6982\u7387\u7f16\u7a0b\u7684\u81ea\u52a8\u5316\u63a8\u7406\u6d41\u7a0b\u3002"}}
{"id": "2508.08308", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08308", "abs": "https://arxiv.org/abs/2508.08308", "authors": ["Chuanruo Fu", "Yuncheng Du"], "title": "First Ask Then Answer: A Framework Design for AI Dialogue Based on Supplementary Questioning with Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) often struggle to deliver accurate and\nactionable answers when user-provided information is incomplete or\nill-specified. We propose a new interaction paradigm, First Ask Then Answer\n(FATA), in which, through prompt words, LLMs are guided to proactively generate\nmultidimensional supplementary questions for users prior to response\ngeneration. Subsequently, by integrating user-provided supplementary\ninformation with the original query through sophisticated prompting techniques,\nwe achieve substantially improved response quality and relevance. In contrast\nto existing clarification approaches -- such as the CLAM framework oriented to\nambiguity and the self-interrogation Self-Ask method -- FATA emphasizes\ncompleteness (beyond mere disambiguation) and user participation (inviting\nhuman input instead of relying solely on model-internal reasoning). It also\nadopts a single-turn strategy: all clarifying questions are produced at once,\nthereby reducing dialogue length and improving efficiency. Conceptually, FATA\nuses the reasoning power of LLMs to scaffold user expression, enabling\nnon-expert users to formulate more comprehensive and contextually relevant\nqueries. To evaluate FATA, we constructed a multi-domain benchmark and compared\nit with two controls: a baseline prompt (B-Prompt) and a context-enhanced\nexpert prompt (C-Prompt). Experimental results show that FATA outperforms\nB-Prompt by approximately 40% in aggregate metrics and exhibits a coefficient\nof variation 8% lower than C-Prompt, indicating superior stability.", "AI": {"tldr": "FATA\u5f15\u5bfcLLM\u5148\u95ee\u95ee\u9898\u518d\u56de\u7b54\uff0c\u4ee5\u6539\u8fdbLLM\u5728\u4fe1\u606f\u4e0d\u5b8c\u6574\u60c5\u51b5\u4e0b\u7684\u54cd\u5e94\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684LLM\u5728\u5904\u7406\u4e0d\u5b8c\u6574\u6216\u4e0d\u660e\u786e\u7684\u7528\u6237\u63d0\u4f9b\u4fe1\u606f\u65f6\uff0c\u96be\u4ee5\u63d0\u4f9b\u51c6\u786e\u548c\u53ef\u64cd\u4f5c\u7684\u7b54\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ea4\u4e92\u8303\u5f0fFATA\uff0c\u8be5\u8303\u5f0f\u5f15\u5bfcLLM\u4e3b\u52a8\u751f\u6210\u591a\u7ef4\u8865\u5145\u95ee\u9898\uff0c\u5e76\u5c06\u7528\u6237\u63d0\u4f9b\u7684\u8865\u5145\u4fe1\u606f\u4e0e\u539f\u59cb\u67e5\u8be2\u96c6\u6210\uff0c\u4ee5\u6539\u8fdb\u54cd\u5e94\u8d28\u91cf\u3002", "result": "FATA\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7ea640%\uff0c\u5e76\u4e14\u53d8\u5f02\u7cfb\u6570\u6bd4\u4e0a\u4e0b\u6587\u589e\u5f3a\u4e13\u5bb6\u63d0\u793a\u4f4e8%\uff0c\u8868\u660e\u5176\u5177\u6709\u66f4\u9ad8\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "FATA\u6846\u67b6\u901a\u8fc7\u5728LLM\u751f\u6210\u7b54\u6848\u524d\u63d0\u51fa\u8865\u5145\u95ee\u9898\u6765\u6539\u8fdbLLM\u7684\u54cd\u5e94\u8d28\u91cf\u548c\u76f8\u5173\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.08344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08344", "abs": "https://arxiv.org/abs/2508.08344", "authors": ["Dongzhuoran Zhou", "Yuqicheng Zhu", "Xiaxia Wang", "Hongkuan Zhou", "Yuan He", "Jiaoyan Chen", "Evgeny Kharlamov", "Steffen Staab"], "title": "What Breaks Knowledge Graph based RAG? Empirical Insights into Reasoning under Incomplete Knowledge", "comment": null, "summary": "Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) is an\nincreasingly explored approach for combining the reasoning capabilities of\nlarge language models with the structured evidence of knowledge graphs.\nHowever, current evaluation practices fall short: existing benchmarks often\ninclude questions that can be directly answered using existing triples in KG,\nmaking it unclear whether models perform reasoning or simply retrieve answers\ndirectly. Moreover, inconsistent evaluation metrics and lenient answer matching\ncriteria further obscure meaningful comparisons. In this work, we introduce a\ngeneral method for constructing benchmarks, together with an evaluation\nprotocol, to systematically assess KG-RAG methods under knowledge\nincompleteness. Our empirical results show that current KG-RAG methods have\nlimited reasoning ability under missing knowledge, often rely on internal\nmemorization, and exhibit varying degrees of generalization depending on their\ndesign.", "AI": {"tldr": "\u65b0\u65b9\u6cd5\u8bc4\u4f30\u4e86\u77e5\u8bc6\u4e0d\u5b8c\u6574\u60c5\u51b5\u4e0bKG-RAG\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u80fd\u529b\u6709\u9650\uff0c\u5e38\u4f9d\u8d56\u8bb0\u5fc6\uff0c\u6cdb\u5316\u80fd\u529b\u4e5f\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\uff0c\u4f8b\u5982\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5305\u542b\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528KG\u4e2d\u73b0\u6709\u4e09\u5143\u7ec4\u56de\u7b54\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u8bc4\u4f30\u6307\u6807\u548c\u7b54\u6848\u5339\u914d\u6807\u51c6\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u5efa\u57fa\u51c6\u7684\u901a\u7528\u65b9\u6cd5\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u7528\u4e8e\u7cfb\u7edf\u5730\u8bc4\u4f30\u77e5\u8bc6\u4e0d\u5b8c\u6574\u60c5\u51b5\u4e0b\u7684KG-RAG\u65b9\u6cd5\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524dKG-RAG\u65b9\u6cd5\u5728\u77e5\u8bc6\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u7ecf\u5e38\u4f9d\u8d56\u4e8e\u5185\u90e8\u8bb0\u5fc6\uff0c\u5e76\u4e14\u6cdb\u5316\u80fd\u529b\u56e0\u8bbe\u8ba1\u800c\u5f02\u3002", "conclusion": "\u5f53\u524dKG-RAG\u65b9\u6cd5\u5728\u77e5\u8bc6\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u7ecf\u5e38\u4f9d\u8d56\u4e8e\u5185\u90e8\u8bb0\u5fc6\uff0c\u5e76\u4e14\u6cdb\u5316\u80fd\u529b\u56e0\u8bbe\u8ba1\u800c\u5f02\u3002"}}
{"id": "2508.08382", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08382", "abs": "https://arxiv.org/abs/2508.08382", "authors": ["Timo Bertram"], "title": "UrzaGPT: LoRA-Tuned Large Language Models for Card Selection in Collectible Card Games", "comment": null, "summary": "Collectible card games (CCGs) are a difficult genre for AI due to their\npartial observability, long-term decision-making, and evolving card sets. Due\nto this, current AI models perform vastly worse than human players at CCG tasks\nsuch as deckbuilding and gameplay. In this work, we introduce\n$\\textit{UrzaGPT}$, a domain-adapted large language model that recommends\nreal-time drafting decisions in $\\textit{Magic: The Gathering}$. Starting from\nan open-weight LLM, we use Low-Rank Adaptation fine-tuning on a dataset of\nannotated draft logs. With this, we leverage the language modeling capabilities\nof LLM, and can quickly adapt to different expansions of the game. We benchmark\n$\\textit{UrzaGPT}$ in comparison to zero-shot LLMs and the state-of-the-art\ndomain-specific model. Untuned, small LLMs like Llama-3-8B are completely\nunable to draft, but the larger GPT-4o achieves a zero-shot performance of\n$43\\%$. Using UrzaGPT to fine-tune smaller models, we achieve an accuracy of\n$66.2\\%$ using only 10,000 steps. Despite this not reaching the capability of\ndomain-specific models, we show that solely using LLMs to draft is possible and\nconclude that using LLMs can enable performant, general, and update-friendly\ndrafting AIs in the future.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86UrzaGPT\uff0c\u4e00\u4e2a\u7528\u4e8eMagic: The Gathering\u5b9e\u65f6\u9009\u724c\u7684LLM\u6a21\u578b\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u96f6\u6837\u672cLLM\uff0c\u663e\u793a\u4e86LLM\u5728CCG AI\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u7684AI\u6a21\u578b\u5728CCG\u4efb\u52a1(\u5982\u6784\u7ec4\u5957\u724c\u548c\u6e38\u620f\u8fc7\u7a0b)\u4e2d\u7684\u8868\u73b0\u8fdc\u900a\u4e8e\u4eba\u7c7b\u73a9\u5bb6\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\uff0c\u4f7f\u7528\u4f4e\u79e9\u81ea\u9002\u5e94\u5fae\u8c03\u6280\u672f\uff0c\u5e76\u4f7f\u7528\u5e26\u6ce8\u91ca\u7684\u9009\u724c\u65e5\u5fd7\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728Magic: The Gathering\u6e38\u620f\u4e2d\uff0cUrzaGPT\u6a21\u578b\u8fbe\u5230\u4e8666.2%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u96f6\u6837\u672cLLM\uff0c\u4f46\u4ecd\u4f4e\u4e8e\u7279\u5b9a\u9886\u57df\u6a21\u578b\u3002", "conclusion": "\u4f7f\u7528LLM\u8fdb\u884c\u96c6\u6362\u5f0f\u5361\u724c\u6e38\u620f(CCG)\u9009\u724c\u662f\u53ef\u884c\u7684\uff0c\u5e76\u80fd\u521b\u5efa\u9ad8\u6027\u80fd\u3001\u901a\u7528\u4e14\u6613\u4e8e\u66f4\u65b0\u7684\u9009\u724cAI\u3002"}}
{"id": "2508.08385", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08385", "abs": "https://arxiv.org/abs/2508.08385", "authors": ["Masataro Asai"], "title": "Bilevel MCTS for Amortized O(1) Node Selection in Classical Planning", "comment": null, "summary": "We study an efficient implementation of Multi-Armed Bandit (MAB)-based\nMonte-Carlo Tree Search (MCTS) for classical planning. One weakness of MCTS is\nthat it spends a significant time deciding which node to expand next. While\nselecting a node from an OPEN list with $N$ nodes has $O(1)$ runtime complexity\nwith traditional array-based priority-queues for dense integer keys, the\ntree-based OPEN list used by MCTS requires $O(\\log N)$, which roughly\ncorresponds to the search depth $d$. In classical planning, $d$ is arbitrarily\nlarge (e.g., $2^k-1$ in $k$-disk Tower-of-Hanoi) and the runtime for node\nselection is significant, unlike in game tree search, where the cost is\nnegligible compared to the node evaluation (rollouts) because $d$ is inherently\nlimited by the game (e.g., $d\\leq 361$ in Go). To improve this bottleneck, we\npropose a bilevel modification to MCTS that runs a best-first search from each\nselected leaf node with an expansion budget proportional to $d$, which achieves\namortized $O(1)$ runtime for node selection, equivalent to the traditional\nqueue-based OPEN list. In addition, we introduce Tree Collapsing, an\nenhancement that reduces action selection steps and further improves the\nperformance.", "AI": {"tldr": "\u6539\u8fdbMCTS\u7b97\u6cd5\uff0c\u901a\u8fc7\u53cc\u5c42\u641c\u7d22\u548c\u6811\u6298\u53e0\uff0c\u5c06\u8282\u70b9\u9009\u62e9\u65f6\u95f4\u590d\u6742\u5ea6\u4eceO(logN)\u964d\u5230O(1)\uff0c\u63d0\u5347\u7ecf\u5178\u89c4\u5212\u6548\u7387\u3002", "motivation": "\u7ecf\u5178\u89c4\u5212\u95ee\u9898\u4e2d\uff0cMCTS \u7684\u8282\u70b9\u9009\u62e9\u6b65\u9aa4\uff08\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a O(log N)\uff09\u6210\u4e3a\u74f6\u9888\uff0c\u672c\u6587\u65e8\u5728\u6539\u8fdb\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u57fa\u4e8e\u591a\u81c2\u8001\u864e\u673a (MAB) \u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22 (MCTS) \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5305\u542b\u53cc\u5c42\u641c\u7d22\u548c\u6811\u6298\u53e0\u6280\u672f\u3002", "result": "\u901a\u8fc7\u53cc\u5c42\u641c\u7d22\u548c\u6811\u6298\u53e0\u6280\u672f\uff0c\u5c06\u8282\u70b9\u9009\u62e9\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u964d\u4f4e\u5230 O(1)\uff0c\u63d0\u9ad8\u4e86\u7b97\u6cd5\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u57fa\u4e8e\u591a\u81c2\u8001\u864e\u673a (MAB) \u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22 (MCTS) \u65b9\u6cd5\uff0c\u7528\u4e8e\u7ecf\u5178\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u5c42\u641c\u7d22\u548c\u6811\u6298\u53e0\u6280\u672f\u5c06\u8282\u70b9\u9009\u62e9\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4ece O(log N) \u964d\u4f4e\u5230 O(1)\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u7b97\u6cd5\u6548\u7387\u3002"}}
{"id": "2508.08442", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08442", "abs": "https://arxiv.org/abs/2508.08442", "authors": ["Niklas Dewally", "\u00d6zg\u00fcr Akg\u00fcn"], "title": "Solver-Aided Expansion of Loops to Avoid Generate-and-Test", "comment": "13 pages, 4 figures, published in ModRef 2025 workshop", "summary": "Constraint modelling languages like MiniZinc and Essence rely on unrolling\nloops (in the form of quantified expressions and comprehensions) during\ncompilation. Standard approaches generate all combinations of induction\nvariables and use partial evaluation to discard those that simplify to identity\nelements of associative-commutative operators (e.g. true for conjunction, 0 for\nsummation). This can be inefficient for problems where most combinations are\nultimately irrelevant. We present a method that avoids full enumeration by\nusing a solver to compute only the combinations required to generate the final\nset of constraints. The resulting model is identical to that produced by\nconventional flattening, but compilation can be significantly faster. This\nimproves the efficiency of translating high-level user models into solver-ready\nform, particularly when induction variables range over large domains with\nselective preconditions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u7f16\u8bd1\u65b9\u6cd5\uff0c\u901a\u8fc7\u6c42\u89e3\u5668\u8ba1\u7b97\u5fc5\u8981\u7ec4\u5408\uff0c\u63d0\u9ad8\u4e86\u7ea6\u675f\u5efa\u6a21\u8bed\u8a00\u7684\u7f16\u8bd1\u6548\u7387\uff0c\u5c24\u5176\u5728\u5f52\u7eb3\u53d8\u91cf\u8303\u56f4\u5f88\u5927\u4e14\u5b58\u5728\u9009\u62e9\u6027\u524d\u63d0\u6761\u4ef6\u65f6\u3002", "motivation": "\u6807\u51c6\u65b9\u6cd5\u751f\u6210\u6240\u6709\u5f52\u7eb3\u53d8\u91cf\u7684\u7ec4\u5408\uff0c\u5e76\u4f7f\u7528\u90e8\u5206\u6c42\u503c\u6765\u4e22\u5f03\u7b80\u5316\u4e3a\u7ed3\u5408-\u4ea4\u6362\u8fd0\u7b97\u7b26\u7684\u6052\u7b49\u5143\u7d20\u7684\u90a3\u4e9b\u7ec4\u5408\uff0c\u8fd9\u5bf9\u4e8e\u5927\u591a\u6570\u7ec4\u5408\u6700\u7ec8\u65e0\u5173\u7d27\u8981\u7684\u95ee\u9898\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u4f7f\u7528\u6c42\u89e3\u5668\u8ba1\u7b97\u4ec5\u751f\u6210\u6700\u7ec8\u7ea6\u675f\u96c6\u6240\u9700\u7684\u7ec4\u5408\uff0c\u907f\u514d\u5b8c\u5168\u679a\u4e3e\u3002", "result": "\u751f\u6210\u7684\u6a21\u578b\u4e0e\u4f20\u7edf\u6241\u5e73\u5316\u751f\u6210\u7684\u6a21\u578b\u76f8\u540c\uff0c\u4f46\u7f16\u8bd1\u901f\u5ea6\u53ef\u80fd\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u907f\u514d\u5b8c\u5168\u679a\u4e3e\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u6c42\u89e3\u5668\u4ec5\u8ba1\u7b97\u751f\u6210\u6700\u7ec8\u7ea6\u675f\u96c6\u6240\u9700\u7684\u7ec4\u5408\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5c06\u9ad8\u7ea7\u7528\u6237\u6a21\u578b\u8f6c\u6362\u4e3a\u6c42\u89e3\u5668\u5c31\u7eea\u5f62\u5f0f\u7684\u6548\u7387\u3002"}}
{"id": "2508.08446", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08446", "abs": "https://arxiv.org/abs/2508.08446", "authors": ["Woojeong Kim", "Junxiong Wang", "Jing Nathan Yan", "Mohamed Abdelfattah", "Alexander M. Rush"], "title": "OverFill: Two-Stage Models for Efficient Language Model Decoding", "comment": "Accepted to COLM 2025", "summary": "Large language models (LLMs) excel across diverse tasks but face significant\ndeployment challenges due to high inference costs. LLM inference comprises\nprefill (compute-bound) and decode (memory-bound) stages, with decode\ndominating latency particularly for long sequences. Current decoder-only models\nhandle both stages uniformly, despite their distinct computational profiles. We\npropose OverFill, which decouples these stages to optimize accuracy-efficiency\ntradeoffs. OverFill begins with a full model for prefill, processing system and\nuser inputs in parallel. It then switches to a dense pruned model, while\ngenerating tokens sequentially. Leveraging more compute during prefill,\nOverFill improves generation quality with minimal latency overhead. Our\n3B-to-1B OverFill configuration outperforms 1B pruned models by 83.2%, while\nthe 8B-to-3B configuration improves over 3B pruned models by 79.2% on average\nacross standard benchmarks. OverFill matches the performance of same-sized\nmodels trained from scratch, while using significantly less training data. Our\ncode is available at https://github.com/friendshipkim/overfill.", "AI": {"tldr": "OverFill\u901a\u8fc7\u89e3\u8026\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\uff0c\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u89e3\u7801\u5668\u6a21\u578b\u7edf\u4e00\u5904\u7406\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u957f\u5e8f\u5217\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOverFill\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u89e3\u8026\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\uff0c\u5206\u522b\u4f7f\u7528\u5b8c\u6574\u6a21\u578b\u548c\u526a\u679d\u6a21\u578b\u8fdb\u884c\u5904\u7406\u3002", "result": "3B\u52301B\u7684OverFill\u914d\u7f6e\u4f18\u4e8e1B\u526a\u679d\u6a21\u578b83.2%\uff0c8B\u52303B\u914d\u7f6e\u4f18\u4e8e3B\u526a\u679d\u6a21\u578b79.2%\u3002OverFill\u4e0e\u540c\u7b49\u89c4\u6a21\u7684\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u76f8\u5f53\uff0c\u4f46\u8bad\u7ec3\u6570\u636e\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "OverFill\u6a21\u578b\u901a\u8fc7\u89e3\u8026\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\uff0c\u4f18\u5316\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u540c\u7b49\u89c4\u6a21\u7684\u6a21\u578b\u3002"}}
{"id": "2508.08477", "categories": ["cs.AI", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.08477", "abs": "https://arxiv.org/abs/2508.08477", "authors": ["Joan Salv\u00e0 Soler", "Gr\u00e9goire de Lambertye"], "title": "A Fast GRASP Metaheuristic for the Trigger Arc TSP with MIP-Based Construction and Multi-Neighborhood Local Search", "comment": "9 pages, 2 figures, 2-column format", "summary": "The Trigger Arc Traveling Salesman Problem (TA-TSP) extends the classical TSP\nby introducing dynamic arc costs that change when specific \\textit{trigger}\narcs are traversed, modeling scenarios such as warehouse operations with\ncompactable storage systems. This paper introduces a GRASP-based metaheuristic\nthat combines multiple construction heuristics with a multi-neighborhood local\nsearch. The construction phase uses mixed-integer programming (MIP) techniques\nto transform the TA-TSP into a sequence of tailored TSP instances, while the\nimprovement phase applies 2-Opt, Swap, and Relocate operators. Computational\nexperiments on MESS 2024 competition instances achieved average optimality gaps\nof 0.77\\% and 0.40\\% relative to the best-known solutions within a 60-second\nlimit. On smaller, synthetically generated datasets, the method produced\nsolutions 11.3\\% better than the Gurobi solver under the same time constraints.\nThe algorithm finished in the top three at MESS 2024, demonstrating its\nsuitability for real-time routing applications with state-dependent travel\ncosts.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u57fa\u4e8eGRASP\u7684\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u89e6\u53d1\u5f27\u65c5\u884c\u5546\u95ee\u9898\uff08TA-TSP\uff09\uff0c\u5e76\u5728MESS 2024\u7ade\u8d5b\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6210\u7ee9\u3002", "motivation": "\u7ecf\u5178\u65c5\u884c\u5546\u95ee\u9898\uff08TSP\uff09\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u52a8\u6001\u5f27\u6210\u672c\u95ee\u9898\uff0c\u4f8b\u5982\u4ed3\u5e93\u64cd\u4f5c\u4e2d\u53ef\u538b\u7f29\u5b58\u50a8\u7cfb\u7edf\u5e26\u6765\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u672c\u6587\u63d0\u51faTA-TSP\u53ca\u5176\u6c42\u89e3\u7b97\u6cd5\u3002", "method": "\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u591a\u79cd\u6784\u9020\u542f\u53d1\u5f0f\u7b97\u6cd5\u548c\u591a\u9886\u57df\u5c40\u90e8\u641c\u7d22\uff0c\u5229\u7528\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\u6280\u672f\u5c06TA-TSP\u8f6c\u5316\u4e3a\u4e00\u7cfb\u5217\u5b9a\u5236\u7684TSP\u5b9e\u4f8b\uff0c\u6539\u8fdb\u9636\u6bb5\u5e94\u75282-Opt\u3001Swap\u548cRelocate\u7b97\u5b50\u3002", "result": "\u5728MESS 2024\u7ade\u8d5b\u5b9e\u4f8b\u4e0a\uff0c\u8be5\u7b97\u6cd5\u572860\u79d2\u5185\u53d6\u5f97\u4e86\u5e73\u5747\u6700\u4f18\u5dee\u8ddd0.77%\u548c0.40%\u7684\u4f18\u5f02\u7ed3\u679c\uff1b\u5728\u8f83\u5c0f\u7684\u5408\u6210\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u7b97\u6cd5\u7684\u7ed3\u679c\u6bd4Gurobi\u6c42\u89e3\u5668\u597d11.3%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eGRASP\u7684\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u89e6\u53d1\u5f27\u65c5\u884c\u5546\u95ee\u9898\uff08TA-TSP\uff09\uff0c\u5e76\u5728MESS 2024\u7ade\u8d5b\u4e2d\u53d6\u5f97\u4e86\u524d\u4e09\u540d\u7684\u6210\u7ee9\uff0c\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u5728\u5177\u6709\u72b6\u6001\u76f8\u5173\u65c5\u884c\u6210\u672c\u7684\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u5e94\u7528\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2508.08486", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08486", "abs": "https://arxiv.org/abs/2508.08486", "authors": ["Parker Whitfill", "Stewy Slocum"], "title": "Beyond Ordinal Preferences: Why Alignment Needs Cardinal Human Feedback", "comment": null, "summary": "Alignment techniques for LLMs rely on optimizing preference-based objectives\n-- where these preferences are typically elicited as ordinal, binary choices\nbetween responses. Recent work has focused on improving label quality or\nmitigating particular biases, but we identify a more fundamental limitation:\nthese methods collect the wrong kind of data. We prove an impossibility result:\nno algorithm relying solely on ordinal comparisons can systematically recover\nthe most preferred model. Intuitively, ordinal data lacks the information\nneeded to resolve tradeoffs -- e.g., fixing a factual error on one prompt\nversus improving style on another. We show that selecting the optimal model\nrequires recovering preferences over \\emph{models} (rather than just\nresponses), which can only be identified given cardinal feedback about response\nquality. To address this, we collect and publicly release a dataset of 25,000\ncardinal judgments using willingness-to-pay elicitations, a well-established\ntool from experimental economics. Empirically, we find that incorporating\ncardinal feedback into preference fine-tuning allows models to prioritize\nhigh-impact improvements and outperform ordinal-only methods on downstream\nbenchmarks, such as Arena-Hard.", "AI": {"tldr": "\u4ec5\u7528\u5e8f\u6570\u6bd4\u8f83\u65e0\u6cd5\u8bad\u7ec3\u6700\u4f73LLM\u6a21\u578b\uff0c\u9700\u7528\u652f\u4ed8\u610f\u613f\u7b49\u57fa\u6570\u53cd\u9988\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u7684LLM\u5bf9\u9f50\u6280\u672f\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u5e8f\u6570\u6bd4\u8f83\u7684\u504f\u597d\u76ee\u6807\uff0c\u5b58\u5728\u65e0\u6cd5\u89e3\u51b3\u6743\u8861\u548c\u6062\u590d\u6700\u4f18\u6a21\u578b\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u652f\u4ed8\u610f\u613f\u6cd5\u6536\u96c6\u57fa\u6570\u5224\u65ad\uff0c\u5e76\u5c06\u5176\u878d\u5165\u504f\u597d\u5fae\u8c03\u4e2d\u3002", "result": "\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u57fa\u4e8e\u57fa\u6570\u5224\u65ad\u7684\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u540e\u7eed\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u4ec5\u4f7f\u7528\u5e8f\u6570\u6bd4\u8f83\u7684\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4ec5\u4f9d\u8d56\u5e8f\u6570\u6bd4\u8f83\u7684\u7b97\u6cd5\u65e0\u6cd5\u7cfb\u7edf\u5730\u6062\u590d\u6700\u4f18\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u652f\u4ed8\u610f\u613f\u6536\u96c6\u57fa\u6570\u5224\u65ad\u7684\u65b9\u6cd5\uff0c\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4ec5\u4f7f\u7528\u5e8f\u6570\u6bd4\u8f83\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.08493", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08493", "abs": "https://arxiv.org/abs/2508.08493", "authors": ["Szymon Jakubicz", "Karol Ku\u017aniak", "Jan Wawszczak", "Pawe\u0142 Gora"], "title": "POMO+: Leveraging starting nodes in POMO for solving Capacitated Vehicle Routing Problem", "comment": null, "summary": "In recent years, reinforcement learning (RL) methods have emerged as a\npromising approach for solving combinatorial problems. Among RL-based models,\nPOMO has demonstrated strong performance on a variety of tasks, including\nvariants of the Vehicle Routing Problem (VRP). However, there is room for\nimprovement for these tasks. In this work, we improved POMO, creating a method\n(\\textbf{POMO+}) that leverages the initial nodes to find a solution in a more\ninformed way. We ran experiments on our new model and observed that our\nsolution converges faster and achieves better results. We validated our models\non the CVRPLIB dataset and noticed improvements in problem instances with up to\n100 customers. We hope that our research in this project can lead to further\nadvancements in the field.", "AI": {"tldr": "\u6539\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5POMO+\u5728VRP\u95ee\u9898\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u6536\u655b\u66f4\u5feb\uff0c\u6548\u679c\u66f4\u597d\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684POMO\u65b9\u6cd5\u5728\u89e3\u51b3\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u4f8b\u5982\u8f66\u8f86\u8def\u5f84\u95ee\u9898 (VRP) \u7684\u53d8\u4f53\u65f6\uff0c\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u6539\u8fdb\u7684POMO (POMO+) \u65b9\u6cd5\uff0c\u5229\u7528\u521d\u59cb\u8282\u70b9\u66f4\u6709\u6548\u5730\u5bfb\u627e\u89e3\u51b3\u65b9\u6848\u3002", "result": "POMO+\u65b9\u6cd5\u5728CVRPLIB\u6570\u636e\u96c6\u4e0a\uff0c\u9488\u5bf9\u6700\u591a100\u4e2a\u5ba2\u6237\u7684\u6848\u4f8b\uff0c\u53d6\u5f97\u4e86\u6bd4POMO\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u7ed3\u679c\u3002", "conclusion": "POMO+\u65b9\u6cd5\u5728CVRPLIB\u6570\u636e\u96c6\u4e0a\uff0c\u9488\u5bf9\u6700\u591a100\u4e2a\u5ba2\u6237\u7684\u6848\u4f8b\uff0c\u901a\u8fc7\u5229\u7528\u521d\u59cb\u8282\u70b9\u66f4\u6709\u6548\u5730\u5bfb\u627e\u89e3\u51b3\u65b9\u6848\uff0c\u53d6\u5f97\u4e86\u6bd4POMO\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u7ed3\u679c\u3002"}}
