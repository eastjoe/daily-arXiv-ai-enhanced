<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [PG-Agent: An Agent Powered by Page Graph](https://arxiv.org/abs/2509.03536)
*Weizhi Chen,Ziwei Wang,Leyang Yang,Sheng Zhou,Xiaoxuan Tang,Jiajun Bu,Yong Li,Wei Jiang*

Main category: cs.AI

TL;DR: 本文提出了一种名为PG-Agent的GUI智能体框架，该框架利用页面图和检索增强生成技术，有效提升了GUI智能体对GUI环境的理解和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体通常利用多步操作的顺序序列作为先验知识，难以捕捉页面间的复杂关系，限制了其泛化能力。

Method: 将顺序操作序列转化为页面图，并利用检索增强生成技术从页面图中检索可靠的感知指导，最终构建PG-Agent框架，该框架包含任务分解策略。

Result: 实验结果表明，PG-Agent在多个基准测试中表现有效，即使页面图的构建数据有限。

Conclusion: PG-Agent框架通过页面图和检索增强生成技术有效提升了GUI智能体的泛化能力，为GUI智能体研究提供了新的方向。

Abstract: Graphical User Interface (GUI) agents possess significant commercial and
social value, and GUI agents powered by advanced multimodal large language
models (MLLMs) have demonstrated remarkable potential. Currently, existing GUI
agents usually utilize sequential episodes of multi-step operations across
pages as the prior GUI knowledge, which fails to capture the complex transition
relationship between pages, making it challenging for the agents to deeply
perceive the GUI environment and generalize to new scenarios. Therefore, we
design an automated pipeline to transform the sequential episodes into page
graphs, which explicitly model the graph structure of the pages that are
naturally connected by actions. To fully utilize the page graphs, we further
introduce Retrieval-Augmented Generation (RAG) technology to effectively
retrieve reliable perception guidelines of GUI from them, and a tailored
multi-agent framework PG-Agent with task decomposition strategy is proposed to
be injected with the guidelines so that it can generalize to unseen scenarios.
Extensive experiments on various benchmarks demonstrate the effectiveness of
PG-Agent, even with limited episodes for page graph construction.

</details>


### [2] [Multilinear and Linear Programs for Partially Identifiable Queries in Quasi-Markovian Structural Causal Models](https://arxiv.org/abs/2509.03548)
*João P. Arroyo,João G. Rodrigues,Daniel Lawand,Denis D. Mauá,Junkyu Lee,Radu Marinescu,Alex Gray,Eduardo R. Laurentino,Fabio G. Cozman*

Main category: cs.AI

TL;DR: 本文研究了因果模型中部分可识别查询的概率界限计算问题，提出了一种新的算法，通过利用内生变量的输入概率简化多线性规划的构建，并在单一干预情况下应用列生成技术提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 研究在内生变量可观测但外生变量未完全指定的情况下，如何计算感兴趣概率的紧致界限。

Method: 提出了一种新的算法，利用内生变量的输入概率简化多线性规划的构建，并在单一干预情况下应用列生成技术计算概率界限。

Result: 实验表明，列生成技术优于现有方法，并证明了外生变量多项式基数表示的可能性。

Conclusion: 本文提出了一种有效的算法来计算部分可识别查询的概率界限，为因果推断提供了新的工具。

Abstract: We investigate partially identifiable queries in a class of causal models. We
focus on acyclic Structural Causal Models that are quasi-Markovian (that is,
each endogenous variable is connected with at most one exogenous confounder).
We look into scenarios where endogenous variables are observed (and a
distribution over them is known), while exogenous variables are not fully
specified. This leads to a representation that is in essence a Bayesian network
where the distribution of root variables is not uniquely determined. In such
circumstances, it may not be possible to precisely compute a probability value
of interest. We thus study the computation of tight probability bounds, a
problem that has been solved by multilinear programming in general, and by
linear programming when a single confounded component is intervened upon. We
present a new algorithm to simplify the construction of such programs by
exploiting input probabilities over endogenous variables. For scenarios with a
single intervention, we apply column generation to compute a probability bound
through a sequence of auxiliary linear integer programs, thus showing that a
representation with polynomial cardinality for exogenous variables is possible.
Experiments show column generation techniques to be superior to existing
methods.

</details>


### [3] [Diffusion-RL Based Air Traffic Conflict Detection and Resolution Method](https://arxiv.org/abs/2509.03550)
*Tonghe Li,Jixin Liu,Weili Zeng,Hao Jiang*

Main category: cs.AI

TL;DR: 本文提出了一种名为Diffusion-AC的新型自主冲突解决框架，该框架利用扩散概率模型克服了现有深度强化学习方法在空中交通冲突检测与解决中的单峰偏差问题，显著提高了安全性并减少了近距离空中碰撞的发生。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法在处理复杂动态约束时存在决策僵局问题，本文旨在通过引入扩散概率模型提高决策灵活性。

Method: 提出了一种名为Diffusion-AC的框架，该框架将扩散概率模型与密度渐进式安全课程(DPSC)相结合，通过反向去噪过程生成多峰动作分布。

Result: 模拟实验表明，Diffusion-AC显著优于现有最先进的DRL基准方法，在高密度场景下成功率达94.1%，近距离空中碰撞发生率降低了约59%。

Conclusion: Diffusion-AC的多峰决策能力使其能够灵活切换有效机动，显著增强了系统的安全裕度。

Abstract: In the context of continuously rising global air traffic, efficient and safe
Conflict Detection and Resolution (CD&R) is paramount for air traffic
management. Although Deep Reinforcement Learning (DRL) offers a promising
pathway for CD&R automation, existing approaches commonly suffer from a
"unimodal bias" in their policies. This leads to a critical lack of
decision-making flexibility when confronted with complex and dynamic
constraints, often resulting in "decision deadlocks." To overcome this
limitation, this paper pioneers the integration of diffusion probabilistic
models into the safety-critical task of CD&R, proposing a novel autonomous
conflict resolution framework named Diffusion-AC. Diverging from conventional
methods that converge to a single optimal solution, our framework models its
policy as a reverse denoising process guided by a value function, enabling it
to generate a rich, high-quality, and multimodal action distribution. This core
architecture is complemented by a Density-Progressive Safety Curriculum (DPSC),
a training mechanism that ensures stable and efficient learning as the agent
progresses from sparse to high-density traffic environments. Extensive
simulation experiments demonstrate that the proposed method significantly
outperforms a suite of state-of-the-art DRL benchmarks. Most critically, in the
most challenging high-density scenarios, Diffusion-AC not only maintains a high
success rate of 94.1% but also reduces the incidence of Near Mid-Air Collisions
(NMACs) by approximately 59% compared to the next-best-performing baseline,
significantly enhancing the system's safety margin. This performance leap stems
from its unique multimodal decision-making capability, which allows the agent
to flexibly switch to effective alternative maneuvers.

</details>


### [4] [Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents](https://arxiv.org/abs/2509.03581)
*Davide Paglieri,Bartłomiej Cupiał,Jonathan Cook,Ulyana Piterbarg,Jens Tuyls,Edward Grefenstette,Jakob Nicolaus Foerster,Jack Parker-Holder,Tim Rocktäschel*

Main category: cs.AI

TL;DR: 本文介绍了一种动态规划框架，用于提高大型语言模型(LLM)的推理能力，该框架允许模型灵活地决定何时分配计算资源进行规划，从而提高样本效率和解决复杂问题的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的RL方法要么总是规划，计算成本高且在长序列任务中性能下降，要么从不规划，性能受限。

Method: 提出一个两阶段训练流程：1) 在多样化合成数据上进行监督微调，使模型能够进行动态规划；2) 在长序列环境中使用RL优化动态规划能力。

Result: 实验表明，该方法训练出的动态规划智能体样本效率更高，能够完成更复杂的任务，并且能够有效地利用人类编写的计划。

Conclusion: 这项工作首次探索了在顺序决策任务中训练LLM智能体进行动态测试时计算分配，为构建更高效、自适应和可控的智能系统铺平了道路。

Abstract: Training large language models (LLMs) to reason via reinforcement learning
(RL) significantly improves their problem-solving capabilities. In agentic
settings, existing methods like ReAct prompt LLMs to explicitly plan before
every action; however, we demonstrate that always planning is computationally
expensive and degrades performance on long-horizon tasks, while never planning
further limits performance. To address this, we introduce a conceptual
framework formalizing dynamic planning for LLM agents, enabling them to
flexibly decide when to allocate test-time compute for planning. We propose a
simple two-stage training pipeline: (1) supervised fine-tuning on diverse
synthetic data to prime models for dynamic planning, and (2) RL to refine this
capability in long-horizon environments. Experiments on the Crafter environment
show that dynamic planning agents trained with this approach are more
sample-efficient and consistently achieve more complex objectives.
Additionally, we demonstrate that these agents can be effectively steered by
human-written plans, surpassing their independent capabilities. To our
knowledge, this work is the first to explore training LLM agents for dynamic
test-time compute allocation in sequential decision-making tasks, paving the
way for more efficient, adaptive, and controllable agentic systems.

</details>


### [5] [Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE](https://arxiv.org/abs/2509.03626)
*Zahra Zehtabi Sabeti Moghaddam,Zeinab Dehghani,Maneeha Rani,Koorosh Aslansefat,Bhupesh Kumar Mishra,Rameez Raja Kureshi,Dhavalkumar Thakker*

Main category: cs.AI

TL;DR: 该论文提出了一种名为KG-SMILE的框架，用于提高基于知识图谱的检索增强生成模型(RAG)的可解释性，通过扰动分析识别对生成输出影响最大的图实体和关系，从而增强RAG的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式AI模型存在幻觉和不可验证的断言问题，尤其在医疗等领域，精度至关重要。检索增强生成(RAG)虽然提高了准确性，但其内部机制仍然不透明。

Method: 提出了一种与方法无关的，基于扰动的框架KG-SMILE，通过控制扰动、计算相似性和训练加权线性代理模型，识别对生成输出影响最大的图实体和关系。

Result: 评估结果表明，KG-SMILE能够产生稳定且符合人类认知的解释，平衡了模型有效性和可解释性。

Conclusion: KG-SMILE框架提升了RAG模型的可解释性和可信度，有助于增强人们对机器学习技术的信任。

Abstract: Generative AI, such as Large Language Models (LLMs), has achieved impressive
progress but still produces hallucinations and unverifiable claims, limiting
reliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves
accuracy by grounding outputs in external knowledge, especially in domains like
healthcare, where precision is vital. However, RAG remains opaque and
essentially a black box, heavily dependent on data quality. We developed a
method-agnostic, perturbation-based framework that provides token and
component-level interoperability for Graph RAG using SMILE and named it as
Knowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing
similarities, and training weighted linear surrogates, KG-SMILE identifies the
graph entities and relations most influential to generated outputs, thereby
making RAG more transparent. We evaluate KG-SMILE using comprehensive
attribution metrics, including fidelity, faithfulness, consistency, stability,
and accuracy. Our findings show that KG-SMILE produces stable, human-aligned
explanations, demonstrating its capacity to balance model effectiveness with
interpretability and thereby fostering greater transparency and trust in
machine learning technologies.

</details>


### [6] [CausalARC: Abstract Reasoning with Causal World Models](https://arxiv.org/abs/2509.03636)
*Jacqueline Maasch,John Kalantari,Kia Khezeli*

Main category: cs.AI

TL;DR: CausalARC:一个用于评估AI在低数据和分布外环境下推理能力的实验平台，基于ARC构建，并利用因果模型进行数据增强。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型难以适应新颖问题和数据限制。

Method: 构建因果ARC平台，利用结构因果模型生成数据，提供观察性、干预性和反事实性反馈。

Result: 在四个语言模型评估场景中验证了CausalARC的有效性：抽象推理、反事实推理、程序合成和因果发现。

Conclusion: CausalARC为评估AI推理能力提供了新的基准。

Abstract: Reasoning requires adaptation to novel problem settings under limited data
and distribution shift. This work introduces CausalARC: an experimental testbed
for AI reasoning in low-data and out-of-distribution regimes, modeled after the
Abstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is
sampled from a fully specified causal world model, formally expressed as a
structural causal model. Principled data augmentations provide observational,
interventional, and counterfactual feedback about the world model in the form
of few-shot, in-context learning demonstrations. As a proof-of-concept, we
illustrate the use of CausalARC for four language model evaluation settings:
(1) abstract reasoning with test-time training, (2) counterfactual reasoning
with in-context learning, (3) program synthesis, and (4) causal discovery with
logical reasoning.

</details>


### [7] [Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations](https://arxiv.org/abs/2509.03644)
*François Olivier,Zied Bouraoui*

Main category: cs.AI

TL;DR: 该论文提出了一种名为Embodied-LM的神经符号系统，通过将图像模式融入大型语言模型（LLM）中，以增强其逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在逻辑推理方面容易出错，缺乏人类理解中稳健的心智表征。

Method: 该系统利用基于图像模式的示意图表征，并使用Answer Set Programming进行声明式空间推理，从而将LLM的理解和逻辑推理建立在具体的认知结构之上。

Result: 实验结果表明，Embodied-LM能够引导LLM通过具体认知结构来解释场景，这些结构可以形式化为可执行程序，并支持更高效、更易解释的逻辑推理。

Conclusion: 该研究为将更复杂和动态的表征整合到LLM中奠定了计算基础。

Abstract: Despite significant progress in natural language understanding, Large
Language Models (LLMs) remain error-prone when performing logical reasoning,
often lacking the robust mental representations that enable human-like
comprehension. We introduce a prototype neurosymbolic system, Embodied-LM, that
grounds understanding and logical reasoning in schematic representations based
on image schemas-recurring patterns derived from sensorimotor experience that
structure human cognition. Our system operationalizes the spatial foundations
of these cognitive structures using declarative spatial reasoning within Answer
Set Programming. Through evaluation on logical deduction problems, we
demonstrate that LLMs can be guided to interpret scenarios through embodied
cognitive structures, that these structures can be formalized as executable
programs, and that the resulting representations support effective logical
reasoning with enhanced interpretability. While our current implementation
focuses on spatial primitives, it establishes the computational foundation for
incorporating more complex and dynamic representations.

</details>


### [8] [Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning](https://arxiv.org/abs/2509.03646)
*Haozhe Wang,Qixin Xu,Che Liu,Junhong Wu,Fangzhen Lin,Wenhu Chen*

Main category: cs.AI

TL;DR: 强化学习增强大型语言模型复杂推理能力，其机制在于分层推理，即战略规划和程序执行的分离。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习算法未能有效利用学习信号，导致学习效率低下。

Method: 提出了一种分层感知信用分配算法(HICRA)，专注于高影响规划token的优化。

Result: HICRA显著优于基线算法，验证了关注战略瓶颈对解锁高级推理至关重要，语义熵是衡量战略探索的优越指标。

Conclusion: 通过关注战略规划瓶颈，改进强化学习算法，从而提升大型语言模型的推理能力。

Abstract: Reinforcement Learning (RL) has proven highly effective at enhancing the
complex reasoning abilities of Large Language Models (LLMs), yet underlying
mechanisms driving this success remain largely opaque. Our analysis reveals
that puzzling phenomena like ``aha moments", ``length-scaling'' and entropy
dynamics are not disparate occurrences but hallmarks of an emergent reasoning
hierarchy, akin to the separation of high-level strategic planning from
low-level procedural execution in human cognition. We uncover a compelling
two-phase dynamic: initially, a model is constrained by procedural correctness
and must improve its low-level skills. The learning bottleneck then decisively
shifts, with performance gains being driven by the exploration and mastery of
high-level strategic planning. This insight exposes a core inefficiency in
prevailing RL algorithms like GRPO, which apply optimization pressure
agnostically and dilute the learning signal across all tokens. To address this,
we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that
concentrates optimization efforts on high-impact planning tokens. HICRA
significantly outperforms strong baselines, demonstrating that focusing on this
strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we
validate semantic entropy as a superior compass for measuring strategic
exploration over misleading metrics such as token-level entropy.

</details>


### [9] [An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series Classification](https://arxiv.org/abs/2509.03649)
*Davide Italo Serramazza,Nikos Papadeas,Zahraa Abdallah,Georgiana Ifrim*

Main category: cs.AI

TL;DR: "本文探究了时间序列分割算法对SHAP方法解释时间序列分类模型预测结果的影响，发现等长分割通常优于其他自定义算法，并提出一种新的归一化技术来提高解释质量。"


<details>
  <summary>Details</summary>
Motivation: "现有SHAP方法计算复杂度高，限制了其在长时序数据中的应用。本文旨在研究如何通过时间序列分割来优化SHAP方法的效率和解释质量。"

Method: "本文研究了八种不同的时间序列分割算法，并使用InterpretTime和AUC Difference两种方法评估了不同分割策略对解释质量的影响，最后提出了一种新的属性归一化技术。"

Result: "实验结果表明，分割的数量比具体的分割方法对解释质量的影响更大，等长分割通常效果最好，并且提出的归一化技术可以有效提高解释质量。"

Conclusion: "等长分割是一种高效且有效的策略，结合提出的归一化技术，可以显著提升SHAP方法在长时序数据上的解释质量。"

Abstract: Explainable AI (XAI) has become an increasingly important topic for
understanding and attributing the predictions made by complex Time Series
Classification (TSC) models. Among attribution methods, SHapley Additive
exPlanations (SHAP) is widely regarded as an excellent attribution method; but
its computational complexity, which scales exponentially with the number of
features, limits its practicality for long time series. To address this, recent
studies have shown that aggregating features via segmentation, to compute a
single attribution value for a group of consecutive time points, drastically
reduces SHAP running time. However, the choice of the optimal segmentation
strategy remains an open question. In this work, we investigated eight
different Time Series Segmentation algorithms to understand how segment
compositions affect the explanation quality. We evaluate these approaches using
two established XAI evaluation methodologies: InterpretTime and AUC Difference.
Through experiments on both Multivariate (MTS) and Univariate Time Series
(UTS), we find that the number of segments has a greater impact on explanation
quality than the specific segmentation method. Notably, equal-length
segmentation consistently outperforms most of the custom time series
segmentation algorithms. Furthermore, we introduce a novel attribution
normalisation technique that weights segments by their length and we show that
it consistently improves attribution quality.

</details>


### [10] [PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming](https://arxiv.org/abs/2509.03728)
*Wesley Hanwen Deng,Sunnie S. Y. Kim,Akshita Jha,Ken Holstein,Motahhare Eslami,Lauren Wilcox,Leon A Gatys*

Main category: cs.AI

TL;DR: PersonaTeaming方法通过在对抗性提示生成过程中引入角色来改进AI模型的红队测试，提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有自动化红队测试方法忽略了身份对策略的影响，该论文旨在通过引入角色来改进自动化红队测试。

Method: 开发PersonaTeaming方法，该方法基于“红队专家”或“普通AI用户”角色对提示进行变异，并使用动态角色生成算法自动生成不同类型的角色。

Result: 实验表明，与RainbowPlus相比，PersonaTeaming方法提高了对抗性提示的攻击成功率（最高达144.1%），同时保持了提示的多样性。

Conclusion: PersonaTeaming方法在自动化红队测试中引入角色，展现出改进红队测试的潜力，为自动化和人工红队测试方法的结合提供了新的思路。

Abstract: Recent developments in AI governance and safety research have called for
red-teaming methods that can effectively surface potential risks posed by AI
models. Many of these calls have emphasized how the identities and backgrounds
of red-teamers can shape their red-teaming strategies, and thus the kinds of
risks they are likely to uncover. While automated red-teaming approaches
promise to complement human red-teaming by enabling larger-scale exploration of
model behavior, current approaches do not consider the role of identity. As an
initial step towards incorporating people's background and identities in
automated red-teaming, we develop and evaluate a novel method, PersonaTeaming,
that introduces personas in the adversarial prompt generation process to
explore a wider spectrum of adversarial strategies. In particular, we first
introduce a methodology for mutating prompts based on either "red-teaming
expert" personas or "regular AI user" personas. We then develop a dynamic
persona-generating algorithm that automatically generates various persona types
adaptive to different seed prompts. In addition, we develop a set of new
metrics to explicitly measure the "mutation distance" to complement existing
diversity measurements of adversarial prompts. Our experiments show promising
improvements (up to 144.1%) in the attack success rates of adversarial prompts
through persona mutation, while maintaining prompt diversity, compared to
RainbowPlus, a state-of-the-art automated red-teaming method. We discuss the
strengths and limitations of different persona types and mutation methods,
shedding light on future opportunities to explore complementarities between
automated and human red-teaming approaches.

</details>


### [11] [The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs](https://arxiv.org/abs/2509.03730)
*Pengrui Han,Rafal Kocielnik,Peiyang Song,Ramit Debnath,Dean Mobbs,Anima Anandkumar,R. Michael Alvarez*

Main category: cs.AI

TL;DR: 该研究系统性地刻画了大型语言模型(LLM)的个性特征，发现指令对齐稳定了特征表达，但自我报告的特征并不能可靠预测行为，且个性注入对行为影响甚微。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖简化的自我报告和启发式提示，缺乏行为验证，该研究旨在系统性地刻画LLM的个性特征。

Method: 研究通过三个维度刻画LLM个性：(1)训练阶段特征的动态变化；(2)自我报告特征在行为任务中的预测效度；(3)个性注入等干预措施的影响。

Result: 指令对齐稳定了特征表达并增强了特征相关性，但自我报告特征不能可靠预测行为，个性注入对行为影响小且不一致。

Conclusion: LLM的个性特征研究需更深入评估，区分表面特征表达和行为一致性。

Abstract: Personality traits have long been studied as predictors of human
behavior.Recent advances in Large Language Models (LLMs) suggest similar
patterns may emerge in artificial systems, with advanced LLMs displaying
consistent behavioral tendencies resembling human traits like agreeableness and
self-regulation. Understanding these patterns is crucial, yet prior work
primarily relied on simplified self-reports and heuristic prompting, with
little behavioral validation. In this study, we systematically characterize LLM
personality across three dimensions: (1) the dynamic emergence and evolution of
trait profiles throughout training stages; (2) the predictive validity of
self-reported traits in behavioral tasks; and (3) the impact of targeted
interventions, such as persona injection, on both self-reports and behavior.
Our findings reveal that instructional alignment (e.g., RLHF, instruction
tuning) significantly stabilizes trait expression and strengthens trait
correlations in ways that mirror human data. However, these self-reported
traits do not reliably predict behavior, and observed associations often
diverge from human patterns. While persona injection successfully steers
self-reports in the intended direction, it exerts little or inconsistent effect
on actual behavior. By distinguishing surface-level trait expression from
behavioral consistency, our findings challenge assumptions about LLM
personality and underscore the need for deeper evaluation in alignment and
interpretability.

</details>


### [12] [Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation](https://arxiv.org/abs/2509.03736)
*James Mooney,Josef Woldense,Zheng Robert Jia,Shirley Anugrah Hayati,My Ha Nguyen,Vipul Raheja,Dongyeop Kang*

Main category: cs.AI

TL;DR: 大型语言模型(LLM)生成的代理人在不同实验环境下行为不一致，不能替代人类参与者进行研究。


<details>
  <summary>Details</summary>
Motivation: 评估LLM代理人能否替代人类参与者，关注代理人内部一致性。

Method: 设计实验揭示代理人内部状态，并在基本对话环境中检查行为，验证行为假设。

Result: 不同模型族和不同大小的LLM都存在显著的内部不一致性，虽然能生成与人类类似的回应，但缺乏内部一致性。

Conclusion: LLM代理人不能准确替代人类参与者，存在关键差距。

Abstract: The impressive capabilities of Large Language Models (LLMs) have fueled the
notion that synthetic agents can serve as substitutes for real participants in
human-subject research. In an effort to evaluate the merits of this claim,
social science researchers have largely focused on whether LLM-generated survey
data corresponds to that of a human counterpart whom the LLM is prompted to
represent. In contrast, we address a more fundamental question: Do agents
maintain internal consistency, retaining similar behaviors when examined under
different experimental settings? To this end, we develop a study designed to
(a) reveal the agent's internal state and (b) examine agent behavior in a basic
dialogue setting. This design enables us to explore a set of behavioral
hypotheses to assess whether an agent's conversation behavior is consistent
with what we would expect from their revealed internal state. Our findings on
these hypotheses show significant internal inconsistencies in LLMs across model
families and at differing model sizes. Most importantly, we find that, although
agents may generate responses matching those of their human counterparts, they
fail to be internally consistent, representing a critical gap in their
capabilities to accurately substitute for real participants in human-subject
research. Our simulation code and data are publicly accessible.

</details>


### [13] [RAGuard: A Novel Approach for in-context Safe Retrieval Augmented Generation for LLMs](https://arxiv.org/abs/2509.03768)
*Connor Walker,Koorosh Aslansefat,Mohammad Naveed Akram,Yiannis Papadopoulos*

Main category: cs.AI

TL;DR: RAGuard框架通过整合安全关键文档和技术手册，增强了检索增强生成模型的安全性，并在保证技术深度的情况下提高了安全覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在处理OSW维护等专业场景时准确性和安全性不足。

Method: 提出RAGuard框架，整合安全关键文档和技术手册，并使用SafetyClamp扩展，确保安全性的同时保持技术深度。

Result: RAGuard和SafetyClamp显著提高了安全召回率，同时保持了较高的技术召回率。

Conclusion: RAGuard和SafetyClamp为在关键维护场景中将安全保障整合到LLM驱动的决策支持系统中，提供了新的标准。

Abstract: Accuracy and safety are paramount in Offshore Wind (OSW) maintenance, yet
conventional Large Language Models (LLMs) often fail when confronted with
highly specialised or unexpected scenarios. We introduce RAGuard, an enhanced
Retrieval-Augmented Generation (RAG) framework that explicitly integrates
safety-critical documents alongside technical manuals.By issuing parallel
queries to two indices and allocating separate retrieval budgets for knowledge
and safety, RAGuard guarantees both technical depth and safety coverage. We
further develop a SafetyClamp extension that fetches a larger candidate pool,
"hard-clamping" exact slot guarantees to safety. We evaluate across sparse
(BM25), dense (Dense Passage Retrieval) and hybrid retrieval paradigms,
measuring Technical Recall@K and Safety Recall@K. Both proposed extensions of
RAG show an increase in Safety Recall@K from almost 0\% in RAG to more than
50\% in RAGuard, while maintaining Technical Recall above 60\%. These results
demonstrate that RAGuard and SafetyClamp have the potential to establish a new
standard for integrating safety assurance into LLM-powered decision support in
critical maintenance contexts.

</details>


### [14] [Leveraging LLM-Based Agents for Intelligent Supply Chain Planning](https://arxiv.org/abs/2509.03811)
*Yongzhi Qi,Jiaheng Yin,Jianshen Zhang,Dongyang Geng,Zhengyu Chen,Hao Hu,Wei Qi,Zuo-Jun Max Shen*

Main category: cs.AI

TL;DR: 本文提出了一种供应链规划代理（SCPA）框架，利用大型语言模型（LLM）提高了京东供应链规划的效率和准确性，减少了人力成本并改善了关键指标。


<details>
  <summary>Details</summary>
Motivation: 解决电商平台供应链规划中数据收集、长期规划和动态调整的难题，提升效率和可靠性。

Method: 构建了一个基于LLM的供应链规划代理（SCPA）框架，该框架能够理解领域知识、理解操作员的需求、分解任务、利用或创建新的工具并返回基于证据的规划报告。

Result: 在京东的实际场景中部署了该框架，有效降低了人力成本，提高了准确性、库存可用性和其他关键指标。

Conclusion: LLM代理在供应链规划中具有可行性，可以有效提升效率和准确性。

Abstract: In supply chain management, planning is a critical concept. The movement of
physical products across different categories, from suppliers to warehouse
management, to sales, and logistics transporting them to customers, entails the
involvement of many entities. It covers various aspects such as demand
forecasting, inventory management, sales operations, and replenishment. How to
collect relevant data from an e-commerce platform's perspective, formulate
long-term plans, and dynamically adjust them based on environmental changes,
while ensuring interpretability, efficiency, and reliability, is a practical
and challenging problem. In recent years, the development of AI technologies,
especially the rapid progress of large language models, has provided new tools
to address real-world issues. In this work, we construct a Supply Chain
Planning Agent (SCPA) framework that can understand domain knowledge,
comprehend the operator's needs, decompose tasks, leverage or create new tools,
and return evidence-based planning reports. We deploy this framework in
JD.com's real-world scenario, demonstrating the feasibility of LLM-agent
applications in the supply chain. It effectively reduced labor and improved
accuracy, stock availability, and other key metrics.

</details>


### [15] [Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning](https://arxiv.org/abs/2509.03817)
*Wei Yang,Jesse Thomason*

Main category: cs.AI

TL;DR: 本文提出了一种元策略协商框架(MPDF)和一种新的强化学习算法SoftRankPO，用于改进大型语言模型的多智能体系统中的复杂推理。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型多智能体系统协作协议固定，忽略了智能体的内部认知能力。

Method: 提出元策略协商框架(MPDF)，智能体学习在“坚持”、“改进”和“让步”等元认知行为上的分散策略，并使用SoftRankPO算法进行强化学习训练。

Result: 在五个基准测试中，MPDF结合SoftRankPO比其他六种最先进的算法平均准确率提高了4-5%。

Conclusion: 这项工作为学习多智能体大型语言模型的自适应元认知策略提供了一种范例，从设计固定协议转向学习动态的、有思考的策略。

Abstract: Multi-agent systems of large language models (LLMs) show promise for complex
reasoning, but their effectiveness is often limited by fixed collaboration
protocols. These frameworks typically focus on macro-level orchestration while
overlooking agents' internal deliberative capabilities. This critical
meta-cognitive blindspot treats agents as passive executors unable to adapt
their strategy based on internal cognitive states like uncertainty or
confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where
agents learn a decentralized policy over a set of high-level meta-cognitive
actions: Persist, Refine, and Concede. To overcome the instability of
traditional policy gradients in this setting, we develop SoftRankPO, a novel
reinforcement learning algorithm. SoftRankPO stabilizes training by shaping
advantages based on the rank of rewards mapped through smooth normal quantiles,
making the learning process robust to reward variance. Experiments show that
MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across
five mathematical and general reasoning benchmarks compared to six
state-of-the-art heuristic and learning-based multi-agent reasoning algorithms.
Our work presents a paradigm for learning adaptive, meta-cognitive policies for
multi-agent LLM systems, shifting the focus from designing fixed protocols to
learning dynamic, deliberative strategies.

</details>


### [16] [What Would an LLM Do? Evaluating Policymaking Capabilities of Large Language Models](https://arxiv.org/abs/2509.03827)
*Pierre Le Coz,Jia An Liu,Debarun Bhattacharjya,Georgina Curto,Serge Stinckwich*

Main category: cs.AI

TL;DR: 大型语言模型(LLM)在评估社会政策（以解决无家可归问题为例）方面的有效性研究，结果显示在加入专家经验和情境校准后，LLM能提供有价值的见解。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在解决全球性社会问题（无家可归）方面的应用潜力，并探讨其与领域专家的意见一致性。

Method: 构建涵盖四个地区的政策决策情景基准，结合基于主体模型的模拟，评估LLM推荐政策的社会影响。

Result: LLM在社会政策制定中展现出良好潜力，尤其是在加入专家经验和情境校准后，能够提供有价值的替代政策。

Conclusion: 负责任的监管和情境校准与本地领域专家的合作，能够提升LLM在社会政策制定中的应用价值。

Abstract: Large language models (LLMs) are increasingly being adopted in high-stakes
domains. Their capacity to process vast amounts of unstructured data, explore
flexible scenarios, and handle a diversity of contextual factors can make them
uniquely suited to provide new insights for the complexity of social
policymaking. This article evaluates whether LLMs' are aligned with domain
experts (and among themselves) to inform social policymaking on the subject of
homelessness alleviation - a challenge affecting over 150 million people
worldwide. We develop a novel benchmark comprised of decision scenarios with
policy choices across four geographies (South Bend, USA; Barcelona, Spain;
Johannesburg, South Africa; Macau SAR, China). The policies in scope are
grounded in the conceptual framework of the Capability Approach for human
development. We also present an automated pipeline that connects the
benchmarked policies to an agent-based model, and we explore the social impact
of the recommended policies through simulated social scenarios. The paper
results reveal promising potential to leverage LLMs for social policy making.
If responsible guardrails and contextual calibrations are introduced in
collaboration with local domain experts, LLMs can provide humans with valuable
insights, in the form of alternative policies at scale.

</details>
