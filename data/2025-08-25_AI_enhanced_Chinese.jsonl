{"id": "2508.15943", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15943", "abs": "https://arxiv.org/abs/2508.15943", "authors": ["Riccardo Andreoni", "Andrei Buliga", "Alessandro Daniele", "Chiara Ghidini", "Marco Montali", "Massimiliano Ronzani"], "title": "T-ILR: a Neurosymbolic Integration for LTLf", "comment": "Accepted for presentation at NeSy 2025. 10 pages", "summary": "State-of-the-art approaches for integrating symbolic knowledge with deep\nlearning architectures have demonstrated promising results in static domains.\nHowever, methods to handle temporal logic specifications remain underexplored.\nThe only existing approach relies on an explicit representation of a\nfinite-state automaton corresponding to the temporal specification. Instead, we\naim at proposing a neurosymbolic framework designed to incorporate temporal\nlogic specifications, expressed in Linear Temporal Logic over finite traces\n(LTLf), directly into deep learning architectures for sequence-based tasks. We\nextend the Iterative Local Refinement (ILR) neurosymbolic algorithm, leveraging\nthe recent introduction of fuzzy LTLf interpretations. We name this proposed\nmethod Temporal Iterative Local Refinement (T-ILR). We assess T-ILR on an\nexisting benchmark for temporal neurosymbolic architectures, consisting of the\nclassification of image sequences in the presence of temporal knowledge. The\nresults demonstrate improved accuracy and computational efficiency compared to\nthe state-of-the-art method.", "AI": {"tldr": "T-ILR\u6846\u67b6\u6709\u6548\u5730\u5c06\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91(LTLf)\u89c4\u8303\u96c6\u6210\u5230\u6df1\u5ea6\u5b66\u4e60\u4e2d\uff0c\u63d0\u5347\u4e86\u65f6\u5e8f\u795e\u7ecf\u7b26\u53f7\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u65f6\u5e8f\u903b\u8f91\u89c4\u8303\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u6709\u9650\u72b6\u6001\u81ea\u52a8\u673a\u7684\u663e\u5f0f\u8868\u793a\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u6269\u5c55\u4e86\u8fed\u4ee3\u5c40\u90e8\u7ec6\u5316(ILR)\u795e\u7ecf\u7b26\u53f7\u7b97\u6cd5\uff0c\u5229\u7528\u6a21\u7ccaLTLf\u89e3\u91ca\u3002", "result": "\u5728\u56fe\u50cf\u5e8f\u5217\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cT-ILR\u65b9\u6cd5\u6bd4\u73b0\u6709\u6280\u672f\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aT-ILR\u7684\u65b0\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u89c4\u8303\u76f4\u63a5\u878d\u5165\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u5e76\u5728\u56fe\u50cf\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6bd4\u73b0\u6709\u6280\u672f\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2508.16033", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.16033", "abs": "https://arxiv.org/abs/2508.16033", "authors": ["Jong-Hwan Jang", "Junho Song", "Yong-Yeon Jo"], "title": "CoFE: A Framework Generating Counterfactual ECG for Explainable Cardiac AI-Diagnostics", "comment": "Demo paper, 5 pages", "summary": "Recognizing the need for explainable AI (XAI) approaches to enable the\nsuccessful integration of AI-based ECG prediction models (AI-ECG) into clinical\npractice, we introduce a framework generating \\textbf{Co}unter\\textbf{F}actual\n\\textbf{E}CGs (i,e., named CoFE) to illustrate how specific features, such as\namplitudes and intervals, influence the model's predictive decisions. To\ndemonstrate the applicability of the CoFE, we present two case studies: atrial\nfibrillation classification and potassium level regression models. The CoFE\nreveals feature changes in ECG signals that align with the established clinical\nknowledge. By clarifying both \\textbf{where valid features appear} in the ECG\nand \\textbf{how they influence the model's predictions}, we anticipate that our\nframework will enhance the interpretability of AI-ECG models and support more\neffective clinical decision-making. Our demonstration video is available at:\nhttps://www.youtube.com/watch?v=YoW0bNBPglQ.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6CoFE\uff0c\u901a\u8fc7\u751f\u6210\u53cd\u4e8b\u5b9eECG\u6765\u89e3\u91caAI-ECG\u6a21\u578b\u7684\u9884\u6d4b\uff0c\u63d0\u9ad8\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u652f\u6301\u66f4\u6709\u6548\u7684\u4e34\u5e8a\u51b3\u7b56\u3002", "motivation": "\u4e3a\u4e86\u4f7f\u57fa\u4e8eAI\u7684ECG\u9884\u6d4b\u6a21\u578b\u80fd\u591f\u6210\u529f\u5730\u6574\u5408\u5230\u4e34\u5e8a\u5b9e\u8df5\u4e2d\uff0c\u9700\u8981\u53ef\u89e3\u91ca\u7684AI (XAI) \u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u53cd\u4e8b\u5b9eECG (CoFE) \u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u91caAI-ECG\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "result": "CoFE\u6846\u67b6\u63ed\u793a\u4e86ECG\u4fe1\u53f7\u4e2d\u4e0e\u65e2\u5b9a\u4e34\u5e8a\u77e5\u8bc6\u76f8\u7b26\u7684\u7279\u5f81\u53d8\u5316\uff0c\u9610\u660e\u4e86\u6709\u6548\u7279\u5f81\u5728ECG\u4e2d\u7684\u4f4d\u7f6e\u4ee5\u53ca\u5b83\u4eec\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u9884\u6d4b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoFE\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u53cd\u4e8b\u5b9eECG\u6765\u89e3\u91caAI-ECG\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\uff0c\u5e76\u5728\u5fc3\u623f\u98a4\u52a8\u5206\u7c7b\u548c\u94be\u6c34\u5e73\u56de\u5f52\u6a21\u578b\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8AI-ECG\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u652f\u6301\u66f4\u6709\u6548\u7684\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2508.16051", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16051", "abs": "https://arxiv.org/abs/2508.16051", "authors": ["Yiheng Hu", "Xiaoyang Wang", "Qing Liu", "Xiwei Xu", "Qian Fu", "Wenjie Zhang", "Liming Zhu"], "title": "MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs", "comment": null, "summary": "Multimodal Multi-hop question answering requires integrating information from\ndiverse sources, such as images and texts, to derive answers. Existing methods\ntypically rely on sequential retrieval and reasoning, where each step builds on\nthe previous output. However, this single-path paradigm makes them vulnerable\nto errors due to misleading intermediate steps. Moreover, developing multimodal\nmodels can be computationally expensive, often requiring extensive training. To\naddress these limitations, we propose a training-free framework guided by an\nAdaptive Planning Graph, which consists of planning, retrieval and reasoning\nmodules. The planning module analyzes the current state of the Adaptive\nPlanning Graph, determines the next action and where to expand the graph, which\nenables dynamic and flexible exploration of reasoning paths. To handle\nretrieval of text to unspecified target modalities, we devise modality-specific\nstrategies that dynamically adapt to distinct data types. Our approach\npreserves the characteristics of multimodal information without costly\ntask-specific training, enabling seamless integration with up-to-date models.\nFinally, the experiments on MultimodalQA and WebQA show that our approach\nmatches or outperforms existing models that rely on training.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u591a\u8df3\u95ee\u7b54\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u89c4\u5212\u56fe\u5b9e\u73b0\u52a8\u6001\u63a8\u7406\u8def\u5f84\u63a2\u7d22\uff0c\u6027\u80fd\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u987a\u5e8f\u68c0\u7d22\u548c\u63a8\u7406\u7684\u5355\u8def\u5f84\u8303\u5f0f\uff0c\u5bb9\u6613\u53d7\u5230\u4e2d\u95f4\u6b65\u9aa4\u9519\u8bef\u7684\u5f71\u54cd\uff0c\u4e14\u5f00\u53d1\u591a\u6a21\u6001\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u89c4\u5212\u56fe\u7684\u65e0\u8bad\u7ec3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u89c4\u5212\u3001\u68c0\u7d22\u548c\u63a8\u7406\u4e09\u4e2a\u6a21\u5757\u3002\u89c4\u5212\u6a21\u5757\u5206\u6790\u5f53\u524d\u72b6\u6001\uff0c\u786e\u5b9a\u4e0b\u4e00\u6b65\u64cd\u4f5c\u548c\u6269\u5c55\u56fe\u7684\u4f4d\u7f6e\uff1b\u68c0\u7d22\u6a21\u5757\u91c7\u7528\u7279\u5b9a\u4e8e\u6a21\u6001\u7684\u7b56\u7565\uff0c\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u6570\u636e\u7c7b\u578b\uff1b\u63a8\u7406\u6a21\u5757\u57fa\u4e8e\u68c0\u7d22\u7ed3\u679c\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728MultimodalQA\u548cWebQA\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u7684\u6027\u80fd\u4e0e\u73b0\u6709\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u597d\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u89c4\u5212\u56fe\u7684\u65e0\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u591a\u8df3\u95ee\u7b54\u95ee\u9898\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u89c4\u5212\u3001\u68c0\u7d22\u548c\u63a8\u7406\u6a21\u5757\u5b9e\u73b0\u52a8\u6001\u7075\u6d3b\u7684\u63a8\u7406\u8def\u5f84\u63a2\u7d22\uff0c\u5e76\u5728MultimodalQA\u548cWebQA\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4e0e\u73b0\u6709\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u597d\u7684\u7ed3\u679c\u3002"}}
{"id": "2508.16054", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16054", "abs": "https://arxiv.org/abs/2508.16054", "authors": ["Sonish Sivarajkumar", "Hang Zhang", "Yuelyu Ji", "Maneesh Bilalpur", "Xizhi Wu", "Chenyu Li", "Min Gu Kwak", "Shyam Visweswaran", "Yanshan Wang"], "title": "Generative Foundation Model for Structured and Unstructured Electronic Health Records", "comment": null, "summary": "Electronic health records (EHRs) are rich clinical data sources but complex\nrepositories of patient data, spanning structured elements (demographics,\nvitals, lab results, codes), unstructured clinical notes and other modalities\nof data. Harnessing this heterogeneity is critical for improving patient\noutcomes. Recent advances in large language models (LLMs) have enabled\nfoundation models that can learn from multiple data modalities and support\nclinical tasks. However, most current approaches simply serialize numeric EHR\ndata into text, which risks losing temporal and quantitative detail. We\nintroduce Generative Deep Patient (GDP), a multimodal foundation model that\nnatively encodes structured EHR time-series via a CNN-Transformer encoder and\nfuses it with unstructured EHRs through cross-modal attention into a\nLLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining,\nwhere it learns to produce clinical narratives from raw patient timelines while\nalso performing masked feature prediction (MFP) and next time-step prediction\n(NTP) to capture temporal dynamics; and (2) multi-task fine-tuning for\nclinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-day\nreadmission). In clinical prediction, GDP demonstrated superior performance on\nMIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and\n30-day readmission AUROC = 0.627. For narrative generation, GDP achieved\nROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation,\nGDP-Instruct scored highest on faithfulness, fluency, and overall clinical\nutility, suggesting reduced hospital documentation workload without sacrificing\naccuracy. Our results demonstrate that a single multimodal foundation model can\nboth predict clinically actionable events and generate high-quality clinical\nnarratives. Furthermore, GDP's flexible architecture can be extended to\nadditional modalities.", "AI": {"tldr": "GDP\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u9884\u6d4b\u4e34\u5e8a\u53ef\u64cd\u4f5c\u4e8b\u4ef6\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4e34\u5e8a\u53d9\u4e8b\uff0c\u5728MIMIC-IV\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5145\u5206\u5229\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4e2d\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\u7684\u5f02\u8d28\u6027\uff0c\u4ee5\u6539\u5584\u60a3\u8005\u9884\u540e\u3002", "method": "GDP\u6a21\u578b\u91c7\u7528CNN-Transformer\u7f16\u7801\u5668\u5bf9\u7ed3\u6784\u5316EHR\u65f6\u95f4\u5e8f\u5217\u8fdb\u884c\u7f16\u7801\uff0c\u5e76\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5c06\u5176\u4e0e\u975e\u7ed3\u6784\u5316EHR\u878d\u5408\u5230\u57fa\u4e8eLLaMA\u7684\u89e3\u7801\u5668\u4e2d\u3002\u6a21\u578b\u8bad\u7ec3\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u548c\u591a\u4efb\u52a1\u5fae\u8c03\u3002", "result": "\u5728MIMIC-IV\u6570\u636e\u96c6\u4e0a\uff0cGDP\u6a21\u578b\u5728\u5fc3\u810f\u8870\u7aed\u30012\u578b\u7cd6\u5c3f\u75c5\u548c30\u5929\u518d\u5165\u9662\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u5f02\u7684AUROC\u503c\uff08\u5206\u522b\u4e3a0.923\u30010.817\u548c0.627\uff09\u3002\u5728\u53d9\u4e8b\u751f\u6210\u65b9\u9762\uff0cROUGE-L\u548cBERTScore-F1\u503c\u4e5f\u8f83\u9ad8\u3002", "conclusion": "Generative Deep Patient (GDP)\u6a21\u578b\u5728\u4e34\u5e8a\u9884\u6d4b\u548c\u4e34\u5e8a\u53d9\u4e8b\u751f\u6210\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5176\u7075\u6d3b\u7684\u67b6\u6784\u53ef\u4ee5\u6269\u5c55\u5230\u5176\u4ed6\u6a21\u6001\u3002"}}
{"id": "2508.16057", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.16057", "abs": "https://arxiv.org/abs/2508.16057", "authors": ["Sijie Yang", "Binyu Lei", "Filip Biljecki"], "title": "Urban Comfort Assessment in the Era of Digital Planning: A Multidimensional, Data-driven, and AI-assisted Framework", "comment": "Presented at 19th International Conference on Computational Urban\n  Planning and Urban Management (CUPUM 2025)", "summary": "Ensuring liveability and comfort is one of the fundamental objectives of\nurban planning. Numerous studies have employed computational methods to assess\nand quantify factors related to urban comfort such as greenery coverage,\nthermal comfort, and walkability. However, a clear definition of urban comfort\nand its comprehensive evaluation framework remain elusive. Our research\nexplores the theoretical interpretations and methodologies for assessing urban\ncomfort within digital planning, emphasising three key dimensions:\nmultidimensional analysis, data support, and AI assistance.", "AI": {"tldr": "\u7f3a\u4e4f\u6e05\u6670\u7684\u57ce\u5e02\u8212\u9002\u6027\u5b9a\u4e49\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u591a\u7ef4\u5ea6\u5206\u6790\u3001\u6570\u636e\u652f\u6301\u548cAI\u8f85\u52a9\u7684\u65b0\u6846\u67b6\u3002", "motivation": "\u786e\u4fdd\u5b9c\u5c45\u6027\u548c\u8212\u9002\u6027\u662f\u57ce\u5e02\u89c4\u5212\u7684\u57fa\u672c\u76ee\u6807\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6e05\u6670\u7684\u57ce\u5e02\u8212\u9002\u6027\u5b9a\u4e49\u548c\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63a2\u7d22\u6027\u7814\u7a76\uff0c\u5bf9\u73b0\u6709\u6587\u732e\u8fdb\u884c\u7efc\u8ff0\u548c\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u591a\u7ef4\u5ea6\u5206\u6790\u3001\u6570\u636e\u652f\u6301\u548cAI\u8f85\u52a9\u7684\u57ce\u5e02\u8212\u9002\u6027\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u6570\u5b57\u89c4\u5212\u4e2d\u8bc4\u4f30\u57ce\u5e02\u8212\u9002\u6027\u7684\u7406\u8bba\u89e3\u91ca\u548c\u65b9\u6cd5\uff0c\u5f3a\u8c03\u591a\u7ef4\u5ea6\u5206\u6790\u3001\u6570\u636e\u652f\u6301\u548cAI\u8f85\u52a9\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u3002"}}
{"id": "2508.16059", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16059", "abs": "https://arxiv.org/abs/2508.16059", "authors": ["Zhuomin Chen", "Dan Li", "Jiahui Zhou", "Shunyu Wu", "Haozheng Ye", "Jian Lou", "See-Kiong Ng"], "title": "Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting", "comment": "To be published in CIKM 2025", "summary": "Time series (TS) data are ubiquitous across various application areas,\nrendering time series forecasting (TSF) a fundamental task. With the astounding\nadvances in large language models (LLMs), a variety of methods have been\ndeveloped to adapt LLMs for time series forecasting. Despite unlocking the\npotential of LLMs in comprehending TS data, existing methods are inherently\nconstrained by their shallow integration of TS information, wherein LLMs\ntypically access TS representations at shallow layers, primarily at the input\nlayer. This causes the influence of TS representations to progressively fade in\ndeeper layers and eventually leads to ineffective adaptation between textual\nembeddings and TS representations. In this paper, we propose the Multi-layer\nSteerable Embedding Fusion (MSEF), a novel framework that enables LLMs to\ndirectly access time series patterns at all depths, thereby mitigating the\nprogressive loss of TS information in deeper layers. Specifically, MSEF\nleverages off-the-shelf time series foundation models to extract semantically\nrich embeddings, which are fused with intermediate text representations across\nLLM layers via layer-specific steering vectors. These steering vectors are\ndesigned to continuously optimize the alignment between time series and textual\nmodalities and facilitate a layer-specific adaptation mechanism that ensures\nefficient few-shot learning capabilities. Experimental results on seven\nbenchmarks demonstrate significant performance improvements by MSEF compared\nwith baselines, with an average reduction of 31.8% in terms of MSE. The code is\navailable at https://github.com/One1sAll/MSEF.", "AI": {"tldr": "MSEF\u6846\u67b6\u901a\u8fc7\u591a\u5c42\u878d\u5408\u65f6\u95f4\u5e8f\u5217\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u65f6\u95f4\u5e8f\u5217\u4fe1\u606f\u6d45\u5c42\u6574\u5408\u5230LLM\u4e2d\uff0c\u5bfc\u81f4\u65f6\u95f4\u5e8f\u5217\u4fe1\u606f\u5728\u6df1\u5c42\u9010\u6e10\u6d88\u5931\uff0c\u5f71\u54cd\u9884\u6d4b\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u591a\u5c42\u53ef\u63a7\u5d4c\u5165\u878d\u5408(MSEF)\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u73b0\u6210\u7684\u65f6\u5e8f\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u8bed\u4e49\u4e30\u5bcc\u7684\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u7279\u5b9a\u5c42\u7684\u8f6c\u5411\u5411\u91cf\u5c06\u5176\u4e0eLLM\u5c42\u7684\u4e2d\u95f4\u6587\u672c\u8868\u793a\u878d\u5408\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMSEF\u4e0e\u57fa\u7ebf\u76f8\u6bd4\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e73\u5747MSE\u964d\u4f4e\u4e8631.8%\u3002", "conclusion": "MSEF\u6846\u67b6\u901a\u8fc7\u5728LLM\u6240\u6709\u5c42\u7ea7\u76f4\u63a5\u8bbf\u95ee\u65f6\u95f4\u5e8f\u5217\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\uff0c\u5e73\u5747MSE\u964d\u4f4e\u4e8631.8%\u3002"}}
{"id": "2508.16072", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16072", "abs": "https://arxiv.org/abs/2508.16072", "authors": ["Zizhen Li", "Chuanhao Li", "Yibin Wang", "Qi Chen", "Diping Song", "Yukang Feng", "Jianwen Sun", "Jiaxin Ai", "Fanrui Zhang", "Mingzhu Sun", "Kaipeng Zhang"], "title": "InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles", "comment": "EMNLP 2025 MainConference", "summary": "LLMs have shown strong performance on human-centric reasoning tasks. While\nprevious evaluations have explored whether LLMs can infer intentions or detect\ndeception, they often overlook the individualized reasoning styles that\ninfluence how people interpret and act in social contexts. Social deduction\ngames (SDGs) provide a natural testbed for evaluating individualized reasoning\nstyles, where different players may adopt diverse but contextually valid\nreasoning strategies under identical conditions. To address this, we introduce\nInMind, a cognitively grounded evaluation framework designed to assess whether\nLLMs can capture and apply personalized reasoning styles in SDGs. InMind\nenhances structured gameplay data with round-level strategy traces and\npost-game reflections, collected under both Observer and Participant modes. It\nsupports four cognitively motivated tasks that jointly evaluate both static\nalignment and dynamic adaptation. As a case study, we apply InMind to the game\nAvalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o\nfrequently rely on lexical cues, struggling to anchor reflections in temporal\ngameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs\nlike DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These\nfindings reveal key limitations in current LLMs' capacity for individualized,\nadaptive reasoning, and position InMind as a step toward cognitively aligned\nhuman-AI interaction.", "AI": {"tldr": "InMind\u6846\u67b6\u8bc4\u4f30LLM\u5728\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u4e2d\u7684\u4e2a\u6027\u5316\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u901a\u7528LLM\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u7684LLM\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u5ffd\u7565\u4e86\u4e2a\u6027\u5316\u63a8\u7406\u98ce\u683c\u5bf9\u793e\u4ea4\u73af\u5883\u4e2d\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u662f\u8bc4\u4f30\u4e2a\u6027\u5316\u63a8\u7406\u7684\u7406\u60f3\u73af\u5883\u3002", "method": "\u63d0\u51faInMind\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u6e38\u620f\u6570\u636e\u3001\u7b56\u7565\u8ffd\u8e2a\u548c\u8d5b\u540e\u53cd\u601d\uff0c\u8bc4\u4f30LLM\u5728\u793e\u4ea4\u63a8\u7406\u6e38\u620f\uff08\u4ee5Avalon\u4e3a\u4f8b\uff09\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u901a\u7528LLM\u4f9d\u8d56\u8bcd\u6c47\u7ebf\u7d22\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u7b56\u7565\uff1b\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u7684LLM\uff08\u5982DeepSeek-R1\uff09\u8868\u73b0\u51fa\u5bf9\u98ce\u683c\u654f\u611f\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u4e2a\u6027\u5316\u3001\u9002\u5e94\u6027\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0cInMind\u6846\u67b6\u6709\u52a9\u4e8e\u8bc4\u4f30LLM\u5728\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u4e2d\u7684\u8ba4\u77e5\u80fd\u529b\u3002"}}
{"id": "2508.16112", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16112", "abs": "https://arxiv.org/abs/2508.16112", "authors": ["Heewoong Noh", "Namkyeong Lee", "Gyoung S. Na", "Kibum Kim", "Chanyoung Park"], "title": "IR-Agent: Expert-Inspired LLM Agents for Structure Elucidation from Infrared Spectra", "comment": null, "summary": "Spectral analysis provides crucial clues for the elucidation of unknown\nmaterials. Among various techniques, infrared spectroscopy (IR) plays an\nimportant role in laboratory settings due to its high accessibility and low\ncost. However, existing approaches often fail to reflect expert analytical\nprocesses and lack flexibility in incorporating diverse types of chemical\nknowledge, which is essential in real-world analytical scenarios. In this\npaper, we propose IR-Agent, a novel multi-agent framework for molecular\nstructure elucidation from IR spectra. The framework is designed to emulate\nexpert-driven IR analysis procedures and is inherently extensible. Each agent\nspecializes in a specific aspect of IR interpretation, and their complementary\nroles enable integrated reasoning, thereby improving the overall accuracy of\nstructure elucidation. Through extensive experiments, we demonstrate that\nIR-Agent not only improves baseline performance on experimental IR spectra but\nalso shows strong adaptability to various forms of chemical information.", "AI": {"tldr": "IR-Agent:\u4e00\u79cd\u65b0\u7684\u591aAgent\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7ea2\u5916\u5149\u8c31\u4e2d\u9610\u660e\u5206\u5b50\u7ed3\u6784\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u5e76\u5177\u6709\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u53cd\u6620\u4e13\u5bb6\u5206\u6790\u6d41\u7a0b\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002", "method": "\u591aAgent\u6846\u67b6\uff0c\u6a21\u62df\u4e13\u5bb6\u9a71\u52a8\u7684\u7ea2\u5916\u5206\u6790\u6d41\u7a0b\u3002", "result": "IR-Agent\u63d0\u9ad8\u4e86\u5b9e\u9a8c\u7ea2\u5916\u5149\u8c31\u7684\u57fa\u7ebf\u6027\u80fd\uff0c\u5e76\u5bf9\u5404\u79cd\u5f62\u5f0f\u7684\u5316\u5b66\u4fe1\u606f\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\u3002", "conclusion": "IR-Agent\u6846\u67b6\u63d0\u9ad8\u4e86\u57fa\u4e8e\u7ea2\u5916\u5149\u8c31\u7684\u5206\u5b50\u7ed3\u6784\u89e3\u6790\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u5404\u79cd\u5316\u5b66\u4fe1\u606f\u7684\u5f3a\u5927\u9002\u5e94\u6027\u3002"}}
{"id": "2508.16117", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.16117", "abs": "https://arxiv.org/abs/2508.16117", "authors": ["Saransh Kumar Gupta", "Rizwan Gulzar Mir", "Lipika Dey", "Partha Pratim Das", "Anirban Sen", "Ramesh Jain"], "title": "Extending FKG.in: Towards a Food Claim Traceability Network", "comment": "10 pages, 3 figures, 1 table, 45 references, ACM International\n  Conference on Multimedia 2025 - Multi-modal Food Computing Workshop", "summary": "The global food landscape is rife with scientific, cultural, and commercial\nclaims about what foods are, what they do, what they should not do, or should\nnot do. These range from rigorously studied health benefits (probiotics improve\ngut health) and misrepresentations (soaked almonds make one smarter) to vague\npromises (superfoods boost immunity) and culturally rooted beliefs (cold foods\ncause coughs). Despite their widespread influence, the infrastructure for\ntracing, verifying, and contextualizing these claims remains fragmented and\nunderdeveloped. In this paper, we propose a Food Claim-Traceability Network\n(FCN) as an extension of FKG.in, a knowledge graph of Indian food that we have\nbeen incrementally building. We also present the ontology design and the\nsemi-automated knowledge curation workflow that we used to develop a proof of\nconcept of FKG.in-FCN using Reddit data and Large Language Models. FCN\nintegrates curated data inputs, structured schemas, and provenance-aware\npipelines for food-related claim extraction and validation. While directly\nlinked to the Indian food knowledge graph as an application, our methodology\nremains application-agnostic and adaptable to other geographic, culinary, or\nregulatory settings. By modeling food claims and their traceability in a\nstructured, verifiable, and explainable way, we aim to contribute to more\ntransparent and accountable food knowledge ecosystems, supporting researchers,\npolicymakers, and most importantly, everyday consumers in navigating a world\nsaturated with dietary assertions.", "AI": {"tldr": "\u6784\u5efa\u98df\u54c1\u58f0\u660e\u53ef\u8ffd\u6eaf\u6027\u7f51\u7edc\uff08FCN\uff09\uff0c\u89e3\u51b3\u98df\u54c1\u4fe1\u606f\u771f\u4f2a\u96be\u8fa8\u7684\u95ee\u9898\u3002", "motivation": "\u5168\u7403\u98df\u54c1\u9886\u57df\u5b58\u5728\u5927\u91cf\u771f\u5047\u96be\u8fa8\u7684\u98df\u54c1\u58f0\u660e\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u8ffd\u8e2a\u548c\u9a8c\u8bc1\u673a\u5236\u3002", "method": "\u57fa\u4e8eFKG.in\u77e5\u8bc6\u56fe\u8c31\uff0c\u5229\u7528Reddit\u6570\u636e\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6784\u5efaFCN\uff0c\u6574\u5408\u6570\u636e\u8f93\u5165\u3001\u7ed3\u6784\u5316\u6a21\u5f0f\u548c\u6eaf\u6e90\u611f\u77e5\u6d41\u7a0b\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u8ffd\u6eaf\u98df\u54c1\u58f0\u660e\u7684\u539f\u578b\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5177\u6709\u5e94\u7528\u65e0\u5173\u6027\uff0c\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u5730\u533a\u548c\u573a\u666f\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u98df\u54c1\u58f0\u660e\u53ef\u8ffd\u6eaf\u6027\u7f51\u7edc\uff08FCN\uff09\u7528\u4e8e\u8ffd\u8e2a\u548c\u9a8c\u8bc1\u98df\u54c1\u76f8\u5173\u7684\u58f0\u660e\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5370\u5ea6\u98df\u7269\u77e5\u8bc6\u56fe\u8c31\u7684\u539f\u578b\u7cfb\u7edf\u3002"}}
{"id": "2508.16129", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16129", "abs": "https://arxiv.org/abs/2508.16129", "authors": ["Ruiqi Wu", "Yuang Yao", "Tengfei Ma", "Chenran Zhang", "Na Su", "Tao Zhou", "Geng Chen", "Wen Fan", "Yi Zhou"], "title": "Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning", "comment": null, "summary": "Multimodal large language models (MLLMs) have recently demonstrated\nremarkable reasoning abilities with reinforcement learning paradigm. Although\nseveral multimodal reasoning models have been explored in the medical domain,\nmost of them focus exclusively on basic reasoning, which refers to shallow\ninference based on visual feature matching. However, real-world clinical\ndiagnosis extends beyond basic reasoning, demanding reasoning processes that\nintegrate heterogeneous clinical information (such as chief complaints and\nmedical history) with multimodal medical imaging data. To bridge this gap, we\nintroduce MM-Retinal-Reason, the first ophthalmic multimodal dataset with the\nfull spectrum of perception and reasoning. It encompasses both basic reasoning\ntasks and complex reasoning tasks, aiming to enhance visual-centric fundamental\nreasoning capabilities and emulate realistic clinical thinking patterns.\nBuilding upon MM-Retinal-Reason, we propose OphthaReason, the first\nophthalmology-specific multimodal reasoning model with step-by-step reasoning\ntraces. To enable flexible adaptation to both basic and complex reasoning\ntasks, we specifically design a novel method called Uncertainty-Aware Dynamic\nThinking (UADT), which estimates sample-level uncertainty via entropy and\ndynamically modulates the model's exploration depth using a shaped advantage\nmechanism. Comprehensive experiments demonstrate that our model achieves\nstate-of-the-art performance on both basic and complex reasoning tasks,\noutperforming general-purpose MLLMs, medical MLLMs, RL-based medical MLLMs, and\nophthalmic MLLMs by at least 24.92\\%, 15.00\\%, 21.20\\%, and 17.66\\%. Project\nPage: \\href{https://github.com/lxirich/OphthaReason}{link}.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u9996\u4e2a\u773c\u79d1\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578bOphthaReason\uff0c\u5e76\u901a\u8fc7UADT\u65b9\u6cd5\u63d0\u5347\u4e86\u5176\u5728\u57fa\u7840\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u5b66\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u5927\u591a\u96c6\u4e2d\u4e8e\u57fa\u4e8e\u89c6\u89c9\u7279\u5f81\u5339\u914d\u7684\u6d45\u5c42\u63a8\u7406\uff08\u57fa\u7840\u63a8\u7406\uff09\uff0c\u65e0\u6cd5\u80dc\u4efb\u9700\u8981\u6574\u5408\u5f02\u6784\u4e34\u5e8a\u4fe1\u606f\uff08\u5982\u4e3b\u8bc9\u548c\u75c5\u53f2\uff09\u548c\u591a\u6a21\u6001\u533b\u5b66\u5f71\u50cf\u6570\u636e\u7684\u590d\u6742\u4e34\u5e8a\u8bca\u65ad\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u52a8\u6001\u601d\u7ef4(UADT)\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u71b5\u4f30\u8ba1\u6837\u672c\u7ea7\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u4f7f\u7528\u5f62\u72b6\u4f18\u52bf\u673a\u5236\u52a8\u6001\u8c03\u8282\u6a21\u578b\u7684\u63a2\u7d22\u6df1\u5ea6\u3002\u6784\u5efa\u4e86MM-Retinal-Reason\u6570\u636e\u96c6\uff0c\u5305\u542b\u57fa\u7840\u63a8\u7406\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u3002", "result": "\u5728\u57fa\u7840\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cOphthaReason\u6a21\u578b\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "conclusion": "OphthaReason\u6a21\u578b\u5728\u57fa\u7840\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u901a\u7528MLLM\u3001\u533b\u5b66MLLM\u3001\u57fa\u4e8eRL\u7684\u533b\u5b66MLLM\u548c\u773c\u79d1MLLM\u3002"}}
{"id": "2508.16172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16172", "abs": "https://arxiv.org/abs/2508.16172", "authors": ["Kai Hu", "Parfait Atchade-Adelomou", "Carlo Adornetto", "Adrian Mora-Carrero", "Luis Alonso-Pastor", "Ariel Noyman", "Yubo Liu", "Kent Larson"], "title": "Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain", "comment": null, "summary": "Understanding human behavior in urban environments is a crucial field within\ncity sciences. However, collecting accurate behavioral data, particularly in\nnewly developed areas, poses significant challenges. Recent advances in\ngenerative agents, powered by Large Language Models (LLMs), have shown promise\nin simulating human behaviors without relying on extensive datasets.\nNevertheless, these methods often struggle with generating consistent,\ncontext-sensitive, and realistic behavioral outputs. To address these\nlimitations, this paper introduces the Preference Chain, a novel method that\nintegrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance\ncontext-aware simulation of human behavior in transportation systems.\nExperiments conducted on the Replica dataset demonstrate that the Preference\nChain outperforms standard LLM in aligning with real-world transportation mode\nchoices. The development of the Mobility Agent highlights potential\napplications of proposed method in urban mobility modeling for emerging cities,\npersonalized travel behavior analysis, and dynamic traffic forecasting. Despite\nlimitations such as slow inference and the risk of hallucination, the method\noffers a promising framework for simulating complex human behavior in\ndata-scarce environments, where traditional data-driven models struggle due to\nlimited data availability.", "AI": {"tldr": "\u5229\u7528RAG\u548cLLM\u6539\u8fdb\u4eba\u7c7b\u884c\u4e3a\u6a21\u62df\uff0c\u5728Replica\u6570\u636e\u96c6\u4e0a\u6548\u679c\u4f18\u4e8e\u6807\u51c6LLM\uff0c\u4f46\u5b58\u5728\u901f\u5ea6\u6162\u548c\u5e7b\u89c9\u98ce\u9669\u3002", "motivation": "\u6a21\u62df\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u4eba\u7c7b\u884c\u4e3a\uff0c\u5c24\u5176\u662f\u5728\u65b0\u5efa\u533a\u57df\u7f3a\u4e4f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u7ed3\u5408\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u4e0eLLM\u7684Preference Chain\u65b9\u6cd5\u3002", "result": "Preference Chain\u5728Replica\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6807\u51c6LLM\uff0c\u5728\u4ea4\u901a\u6a21\u5f0f\u9009\u62e9\u65b9\u9762\u66f4\u7b26\u5408\u73b0\u5b9e\u4e16\u754c\u3002Mobility Agent\u7684\u5f00\u53d1\u7a81\u51fa\u4e86\u8be5\u65b9\u6cd5\u5728\u57ce\u5e02\u4ea4\u901a\u5efa\u6a21\u3001\u4e2a\u6027\u5316\u51fa\u884c\u884c\u4e3a\u5206\u6790\u548c\u52a8\u6001\u4ea4\u901a\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "Preference Chain\u65b9\u6cd5\u5728Replica\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6807\u51c6LLM\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e0b\u6a21\u62df\u590d\u6742\u4eba\u7c7b\u884c\u4e3a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u6846\u67b6\uff0c\u5c3d\u7ba1\u5b58\u5728\u63a8\u7406\u901f\u5ea6\u6162\u548c\u5e7b\u89c9\u98ce\u9669\u7b49\u5c40\u9650\u6027\u3002"}}
{"id": "2508.16204", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.16204", "abs": "https://arxiv.org/abs/2508.16204", "authors": ["Jo\u00e3o Abrantes", "Robert Tjarko Lange", "Yujin Tang"], "title": "Competition and Attraction Improve Model Fusion", "comment": "Accepted at GECCO 2025 as a full paper", "summary": "Model merging is a powerful technique for integrating the specialized\nknowledge of multiple machine learning models into a single model. However,\nexisting methods require manually partitioning model parameters into fixed\ngroups for merging, which restricts the exploration of potential combinations\nand limits performance. To overcome these limitations, we propose Model Merging\nof Natural Niches (M2N2), an evolutionary algorithm with three key features:\n(1) dynamic adjustment of merging boundaries to progressively explore a broader\nrange of parameter combinations; (2) a diversity preservation mechanism\ninspired by the competition for resources in nature, to maintain a population\nof diverse, high-performing models that are particularly well-suited for\nmerging; and (3) a heuristicbased attraction metric to identify the most\npromising pairs of models for fusion. Our experimental results demonstrate, for\nthe first time, that model merging can be used to evolve models entirely from\nscratch. Specifically, we apply M2N2 to evolve MNIST classifiers from scratch\nand achieve performance comparable to CMA-ES, while being computationally more\nefficient. Furthermore, M2N2 scales to merge specialized language and image\ngeneration models, achieving state-of-the-art performance. Notably, it\npreserves crucial model capabilities beyond those explicitly optimized by the\nfitness function, highlighting its robustness and versatility. Our code is\navailable at https://github.com/SakanaAI/natural_niches", "AI": {"tldr": "M2N2\u662f\u4e00\u79cd\u65b0\u578b\u6a21\u578b\u5408\u5e76\u8fdb\u5316\u7b97\u6cd5\uff0c\u80fd\u591f\u4ece\u96f6\u5f00\u59cb\u8fdb\u5316\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u9700\u8981\u624b\u52a8\u5212\u5206\u6a21\u578b\u53c2\u6570\uff0c\u9650\u5236\u4e86\u6f5c\u5728\u7ec4\u5408\u7684\u63a2\u7d22\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aM2N2\u7684\u8fdb\u5316\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5177\u6709\u52a8\u6001\u8c03\u6574\u5408\u5e76\u8fb9\u754c\u3001\u591a\u6837\u6027\u4fdd\u6301\u673a\u5236\u548c\u542f\u53d1\u5f0f\u5438\u5f15\u5ea6\u91cf\u4e09\u4e2a\u5173\u952e\u7279\u6027\u3002", "result": "M2N2\u5728MNIST\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4e0eCMA-ES\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff1b\u5728\u8bed\u8a00\u548c\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u5408\u5e76\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4fdd\u6301\u4e86\u5173\u952e\u7684\u6a21\u578b\u80fd\u529b\u3002", "conclusion": "M2N2\u7b97\u6cd5\u80fd\u591f\u4ece\u96f6\u5f00\u59cb\u8fdb\u5316\u6a21\u578b\uff0c\u5e76\u5728MNIST\u5206\u7c7b\u548c\u8bed\u8a00/\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5408\u5e76\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u5173\u952e\u80fd\u529b\u3002"}}
{"id": "2508.16277", "categories": ["cs.AI", "cs.HC", "68T01, 68T05, 68T42, 91A80", "I.2; K.4"], "pdf": "https://arxiv.org/pdf/2508.16277", "abs": "https://arxiv.org/abs/2508.16277", "authors": ["Alexandru Tugui"], "title": "The next question after Turing's question: Introducing the Grow-AI test", "comment": "9th International Conference on Inventive Systems and Control ICISC\n  2025", "summary": "This study aims to extend the framework for assessing artificial\nintelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom),\ndesigned to answer the question \"Can machines grow up?\" -- a natural successor\nto the Turing Test. The methodology applied is based on a system of six primary\ncriteria (C1-C6), each assessed through a specific \"game\", divided into four\narenas that explore both the human dimension and its transposition into AI. All\ndecisions and actions of the entity are recorded in a standardized AI Journal,\nthe primary source for calculating composite scores. The assessment uses the\nprior expert method to establish initial weights, and the global score -- Grow\nUp Index -- is calculated as the arithmetic mean of the six scores, with\ninterpretation on maturity thresholds. The results show that the methodology\nallows for a coherent and comparable assessment of the level of \"growth\" of AI\nentities, regardless of their type (robots, software agents, LLMs). The\nmulti-game structure highlights strengths and vulnerable areas, and the use of\na unified journal guarantees traceability and replicability in the evaluation.\nThe originality of the work lies in the conceptual transposition of the process\nof \"growing\" from the human world to that of artificial intelligence, in an\nintegrated testing format that combines perspectives from psychology, robotics,\ncomputer science, and ethics. Through this approach, GROW-AI not only measures\nperformance but also captures the evolutionary path of an AI entity towards\nmaturity.", "AI": {"tldr": "GROW-AI\u6846\u67b6\u901a\u8fc7\u516d\u4e2a\u6807\u51c6\u548c\u56db\u4e2a\u9886\u57df\u7684\u201c\u6e38\u620f\u201d\u8bc4\u4f30AI\u7684\u6210\u957f\u6210\u719f\u5ea6\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u4e14\u53ef\u590d\u5236\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u8bc4\u4f30AI\u7684\u201c\u6210\u957f\u201d\uff0c\u65e8\u5728\u56de\u7b54\u201c\u673a\u5668\u80fd\u5426\u6210\u957f\uff1f\u201d\u8fd9\u4e2a\u95ee\u9898\uff0c\u8d85\u8d8a\u56fe\u7075\u6d4b\u8bd5\u3002", "method": "GROW-AI\u6846\u67b6\u57fa\u4e8e\u516d\u4e2a\u4e3b\u8981\u6807\u51c6\uff08C1-C6\uff09\uff0c\u6bcf\u4e2a\u6807\u51c6\u901a\u8fc7\u7279\u5b9a\u7684\u201c\u6e38\u620f\u201d\u8fdb\u884c\u8bc4\u4f30\uff0c\u5206\u4e3a\u56db\u4e2a\u9886\u57df\uff0c\u63a2\u7d22\u4eba\u7c7b\u7ef4\u5ea6\u53ca\u5176\u5728AI\u4e2d\u7684\u8f6c\u6362\u3002\u8bc4\u4f30\u4f7f\u7528\u4e13\u5bb6\u5148\u9a8c\u65b9\u6cd5\u786e\u5b9a\u521d\u59cb\u6743\u91cd\uff0c\u6700\u7ec8\u8ba1\u7b97\u201c\u6210\u957f\u6307\u6570\u201d\u3002", "result": "\u8be5\u65b9\u6cd5\u5141\u8bb8\u5bf9AI\u5b9e\u4f53\u7684\u201c\u6210\u957f\u201d\u6c34\u5e73\u8fdb\u884c\u8fde\u8d2f\u4e14\u53ef\u6bd4\u8f83\u7684\u8bc4\u4f30\uff0c\u591a\u6e38\u620f\u7ed3\u6784\u80fd\u7a81\u51fa\u5176\u4f18\u7f3a\u70b9\uff0c\u7edf\u4e00\u7684\u65e5\u5fd7\u4fdd\u8bc1\u4e86\u8bc4\u4f30\u7684\u53ef\u8ffd\u6eaf\u6027\u548c\u53ef\u590d\u5236\u6027\u3002", "conclusion": "GROW-AI\u6846\u67b6\u80fd\u6709\u6548\u8bc4\u4f30AI\u7684\u201c\u6210\u957f\u201d\u6c34\u5e73\uff0c\u65e0\u8bbaAI\u5b9e\u4f53\u7684\u7c7b\u578b\u5982\u4f55\uff0c\u90fd\u80fd\u8fdb\u884c\u8fde\u8d2f\u4e14\u53ef\u6bd4\u8f83\u7684\u8bc4\u4f30\u3002\u591a\u6e38\u620f\u7ed3\u6784\u7a81\u51fa\u4e86AI\u7684\u4f18\u52bf\u548c\u52a3\u52bf\uff0c\u7edf\u4e00\u7684AI\u65e5\u5fd7\u4fdd\u8bc1\u4e86\u8bc4\u4f30\u7684\u53ef\u8ffd\u6eaf\u6027\u548c\u53ef\u590d\u5236\u6027\u3002"}}
{"id": "2508.16279", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16279", "abs": "https://arxiv.org/abs/2508.16279", "authors": ["Dawei Gao", "Zitao Li", "Yuexiang Xie", "Weirui Kuang", "Liuyi Yao", "Bingchen Qian", "Zhijian Ma", "Yue Cui", "Haohao Luo", "Shen Li", "Lu Yi", "Yi Yu", "Shiqi He", "Zhiling Luo", "Wenmeng Zhou", "Zhicheng Zhang", "Xuguang He", "Ziqian Chen", "Weikai Liao", "Farruh Isakulovich Kushnazarov", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "title": "AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications", "comment": null, "summary": "Driven by rapid advancements of Large Language Models (LLMs), agents are\nempowered to combine intrinsic knowledge with dynamic tool use, greatly\nenhancing their capacity to address real-world tasks. In line with such an\nevolution, AgentScope introduces major improvements in a new version (1.0),\ntowards comprehensively supporting flexible and efficient tool-based\nagent-environment interactions for building agentic applications. Specifically,\nwe abstract foundational components essential for agentic applications and\nprovide unified interfaces and extensible modules, enabling developers to\neasily leverage the latest progress, such as new models and MCPs. Furthermore,\nwe ground agent behaviors in the ReAct paradigm and offer advanced agent-level\ninfrastructure based on a systematic asynchronous design, which enriches both\nhuman-agent and agent-agent interaction patterns while improving execution\nefficiency. Building on this foundation, we integrate several built-in agents\ntailored to specific practical scenarios. AgentScope also includes robust\nengineering support for developer-friendly experiences. We provide a scalable\nevaluation module with a visual studio interface, making the development of\nlong-trajectory agentic applications more manageable and easier to trace. In\naddition, AgentScope offers a runtime sandbox to ensure safe agent execution\nand facilitates rapid deployment in production environments. With these\nenhancements, AgentScope provides a practical foundation for building scalable,\nadaptive, and effective agentic applications.", "AI": {"tldr": "AgentScope 1.0 \u662f\u4e00\u4e2a\u6539\u8fdb\u7684\u5de5\u5177\u578b\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u652f\u6301\u7075\u6d3b\u9ad8\u6548\u7684\u5de5\u5177\u4f7f\u7528\uff0c\u5e76\u63d0\u4f9b\u5f00\u53d1\u8005\u53cb\u597d\u7684\u5de5\u7a0b\u652f\u6301\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u667a\u80fd\u4f53\u80fd\u591f\u7ed3\u5408\u5185\u5728\u77e5\u8bc6\u548c\u52a8\u6001\u5de5\u5177\u4f7f\u7528\uff0cAgentScope\u65e8\u5728\u5168\u9762\u652f\u6301\u7075\u6d3b\u9ad8\u6548\u7684\u57fa\u4e8e\u5de5\u5177\u7684\u4ee3\u7406\u73af\u5883\u4ea4\u4e92\u3002", "method": "AgentScope 1.0 \u62bd\u8c61\u4e86\u4ee3\u7406\u5e94\u7528\u7a0b\u5e8f\u7684\u57fa\u672c\u7ec4\u4ef6\uff0c\u63d0\u4f9b\u7edf\u4e00\u63a5\u53e3\u548c\u53ef\u6269\u5c55\u6a21\u5757\uff0c\u652f\u6301\u6700\u65b0\u7684\u6a21\u578b\u548cMCP\uff0c\u5e76\u57fa\u4e8e\u5f02\u6b65\u8bbe\u8ba1\u6784\u5efa\u4e86\u9ad8\u7ea7\u4ee3\u7406\u57fa\u7840\u8bbe\u65bd\u3002", "result": "AgentScope 1.0 \u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u6a21\u5757\u3001\u53ef\u89c6\u5316Studio\u754c\u9762\u3001\u8fd0\u884c\u65f6\u6c99\u7bb1\u7b49\uff0c\u65b9\u4fbf\u5f00\u53d1\u8005\u6784\u5efa\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u4e14\u6709\u6548\u7684\u4ee3\u7406\u5e94\u7528\u7a0b\u5e8f\u3002", "conclusion": "AgentScope 1.0 \u663e\u8457\u6539\u8fdb\u4e86\u57fa\u4e8e\u5de5\u5177\u7684\u667a\u80fd\u4f53\u4e0e\u73af\u5883\u4ea4\u4e92\uff0c\u4e3a\u6784\u5efa\u4ee3\u7406\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\u3002"}}
{"id": "2508.16292", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.16292", "abs": "https://arxiv.org/abs/2508.16292", "authors": ["Wen-Han Hsieh", "Elvis Hsieh", "Dantong Niu", "Trevor Darrell", "Roei Herzig", "David M. Chan"], "title": "Do What? Teaching Vision-Language-Action Models to Reject the Impossible", "comment": "9 pages, 2 figures, 1 table", "summary": "Recently, Vision-Language-Action (VLA) models have demonstrated strong\nperformance on a range of robotic tasks. These models rely on multimodal\ninputs, with language instructions playing a crucial role -- not only in\npredicting actions, but also in robustly interpreting user intent, even when\nthe requests are impossible to fulfill. In this work, we investigate how VLAs\ncan recognize, interpret, and respond to false-premise instructions: natural\nlanguage commands that reference objects or conditions absent from the\nenvironment. We propose Instruct-Verify-and-Act (IVA), a unified framework that\n(i) detects when an instruction cannot be executed due to a false premise, (ii)\nengages in language-based clarification or correction, and (iii) grounds\nplausible alternatives in perception and action. Towards this end, we construct\na large-scale instruction tuning setup with structured language prompts and\ntrain a VLA model capable of handling both accurate and erroneous requests. Our\napproach leverages a contextually augmented, semi-synthetic dataset containing\npaired positive and false-premise instructions, enabling robust detection and\nnatural language correction. Our experiments show that IVA improves false\npremise detection accuracy by 97.56% over baselines, while increasing\nsuccessful responses in false-premise scenarios by 50.78%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIVA\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u673a\u5668\u4eba\u5bf9\u9519\u8bef\u524d\u63d0\u6307\u4ee4\u7684\u5904\u7406\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8(VLA)\u6a21\u578b\u5728\u5904\u7406\u9519\u8bef\u524d\u63d0\u6307\u4ee4\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u672c\u6587\u65e8\u5728\u7814\u7a76VLA\u6a21\u578b\u5982\u4f55\u8bc6\u522b\u3001\u89e3\u91ca\u548c\u54cd\u5e94\u8fd9\u7c7b\u6307\u4ee4\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6307\u4ee4-\u9a8c\u8bc1-\u884c\u52a8(IVA)\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u68c0\u6d4b\u3001\u89e3\u91ca\u548c\u54cd\u5e94\u9519\u8bef\u524d\u63d0\u6307\u4ee4\uff0c\u5e76\u5229\u7528\u4e0a\u4e0b\u6587\u589e\u5f3a\u3001\u534a\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cIVA\u6846\u67b6\u5728\u9519\u8bef\u524d\u63d0\u6307\u4ee4\u68c0\u6d4b\u548c\u6210\u529f\u54cd\u5e94\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "IVA\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u9519\u8bef\u524d\u63d0\u6307\u4ee4\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0897.56%\uff09\u548c\u6210\u529f\u54cd\u5e94\u7387\uff0850.78%\uff09\uff0c\u6709\u6548\u5904\u7406\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8bed\u8a00\u6307\u4ee4\u7684\u9519\u8bef\u524d\u63d0\u95ee\u9898\u3002"}}
{"id": "2508.16352", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.16352", "abs": "https://arxiv.org/abs/2508.16352", "authors": ["Nasir Khan", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil", "Sinem Coleri"], "title": "Causal Beam Selection for Reliable Initial Access in AI-driven Beam Management", "comment": null, "summary": "Efficient and reliable beam alignment is a critical requirement for mmWave\nmultiple-input multiple-output (MIMO) systems, especially in 6G and beyond,\nwhere communication must be fast, adaptive, and resilient to real-world\nuncertainties. Existing deep learning (DL)-based beam alignment methods often\nneglect the underlying causal relationships between inputs and outputs, leading\nto limited interpretability, poor generalization, and unnecessary beam sweeping\noverhead. In this work, we propose a causally-aware DL framework that\nintegrates causal discovery into beam management pipeline. Particularly, we\npropose a novel two-stage causal beam selection algorithm to identify a minimal\nset of relevant inputs for beam prediction. First, causal discovery learns a\nBayesian graph capturing dependencies between received power inputs and the\noptimal beam. Then, this graph guides causal feature selection for the DL-based\nclassifier. Simulation results reveal that the proposed causal beam selection\nmatches the performance of conventional methods while drastically reducing\ninput selection time by 94.4% and beam sweeping overhead by 59.4% by focusing\nonly on causally relevant features.", "AI": {"tldr": "\u5229\u7528\u56e0\u679c\u53d1\u73b0\u6280\u672f\u6539\u8fdb\u6beb\u7c73\u6ce2MIMO\u7cfb\u7edf\u6ce2\u675f\u5bf9\u51c6\uff0c\u663e\u8457\u964d\u4f4e\u65f6\u95f4\u548c\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6ce2\u675f\u5bf9\u51c6\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5bfc\u81f4\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u4ee5\u53ca\u4e0d\u5fc5\u8981\u7684\u6ce2\u675f\u626b\u63cf\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u56e0\u679c\u6ce2\u675f\u9009\u62e9\u7b97\u6cd5\uff0c\u9996\u5148\u4f7f\u7528\u56e0\u679c\u53d1\u73b0\u5b66\u4e60\u8d1d\u53f6\u65af\u56fe\u4ee5\u6355\u83b7\u63a5\u6536\u529f\u7387\u8f93\u5165\u548c\u6700\u4f73\u6ce2\u675f\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u7136\u540e\u8be5\u56fe\u6307\u5bfc\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5206\u7c7b\u5668\u7684\u56e0\u679c\u7279\u5f81\u9009\u62e9\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u56e0\u679c\u6ce2\u675f\u9009\u62e9\u65b9\u6cd5\u7684\u6027\u80fd\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u901a\u8fc7\u4ec5\u5173\u6ce8\u56e0\u679c\u76f8\u5173\u7684\u7279\u5f81\uff0c\u5c06\u8f93\u5165\u9009\u62e9\u65f6\u95f4\u51cf\u5c11\u4e86 94.4%\uff0c\u5c06\u6ce2\u675f\u626b\u63cf\u5f00\u9500\u51cf\u5c11\u4e86 59.4%\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u611f\u77e5\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u56e0\u679c\u53d1\u73b0\u96c6\u6210\u5230\u6ce2\u675f\u7ba1\u7406\u7ba1\u9053\u4e2d\uff0c\u901a\u8fc7\u56e0\u679c\u7279\u5f81\u9009\u62e9\u51cf\u5c11\u8f93\u5165\u9009\u62e9\u65f6\u95f4\u548c\u6ce2\u675f\u626b\u63cf\u5f00\u9500\uff0c\u5728\u6027\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u3002"}}
