{"id": "2508.11836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11836", "abs": "https://arxiv.org/abs/2508.11836", "authors": ["Dave Goel", "Matthew Guzdial", "Anurag Sarkar"], "title": "Finite Automata Extraction: Low-data World Model Learning as Programs from Gameplay Video", "comment": null, "summary": "World models are defined as a compressed spatial and temporal learned\nrepresentation of an environment. The learned representation is typically a\nneural network, making transfer of the learned environment dynamics and\nexplainability a challenge. In this paper, we propose an approach, Finite\nAutomata Extraction (FAE), that learns a neuro-symbolic world model from\ngameplay video represented as programs in a novel domain-specific language\n(DSL): Retro Coder. Compared to prior world model approaches, FAE learns a more\nprecise model of the environment and more general code than prior DSL-based\napproaches.", "AI": {"tldr": "\u4f7f\u7528FAE\u4ece\u6e38\u620f\u89c6\u9891\u4e2d\u5b66\u4e60\u66f4\u7cbe\u786e\u3001\u66f4\u901a\u7528\u7684\u795e\u7ecf\u7b26\u53f7\u4e16\u754c\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u4e16\u754c\u6a21\u578b\u65b9\u6cd5\u96be\u4ee5\u8fc1\u79fb\u5b66\u4e60\u5230\u7684\u73af\u5883\u52a8\u6001\u548c\u89e3\u91ca\u6027\uff0c\u8be5\u8bba\u6587\u65e8\u5728\u5b66\u4e60\u66f4\u7cbe\u786e\u548c\u901a\u7528\u7684\u73af\u5883\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u9650\u81ea\u52a8\u673a\u63d0\u53d6 (FAE) \u65b9\u6cd5\uff0c\u4ece\u6e38\u620f\u89c6\u9891\u4e2d\u5b66\u4e60\u795e\u7ecf\u7b26\u53f7\u4e16\u754c\u6a21\u578b\u3002\u4f7f\u7528Retro Coder DSL\u5c06\u6e38\u620f\u89c6\u9891\u8868\u793a\u4e3a\u7a0b\u5e8f\u3002", "result": "FAE\u5b66\u4e60\u5230\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u7cbe\u786e\u7684\u73af\u5883\u6a21\u578b\u548c\u66f4\u901a\u7528\u7684\u4ee3\u7801\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u6e38\u620f\u89c6\u9891\u4e2d\u5b66\u4e60\u795e\u7ecf\u7b26\u53f7\u4e16\u754c\u6a21\u578b\u7684\u65b9\u6cd5\u2014\u2014\u6709\u9650\u81ea\u52a8\u673a\u63d0\u53d6 (FAE)\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u65b0\u578b\u9886\u57df\u7279\u5b9a\u8bed\u8a00 (DSL) Retro Coder \u5c06\u6e38\u620f\u89c6\u9891\u8868\u793a\u4e3a\u7a0b\u5e8f\uff0c\u5b66\u4e60\u5230\u7684\u6a21\u578b\u6bd4\u4ee5\u5f80\u65b9\u6cd5\u66f4\u7cbe\u786e\u3001\u4ee3\u7801\u66f4\u901a\u7528\u3002"}}
{"id": "2508.11850", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11850", "abs": "https://arxiv.org/abs/2508.11850", "authors": ["Milad Yazdani", "Mahdi Mostajabdaveh", "Samin Aref", "Zirui Zhou"], "title": "EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models", "comment": null, "summary": "Integer programming lies at the heart of crucial combinatorial optimization\ntasks but remains challenging due to its NP-hard nature. An effective approach\nfor practically solving integer programs is the manual design of acceleration\ncuts, i.e. inequalities that improve solver performance. However, this creative\nprocess demands deep expertise and is yet to be automated. Our proposed\nframework, EvoCut, automates the generation of acceleration cuts by combining\nlarge language models (LLMs) with an evolutionary search. EvoCut (i)\ninitializes a diverse population of candidate cuts via an LLM-based initializer\nagent; (ii) for each cut empirically evaluates both preservation of the optimal\nsolution and its ability to cut off fractional solutions across a verification\nset; and (iii) iteratively refines the population through evolutionary\ncrossover and mutation agents. We quantify each cut's utility by its relative\nreduction in the solver's optimality gap. Our comparisons against standard\ninteger programming practice show that EvoCut reduces optimality gap by 17-57%\nwithin a fixed time. It obtains the same solutions up to 4 times as fast, and\nobtains higher-quality solutions within the same time limit. Requiring no human\nexpert input, EvoCut reliably generates, improves, and empirically verifies\ncuts that generalize to unseen instances. The code is available at\nhttps://github.com/milad1378yz/EvoCut.", "AI": {"tldr": "EvoCut \u81ea\u52a8\u751f\u6210\u6574\u6570\u89c4\u5212\u52a0\u901f\u5207\u5272\uff0c\u663e\u8457\u63d0\u5347\u6c42\u89e3\u6548\u7387\u548c\u89e3\u7684\u8d28\u91cf\u3002", "motivation": "\u6574\u6570\u89c4\u5212\u6c42\u89e3\u7684NP-hard\u7279\u6027\u5bfc\u81f4\u6c42\u89e3\u56f0\u96be\uff0c\u4eba\u5de5\u8bbe\u8ba1\u52a0\u901f\u5207\u5272\u8017\u65f6\u8d39\u529b\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u8fdb\u5316\u641c\u7d22\u7b97\u6cd5\u81ea\u52a8\u751f\u6210\u52a0\u901f\u5207\u5272\u3002", "result": "EvoCut\u51cf\u5c11\u4e86\u6700\u4f18\u6027\u5dee\u8ddd17-57%\uff0c\u6c42\u89e3\u901f\u5ea6\u63d0\u9ad8\u6700\u9ad8\u8fbe4\u500d\uff0c\u5728\u76f8\u540c\u65f6\u95f4\u5185\u83b7\u5f97\u66f4\u9ad8\u8d28\u91cf\u7684\u89e3\u3002", "conclusion": "EvoCut\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u8fdb\u5316\u641c\u7d22\uff0c\u81ea\u52a8\u5316\u751f\u6210\u52a0\u901f\u5207\u5272\uff0c\u63d0\u9ad8\u6574\u6570\u89c4\u5212\u6c42\u89e3\u6548\u7387\uff0c\u51cf\u5c11\u6700\u4f18\u6027\u5dee\u8ddd17-57%\uff0c\u901f\u5ea6\u63d0\u5347\u6700\u9ad8\u8fbe4\u500d\u3002"}}
{"id": "2508.11860", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11860", "abs": "https://arxiv.org/abs/2508.11860", "authors": ["Frazier N. Baker", "Daniel Adu-Ampratwum", "Reza Averly", "Botao Yu", "Huan Sun", "Xia Ning"], "title": "LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework", "comment": "24 pages, 5 figures", "summary": "Large language model (LLM) agent evaluators leverage specialized tools to\nground the rational decision-making of LLMs, making them well-suited to aid in\nscientific discoveries, such as constrained retrosynthesis planning.\nConstrained retrosynthesis planning is an essential, yet challenging, process\nwithin chemistry for identifying synthetic routes from commercially available\nstarting materials to desired target molecules, subject to practical\nconstraints. Here, we present LARC, the first LLM-based Agentic framework for\nRetrosynthesis planning under Constraints. LARC incorporates agentic constraint\nevaluation, through an Agent-as-a-Judge, directly into the retrosynthesis\nplanning process, using agentic feedback grounded in tool-based reasoning to\nguide and constrain route generation. We rigorously evaluate LARC on a\ncarefully curated set of 48 constrained retrosynthesis planning tasks across 3\nconstraint types. LARC achieves a 72.9% success rate on these tasks, vastly\noutperforming LLM baselines and approaching human expert-level success in\nsubstantially less time. The LARC framework is extensible, and serves as a\nfirst step towards an effective agentic tool or a co-scientist to human experts\nfor constrained retrosynthesis.", "AI": {"tldr": "LARC\u6846\u67b6\u662f\u4e00\u4e2a\u6709\u6548\u7684\u57fa\u4e8eLLM\u7684Agent\u6846\u67b6\uff0c\u7528\u4e8e\u7ea6\u675f\u9006\u5408\u6210\u89c4\u5212\uff0c\u5176\u6210\u529f\u7387\u9ad8\u8fbe72.9%\uff0c\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73\u3002", "motivation": "\u89e3\u51b3\u7ea6\u675f\u9006\u5408\u6210\u89c4\u5212\u8fd9\u4e00\u5316\u5b66\u9886\u57df\u4e2d\u7684\u96be\u9898\uff0c\u4e3a\u79d1\u5b66\u53d1\u73b0\u63d0\u4f9b\u5e2e\u52a9\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aLARC\u7684\u57fa\u4e8eLLM\u7684Agent\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06Agent-as-a-Judge\u673a\u5236\u878d\u5165\u9006\u5408\u6210\u89c4\u5212\u8fc7\u7a0b\uff0c\u5229\u7528\u57fa\u4e8e\u5de5\u5177\u7684\u63a8\u7406\u8fdb\u884c\u7ea6\u675f\u8bc4\u4f30\u548c\u6307\u5bfc\u8def\u7ebf\u751f\u6210\u3002", "result": "LARC\u572848\u4e2a\u7ea6\u675f\u9006\u5408\u6210\u89c4\u5212\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e8672.9%\u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8eLLM\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u6846\u67b6LARC\u5728\u7ea6\u675f\u9006\u5408\u6210\u89c4\u5212\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u6210\u529f\u7387\u8fbe\u523072.9%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u7684\u6c34\u5e73\u3002"}}
{"id": "2508.11894", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11894", "abs": "https://arxiv.org/abs/2508.11894", "authors": ["Ao Li", "Bin Yan", "Bingfeng Cai", "Chenxi Li", "Cunzhong Zhao", "Fugen Yao", "Gaoqiang Liu", "Guanjun Jiang", "Jian Xu", "Liang Dong", "Liansheng Sun", "Rongshen Zhang", "Xiaolei Gui", "Xin Liu", "Xin Shang", "Yao Wu", "Yu Cao", "Zhenxin Ma", "Zhuang Jia"], "title": "QuarkMed Medical Foundation Model Technical Report", "comment": "20 pages", "summary": "Recent advancements in large language models have significantly accelerated\ntheir adoption in healthcare applications, including AI-powered medical\nconsultations, diagnostic report assistance, and medical search tools. However,\nmedical tasks often demand highly specialized knowledge, professional accuracy,\nand customization capabilities, necessitating a robust and reliable foundation\nmodel. QuarkMed addresses these needs by leveraging curated medical data\nprocessing, medical-content Retrieval-Augmented Generation (RAG), and a\nlarge-scale, verifiable reinforcement learning pipeline to develop a\nhigh-performance medical foundation model. The model achieved 70% accuracy on\nthe Chinese Medical Licensing Examination, demonstrating strong generalization\nacross diverse medical benchmarks. QuarkMed offers a powerful yet versatile\npersonal medical AI solution, already serving over millions of users at\nai.quark.cn.", "AI": {"tldr": "QuarkMed: \u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u533b\u7597\u57fa\u7840\u6a21\u578b\uff0c\u51c6\u786e\u7387\u9ad8\uff0c\u7528\u6237\u4f17\u591a\u3002", "motivation": "\u6ee1\u8db3\u533b\u7597\u4efb\u52a1\u5bf9\u4e13\u4e1a\u77e5\u8bc6\u3001\u51c6\u786e\u6027\u548c\u5b9a\u5236\u80fd\u529b\u7684\u9700\u6c42\u3002", "method": "\u5229\u7528\u7cbe\u9009\u7684\u533b\u7597\u6570\u636e\u5904\u7406\u3001\u533b\u7597\u5185\u5bb9\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u548c\u5927\u89c4\u6a21\u53ef\u9a8c\u8bc1\u5f3a\u5316\u5b66\u4e60\u6d41\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u4e2d\u534e\u533b\u5e08\u6267\u4e1a\u8003\u8bd5\u4e2d\u8fbe\u5230 70% \u7684\u51c6\u786e\u7387\u3002", "conclusion": "QuarkMed \u662f\u4e00\u6b3e\u9ad8\u6027\u80fd\u7684\u533b\u7597\u57fa\u7840\u6a21\u578b\uff0c\u5728\u4e2d\u534e\u533b\u5e08\u6267\u4e1a\u8003\u8bd5\u4e2d\u53d6\u5f97\u4e86 70% \u7684\u51c6\u786e\u7387\uff0c\u5e76\u5df2\u670d\u52a1\u6570\u767e\u4e07\u7528\u6237\u3002"}}
{"id": "2508.11944", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11944", "abs": "https://arxiv.org/abs/2508.11944", "authors": ["Hongtao Liu", "Zhicheng Du", "Zihe Wang", "Weiran Shen"], "title": "CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs", "comment": null, "summary": "Game-playing ability serves as an indicator for evaluating the strategic\nreasoning capability of large language models (LLMs). While most existing\nstudies rely on utility performance metrics, which are not robust enough due to\nvariations in opponent behavior and game structure. To address this limitation,\nwe propose \\textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation\nframework inspired by the cognitive hierarchy models from behavioral economics.\nWe hypothesize that agents have bounded rationality -- different agents behave\nat varying reasoning depths/levels. We evaluate LLMs' strategic reasoning\nthrough a three-phase systematic framework, utilizing behavioral data from six\nstate-of-the-art LLMs across fifteen carefully selected normal-form games.\nExperiments show that LLMs exhibit consistent strategic reasoning levels across\ndiverse opponents, confirming the framework's robustness and generalization\ncapability. We also analyze the effects of two key mechanisms (Chat Mechanism\nand Memory Mechanism) on strategic reasoning performance. Results indicate that\nthe Chat Mechanism significantly degrades strategic reasoning, whereas the\nMemory Mechanism enhances it. These insights position CHBench as a promising\ntool for evaluating LLM capabilities, with significant potential for future\nresearch and practical applications.", "AI": {"tldr": "CHBench\u6846\u67b6\u662f\u4e00\u79cd\u65b0\u7684\u3001\u9c81\u68d2\u7684LLM\u7b56\u7565\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u5de5\u5177\uff0c\u8bb0\u5fc6\u673a\u5236\u63d0\u5347\u6027\u80fd\uff0c\u804a\u5929\u673a\u5236\u964d\u4f4e\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30LLM\u7b56\u7565\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6548\u7528\u6027\u80fd\u6307\u6807\uff0c\u5176\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba4\u77e5\u5c42\u6b21\u6a21\u578b\u7684\u8bc4\u4f30\u6846\u67b6CHBench\uff0c\u5e76\u901a\u8fc7\u5bf9\u516d\u4e2aLLM\u5728\u5341\u4e94\u4e2a\u7cbe\u5fc3\u9009\u62e9\u7684\u6b63\u89c4\u5f62\u5f0f\u535a\u5f08\u4e2d\u7684\u884c\u4e3a\u6570\u636e\u8fdb\u884c\u4e09\u9636\u6bb5\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "CHBench\u6846\u67b6\u5728\u4e0d\u540c\u5bf9\u624b\u95f4\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u7b56\u7565\u63a8\u7406\u6c34\u5e73\uff0c\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u8bb0\u5fc6\u673a\u5236\u589e\u5f3a\u4e86\u7b56\u7565\u63a8\u7406\uff0c\u800c\u804a\u5929\u673a\u5236\u5219\u4f1a\u964d\u4f4e\u5176\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7b56\u7565\u63a8\u7406\u80fd\u529b\u7684\u6846\u67b6CHBench\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u8ba4\u77e5\u5c42\u6b21\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5bf9\u516d\u4e2aLLM\u5728\u5341\u4e94\u4e2a\u535a\u5f08\u4e2d\u7684\u884c\u4e3a\u6570\u636e\u5206\u6790\uff0c\u8bc4\u4f30\u4e86LLM\u7684\u7b56\u7565\u63a8\u7406\u6c34\u5e73\uff0c\u7ed3\u679c\u8868\u660eLLM\u5728\u4e0d\u540c\u5bf9\u624b\u95f4\u7684\u7b56\u7565\u63a8\u7406\u6c34\u5e73\u4e00\u81f4\uff0c\u4e14\u8bb0\u5fc6\u673a\u5236\u589e\u5f3a\u4e86\u7b56\u7565\u63a8\u7406\uff0c\u800c\u804a\u5929\u673a\u5236\u5219\u4f1a\u964d\u4f4e\u5176\u6027\u80fd\u3002"}}
{"id": "2508.11953", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11953", "abs": "https://arxiv.org/abs/2508.11953", "authors": ["Yuan Li", "Zhengzhong Liu", "Eric Xing"], "title": "Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models", "comment": null, "summary": "Optimizing data mixtures for supervised fine-tuning (SFT) of large language\nmodels (LLMs) is critical for developing general-purpose models, yet this area\nremains underexplored. In this paper, we frame data mixing as an optimization\nproblem and introduce a novel method designed to minimize validation loss. Our\napproach parametrizes the loss by modeling effective data transferred and\nleveraging scaling laws for fine-tuning. By experimenting with various\nsmall-scale data mixtures, we fit these parameters and derive the optimal\nweights. We provide both mathematical proofs and empirical results\ndemonstrating that our algorithm achieves excellent overall and individual\nperformance across all domains. Through controlled experiments, we show that\nmodels trained with our optimized weights perform on par with those using\noptimal weights determined via grid search, with per-domain loss only 0.66%\nhigher than the best domain loss from grid search on average. Additionally, we\nshow that reweighting popular SFT datasets using our method improves both\nvalidation loss and downstream performance. Finally, we discuss how our method\ncan generalize to guide data selection for domain-specific models and provide\ninsights into SFT.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u6570\u636e\u6df7\u5408\u4f18\u5316\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u76d1\u7763\u5fae\u8c03\u6027\u80fd\uff0c\u5176\u7ed3\u679c\u4e0e\u7f51\u683c\u641c\u7d22\u76f8\u5f53\u3002", "motivation": "\u4f18\u5316\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76d1\u7763\u5fae\u8c03\u7684\u6570\u636e\u6df7\u5408\uff0c\u4ee5\u5f00\u53d1\u901a\u7528\u7684\u6a21\u578b\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u6570\u636e\u6df7\u5408\u5efa\u6a21\u4e3a\u4e00\u4e2a\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u635f\u5931\u51fd\u6570\u5e76\u5229\u7528\u5fae\u8c03\u7f29\u653e\u5b9a\u5f8b\u6765\u4f18\u5316\u6570\u636e\u6743\u91cd\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5404\u4e2a\u9886\u57df\u90fd\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6574\u4f53\u548c\u4e2a\u4f53\u6027\u80fd\uff0c\u4e0e\u7f51\u683c\u641c\u7d22\u76f8\u6bd4\uff0c\u6bcf\u4e2a\u9886\u57df\u7684\u635f\u5931\u5e73\u5747\u4ec5\u9ad8\u51fa 0.66%\u3002\u91cd\u65b0\u52a0\u6743\u6d41\u884c\u7684 SFT \u6570\u636e\u96c6\u4e5f\u63d0\u9ad8\u4e86\u9a8c\u8bc1\u635f\u5931\u548c\u4e0b\u6e38\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u6df7\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u53c2\u6570\u5316\u635f\u5931\u51fd\u6570\u5e76\u5229\u7528\u5fae\u8c03\u7f29\u653e\u5b9a\u5f8b\u6765\u6700\u5c0f\u5316\u9a8c\u8bc1\u635f\u5931\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u4e0e\u7f51\u683c\u641c\u7d22\u7ed3\u679c\u76f8\u5f53\u3002"}}
{"id": "2508.11954", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11954", "abs": "https://arxiv.org/abs/2508.11954", "authors": ["Sehyuk Park", "Soyeon Caren Han", "Eduard Hovy"], "title": "UniCast: A Unified Multimodal Prompting Framework for Time Series Forecasting", "comment": null, "summary": "Time series forecasting is a foundational task across domains, such as\nfinance, healthcare, and environmental monitoring. While recent advances in\nTime Series Foundation Models (TSFMs) have demonstrated strong generalisation\nthrough large-scale pretraining, existing models operate predominantly in a\nunimodal setting, ignoring the rich multimodal context, such as visual and\ntextual signals, that often accompanies time series data in real-world\nscenarios. This paper introduces a novel parameter-efficient multimodal\nframework, UniCast, that extends TSFMs to jointly leverage time series, vision,\nand text modalities for enhanced forecasting performance. Our method integrates\nmodality-specific embeddings from pretrained Vision and Text Encoders with a\nfrozen TSFM via soft prompt tuning, enabling efficient adaptation with minimal\nparameter updates. This design not only preserves the generalisation strength\nof the foundation model but also enables effective cross-modal interaction.\nExtensive experiments across diverse time-series forecasting benchmarks\ndemonstrate that UniCast consistently and significantly outperforms all\nexisting TSFM baselines. The findings highlight the critical role of multimodal\ncontext in advancing the next generation of general-purpose time series\nforecasters.", "AI": {"tldr": "UniCast\u662f\u4e00\u4e2a\u53c2\u6570\u9ad8\u6548\u7684\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u5b83\u5229\u7528\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709TSFM\u4e3b\u8981\u5728\u5355\u6a21\u6001\u73af\u5883\u4e0b\u8fd0\u884c\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u573a\u666f\u4e2d\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u901a\u5e38\u4f34\u968f\u7684\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\uff0c\u4f8b\u5982\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u53f7\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u53c2\u6570\u9ad8\u6548\u7684\u591a\u6a21\u6001\u6846\u67b6UniCast\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u8f6f\u63d0\u793a\u5fae\u8c03\u5c06\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u548c\u6587\u672c\u7f16\u7801\u5668\u7684\u6a21\u6001\u7279\u5b9a\u5d4c\u5165\u4e0e\u51bb\u7ed3\u7684TSFM\u96c6\u6210\u3002", "result": "\u5728\u4e0d\u540c\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUniCast\u59cb\u7ec8\u4e14\u663e\u8457\u5730\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u7684TSFM\u57fa\u7ebf\u3002", "conclusion": "UniCast\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u73b0\u6709TSFM\u57fa\u7ebf\uff0c\u7a81\u51fa\u4e86\u591a\u6a21\u6001\u8bed\u5883\u5728\u53d1\u5c55\u4e0b\u4e00\u4ee3\u901a\u7528\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5668\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2508.11959", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11959", "abs": "https://arxiv.org/abs/2508.11959", "authors": ["Xuanxiang Huang", "Olivier L\u00e9toff\u00e9", "Joao Marques-Silva"], "title": "Rigorous Feature Importance Scores based on Shapley Value and Banzhaf Index", "comment": null, "summary": "Feature attribution methods based on game theory are ubiquitous in the field\nof eXplainable Artificial Intelligence (XAI). Recent works proposed rigorous\nfeature attribution using logic-based explanations, specifically targeting\nhigh-stakes uses of machine learning (ML) models. Typically, such works exploit\nweak abductive explanation (WAXp) as the characteristic function to assign\nimportance to features. However, one possible downside is that the contribution\nof non-WAXp sets is neglected. In fact, non-WAXp sets can also convey important\ninformation, because of the relationship between formal explanations (XPs) and\nadversarial examples (AExs). Accordingly, this paper leverages Shapley value\nand Banzhaf index to devise two novel feature importance scores. We take into\naccount non-WAXp sets when computing feature contribution, and the novel scores\nquantify how effective each feature is at excluding AExs. Furthermore, the\npaper identifies properties and studies the computational complexity of the\nproposed scores.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u65b0\u7684\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u5206\u65b9\u6cd5\uff0c\u6709\u6548\u5229\u7528\u975eWAXp\u96c6\u5408\u4fe1\u606f\uff0c\u63d0\u5347\u4e86\u7279\u5f81\u5f52\u56e0\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u5ffd\u7565\u4e86\u975eWAXp\u96c6\u5408\u7684\u8d21\u732e\uff0c\u800c\u8fd9\u4e9b\u96c6\u5408\u53ef\u80fd\u5305\u542b\u91cd\u8981\u4fe1\u606f\u3002", "method": "\u5229\u7528Shapley\u503c\u548cBanzhaf\u6307\u6570\uff0c\u8003\u8651\u975eWAXp\u96c6\u5408\u8ba1\u7b97\u7279\u5f81\u8d21\u732e\u3002", "result": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u5206\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u7684\u6027\u8d28\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u9896\u7684\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u5206\u65b9\u6cd5\uff0c\u8003\u8651\u4e86\u975eWAXp\u96c6\u5408\u5728\u8ba1\u7b97\u7279\u5f81\u8d21\u732e\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u91cf\u5316\u4e86\u6bcf\u4e2a\u7279\u5f81\u5728\u6392\u9664\u5bf9\u6297\u6837\u672c\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.11975", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11975", "abs": "https://arxiv.org/abs/2508.11975", "authors": ["Gongyao Jiang", "Qiong Luo"], "title": "Chart-CoCa: Self-Improving Chart Understanding of Vision LMs via Code-Driven Synthesis and Candidate-Conditioned Answering", "comment": "Accepted to CIKM 2025", "summary": "Vision Language Models (VLMs) often struggle with chart understanding tasks,\nparticularly in accurate chart description and complex reasoning. Synthetic\ndata generation is a promising solution, while usually facing the challenge of\nnoise labels. To address this challenge, we first introduce a chart synthesis\npipeline that generates aligned chart-question-answer triplets through code\ngeneration and execution, ensuring the reliability of synthetic data without\nhuman intervention. Furthermore, inspired by test-time scaling that increases\ninference budget and thereby improves performance, we design a\ncandidate-conditioned answering process. The VLM first generates multiple\nresponses per query, and then synthesizes the final answer by contextualizing\nthese candidates. Experiments demonstrate significant improvements, with up to\n15.50 points accuracy gain over the initial VLM, in a fully self-improving\nparadigm without either human-labeled data or external models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u548c\u5019\u9009\u6761\u4ef6\u56de\u7b54\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86VLMs\u5bf9\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u56fe\u8868\u7406\u89e3\u4efb\u52a1\uff0c\u7279\u522b\u662f\u51c6\u786e\u7684\u56fe\u8868\u63cf\u8ff0\u548c\u590d\u6742\u7684\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5408\u6210\u6570\u636e\u751f\u6210\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u901a\u5e38\u9762\u4e34\u566a\u58f0\u6807\u7b7e\u7684\u6311\u6218\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56fe\u8868\u5408\u6210\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u548c\u6267\u884c\u751f\u6210\u5bf9\u9f50\u7684\u56fe\u8868-\u95ee\u9898-\u7b54\u6848\u4e09\u5143\u7ec4\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5019\u9009\u6761\u4ef6\u56de\u7b54\u8fc7\u7a0b\uff0c\u8be5\u8fc7\u7a0b\u9996\u5148\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u751f\u6210\u591a\u4e2a\u54cd\u5e94\uff0c\u7136\u540e\u901a\u8fc7\u4e0a\u4e0b\u6587\u5173\u8054\u8fd9\u4e9b\u5019\u9009\u7b54\u6848\u6765\u5408\u6210\u6700\u7ec8\u7b54\u6848\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b8c\u5168\u81ea\u6539\u8fdb\u7684\u8303\u5f0f\u4e2d\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u6216\u5916\u90e8\u6a21\u578b\uff0c\u5c31\u80fd\u663e\u8457\u63d0\u9ad8VLMs\u7684\u51c6\u786e\u6027\uff0c\u6700\u9ad8\u53ef\u8fbe15.50\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u548c\u6267\u884c\u751f\u6210\u56fe\u8868-\u95ee\u9898-\u7b54\u6848\u4e09\u5143\u7ec4\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5019\u9009\u6761\u4ef6\u56de\u7b54\u8fc7\u7a0b\uff0c\u4ee5\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5bf9\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u7684\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8VLMs\u7684\u51c6\u786e\u6027\uff0c\u6700\u9ad8\u53ef\u8fbe15.50\u4e2a\u767e\u5206\u70b9\u3002"}}
{"id": "2508.11987", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11987", "abs": "https://arxiv.org/abs/2508.11987", "authors": ["Zhiyuan Zeng", "Jiashuo Liu", "Siyuan Chen", "Tianci He", "Yali Liao", "Jinpeng Wang", "Zaiyuan Wang", "Yang Yang", "Lingyue Yin", "Mingren Yin", "Zhenwei Zhu", "Tianle Cai", "Zehui Chen", "Jiecao Chen", "Yantao Du", "Xiang Gao", "Jiacheng Guo", "Liang Hu", "Jianpeng Jiao", "Xiangsheng Li", "Jingkai Liu", "Shuang Ni", "Zhoufutu Wen", "Ge Zhang", "Kaiyuan Zhang", "Xin Zhou", "Jose Blanchet", "Xipeng Qiu", "Mengdi Wang", "Wenhao Huang"], "title": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction", "comment": "Technical report, 51 pages", "summary": "Future prediction is a complex task for LLM agents, requiring a high level of\nanalytical thinking, information gathering, contextual understanding, and\ndecision-making under uncertainty. Agents must not only gather and interpret\nvast amounts of dynamic information but also integrate diverse data sources,\nweigh uncertainties, and adapt predictions based on emerging trends, just as\nhuman experts do in fields like politics, economics, and finance. Despite its\nimportance, no large-scale benchmark exists for evaluating agents on future\nprediction, largely due to challenges in handling real-time updates and\nretrieving timely, accurate answers. To address this, we introduce\n$\\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically\ndesigned for LLM agents performing future prediction tasks. FutureX is the\nlargest and most diverse live benchmark for future prediction, supporting\nreal-time daily updates and eliminating data contamination through an automated\npipeline for question gathering and answer collection. We evaluate 25 LLM/agent\nmodels, including those with reasoning, search capabilities, and integration of\nexternal tools such as the open-source Deep Research Agent and closed-source\nDeep Research models. This comprehensive evaluation assesses agents' adaptive\nreasoning and performance in dynamic environments. Additionally, we provide\nin-depth analyses of agents' failure modes and performance pitfalls in\nfuture-oriented tasks, including the vulnerability to fake web pages and the\ntemporal validity. Our goal is to establish a dynamic, contamination-free\nevaluation standard that drives the development of LLM agents capable of\nperforming at the level of professional human analysts in complex reasoning and\npredictive thinking.", "AI": {"tldr": "FutureX\u57fa\u51c6\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u7684\u672a\u6765\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u5728\u8be5\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9LLM\u4ee3\u7406\u672a\u6765\u9884\u6d4b\u80fd\u529b\u7684\u5927\u89c4\u6a21\u57fa\u51c6\uff0cFutureX\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aFutureX\u7684\u52a8\u6001\u5b9e\u65f6\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u7684\u672a\u6765\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u5bf925\u4e2aLLM/agent\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u4e8625\u4e2aLLM/agent\u6a21\u578b\uff0c\u5206\u6790\u4e86\u4ee3\u7406\u7684\u5931\u8d25\u6a21\u5f0f\u548c\u6027\u80fd\u7f3a\u9677\uff0c\u4f8b\u5982\u5bf9\u865a\u5047\u7f51\u9875\u7684\u8106\u5f31\u6027\u548c\u65f6\u95f4\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aFutureX\u7684\u52a8\u6001\u5b9e\u65f6\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u5728\u672a\u6765\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u5bf925\u4e2a\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5206\u6790\u4e86\u5176\u4e0d\u8db3\u4e4b\u5904\u3002"}}
{"id": "2508.11991", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11991", "abs": "https://arxiv.org/abs/2508.11991", "authors": ["Weihao Sun"], "title": "Modeling Relational Logic Circuits for And-Inverter Graph Convolutional Network", "comment": null, "summary": "The automation of logic circuit design enhances chip performance, energy\nefficiency, and reliability, and is widely applied in the field of Electronic\nDesign Automation (EDA).And-Inverter Graphs (AIGs) efficiently represent,\noptimize, and verify the functional characteristics of digital circuits,\nenhancing the efficiency of EDA development.Due to the complex structure and\nlarge scale of nodes in real-world AIGs, accurate modeling is challenging,\nleading to existing work lacking the ability to jointly model functional and\nstructural characteristics, as well as insufficient dynamic information\npropagation capability.To address the aforementioned challenges, we propose\nAIGer.Specifically, AIGer consists of two components: 1) Node logic feature\ninitialization embedding component and 2) AIGs feature learning network\ncomponent.The node logic feature initialization embedding component projects\nlogic nodes, such as AND and NOT, into independent semantic spaces, to enable\neffective node embedding for subsequent processing.Building upon this, the AIGs\nfeature learning network component employs a heterogeneous graph convolutional\nnetwork, designing dynamic relationship weight matrices and differentiated\ninformation aggregation approaches to better represent the original structure\nand information of AIGs.The combination of these two components enhances\nAIGer's ability to jointly model functional and structural characteristics and\nimproves its message passing capability. Experimental results indicate that\nAIGer outperforms the current best models in the Signal Probability Prediction\n(SSP) task, improving MAE and MSE by 18.95\\% and 44.44\\%, respectively. In the\nTruth Table Distance Prediction (TTDP) task, AIGer achieves improvements of\n33.57\\% and 14.79\\% in MAE and MSE, respectively, compared to the\nbest-performing models.", "AI": {"tldr": "AIGer, a new model for AIG analysis, significantly improves accuracy in predicting signal probability and truth table distance.", "motivation": "Existing methods lack the ability to jointly model functional and structural characteristics of AIGs and have insufficient dynamic information propagation capability.", "method": "AIGer employs a heterogeneous graph convolutional network with dynamic relationship weight matrices and differentiated information aggregation approaches to model functional and structural characteristics of AIGs.", "result": "AIGer improves MAE and MSE by 18.95% and 44.44% in SSP, and by 33.57% and 14.79% in TTDP, respectively, compared to state-of-the-art models.", "conclusion": "AIGer, a novel model consisting of a node logic feature initialization embedding component and an AIGs feature learning network component, significantly outperforms existing models in Signal Probability Prediction (SSP) and Truth Table Distance Prediction (TTDP) tasks."}}
{"id": "2508.11995", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.11995", "abs": "https://arxiv.org/abs/2508.11995", "authors": ["Xuyang Zhao", "Shiwan Zhao", "Hualong Yu", "Liting Zhang", "Qicheng Li"], "title": "AgentCDM: Enhancing Multi-Agent Collaborative Decision-Making via ACH-Inspired Structured Reasoning", "comment": null, "summary": "Multi-agent systems (MAS) powered by large language models (LLMs) hold\nsignificant promise for solving complex decision-making tasks. However, the\ncore process of collaborative decision-making (CDM) within these systems\nremains underexplored. Existing approaches often rely on either ``dictatorial\"\nstrategies that are vulnerable to the cognitive biases of a single agent, or\n``voting-based\" methods that fail to fully harness collective intelligence. To\naddress these limitations, we propose \\textbf{AgentCDM}, a structured framework\nfor enhancing collaborative decision-making in LLM-based multi-agent systems.\nDrawing inspiration from the Analysis of Competing Hypotheses (ACH) in\ncognitive science, AgentCDM introduces a structured reasoning paradigm that\nsystematically mitigates cognitive biases and shifts decision-making from\npassive answer selection to active hypothesis evaluation and construction. To\ninternalize this reasoning process, we develop a two-stage training paradigm:\nthe first stage uses explicit ACH-inspired scaffolding to guide the model\nthrough structured reasoning, while the second stage progressively removes this\nscaffolding to encourage autonomous generalization. Experiments on multiple\nbenchmark datasets demonstrate that AgentCDM achieves state-of-the-art\nperformance and exhibits strong generalization, validating its effectiveness in\nimproving the quality and robustness of collaborative decisions in MAS.", "AI": {"tldr": "AgentCDM\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u6709\u6548\u63d0\u5347\u4e86\u57fa\u4e8eLLM\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u534f\u4f5c\u51b3\u7b56\u80fd\u529b\uff0c\u53d6\u5f97\u4e86SOTA\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u534f\u4f5c\u51b3\u7b56\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u5bb9\u6613\u53d7\u5355\u4e00\u4ee3\u7406\u8ba4\u77e5\u504f\u5dee\u5f71\u54cd\u7684\u201c\u72ec\u88c1\u201d\u7b56\u7565\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u65e0\u6cd5\u5145\u5206\u5229\u7528\u96c6\u4f53\u667a\u6167\u7684\u201c\u6295\u7968\u201d\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAgentCDM\u7684\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u501f\u9274\u4e86\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7ade\u4e89\u6027\u5047\u8bbe\u5206\u6790\uff08ACH\uff09\u7684\u601d\u60f3\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff08\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u663e\u5f0f\u652f\u67b6\u6307\u5bfc\u6a21\u578b\u8fdb\u884c\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u7b2c\u4e8c\u9636\u6bb5\u9010\u6b65\u53bb\u9664\u652f\u67b6\u4ee5\u9f13\u52b1\u81ea\u4e3b\u6cdb\u5316\uff09\u5c06\u51b3\u7b56\u8fc7\u7a0b\u4ece\u88ab\u52a8\u7b54\u6848\u9009\u62e9\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u5047\u8bbe\u8bc4\u4f30\u548c\u6784\u5efa\u3002", "result": "AgentCDM\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u6539\u8fdbMAS\u534f\u4f5c\u51b3\u7b56\u8d28\u91cf\u548c\u7a33\u5065\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "AgentCDM\u6846\u67b6\u5728\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u63d0\u9ad8\u4e86\u534f\u4f5c\u51b3\u7b56\u7684\u8d28\u91cf\u548c\u7a33\u5065\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.12022", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12022", "abs": "https://arxiv.org/abs/2508.12022", "authors": ["Dorsa Macky Aleagha", "Payam Zohari", "Mostafa Haghir Chehreghani"], "title": "AI Models for Depressive Disorder Detection and Diagnosis: A Review", "comment": null, "summary": "Major Depressive Disorder is one of the leading causes of disability\nworldwide, yet its diagnosis still depends largely on subjective clinical\nassessments. Integrating Artificial Intelligence (AI) holds promise for\ndeveloping objective, scalable, and timely diagnostic tools. In this paper, we\npresent a comprehensive survey of state-of-the-art AI methods for depression\ndetection and diagnosis, based on a systematic review of 55 key studies. We\nintroduce a novel hierarchical taxonomy that structures the field by primary\nclinical task (diagnosis vs. prediction), data modality (text, speech,\nneuroimaging, multimodal), and computational model class (e.g., graph neural\nnetworks, large language models, hybrid approaches). Our in-depth analysis\nreveals three major trends: the predominance of graph neural networks for\nmodeling brain connectivity, the rise of large language models for linguistic\nand conversational data, and an emerging focus on multimodal fusion,\nexplainability, and algorithmic fairness. Alongside methodological insights, we\nprovide an overview of prominent public datasets and standard evaluation\nmetrics as a practical guide for researchers. By synthesizing current advances\nand highlighting open challenges, this survey offers a comprehensive roadmap\nfor future innovation in computational psychiatry.", "AI": {"tldr": "\u4eba\u5de5\u667a\u80fd\u5728\u6291\u90c1\u75c7\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u7efc\u8ff0\uff0c\u6307\u51fa\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u878d\u5408\u7684\u8d8b\u52bf", "motivation": "\u6291\u90c1\u75c7\u8bca\u65ad\u4e3b\u8981\u4f9d\u8d56\u4e3b\u89c2\u4e34\u5e8a\u8bc4\u4f30\uff0c\u4eba\u5de5\u667a\u80fd\u7684\u6574\u5408\u6709\u671b\u5f00\u53d1\u5ba2\u89c2\u3001\u53ef\u6269\u5c55\u548c\u53ca\u65f6\u7684\u8bca\u65ad\u5de5\u5177\u3002", "method": "\u7cfb\u7edf\u7efc\u8ff0\u4e8655\u4e2a\u5173\u952e\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5c42\u5206\u7c7b\u6cd5\uff0c\u5e76\u5bf9\u5e38\u89c1\u516c\u5171\u6570\u636e\u96c6\u548c\u6807\u51c6\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u4e86\u6982\u8ff0\u3002", "result": "\u63ed\u793a\u4e86\u8be5\u9886\u57df\u7684\u4e09\u5927\u8d8b\u52bf\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u5bf9\u4eba\u5de5\u667a\u80fd\u5728\u6291\u90c1\u75c7\u68c0\u6d4b\u548c\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u7efc\u8ff0\uff0c\u603b\u7ed3\u4e86\u8be5\u9886\u57df\u7684\u4e09\u5927\u8d8b\u52bf\uff1a\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u8111\u8fde\u63a5\u5efa\u6a21\u4e2d\u7684\u4e3b\u5bfc\u5730\u4f4d\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u548c\u4f1a\u8bdd\u6570\u636e\u4e2d\u7684\u5174\u8d77\u4ee5\u53ca\u5bf9\u591a\u6a21\u6001\u878d\u5408\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7b97\u6cd5\u516c\u5e73\u6027\u7684\u5173\u6ce8\u3002"}}
{"id": "2508.12026", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12026", "abs": "https://arxiv.org/abs/2508.12026", "authors": ["Szymon Pawlonka", "Miko\u0142aj Ma\u0142ki\u0144ski", "Jacek Ma\u0144dziuk"], "title": "Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems", "comment": null, "summary": "Bongard Problems (BPs) provide a challenging testbed for abstract visual\nreasoning (AVR), requiring models to identify visual concepts fromjust a few\nexamples and describe them in natural language. Early BP benchmarks featured\nsynthetic black-and-white drawings, which might not fully capture the\ncomplexity of real-world scenes. Subsequent BP datasets employed real-world\nimages, albeit the represented concepts are identifiable from high-level image\nfeatures, reducing the task complexity. Differently, the recently released\nBongard-RWR dataset aimed at representing abstract concepts formulated in the\noriginal BPs using fine-grained real-world images. Its manual construction,\nhowever, limited the dataset size to just $60$ instances, constraining\nevaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset\ncomposed of $5\\,400$ instances that represent original BP abstract concepts\nusing real-world-like images generated via a vision language model (VLM)\npipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually\ncurated images and generate new descriptions aligned with the underlying\nconcepts, use Flux.1-dev to synthesize images from these descriptions, and\nmanually verify that the generated images faithfully reflect the intended\nconcepts. We evaluate state-of-the-art VLMs across diverse BP formulations,\nincluding binary and multiclass classification, as well as textual answer\ngeneration. Our findings reveal that while VLMs can recognize coarse-grained\nvisual concepts, they consistently struggle with discerning fine-grained\nconcepts, highlighting limitations in their reasoning capabilities.", "AI": {"tldr": "Bongard-RWR+\u6570\u636e\u96c6\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e865400\u4e2a\u5305\u542b\u7ec6\u7c92\u5ea6\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u7684Bongard\u95ee\u9898\u5b9e\u4f8b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u4ecd\u6709\u5f85\u63d0\u9ad8", "motivation": "\u73b0\u6709\u7684Bongard\u95ee\u9898\u6570\u636e\u96c6\u5b58\u5728\u89c4\u6a21\u5c0f\u6216\u6982\u5ff5\u8fc7\u4e8e\u7b80\u5355\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u5145\u5206\u6d4b\u8bd5\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u6a21\u578b\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528Pixtral-12B\u63cf\u8ff0\u624b\u52a8\u7b56\u5212\u7684\u56fe\u50cf\u5e76\u751f\u6210\u4e0e\u57fa\u7840\u6982\u5ff5\u4e00\u81f4\u7684\u65b0\u63cf\u8ff0\uff0c\u4f7f\u7528Flux.1-dev\u6839\u636e\u8fd9\u4e9b\u63cf\u8ff0\u5408\u6210\u56fe\u50cf\uff0c\u5e76\u624b\u52a8\u9a8c\u8bc1\u751f\u6210\u7684\u56fe\u50cf\u662f\u5426\u5fe0\u5b9e\u5730\u53cd\u6620\u4e86\u9884\u671f\u7684\u6982\u5ff5\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684VLMs\u5728\u8bc6\u522b\u7ec6\u7c92\u5ea6\u89c6\u89c9\u6982\u5ff5\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u8fd9\u7a81\u51fa\u4e86\u5b83\u4eec\u5728\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4ecb\u7ecd\u4e86Bongard-RWR+\uff0c\u4e00\u4e2a\u5305\u542b5400\u4e2a\u5b9e\u4f8b\u7684BP\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u7c7b\u4f3c\u771f\u5b9e\u4e16\u754c\u7684\u56fe\u50cf\u6765\u8868\u793a\u539f\u59cbBP\u62bd\u8c61\u6982\u5ff5\u3002\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136VLMs\u80fd\u591f\u8bc6\u522b\u7c97\u7c92\u5ea6\u7684\u89c6\u89c9\u6982\u5ff5\uff0c\u4f46\u5b83\u4eec\u5728\u8bc6\u522b\u7ec6\u7c92\u5ea6\u6982\u5ff5\u65b9\u9762\u59cb\u7ec8\u5b58\u5728\u56f0\u96be\uff0c\u7a81\u51fa\u4e86\u5176\u63a8\u7406\u80fd\u529b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.12027", "categories": ["cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.12027", "abs": "https://arxiv.org/abs/2508.12027", "authors": ["Filippo Torresan", "Keisuke Suzuki", "Ryota Kanai", "Manuel Baltieri"], "title": "Active inference for action-unaware agents", "comment": "59 pages, 47 figures", "summary": "Active inference is a formal approach to study cognition based on the notion\nthat adaptive agents can be seen as engaging in a process of approximate\nBayesian inference, via the minimisation of variational and expected free\nenergies. Minimising the former provides an account of perceptual processes and\nlearning as evidence accumulation, while minimising the latter describes how\nagents select their actions over time. In this way, adaptive agents are able to\nmaximise the likelihood of preferred observations or states, given a generative\nmodel of the environment. In the literature, however, different strategies have\nbeen proposed to describe how agents can plan their future actions. While they\nall share the notion that some kind of expected free energy offers an\nappropriate way to score policies, sequences of actions, in terms of their\ndesirability, there are different ways to consider the contribution of past\nmotor experience to the agent's future behaviour. In some approaches, agents\nare assumed to know their own actions, and use such knowledge to better plan\nfor the future. In other approaches, agents are unaware of their actions, and\nmust infer their motor behaviour from recent observations in order to plan for\nthe future. This difference reflects a standard point of departure in two\nleading frameworks in motor control based on the presence, or not, of an\nefference copy signal representing knowledge about an agent's own actions. In\nthis work we compare the performances of action-aware and action-unaware agents\nin two navigations tasks, showing how action-unaware agents can achieve\nperformances comparable to action-aware ones while at a severe disadvantage.", "AI": {"tldr": "\u5728\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\u4e0b\uff0c\u5373\u4f7f\u4e0d\u77e5\u9053\u81ea\u8eab\u884c\u4e3a\uff0c\u667a\u80fd\u4f53\u4e5f\u80fd\u5728\u5bfc\u822a\u4efb\u52a1\u4e2d\u53d6\u5f97\u826f\u597d\u6027\u80fd\u3002", "motivation": "\u63a2\u7a76\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\u4e0b\uff0c\u667a\u80fd\u4f53\u662f\u5426\u9700\u8981\u77e5\u9053\u81ea\u8eab\u884c\u4e3a\u624d\u80fd\u6709\u6548\u89c4\u5212\u672a\u6765\u884c\u52a8\u3002", "method": "\u901a\u8fc7\u5728\u4e24\u4e2a\u5bfc\u822a\u4efb\u52a1\u4e2d\u6bd4\u8f83\u77e5\u9053\u81ea\u8eab\u884c\u4e3a\u548c\u4e0d\u77e5\u9053\u81ea\u8eab\u884c\u4e3a\u7684\u667a\u80fd\u4f53\uff0c\u8bc4\u4f30\u4e24\u79cd\u7b56\u7565\u7684\u6027\u80fd\u3002", "result": "\u4e0d\u77e5\u9053\u81ea\u8eab\u884c\u4e3a\u7684\u667a\u80fd\u4f53\u5728\u6027\u80fd\u4e0a\u53ef\u4ee5\u4e0e\u77e5\u9053\u81ea\u8eab\u884c\u4e3a\u7684\u667a\u80fd\u4f53\u76f8\u5ab2\u7f8e\u3002", "conclusion": "\u6bd4\u8f83\u4e86\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\u4e0b\u4e24\u79cd\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0c\u77e5\u9053\u81ea\u8eab\u884c\u4e3a\u548c\u4e0d\u77e5\u9053\u81ea\u8eab\u884c\u4e3a\u7684\u667a\u80fd\u4f53\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u77e5\u9053\u81ea\u8eab\u884c\u4e3a\u7684\u667a\u80fd\u4f53\u5373\u4f7f\u5904\u4e8e\u52a3\u52bf\uff0c\u4e5f\u80fd\u53d6\u5f97\u4e0e\u77e5\u9053\u81ea\u8eab\u884c\u4e3a\u7684\u667a\u80fd\u4f53\u76f8\u5f53\u7684\u6027\u80fd\u3002"}}
