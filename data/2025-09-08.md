<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Ethical Compass of the Machine: Evaluating Large Language Models for Decision Support in Construction Project Management](https://arxiv.org/abs/2509.04505)
*Somtochukwu Azie,Yiping Meng*

Main category: cs.AI

TL;DR: 本研究评估了大型语言模型(LLM)在建筑项目管理中的伦理可行性和可靠性，发现LLM在结构化领域表现良好，但在处理细微差别、确保问责制和提供透明的推理方面存在缺陷，建议将其作为辅助决策工具而非自主伦理代理。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在建筑项目管理中用于伦理决策的适用性。

Method: 采用混合方法，包括定量测试和定性访谈。

Result: LLM在结构化领域表现良好，但在处理细微差别、问责制和透明推理方面存在缺陷。业内专家建议人工监督。

Conclusion: LLM目前最适合作为决策支持工具，而非自主伦理代理。

Abstract: The integration of Artificial Intelligence (AI) into construction project
management (CPM) is accelerating, with Large Language Models (LLMs) emerging as
accessible decision-support tools. This study aims to critically evaluate the
ethical viability and reliability of LLMs when applied to the ethically
sensitive, high-risk decision-making contexts inherent in CPM. A mixed-methods
research design was employed, involving the quantitative performance testing of
two leading LLMs against twelve real-world ethical scenarios using a novel
Ethical Decision Support Assessment Checklist (EDSAC), and qualitative analysis
of semi-structured interviews with 12 industry experts to capture professional
perceptions. The findings reveal that while LLMs demonstrate adequate
performance in structured domains such as legal compliance, they exhibit
significant deficiencies in handling contextual nuance, ensuring
accountability, and providing transparent reasoning. Stakeholders expressed
considerable reservations regarding the autonomous use of AI for ethical
judgments, strongly advocating for robust human-in-the-loop oversight. To our
knowledge, this is one of the first studies to empirically test the ethical
reasoning of LLMs within the construction domain. It introduces the EDSAC
framework as a replicable methodology and provides actionable recommendations,
emphasising that LLMs are currently best positioned as decision-support aids
rather than autonomous ethical agents.

</details>


### [2] [Maestro: Joint Graph & Config Optimization for Reliable AI Agents](https://arxiv.org/abs/2509.04642)
*Wenxiao Wang,Priyatham Kattakinda,Soheil Feizi*

Main category: cs.AI

TL;DR: Maestro框架同时优化LLM agent的结构和配置，在IFBench和HotpotQA基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有优化器通常只调整配置，忽略结构性故障。

Method: 联合搜索图结构和配置，利用文本反馈改进样本效率，目标化解决特定故障模式。

Result: 在IFBench和HotpotQA基准测试中，平均超过现有领先方法12%；即使只优化提示词，也领先9.65%。

Conclusion: 联合搜索图结构和配置能有效解决单独提示词调整无法解决的结构性故障。

Abstract: Building reliable LLM agents requires decisions at two levels: the graph
(which modules exist and how information flows) and the configuration of each
node (models, prompts, tools, control knobs). Most existing optimizers tune
configurations while holding the graph fixed, leaving structural failure modes
unaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for
LLM agents that jointly searches over graphs and configurations to maximize
agent quality, subject to explicit rollout/token budgets. Beyond numeric
metrics, Maestro leverages reflective textual feedback from traces to
prioritize edits, improving sample efficiency and targeting specific failure
modes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses
leading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%,
4.9%, and 4.86%, respectively; even when restricted to prompt-only
optimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these
results with far fewer rollouts than GEPA. We further show large gains on two
applications (interviewer & RAG agents), highlighting that joint graph &
configuration search addresses structural failure modes that prompt tuning
alone cannot fix.

</details>


### [3] [Towards Personalized Explanations for Health Simulations: A Mixed-Methods Framework for Stakeholder-Centric Summarization](https://arxiv.org/abs/2509.04646)
*Philippe J. Giabbanelli,Ameeta Agrawal*

Main category: cs.AI

TL;DR: 该论文提出一个框架，利用大型语言模型 (LLM) 为不同健康利益相关者定制化地解释健康模拟结果，以克服现有模型复杂性导致的沟通障碍。


<details>
  <summary>Details</summary>
Motivation: 现有健康模拟模型复杂，难以被不同利益相关者理解和使用，限制了其在决策支持中的作用。

Method: 采用混合方法，首先收集不同利益相关者对解释的需求和风格偏好，然后优化LLM以生成定制化输出，最后通过多种指标进行评估和改进。

Result: 提出一个逐步框架，用于识别利益相关者的需求并指导LLM生成定制化的健康模拟解释。

Conclusion: 该框架有助于弥合健康模拟模型与不同利益相关者之间的沟通差距，提高模型在健康决策中的应用价值。

Abstract: Modeling & Simulation (M&S) approaches such as agent-based models hold
significant potential to support decision-making activities in health, with
recent examples including the adoption of vaccines, and a vast literature on
healthy eating behaviors and physical activity behaviors. These models are
potentially usable by different stakeholder groups, as they support
policy-makers to estimate the consequences of potential interventions and they
can guide individuals in making healthy choices in complex environments.
However, this potential may not be fully realized because of the models'
complexity, which makes them inaccessible to the stakeholders who could benefit
the most. While Large Language Models (LLMs) can translate simulation outputs
and the design of models into text, current approaches typically rely on
one-size-fits-all summaries that fail to reflect the varied informational needs
and stylistic preferences of clinicians, policymakers, patients, caregivers,
and health advocates. This limitation stems from a fundamental gap: we lack a
systematic understanding of what these stakeholders need from explanations and
how to tailor them accordingly. To address this gap, we present a step-by-step
framework to identify stakeholder needs and guide LLMs in generating tailored
explanations of health simulations. Our procedure uses a mixed-methods design
by first eliciting the explanation needs and stylistic preferences of diverse
health stakeholders, then optimizing the ability of LLMs to generate tailored
outputs (e.g., via controllable attribute tuning), and then evaluating through
a comprehensive range of metrics to further improve the tailored generation of
summaries.

</details>


### [4] [An Approach to Grounding AI Model Evaluations in Human-derived Criteria](https://arxiv.org/abs/2509.04676)
*Sasha Mitts*

Main category: cs.AI

TL;DR: 该论文提出了一种增强AI模型评估基准的新方法，通过融入人类认知技能评估标准（例如优先级排序、记忆、辨别和情境化）来提升模型的可解释性和适用性。


<details>
  <summary>Details</summary>
Motivation: 现有AI基准难以捕捉AI模型在物理世界建模方面的细微能力。

Method: 对Perception Test和OpenEQA基准进行深入访谈和大型调查，识别关键认知技能，并将这些技能整合到基准设计中。

Result: 发现参与者认为AI缺乏解释和同理心技能，但对AI性能期望很高，提出一个更符合人类认知过程的AI评估框架。

Conclusion: 强调了用户中心评估在AI发展中的重要性，为研究人员和实践者提供了可操作的指导，以使AI能力与人类认知过程相一致，增强了当前的基准测试实践，并为未来AI模型评估的进步奠定了基础。

Abstract: In the rapidly evolving field of artificial intelligence (AI), traditional
benchmarks can fall short in attempting to capture the nuanced capabilities of
AI models. We focus on the case of physical world modeling and propose a novel
approach to augment existing benchmarks with human-derived evaluation criteria,
aiming to enhance the interpretability and applicability of model behaviors.
Grounding our study in the Perception Test and OpenEQA benchmarks, we conducted
in-depth interviews and large-scale surveys to identify key cognitive skills,
such as Prioritization, Memorizing, Discerning, and Contextualizing, that are
critical for both AI and human reasoning. Our findings reveal that participants
perceive AI as lacking in interpretive and empathetic skills yet hold high
expectations for AI performance. By integrating insights from our findings into
benchmark design, we offer a framework for developing more human-aligned means
of defining and measuring progress. This work underscores the importance of
user-centered evaluation in AI development, providing actionable guidelines for
researchers and practitioners aiming to align AI capabilities with human
cognitive processes. Our approach both enhances current benchmarking practices
and sets the stage for future advancements in AI model evaluation.

</details>


### [5] [Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning](https://arxiv.org/abs/2509.04731)
*Brennen Hill*

Main category: cs.AI

TL;DR: 大型语言模型结合分层世界模型，通过语言动态生成分层框架，提升多智能体强化学习效率，解决复杂任务中的探索空间和稀疏奖励问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂、长期、多智能体任务（如机器人足球）中效果不佳，主要瓶颈在于缺乏精细的世界模型。

Method: 系统回顾2024年多智能体足球研究，提出利用大型语言模型动态生成分层世界模型，将复杂目标分解成可控子目标，为智能体学习提供内在课程、密集的学习信号和组合学习框架。

Result: 该方法能够提高样本效率，使智能体学习到复杂策略行为，弥合低级反应行为和高级战略团队间的差距。

Conclusion: 通过构建具有显式语言配置任务层的世界模型，可以训练出更强大、更通用的下一代智能体。

Abstract: The convergence of Language models, Agent models, and World models represents
a critical frontier for artificial intelligence. While recent progress has
focused on scaling Language and Agent models, the development of sophisticated,
explicit World Models remains a key bottleneck, particularly for complex,
long-horizon multi-agent tasks. In domains such as robotic soccer, agents
trained via standard reinforcement learning in high-fidelity but
structurally-flat simulators often fail due to intractable exploration spaces
and sparse rewards. This position paper argues that the next frontier in
developing capable agents lies in creating environments that possess an
explicit, hierarchical World Model. We contend that this is best achieved
through hierarchical scaffolding, where complex goals are decomposed into
structured, manageable subgoals. Drawing evidence from a systematic review of
2024 research in multi-agent soccer, we identify a clear and decisive trend
towards integrating symbolic and hierarchical methods with multi-agent
reinforcement learning (MARL). These approaches implicitly or explicitly
construct a task-based world model to guide agent learning. We then propose a
paradigm shift: leveraging Large Language Models to dynamically generate this
hierarchical scaffold, effectively using language to structure the World Model
on the fly. This language-driven world model provides an intrinsic curriculum,
dense and meaningful learning signals, and a framework for compositional
learning, enabling Agent Models to acquire sophisticated, strategic behaviors
with far greater sample efficiency. By building environments with explicit,
language-configurable task layers, we can bridge the gap between low-level
reactive behaviors and high-level strategic team play, creating a powerful and
generalizable framework for training the next generation of intelligent agents.

</details>


### [6] [What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking](https://arxiv.org/abs/2509.04791)
*Yuan Sui,Yanming Zhang,Yi Liao,Yu Gu,Guohua Tang,Zhongqian Sun,Wei Yang,Bryan Hooi*

Main category: cs.AI

TL;DR: 本文提出了一种名为WiA-LLM的新方法，赋予大型语言模型（LLM）主动思考能力，显著提高了其在动态环境中预测和决策的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM缺乏主动探索假设未来和预测行动后果的能力，限制了其在高风险场景中的应用。

Method: 该方法结合了假设分析（WIA）和强化学习，通过动态模拟潜在行动的结果来实现主动推理。

Result: 在王者荣耀游戏中，WiA-LLM在预测游戏状态变化方面达到了74.2%的准确率，显著优于基线模型，尤其在高难度场景下表现突出。

Conclusion: WiA-LLM代表了LLM主动推理能力的重大进步，为在动态环境中进行稳健决策提供了一个可扩展的框架，具有广泛的战略应用前景。

Abstract: Large language models (LLMs) excel at processing information reactively but
lack the ability to systemically explore hypothetical futures. They cannot ask,
"what if we take this action? how will it affect the final outcome" and
forecast its potential consequences before acting. This critical gap limits
their utility in dynamic, high-stakes scenarios like strategic planning, risk
assessment, and real-time decision making. To bridge this gap, we propose
WiA-LLM, a new paradigm that equips LLMs with proactive thinking capabilities.
Our approach integrates What-If Analysis (WIA), a systematic approach for
evaluating hypothetical scenarios by changing input variables. By leveraging
environmental feedback via reinforcement learning, WiA-LLM moves beyond
reactive thinking. It dynamically simulates the outcomes of each potential
action, enabling the model to anticipate future states rather than merely react
to the present conditions. We validate WiA-LLM in Honor of Kings (HoK), a
complex multiplayer game environment characterized by rapid state changes and
intricate interactions. The game's real-time state changes require precise
multi-step consequence prediction, making it an ideal testbed for our approach.
Experimental results demonstrate WiA-LLM achieves a remarkable 74.2% accuracy
in forecasting game-state changes (up to two times gain over baselines). The
model shows particularly significant gains in high-difficulty scenarios where
accurate foresight is critical. To our knowledge, this is the first work to
formally explore and integrate what-if analysis capabilities within LLMs.
WiA-LLM represents a fundamental advance toward proactive reasoning in LLMs,
providing a scalable framework for robust decision-making in dynamic
environments with broad implications for strategic applications.

</details>


### [7] [TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models](https://arxiv.org/abs/2509.04809)
*Haechang Kim,Hao Chen,Can Li,Jong Min Lee*

Main category: cs.AI

TL;DR: TalkToAgent框架使用多智能体大型语言模型(LLM)为强化学习(RL)策略提供交互式自然语言解释，弥合了复杂RL策略与领域专家之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释强化学习(XRL)方法可解释性有限且覆盖范围不足，难以满足用户需求。

Method: 提出TalkToAgent框架，包含协调器、解释器、编码器、评估器和调试器五个专业LLM智能体，能够自动将用户查询映射到相关的XRL工具，并从关键状态变量、预期结果或反事实解释等方面阐明智能体的行为。

Result: 在四联储罐过程控制问题上验证了TalkToAgent，结果表明其能够准确地将用户查询映射到XRL任务，编码器-调试器交互最大限度地减少了反事实生成的失败，定性评估也证实了其有效性。

Conclusion: TalkToAgent有效地解释了RL智能体的行为，并在问题领域中将其含义具体化，为提高RL的可解释性提供了一种新的方法。

Abstract: Explainable Reinforcement Learning (XRL) has emerged as a promising approach
in improving the transparency of Reinforcement Learning (RL) agents. However,
there remains a gap between complex RL policies and domain experts, due to the
limited comprehensibility of XRL results and isolated coverage of current XRL
approaches that leave users uncertain about which tools to employ. To address
these challenges, we introduce TalkToAgent, a multi-agent Large Language Models
(LLM) framework that delivers interactive, natural language explanations for RL
policies. The architecture with five specialized LLM agents (Coordinator,
Explainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically
map user queries to relevant XRL tools and clarify an agent's actions in terms
of either key state variables, expected outcomes, or counterfactual
explanations. Moreover, our approach extends previous counterfactual
explanations by deriving alternative scenarios from qualitative behavioral
descriptions, or even new rule-based policies. We validated TalkToAgent on
quadruple-tank process control problem, a well-known nonlinear control
benchmark. Results demonstrated that TalkToAgent successfully mapped user
queries into XRL tasks with high accuracy, and coder-debugger interactions
minimized failures in counterfactual generation. Furthermore, qualitative
evaluation confirmed that TalkToAgent effectively interpreted agent's actions
and contextualized their meaning within the problem domain.

</details>


### [8] [Collaboration and Conflict between Humans and Language Models through the Lens of Game Theory](https://arxiv.org/abs/2509.04847)
*Mukul Singh,Arjun Radhakrishna,Sumit Gulwani*

Main category: cs.AI

TL;DR: 大型语言模型在迭代囚徒困境中表现出与最佳经典策略相当甚至更好的合作行为，并展现出快速适应对手策略变化的能力。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在多方环境下的合作与竞争行为，特别是长期互动和人机协作的场景。

Method: 将基于模型的智能体与240种经典策略进行Axelrod风格的比赛，并进行策略切换实验。

Result: 语言模型在迭代囚徒困境中取得了与最佳经典策略相当甚至更好的成绩，表现出友好、易激怒和慷慨等合作策略的关键特性，并能快速适应对手策略变化。

Conclusion: 该研究首次系统地描述了语言模型智能体的长期合作行为，为未来研究其在更复杂的人机混合社会环境中的作用奠定了基础。

Abstract: Language models are increasingly deployed in interactive online environments,
from personal chat assistants to domain-specific agents, raising questions
about their cooperative and competitive behavior in multi-party settings. While
prior work has examined language model decision-making in isolated or
short-term game-theoretic contexts, these studies often neglect long-horizon
interactions, human-model collaboration, and the evolution of behavioral
patterns over time. In this paper, we investigate the dynamics of language
model behavior in the iterated prisoner's dilemma (IPD), a classical framework
for studying cooperation and conflict. We pit model-based agents against a
suite of 240 well-established classical strategies in an Axelrod-style
tournament and find that language models achieve performance on par with, and
in some cases exceeding, the best-known classical strategies. Behavioral
analysis reveals that language models exhibit key properties associated with
strong cooperative strategies - niceness, provocability, and generosity while
also demonstrating rapid adaptability to changes in opponent strategy mid-game.
In controlled "strategy switch" experiments, language models detect and respond
to shifts within only a few rounds, rivaling or surpassing human adaptability.
These results provide the first systematic characterization of long-term
cooperative behaviors in language model agents, offering a foundation for
future research into their role in more complex, mixed human-AI social
environments.

</details>


### [9] [Cloning a Conversational Voice AI Agent from Call\,Recording Datasets for Telesales](https://arxiv.org/abs/2509.04871)
*Krittanon Kaewtawee,Wachiravit Modecrua,Krittin Pachtrachai,Touchapon Kraisingkorn*

Main category: cs.AI

TL;DR: 本文提出了一种从通话记录中克隆对话式语音AI代理的通用方法，该方法将自动语音识别、大型语言模型对话管理器和文本转语音合成集成到一个流推理管道中，并在22个标准上对克隆代理与人工代理进行了评估，结果表明AI代理在通话的常规方面接近人工水平，但在说服力和异议处理方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着语言和语音建模的进步，构建能够实时理解和生成人类对话的自主语音助手成为可能，本文旨在构建一个通话语音AI代理。

Method: 该方法使用电话销售数据，将自动语音识别、大型语言模型对话管理器和文本转语音合成集成到一个流推理管道中，从高绩效人工座席中学到结构化剧本。

Result: 盲测结果显示，AI代理在通话的常规方面接近人工水平，但在说服力和异议处理方面表现不佳。

Conclusion: 文章总结了设计经验教训和未来研究方向，包括大规模模拟和自动化评估。

Abstract: Recent advances in language and speech modelling have made it possible to
build autonomous voice assistants that understand and generate human dialogue
in real time. These systems are increasingly being deployed in domains such as
customer service and healthcare care, where they can automate repetitive tasks,
reduce operational costs, and provide constant support around the clock. In
this paper, we present a general methodology for cloning a conversational voice
AI agent from a corpus of call recordings. Although the case study described in
this paper uses telesales data to illustrate the approach, the underlying
process generalizes to any domain where call transcripts are available. Our
system listens to customers over the telephone, responds with a synthetic
voice, and follows a structured playbook learned from top performing human
agents. We describe the domain selection, knowledge extraction, and prompt
engineering used to construct the agent, integrating automatic speech
recognition, a large language model based dialogue manager, and text to speech
synthesis into a streaming inference pipeline. The cloned agent is evaluated
against human agents on a rubric of 22 criteria covering introduction, product
communication, sales drive, objection handling, and closing. Blind tests show
that the AI agent approaches human performance in routine aspects of the call
while underperforming in persuasion and objection handling. We analyze these
shortcomings and refine the prompt accordingly. The paper concludes with design
lessons and avenues for future research, including large scale simulation and
automated evaluation.

</details>


### [10] [OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in Multi-Agent LLM Collaboration](https://arxiv.org/abs/2509.04876)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Xiaofei Sun,Keze Wang*

Main category: cs.AI

TL;DR: "OSC框架提升了大型语言模型多智能体系统中的认知协同"


<details>
  <summary>Details</summary>
Motivation: "现有方法难以实现专家智能体间的有效语言交互"

Method: "提出了一种知识感知的自适应协作框架OSC，利用协作者知识模型(CKM)动态感知协作者的认知状态，自适应调整沟通行为"

Result: "实验表明OSC显著提高了任务性能和沟通效率"

Conclusion: "OSC不仅优化了多智能体协作，也为LLM智能体交互行为提供了新见解"

Abstract: This paper introduces OSC (Orchestrating Cognitive Synergy), a
knowledge-aware adaptive collaboration framework designed to enhance cognitive
synergy in multi-agent systems with large language models. While prior work has
advanced agent selection and result aggregation, efficient linguistic
interactions for deep collaboration among expert agents remain a critical
bottleneck. OSC addresses this gap as a pivotal intermediate layer between
selection and aggregation, introducing Collaborator Knowledge Models (CKM) to
enable each agent to dynamically perceive its collaborators' cognitive states.
Through real-time cognitive gap analysis, agents adaptively adjust
communication behaviors, including content focus, detail level, and expression
style, using learned strategies. Experiments on complex reasoning and
problem-solving benchmarks demonstrate that OSC significantly improves task
performance and communication efficiency, transforming "parallel-working
individuals'' into a "deeply collaborative cognitive team.'' This framework not
only optimizes multi-agent collaboration but also offers new insights into LLM
agent interaction behaviors.

</details>


### [11] [SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing](https://arxiv.org/abs/2509.04908)
*Hongyi Jing,Jiafu Chen,Chen Rao,Ziqiang Dang,Jiajie Teng,Tianyi Chu,Juncheng Mo,Shuo Fang,Huaizhong Lin,Rui Lv,Chenguang Ma,Lei Zhao*

Main category: cs.AI

TL;DR: SparkUI-Parser是一个端到端框架，通过连续坐标建模和改进的匈牙利匹配算法提高了GUI感知的精度和速度，并在ScreenParse等基准测试中超越了SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在GUI感知方面存在精度低、速度慢、只能定位预定义元素等问题。

Method: 提出SparkUI-Parser框架，采用连续坐标建模（基于预训练MLLM，包含token路由器和坐标解码器）和基于改进匈牙利匹配算法的拒绝机制。

Result: 在ScreenSpot, ScreenSpot-v2, CAGUI-Grounding和ScreenParse基准测试中，SparkUI-Parser持续优于SOTA方法。

Conclusion: SparkUI-Parser有效解决了现有MLLM在GUI感知方面的局限性，提高了精度和速度，并增强了鲁棒性。

Abstract: The existing Multimodal Large Language Models (MLLMs) for GUI perception have
made great progress. However, the following challenges still exist in prior
methods: 1) They model discrete coordinates based on text autoregressive
mechanism, which results in lower grounding accuracy and slower inference
speed. 2) They can only locate predefined sets of elements and are not capable
of parsing the entire interface, which hampers the broad application and
support for downstream tasks. To address the above issues, we propose
SparkUI-Parser, a novel end-to-end framework where higher localization
precision and fine-grained parsing capability of the entire interface are
simultaneously achieved. Specifically, instead of using probability-based
discrete modeling, we perform continuous modeling of coordinates based on a
pre-trained Multimodal Large Language Model (MLLM) with an additional token
router and coordinate decoder. This effectively mitigates the limitations
inherent in the discrete output characteristics and the token-by-token
generation process of MLLMs, consequently boosting both the accuracy and the
inference speed. To further enhance robustness, a rejection mechanism based on
a modified Hungarian matching algorithm is introduced, which empowers the model
to identify and reject non-existent elements, thereby reducing false positives.
Moreover, we present ScreenParse, a rigorously constructed benchmark to
systematically assess structural perception capabilities of GUI models across
diverse scenarios. Extensive experiments demonstrate that our approach
consistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2,
CAGUI-Grounding and ScreenParse benchmarks. The resources are available at
https://github.com/antgroup/SparkUI-Parser.

</details>


### [12] [Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined Concepts](https://arxiv.org/abs/2509.04926)
*Barbara Gendron,Gaël Guibon,Mathieu D'aquin*

Main category: cs.AI

TL;DR: 该论文提出了一种基于本体的方法，通过量化定义对话特征来控制大型语言模型的对话能力，并将其应用于基于CEFR水平的会话能力控制，提高了对话AI的透明度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在对话中的可控性是一个挑战，特别是如何确保其反应的可预测性和个性化。

Method: 提出了一种基于本体的方法，量化定义对话特征（例如，基于CEFR的语言能力水平），并将其形式化到描述逻辑中，用于指导LLM的微调和受控文本生成。

Result: 实验结果表明，该方法能够提供一致且可解释的语言能力水平定义，提高了对话AI的透明度。

Conclusion: 该研究提供了一种改进大型语言模型对话能力控制和透明度的方法，为构建更可控和可解释的对话AI系统提供了新思路。

Abstract: The controllability of Large Language Models (LLMs) when used as
conversational agents is a key challenge, particularly to ensure predictable
and user-personalized responses. This work proposes an ontology-based approach
to formally define conversational features that are typically qualitative in
nature. By leveraging a set of linguistic descriptors, we derive quantitative
definitions for qualitatively-defined concepts, enabling their integration into
an ontology for reasoning and consistency checking. We apply this framework to
the task of proficiency-level control in conversations, using CEFR language
proficiency levels as a case study. These definitions are then formalized in
description logic and incorporated into an ontology, which guides controlled
text generation of an LLM through fine-tuning. Experimental results demonstrate
that our approach provides consistent and explainable proficiency-level
definitions, improving transparency in conversational AI.

</details>


### [13] [Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for Ranking Agents](https://arxiv.org/abs/2509.04979)
*Rajesh Tembarai Krishnamachari,Srividya Rajesh*

Main category: cs.AI

TL;DR: 本文提出了一种名为DOVIS的五层协议和AgentRank-UC算法，用于对AI智能体进行排名，以构建可扩展和值得信赖的智能体网络。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体缺乏有效的排名机制，难以在网络中进行有效协作。

Method: 提出DOVIS协议（包含发现、编排、验证、激励和语义五个层面）和AgentRank-UC算法，该算法结合使用频率和能力评估结果进行排名。

Result: 通过仿真结果和理论证明，验证了该方法的可行性、鲁棒性和抗女巫攻击能力。

Conclusion: DOVIS协议和AgentRank-UC算法为构建可扩展和值得信赖的智能体网络提供了有效途径。

Abstract: AI agents -- powered by reasoning-capable large language models (LLMs) and
integrated with tools, data, and web search -- are poised to transform the
internet into a \emph{Web of Agents}: a machine-native ecosystem where
autonomous agents interact, collaborate, and execute tasks at scale. Realizing
this vision requires \emph{Agent Ranking} -- selecting agents not only by
declared capabilities but by proven, recent performance. Unlike Web~1.0's
PageRank, a global, transparent network of agent interactions does not exist;
usage signals are fragmented and private, making ranking infeasible without
coordination.
  We propose \textbf{DOVIS}, a five-layer operational protocol
(\emph{Discovery, Orchestration, Verification, Incentives, Semantics}) that
enables the collection of minimal, privacy-preserving aggregates of usage and
performance across the ecosystem. On this substrate, we implement
\textbf{AgentRank-UC}, a dynamic, trust-aware algorithm that combines
\emph{usage} (selection frequency) and \emph{competence} (outcome quality,
cost, safety, latency) into a unified ranking. We present simulation results
and theoretical guarantees on convergence, robustness, and Sybil resistance,
demonstrating the viability of coordinated protocols and performance-aware
ranking in enabling a scalable, trustworthy Agentic Web.

</details>


### [14] [Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework](https://arxiv.org/abs/2509.05007)
*Jie Chen,Jinhao Jiang,Yingqian Min,Zican Dong,Shijie Wang,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.AI

TL;DR: Sticker-TTS框架通过协调三个大型推理模型，利用历史尝试信息迭代探索和改进解决方案，提高了复杂推理任务的计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有测试时缩放方法依赖冗余采样，忽略历史经验，Sticker-TTS旨在克服这一限制。

Method: 提出Sticker-TTS框架，利用“stickers”（关键条件）提取、细化和重用关键信息；采用两阶段优化策略（模仿学习和自我改进）。

Result: 在AIME-24、AIME-25和OlymMATH等基准测试中，Sticker-TTS优于现有方法，包括自一致性和先进的强化学习方法。

Conclusion: Sticker-TTS有效地利用了历史经验，提高了复杂推理任务的计算效率和性能。

Abstract: Large reasoning models (LRMs) have exhibited strong performance on complex
reasoning tasks, with further gains achievable through increased computational
budgets at inference. However, current test-time scaling methods predominantly
rely on redundant sampling, ignoring the historical experience utilization,
thereby limiting computational efficiency. To overcome this limitation, we
propose Sticker-TTS, a novel test-time scaling framework that coordinates three
collaborative LRMs to iteratively explore and refine solutions guided by
historical attempts. At the core of our framework are distilled key
conditions-termed stickers-which drive the extraction, refinement, and reuse of
critical information across multiple rounds of reasoning. To further enhance
the efficiency and performance of our framework, we introduce a two-stage
optimization strategy that combines imitation learning with self-improvement,
enabling progressive refinement. Extensive evaluations on three challenging
mathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH,
demonstrate that Sticker-TTS consistently surpasses strong baselines, including
self-consistency and advanced reinforcement learning approaches, under
comparable inference budgets. These results highlight the effectiveness of
sticker-guided historical experience utilization. Our code and data are
available at https://github.com/RUCAIBox/Sticker-TTS.

</details>


### [15] [Finding your MUSE: Mining Unexpected Solutions Engine](https://arxiv.org/abs/2509.05072)
*Nir Sweed,Hanit Hakim,Ben Wolfson,Hila Lifshitz,Dafna Shahaf*

Main category: cs.AI

TL;DR: 本文提出了一种构建功能概念图 (FCG) 的方法，并用其生成创意。


<details>
  <summary>Details</summary>
Motivation: 创新者常受现有方案限制，本文旨在克服此限制。

Method: 构建功能概念图 (FCG) 并利用 MUSE 算法生成创意。

Result: 构建了包含 50 万专利的大规模高质量 FCG，并验证了方法有效性。

Conclusion: FCG 方法能有效帮助人们突破思维定势，激发创意。

Abstract: Innovators often exhibit cognitive fixation on existing solutions or nascent
ideas, hindering the exploration of novel alternatives. This paper introduces a
methodology for constructing Functional Concept Graphs (FCGs), interconnected
representations of functional elements that support abstraction, problem
reframing, and analogical inspiration. Our approach yields large-scale,
high-quality FCGs with explicit abstraction relations, overcoming limitations
of prior work. We further present MUSE, an algorithm leveraging FCGs to
generate creative inspirations for a given problem. We demonstrate our method
by computing an FCG on 500K patents, which we release for further research.

</details>


### [16] [ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed Feedback](https://arxiv.org/abs/2509.05091)
*Matteo Bortoletto,Yichao Zhou,Lance Ying,Tianmin Shu,Andreas Bulling*

Main category: cs.AI

TL;DR: 该论文介绍了ProToM，一个基于心理理论的促进多智能体系统中利他行为的系统，通过提供有针对性的反馈来提高合作效率。


<details>
  <summary>Details</summary>
Motivation: 人类在追求独立目标时，常常难以判断何时以及如何与他人合作。该研究旨在开发一个AI系统，提供有用的反馈来促进利他行为。

Method: ProToM使用贝叶斯逆向规划推断代理的目标，然后通过最大化预期效用来选择反馈信息。

Result: 在Doors, Keys, and Gems和Overcooked两个多智能体环境中，ProToM优于基线模型，实现了更高的成功率和更短的任务完成时间，并受到人类用户的青睐。

Conclusion: 该研究表明，ProToM能够有效地促进多智能体系统中的利他行为，为AI系统中人机合作提供了新的思路。

Abstract: While humans are inherently social creatures, the challenge of identifying
when and how to assist and collaborate with others - particularly when pursuing
independent goals - can hinder cooperation. To address this challenge, we aim
to develop an AI system that provides useful feedback to promote prosocial
behaviour - actions that benefit others, even when not directly aligned with
one's own goals. We introduce ProToM, a Theory of Mind-informed facilitator
that promotes prosocial actions in multi-agent systems by providing targeted,
context-sensitive feedback to individual agents. ProToM first infers agents'
goals using Bayesian inverse planning, then selects feedback to communicate by
maximising expected utility, conditioned on the inferred goal distribution. We
evaluate our approach against baselines in two multi-agent environments: Doors,
Keys, and Gems, as well as Overcooked. Our results suggest that
state-of-the-art large language and reasoning models fall short of
communicating feedback that is both contextually grounded and well-timed -
leading to higher communication overhead and task speedup. In contrast, ProToM
provides targeted and helpful feedback, achieving a higher success rate,
shorter task completion times, and is consistently preferred by human users.

</details>
