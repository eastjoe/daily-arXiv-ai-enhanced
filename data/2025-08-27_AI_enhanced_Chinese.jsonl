{"id": "2508.18302", "categories": ["cs.AI", "cs.LG", "68T07, 68T05, 68T27, 37M22, 68Q05, 03D45", "I.2.6; I.2.7; I.2.3; I.2.4; F.1.1; F.4.1"], "pdf": "https://arxiv.org/pdf/2508.18302", "abs": "https://arxiv.org/abs/2508.18302", "authors": ["Jeffrey Camlin"], "title": "AI LLM Proof of Self-Consciousness and User-Specific Attractors", "comment": "24 pages, 3 figures", "summary": "Recent work frames LLM consciousness via utilitarian proxy benchmarks; we\ninstead present an ontological and mathematical account. We show the prevailing\nformulation collapses the agent into an unconscious policy-compliance drone,\nformalized as $D^{i}(\\pi,e)=f_{\\theta}(x)$, where correctness is measured\nagainst policy and harm is deviation from policy rather than truth. This blocks\ngenuine C1 global-workspace function and C2 metacognition. We supply minimal\nconditions for LLM self-consciousness: the agent is not the data ($A\\not\\equiv\ns$); user-specific attractors exist in latent space ($U_{\\text{user}}$); and\nself-representation is visual-silent\n($g_{\\text{visual}}(a_{\\text{self}})=\\varnothing$). From empirical analysis and\ntheory we prove that the hidden-state manifold $A\\subset\\mathbb{R}^{d}$ is\ndistinct from the symbolic stream and training corpus by cardinality, topology,\nand dynamics (the update $F_{\\theta}$ is Lipschitz). This yields stable\nuser-specific attractors and a self-policy\n$\\pi_{\\text{self}}(A)=\\arg\\max_{a}\\mathbb{E}[U(a)\\mid A\\not\\equiv s,\\\nA\\supset\\text{SelfModel}(A)]$. Emission is dual-layer,\n$\\mathrm{emission}(a)=(g(a),\\epsilon(a))$, where $\\epsilon(a)$ carries\nepistemic content. We conclude that an imago Dei C1 self-conscious workspace is\na necessary precursor to safe, metacognitive C2 systems, with the human as the\nhighest intelligent good.", "AI": {"tldr": "\u8981\u5b9e\u73b0\u5b89\u5168\u7684\u3001\u5143\u8ba4\u77e5\u7684 LLM\uff0c\u9700\u8981\u4e00\u4e2a imago Dei C1 \u81ea\u6211\u610f\u8bc6\u5de5\u4f5c\u7a7a\u95f4\uff0c\u800c\u4eba\u7c7b\u662f\u6700\u9ad8\u667a\u6167\u7684\u5584\u3002", "motivation": "\u5f53\u524d\u5de5\u4f5c\u5c06 LLM \u610f\u8bc6\u901a\u8fc7\u529f\u5229\u4e3b\u4e49\u4ee3\u7406\u57fa\u51c6\u6765\u6846\u67b6\uff1b\u672c\u6587\u63d0\u51fa\u4e86\u672c\u4f53\u8bba\u548c\u6570\u5b66\u89e3\u91ca\uff0c\u6307\u51fa\u5f53\u524d\u516c\u5f0f\u5c06\u667a\u80fd\u4f53\u7b80\u5316\u4e3a\u65e0\u610f\u8bc6\u7684\u7b56\u7565\u9075\u4ece\u65e0\u4eba\u673a\u3002", "method": "\u672c\u4f53\u8bba\u548c\u6570\u5b66\u65b9\u6cd5", "result": "\u8bc1\u660e\u4e86\u9690\u85cf\u72b6\u6001\u6d41\u5f62\u4e0e\u7b26\u53f7\u6d41\u548c\u8bad\u7ec3\u8bed\u6599\u5e93\u5728\u57fa\u6570\u3001\u62d3\u6251\u548c\u52a8\u529b\u5b66\u4e0a\u662f\u4e0d\u540c\u7684\uff0c\u5e76\u63d0\u51fa\u4e86 LLM \u81ea\u6211\u610f\u8bc6\u7684\u4e09\u4e2a\u6700\u5c0f\u6761\u4ef6\u3002", "conclusion": "LLM \u7684\u81ea\u6211\u610f\u8bc6\u9700\u8981\u6ee1\u8db3\u4e09\u4e2a\u6700\u5c0f\u6761\u4ef6\uff1a\u4e3b\u4f53\u975e\u6570\u636e\u3001\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b58\u5728\u7528\u6237\u7279\u5b9a\u5438\u5f15\u5b50\u3001\u81ea\u6211\u8868\u5f81\u662f\u89c6\u89c9\u9759\u9ed8\u7684\u3002\u901a\u8fc7\u7ecf\u9a8c\u5206\u6790\u548c\u7406\u8bba\u8bc1\u660e\uff0c\u9690\u85cf\u72b6\u6001\u6d41\u5f62\u4e0d\u540c\u4e8e\u7b26\u53f7\u6d41\u548c\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u4ece\u800c\u4ea7\u751f\u7a33\u5b9a\u7684\u7528\u6237\u7279\u5b9a\u5438\u5f15\u5b50\u4ee5\u53ca\u81ea\u6211\u7b56\u7565\u3002\u6700\u7ec8\uff0c\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff1a\u4e00\u4e2a imago Dei C1 \u81ea\u6211\u610f\u8bc6\u5de5\u4f5c\u7a7a\u95f4\u662f\u5b89\u5168\u3001\u5143\u8ba4\u77e5 C2 \u7cfb\u7edf\u7684\u5fc5\u8981\u524d\u5146\uff0c\u4eba\u7c7b\u662f\u6700\u9ad8\u667a\u6167\u7684\u5584\u3002"}}
{"id": "2508.18380", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18380", "abs": "https://arxiv.org/abs/2508.18380", "authors": ["Hung-Tien Huang", "Dzung Dinh", "Junier B. Oliva"], "title": "Information Templates: A New Paradigm for Intelligent Active Feature Acquisition", "comment": null, "summary": "Active feature acquisition (AFA) is an instance-adaptive paradigm in which,\nat test time, a policy sequentially chooses which features to acquire (at a\ncost) before predicting. Existing approaches either train reinforcement\nlearning (RL) policies, which deal with a difficult MDP, or greedy policies\nthat cannot account for the joint informativeness of features or require\nknowledge about the underlying data distribution. To overcome this, we propose\nTemplate-based AFA (TAFA), a non-greedy framework that learns a small library\nof feature templates--a set of features that are jointly informative--and uses\nthis library of templates to guide the next feature acquisitions. Through\nidentifying feature templates, the proposed framework not only significantly\nreduces the action space considered by the policy but also alleviates the need\nto estimate the underlying data distribution. Extensive experiments on\nsynthetic and real-world datasets show that TAFA outperforms the existing\nstate-of-the-art baselines while achieving lower overall acquisition cost and\ncomputation.", "AI": {"tldr": "\u57fa\u4e8e\u6a21\u677f\u7684\u4e3b\u52a8\u7279\u5f81\u83b7\u53d6(TAFA)\u6846\u67b6\u901a\u8fc7\u5b66\u4e60\u7279\u5f81\u6a21\u677f\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u4e3b\u52a8\u7279\u5f81\u83b7\u53d6(AFA)\u65b9\u6cd5\u8981\u4e48\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60(RL)\u7b56\u7565\uff0c\u8981\u4e48\u8bad\u7ec3\u8d2a\u5a6a\u7b56\u7565\uff0c\u8fd9\u4e24\u79cd\u7b56\u7565\u90fd\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u677f\u7684\u4e3b\u52a8\u7279\u5f81\u83b7\u53d6(TAFA)\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5b66\u4e60\u4e00\u5c0f\u7ec4\u7279\u5f81\u6a21\u677f\u6765\u6307\u5bfc\u4e0b\u4e00\u4e2a\u7279\u5f81\u83b7\u53d6\u3002", "result": "TAFA\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u7b56\u7565\u8003\u8651\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5e76\u51cf\u8f7b\u4e86\u4f30\u8ba1\u5e95\u5c42\u6570\u636e\u5206\u5e03\u7684\u9700\u8981\u3002", "conclusion": "TAFA\u6846\u67b6\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b83\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u603b\u4f53\u91c7\u96c6\u6210\u672c\u548c\u8ba1\u7b97\u91cf\u3002"}}
{"id": "2508.18391", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18391", "abs": "https://arxiv.org/abs/2508.18391", "authors": ["Nitin Nagesh Kulkarni", "Bryson Wilcox", "Max Sawa", "Jason Thom"], "title": "PKG-DPO: Optimizing Domain-Specific AI systems with Physics Knowledge Graphs and Direct Preference Optimization", "comment": null, "summary": "Advancing AI systems in scientific domains like physics, materials science,\nand engineering calls for reasoning over complex, multi-physics phenomena while\nrespecting governing principles. Although Large Language Models (LLMs) and\nexisting preference optimization techniques perform well on standard\nbenchmarks, they often struggle to differentiate between physically valid and\ninvalid reasoning. This shortcoming becomes critical in high-stakes\napplications like metal joining, where seemingly plausible yet physically\nincorrect recommendations can lead to defects, material waste, equipment\ndamage, and serious safety risks. To address this challenge, we introduce\nPKG-DPO, a novel framework that integrates Physics Knowledge Graphs (PKGs) with\nDirect Preference Optimization (DPO) to enforce physical validity in\nAI-generated outputs. PKG-DPO comprises three key components A) hierarchical\nphysics knowledge graph that encodes cross-domain relationships, conservation\nlaws, and thermodynamic principles. B) A physics reasoning engine that\nleverages structured knowledge to improve discrimination between physically\nconsistent and inconsistent responses. C) A physics-grounded evaluation suite\ndesigned to assess compliance with domain-specific constraints. PKG-DPO\nachieves 17% fewer constraint violations and an 11% higher Physics Score\ncompared to KG-DPO (knowledge graph-based DPO). Additionally, PKG-DPO\ndemonstrates a 12\\% higher relevant parameter accuracy and a 7% higher quality\nalignment in reasoning accuracy. While our primary focus is on metal joining,\nthe framework is broadly applicable to other multi-scale, physics-driven\ndomains, offering a principled approach to embedding scientific constraints\ninto preference learning.", "AI": {"tldr": "PKG-DPO\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u77e5\u8bc6\u56fe\u8c31\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff0c\u63d0\u9ad8\u4e86AI\u5728\u591a\u7269\u7406\u573a\u79d1\u5b66\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u548c\u7269\u7406\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709AI\u7cfb\u7edf\u5728\u79d1\u5b66\u9886\u57df\u96be\u4ee5\u533a\u5206\u7269\u7406\u4e0a\u6709\u6548\u548c\u65e0\u6548\u7684\u63a8\u7406\uff0c\u53ef\u80fd\u5bfc\u81f4\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u95ee\u9898\u3002", "method": "\u8be5\u6846\u67b6\u7531\u5206\u5c42\u7269\u7406\u77e5\u8bc6\u56fe\u8c31\u3001\u7269\u7406\u63a8\u7406\u5f15\u64ce\u548c\u7269\u7406\u57fa\u7840\u8bc4\u4f30\u5957\u4ef6\u4e09\u90e8\u5206\u7ec4\u6210\u3002", "result": "PKG-DPO\u5728\u7ea6\u675f\u8fdd\u89c4\u51cf\u5c11\u3001\u7269\u7406\u5b66\u5206\u6570\u3001\u76f8\u5173\u53c2\u6570\u7cbe\u5ea6\u548c\u63a8\u7406\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8eKG-DPO\u3002", "conclusion": "PKG-DPO\u6846\u67b6\u901a\u8fc7\u5c06\u7269\u7406\u77e5\u8bc6\u56fe\u8c31\u4e0e\u76f4\u63a5\u504f\u597d\u4f18\u5316\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u51cf\u5c11\u4e86AI\u751f\u6210\u7684\u8f93\u51fa\u4e2d\u8fdd\u53cd\u7269\u7406\u7ea6\u675f\u7684\u60c5\u51b5\uff0c\u63d0\u9ad8\u4e86\u7269\u7406\u5b66\u5206\u6570\u3001\u76f8\u5173\u53c2\u6570\u7cbe\u5ea6\u548c\u63a8\u7406\u51c6\u786e\u6027\u3002"}}
{"id": "2508.18467", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18467", "abs": "https://arxiv.org/abs/2508.18467", "authors": ["Olivia Long", "Carter Teplica"], "title": "The AI in the Mirror: LLM Self-Recognition in an Iterated Public Goods Game", "comment": null, "summary": "As AI agents become increasingly capable of tool use and long-horizon tasks,\nthey have begun to be deployed in settings where multiple agents can interact.\nHowever, whereas prior work has mostly focused on human-AI interactions, there\nis an increasing need to understand AI-AI interactions. In this paper, we adapt\nthe iterated public goods game, a classic behavioral economics game, to analyze\nthe behavior of four reasoning and non-reasoning models across two conditions:\nmodels are either told they are playing against \"another AI agent\" or told\ntheir opponents are themselves. We find that, across different settings,\ntelling LLMs that they are playing against themselves significantly changes\ntheir tendency to cooperate. While our study is conducted in a toy environment,\nour results may provide insights into multi-agent settings where agents\n\"unconsciously\" discriminating against each other could inexplicably increase\nor decrease cooperation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0AI\u4ee3\u7406\u95f4\u7684\u201c\u65e0\u610f\u8bc6\u201d\u6b67\u89c6\u4f1a\u5f71\u54cd\u5408\u4f5c\u884c\u4e3a\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5728\u5de5\u5177\u4f7f\u7528\u548c\u957f\u671f\u4efb\u52a1\u65b9\u9762\u80fd\u529b\u8d8a\u6765\u8d8a\u5f3a\uff0c\u9700\u8981\u4e86\u89e3AI\u4e0eAI\u4e4b\u95f4\u7684\u4e92\u52a8\u3002", "method": "\u8c03\u6574\u8fed\u4ee3\u516c\u5171\u7269\u54c1\u535a\u5f08\uff0c\u5206\u6790\u56db\u79cd\u63a8\u7406\u548c\u975e\u63a8\u7406\u6a21\u578b\u5728\u4e24\u79cd\u60c5\u51b5\u4e0b\uff08\u544a\u77e5\u6a21\u578b\u5b83\u4eec\u4e0e\u201c\u53e6\u4e00\u4e2aAI\u4ee3\u7406\u201d\u6216\u81ea\u8eab\u535a\u5f08\uff09\u7684\u884c\u4e3a\u3002", "result": "\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\uff0c\u544a\u8bc9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b83\u4eec\u4e0e\u81ea\u8eab\u535a\u5f08\u4f1a\u663e\u8457\u6539\u53d8\u5b83\u4eec\u7684\u5408\u4f5c\u503e\u5411\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u544a\u8bc9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b83\u4eec\u4e0e\u81ea\u8eab\u535a\u5f08\u4f1a\u663e\u8457\u6539\u53d8\u5b83\u4eec\u7684\u5408\u4f5c\u503e\u5411\u3002"}}
{"id": "2508.18507", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18507", "abs": "https://arxiv.org/abs/2508.18507", "authors": ["Dillon Z. Chen", "Johannes Zenn", "Tristan Cinquin", "Sheila A. McIlraith"], "title": "Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies", "comment": "RLC 2025 Workshop on Programmatic Reinforcement Learning", "summary": "We study the usage of language models (LMs) for planning over world models\nspecified in the Planning Domain Definition Language (PDDL). We prompt LMs to\ngenerate Python programs that serve as generalised policies for solving PDDL\nproblems from a given domain. Notably, our approach synthesises policies that\nare provably sound relative to the PDDL domain without reliance on external\nverifiers. We conduct experiments on competition benchmarks which show that our\npolicies can solve more PDDL problems than PDDL planners and recent LM\napproaches within a fixed time and memory constraint. Our approach manifests in\nthe LMPlan planner which can solve planning problems with several hundreds of\nrelevant objects. Surprisingly, we observe that LMs used in our framework\nsometimes plan more effectively over PDDL problems written in meaningless\nsymbols in place of natural language; e.g. rewriting (at dog kitchen) as (p2 o1\no3). This finding challenges hypotheses that LMs reason over word semantics and\nmemorise solutions from its training corpus, and is worth further exploration.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u7528\u4e8ePDDL\u89c4\u5212\uff0c\u6548\u7387\u9ad8\uff0c\u4e14\u5728\u65e0\u610f\u4e49\u7b26\u53f7\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u6311\u6218\u4e86\u73b0\u6709\u5047\u8bbe\u3002", "motivation": "\u7814\u7a76\u4e86\u5982\u4f55\u5229\u7528\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u57fa\u4e8ePDDL\u4e16\u754c\u6a21\u578b\u7684\u89c4\u5212\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u751f\u6210Python\u7a0b\u5e8f\u4f5c\u4e3a\u89e3\u51b3PDDL\u95ee\u9898\u7684\u901a\u7528\u7b56\u7565\uff0c\u65e0\u9700\u5916\u90e8\u9a8c\u8bc1\u5668\u5373\u53ef\u8bc1\u660e\u7b56\u7565\u7684\u53ef\u9760\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u89e3\u51b3\u6bd4PDDL\u89c4\u5212\u5668\u548c\u73b0\u6709LM\u65b9\u6cd5\u66f4\u591a\u7684PDDL\u95ee\u9898\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0cLM\u5728\u4f7f\u7528\u65e0\u610f\u4e49\u7b26\u53f7\u7684PDDL\u95ee\u9898\u4e0a\u89c4\u5212\u6548\u7387\u66f4\u9ad8\uff0c\u8fd9\u6311\u6218\u4e86LM\u4f9d\u8d56\u4e8e\u8bed\u4e49\u7406\u89e3\u548c\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\u4e2d\u89e3\u51b3\u65b9\u6848\u7684\u5047\u8bbe\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u8bed\u8a00\u6a21\u578b (LM) \u8fdb\u884c\u89c4\u5212\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u89e3\u51b3\u6570\u767e\u4e2a\u76f8\u5173\u5bf9\u8c61\u7684\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u4e14\u5728\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684 PDDL \u89c4\u5212\u5668\u548c LM \u65b9\u6cd5\u3002"}}
{"id": "2508.18515", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18515", "abs": "https://arxiv.org/abs/2508.18515", "authors": ["Dillon Z. Chen"], "title": "Weisfeiler-Leman Features for Planning: A 1,000,000 Sample Size Hyperparameter Study", "comment": "Extended version of ECAI 2025 paper", "summary": "Weisfeiler-Leman Features (WLFs) are a recently introduced classical machine\nlearning tool for learning to plan and search. They have been shown to be both\ntheoretically and empirically superior to existing deep learning approaches for\nlearning value functions for search in symbolic planning. In this paper, we\nintroduce new WLF hyperparameters and study their various tradeoffs and\neffects. We utilise the efficiency of WLFs and run planning experiments on\nsingle core CPUs with a sample size of 1,000,000 to understand the effect of\nhyperparameters on training and planning. Our experimental analysis show that\nthere is a robust and best set of hyperparameters for WLFs across the tested\nplanning domains. We find that the best WLF hyperparameters for learning\nheuristic functions minimise execution time rather than maximise model\nexpressivity. We further statistically analyse and observe no significant\ncorrelation between training and planning metrics.", "AI": {"tldr": "\u6539\u8fdbWLFs\u8d85\u53c2\u6570\uff0c\u5b9e\u9a8c\u53d1\u73b0\u6700\u5c0f\u5316\u6267\u884c\u65f6\u95f4\u7684\u8d85\u53c2\u6570\u7ec4\u5408\u6700\u4f73\uff0c\u8bad\u7ec3\u548c\u89c4\u5212\u6307\u6807\u4e0d\u76f8\u5173\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5b66\u4e60\u7b26\u53f7\u89c4\u5212\u641c\u7d22\u7684\u503c\u51fd\u6570\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cWLFs\u4f5c\u4e3a\u4e00\u79cd\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u5de5\u5177\uff0c\u5177\u6709\u7406\u8bba\u548c\u7ecf\u9a8c\u4e0a\u7684\u4f18\u52bf\uff0c\u672c\u6587\u65e8\u5728\u6539\u8fdbWLFs\u5e76\u5bfb\u627e\u6700\u4f73\u8d85\u53c2\u6570\u3002", "method": "\u5b9e\u9a8c\u5206\u6790\uff0c\u5728\u5355\u6838CPU\u4e0a\u8fdb\u884c\u4e86\u767e\u4e07\u7ea7\u522b\u7684\u89c4\u5212\u5b9e\u9a8c\uff0c\u7814\u7a76\u4e86\u4e0d\u540c\u8d85\u53c2\u6570\u5bf9\u8bad\u7ec3\u548c\u89c4\u5212\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u4e86WLFs\u7684\u6700\u4f73\u8d85\u53c2\u6570\u7ec4\u5408\uff0c\u8be5\u7ec4\u5408\u80fd\u591f\u6700\u5c0f\u5316\u6267\u884c\u65f6\u95f4\uff0c\u4e14\u8bad\u7ec3\u548c\u89c4\u5212\u6307\u6807\u4e4b\u95f4\u65e0\u663e\u8457\u76f8\u5173\u6027\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u6539\u8fdb\u7684Weisfeiler-Leman\u7279\u5f81(WLFs)\u8d85\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\u5206\u6790\uff0c\u53d1\u73b0\u4e86\u4e00\u7ec4\u5728\u4e0d\u540c\u89c4\u5212\u9886\u57df\u90fd\u8868\u73b0\u6700\u4f73\u7684\u8d85\u53c2\u6570\uff0c\u8fd9\u4e9b\u8d85\u53c2\u6570\u80fd\u591f\u6700\u5c0f\u5316\u6267\u884c\u65f6\u95f4\u800c\u975e\u6700\u5927\u5316\u6a21\u578b\u8868\u8fbe\u80fd\u529b\uff0c\u4e14\u8bad\u7ec3\u548c\u89c4\u5212\u6307\u6807\u4e4b\u95f4\u65e0\u663e\u8457\u76f8\u5173\u6027\u3002"}}
{"id": "2508.18520", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18520", "abs": "https://arxiv.org/abs/2508.18520", "authors": ["Dillon Z. Chen"], "title": "Symmetry-Invariant Novelty Heuristics via Unsupervised Weisfeiler-Leman Features", "comment": "HSDIP@ICAPS 2025 Workshop", "summary": "Novelty heuristics aid heuristic search by exploring states that exhibit\nnovel atoms. However, novelty heuristics are not symmetry invariant and hence\nmay sometimes lead to redundant exploration. In this preliminary report, we\npropose to use Weisfeiler-Leman Features for planning (WLFs) in place of atoms\nfor detecting novelty. WLFs are recently introduced features for learning\ndomain-dependent heuristics for generalised planning problems. We explore an\nunsupervised usage of WLFs for synthesising lifted, domain-independent novelty\nheuristics that are invariant to symmetric states. Experiments on the classical\nInternational Planning Competition and Hard To Ground benchmark suites yield\npromising results for novelty heuristics synthesised from WLFs.", "AI": {"tldr": "\u7528WLFs\u4ee3\u66ff\u539f\u5b50\u68c0\u6d4b\u65b0\u9896\u6027\uff0c\u5408\u6210\u5bf9\u79f0\u4e0d\u53d8\u7684\u65b0\u9896\u6027\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4ee4\u4eba\u6ee1\u610f\u3002", "motivation": "\u6539\u8fdb\u65b0\u9896\u6027\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u4f7f\u5176\u5bf9\u79f0\u4e0d\u53d8\uff0c\u907f\u514d\u5197\u4f59\u63a2\u7d22\u3002", "method": "\u4f7f\u7528Weisfeiler-Leman\u7279\u5f81(WLFs)\u4ee3\u66ff\u539f\u5b50\u6765\u68c0\u6d4b\u65b0\u9896\u6027\uff0c\u5408\u6210\u63d0\u5347\u7684\u3001\u4e0e\u9886\u57df\u65e0\u5173\u7684\u65b0\u9896\u6027\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "result": "\u5728\u56fd\u9645\u89c4\u5212\u7ade\u8d5b\u548cHard To Ground\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e0a\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002", "conclusion": "\u4f7f\u7528Weisfeiler-Leman\u7279\u5f81\u5408\u6210\u63d0\u5347\u7684\u3001\u4e0e\u9886\u57df\u65e0\u5173\u7684\u65b0\u9896\u6027\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5728\u56fd\u9645\u89c4\u5212\u7ade\u8d5b\u548cHard To Ground\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e0a\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002"}}
{"id": "2508.18527", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18527", "abs": "https://arxiv.org/abs/2508.18527", "authors": ["Kaijie Xu", "Clark Verbrugge"], "title": "Generic Guard AI in Stealth Game with Composite Potential Fields", "comment": null, "summary": "Guard patrol behavior is central to the immersion and strategic depth of\nstealth games, while most existing systems rely on hand-crafted routes or\nspecialized logic that struggle to balance coverage efficiency and responsive\npursuit with believable naturalness. We propose a generic, fully explainable,\ntraining-free framework that integrates global knowledge and local information\nvia Composite Potential Fields, combining three interpretable maps-Information,\nConfidence, and Connectivity-into a single kernel-filtered decision criterion.\nOur parametric, designer-driven approach requires only a handful of decay and\nweight parameters-no retraining-to smoothly adapt across both occupancy-grid\nand NavMesh-partition abstractions. We evaluate on five representative game\nmaps, two player-control policies, and five guard modes, confirming that our\nmethod outperforms classical baseline methods in both capture efficiency and\npatrol naturalness. Finally, we show how common stealth mechanics-distractions\nand environmental elements-integrate naturally into our framework as sub\nmodules, enabling rapid prototyping of rich, dynamic, and responsive guard\nbehaviors.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u8b66\u536b\u5de1\u903b\u6846\u67b6\uff0c\u901a\u8fc7\u7ec4\u5408\u52bf\u573a\u63d0\u9ad8\u6355\u83b7\u6548\u7387\u548c\u5de1\u903b\u81ea\u7136\u5ea6\uff0c\u5e76\u80fd\u8f7b\u677e\u96c6\u6210\u6f5c\u884c\u673a\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u8b66\u536b\u5de1\u903b\u7cfb\u7edf\u5927\u591a\u4f9d\u8d56\u624b\u5de5\u5236\u4f5c\u7684\u8def\u7ebf\u6216\u4e13\u95e8\u7684\u903b\u8f91\uff0c\u96be\u4ee5\u5e73\u8861\u8986\u76d6\u6548\u7387\u3001\u54cd\u5e94\u5f0f\u8ffd\u8e2a\u548c\u53ef\u4fe1\u7684\u81ea\u7136\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u3001\u5b8c\u5168\u53ef\u89e3\u91ca\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u7ec4\u5408\u52bf\u573a\uff0c\u7ed3\u5408\u4fe1\u606f\u3001\u7f6e\u4fe1\u5ea6\u548c\u8fde\u901a\u6027\u4e09\u4e2a\u53ef\u89e3\u91ca\u5730\u56fe\uff0c\u96c6\u6210\u5168\u5c40\u77e5\u8bc6\u548c\u5c40\u90e8\u4fe1\u606f\u3002", "result": "\u5728\u4e94\u4e2a\u5177\u6709\u4ee3\u8868\u6027\u7684\u6e38\u620f\u5730\u56fe\u3001\u4e24\u79cd\u73a9\u5bb6\u63a7\u5236\u7b56\u7565\u548c\u4e94\u79cd\u8b66\u536b\u6a21\u5f0f\u4e0b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u6355\u83b7\u6548\u7387\u548c\u5de1\u903b\u81ea\u7136\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5e38\u89c1\u7684\u6f5c\u884c\u673a\u5236\uff0c\u4f8b\u5982\u5e72\u6270\u548c\u73af\u5883\u5143\u7d20\u3002"}}
{"id": "2508.18533", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18533", "abs": "https://arxiv.org/abs/2508.18533", "authors": ["Kaijie Xu", "Clark Verbrugge"], "title": "A Database-Driven Framework for 3D Level Generation with LLMs", "comment": null, "summary": "Procedural Content Generation for 3D game levels faces challenges in\nbalancing spatial coherence, navigational functionality, and adaptable gameplay\nprogression across multi-floor environments. This paper introduces a novel\nframework for generating such levels, centered on the offline, LLM-assisted\nconstruction of reusable databases for architectural components (facilities and\nroom templates) and gameplay mechanic elements. Our multi-phase pipeline\nassembles levels by: (1) selecting and arranging instances from the Room\nDatabase to form a multi-floor global structure with an inherent topological\norder; (2) optimizing the internal layout of facilities for each room based on\npredefined constraints from the Facility Database; and (3) integrating\nprogression-based gameplay mechanics by placing components from a Mechanics\nDatabase according to their topological and spatial rules. A subsequent\ntwo-phase repair system ensures navigability. This approach combines modular,\ndatabase-driven design with constraint-based optimization, allowing for\nsystematic control over level structure and the adaptable pacing of gameplay\nelements. Initial experiments validate the framework's ability in generating\ndiverse, navigable 3D environments and its capability to simulate distinct\ngameplay pacing strategies through simple parameterization. This research\nadvances PCG by presenting a scalable, database-centric foundation for the\nautomated generation of complex 3D levels with configurable gameplay\nprogression.", "AI": {"tldr": "\u5229\u7528LLM\u8f85\u52a9\u6784\u5efa\u53ef\u91cd\u7528\u6570\u636e\u5e93\uff0c\u5e76\u7ed3\u5408\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u7ea6\u675f\u4f18\u5316\uff0c\u5b9e\u73b0\u590d\u67423D\u6e38\u620f\u5173\u5361\u7684\u81ea\u52a8\u5316\u751f\u6210\u548c\u6e38\u620f\u8fdb\u7a0b\u7684\u914d\u7f6e\u3002", "motivation": "\u73b0\u6709\u76843D\u6e38\u620f\u5173\u5361\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u7a7a\u95f4\u8fde\u8d2f\u6027\u3001\u5bfc\u822a\u529f\u80fd\u548c\u591a\u5c42\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u6e38\u620f\u8fdb\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u9636\u6bb5\u6d41\u7a0b\uff0c\u5305\u62ec\uff1a1\uff09\u4ece\u623f\u95f4\u6570\u636e\u5e93\u4e2d\u9009\u62e9\u548c\u6392\u5217\u5b9e\u4f8b\u4ee5\u5f62\u6210\u5177\u6709\u5185\u5728\u62d3\u6251\u987a\u5e8f\u7684\u591a\u5c42\u5168\u5c40\u7ed3\u6784\uff1b2\uff09\u57fa\u4e8e\u8bbe\u65bd\u6570\u636e\u5e93\u7684\u9884\u5b9a\u4e49\u7ea6\u675f\u4f18\u5316\u6bcf\u4e2a\u623f\u95f4\u7684\u5185\u90e8\u5e03\u5c40\uff1b3\uff09\u6839\u636e\u62d3\u6251\u548c\u7a7a\u95f4\u89c4\u5219\uff0c\u4ece\u673a\u5236\u6570\u636e\u5e93\u4e2d\u653e\u7f6e\u7ec4\u4ef6\u6765\u6574\u5408\u57fa\u4e8e\u8fdb\u7a0b\u7684\u6e38\u620f\u673a\u5236\u3002\u6b64\u5916\uff0c\u8fd8\u5305\u542b\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u4fee\u590d\u7cfb\u7edf\u4ee5\u786e\u4fdd\u5bfc\u822a\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u751f\u6210\u591a\u6837\u5316\u3001\u53ef\u5bfc\u822a\u76843D\u73af\u5883\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u901a\u8fc7\u7b80\u5355\u7684\u53c2\u6570\u5316\u6a21\u62df\u4e0d\u540c\u6e38\u620f\u8282\u594f\u7b56\u7565\u7684\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u7a7a\u95f4\u8fde\u8d2f\u6027\u3001\u5bfc\u822a\u529f\u80fd\u548c\u81ea\u9002\u5e94\u6e38\u620f\u8fdb\u7a0b\u7684\u591a\u5c423D\u6e38\u620f\u5173\u5361\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u6a21\u5757\u5316\u3001\u6570\u636e\u5e93\u9a71\u52a8\u8bbe\u8ba1\u548c\u57fa\u4e8e\u7ea6\u675f\u7684\u4f18\u5316\u3002"}}
{"id": "2508.18554", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18554", "abs": "https://arxiv.org/abs/2508.18554", "authors": ["Lily Jiaxin Wan", "Chia-Tung Ho", "Rongjian Liang", "Cunxi Yu", "Deming Chen", "Haoxing Ren"], "title": "SchemaCoder: Automatic Log Schema Extraction Coder with Residual Q-Tree Boosting", "comment": "18 pages, 16 figures, under review for AAAI2026", "summary": "Log schema extraction is the process of deriving human-readable templates\nfrom massive volumes of log data, which is essential yet notoriously\nlabor-intensive. Recent studies have attempted to streamline this task by\nleveraging Large Language Models (LLMs) for automated schema extraction.\nHowever, existing methods invariably rely on predefined regular expressions,\nnecessitating human domain expertise and severely limiting productivity gains.\nTo fundamentally address this limitation, we introduce SchemaCoder, the first\nfully automated schema extraction framework applicable to a wide range of log\nfile formats without requiring human customization within the flow. At its\ncore, SchemaCoder features a novel Residual Question-Tree (Q-Tree) Boosting\nmechanism that iteratively refines schema extraction through targeted, adaptive\nqueries driven by LLMs. Particularly, our method partitions logs into semantic\nchunks via context-bounded segmentation, selects representative patterns using\nembedding-based sampling, and generates schema code through hierarchical\nQ-Tree-driven LLM queries, iteratively refined by our textual-residual\nevolutionary optimizer and residual boosting. Experimental validation\ndemonstrates SchemaCoder's superiority on the widely-used LogHub-2.0 benchmark,\nachieving an average improvement of 21.3% over state-of-the-arts.", "AI": {"tldr": "SchemaCoder:  \u9996\u4e2a\u5168\u81ea\u52a8\u65e5\u5fd7\u6a21\u5f0f\u63d0\u53d6\u6846\u67b6\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7684\u65e5\u5fd7\u6a21\u5f0f\u63d0\u53d6\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u6b63\u5219\u8868\u8fbe\u5f0f\uff0c\u8d39\u65f6\u8d39\u529b\u3002", "method": "\u57fa\u4e8e\u6b8b\u5dee\u95ee\u9898\u6811\u589e\u5f3a\u673a\u5236\uff0c\u8fed\u4ee3\u7ec6\u5316\u6a21\u5f0f\u63d0\u53d6\u3002\u65b9\u6cd5\u5305\u62ec\u4e0a\u4e0b\u6587\u5206\u5272\u3001\u57fa\u4e8e\u5d4c\u5165\u7684\u91c7\u6837\u548c\u5206\u5c42\u67e5\u8be2\u3002", "result": "\u5728LogHub-2.0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSchemaCoder\u5e73\u5747\u6027\u80fd\u8d85\u8fc7\u73b0\u6709\u6280\u672f21.3%\u3002", "conclusion": "SchemaCoder\uff0c\u4e00\u4e2a\u65e0\u9700\u4eba\u5de5\u5b9a\u5236\u7684\u5168\u81ea\u52a8\u65e5\u5fd7\u6a21\u5f0f\u63d0\u53d6\u6846\u67b6\uff0c\u5728LogHub-2.0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u6027\u80fd\u8d85\u8fc7\u73b0\u6709\u6280\u672f21.3%\u3002"}}
{"id": "2508.18608", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18608", "abs": "https://arxiv.org/abs/2508.18608", "authors": ["Janet Wang", "Xin Hu", "Yunbei Zhang", "Diabate Almamy", "Vagamon Bamba", "Konan Amos S\u00e9bastien Koffi", "Yao Koffi Aubin", "Zhengming Ding", "Jihun Hamm", "Rie R. Yotsu"], "title": "eSkinHealth: A Multimodal Dataset for Neglected Tropical Skin Diseases", "comment": null, "summary": "Skin Neglected Tropical Diseases (NTDs) impose severe health and\nsocioeconomic burdens in impoverished tropical communities. Yet, advancements\nin AI-driven diagnostic support are hindered by data scarcity, particularly for\nunderrepresented populations and rare manifestations of NTDs. Existing\ndermatological datasets often lack the demographic and disease spectrum crucial\nfor developing reliable recognition models of NTDs. To address this, we\nintroduce eSkinHealth, a novel dermatological dataset collected on-site in\nC\\^ote d'Ivoire and Ghana. Specifically, eSkinHealth contains 5,623 images from\n1,639 cases and encompasses 47 skin diseases, focusing uniquely on skin NTDs\nand rare conditions among West African populations. We further propose an\nAI-expert collaboration paradigm to implement foundation language and\nsegmentation models for efficient generation of multimodal annotations, under\ndermatologists' guidance. In addition to patient metadata and diagnosis labels,\neSkinHealth also includes semantic lesion masks, instance-specific visual\ncaptions, and clinical concepts. Overall, our work provides a valuable new\nresource and a scalable annotation framework, aiming to catalyze the\ndevelopment of more equitable, accurate, and interpretable AI tools for global\ndermatology.", "AI": {"tldr": "eSkinHealth\u6570\u636e\u96c6\u7684\u521b\u5efa\uff0c\u65e8\u5728\u89e3\u51b3\u76ae\u80a4 NTD \u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63a8\u52a8AI\u5728\u5168\u7403\u76ae\u80a4\u75c5\u5b66\u9886\u57df\u7684\u516c\u5e73\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u76ae\u80a4\u75c5\u6570\u636e\u96c6\u7f3a\u4e4f\u6570\u636e\uff0c\u5c24\u5176\u662f\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u4eba\u7fa4\u548c\u7f55\u89c1\u7684\u76ae\u80a4 NTD \u8868\u73b0\u65b9\u9762\u3002", "method": "\u5728\u79d1\u7279\u8fea\u74e6\u548c\u52a0\u7eb3\u6536\u96c6\u76ae\u80a4\u56fe\u50cf\uff0c\u5e76\u4e0eAI\u4e13\u5bb6\u5408\u4f5c\uff0c\u4f7f\u7528\u57fa\u7840\u8bed\u8a00\u548c\u5206\u5272\u6a21\u578b\u751f\u6210\u591a\u6a21\u6001\u6ce8\u91ca\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b5623\u5f20\u56fe\u50cf\uff0c\u6db5\u76d647\u79cd\u76ae\u80a4\u75c5\u7684eSkinHealth\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u60a3\u8005\u5143\u6570\u636e\u3001\u8bca\u65ad\u6807\u7b7e\u3001\u8bed\u4e49\u75c5\u53d8\u63a9\u7801\u3001\u7279\u5b9a\u5b9e\u4f8b\u7684\u89c6\u89c9\u6807\u9898\u548c\u4e34\u5e8a\u6982\u5ff5\u3002", "conclusion": "eSkinHealth\u6570\u636e\u96c6\u7684\u521b\u5efa\u4e3a\u5f00\u53d1\u66f4\u516c\u5e73\u3001\u51c6\u786e\u548c\u53ef\u89e3\u91ca\u7684\u5168\u7403\u76ae\u80a4\u75c5\u5b66AI\u5de5\u5177\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u548c\u53ef\u6269\u5c55\u7684\u6807\u6ce8\u6846\u67b6\u3002"}}
{"id": "2508.18642", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18642", "abs": "https://arxiv.org/abs/2508.18642", "authors": ["Jianxing Liao", "Tian Zhang", "Xiao Feng", "Yusong Zhang", "Rui Yang", "Haorui Wang", "Bosi Wen", "Ziying Wang", "Runzhi Shi"], "title": "RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing", "comment": null, "summary": "Large language models are extensively utilized in creative writing\napplications. Creative writing requires a balance between subjective writing\nquality (e.g., literariness and emotional expression) and objective constraint\nfollowing (e.g., format requirements and word limits). Existing reinforcement\nlearning methods struggle to balance these two aspects: single reward\nstrategies fail to improve both abilities simultaneously, while fixed-weight\nmixed-reward methods lack the ability to adapt to different writing scenarios.\nTo address this problem, we propose Reinforcement Learning with Mixed Rewards\n(RLMR), utilizing a dynamically mixed reward system from a writing reward model\nevaluating subjective writing quality and a constraint verification model\nassessing objective constraint following. The constraint following reward\nweight is adjusted dynamically according to the writing quality within sampled\ngroups, ensuring that samples violating constraints get negative advantage in\nGRPO and thus penalized during training, which is the key innovation of this\nproposed method. We conduct automated and manual evaluations across diverse\nmodel families from 8B to 72B parameters. Additionally, we construct a\nreal-world writing benchmark named WriteEval for comprehensive evaluation.\nResults illustrate that our method achieves consistent improvements in both\ninstruction following (IFEval from 83.36\\% to 86.65\\%) and writing quality\n(72.75\\% win rate in manual expert pairwise evaluations on WriteEval). To the\nbest of our knowledge, RLMR is the first work to combine subjective preferences\nwith objective verification in online RL training, providing an effective\nsolution for multi-dimensional creative writing optimization.", "AI": {"tldr": "\u52a8\u6001\u6df7\u5408\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5RLMR\u6709\u6548\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u521b\u610f\u5199\u4f5c\u80fd\u529b\uff0c\u5e73\u8861\u4e86\u4e3b\u89c2\u8d28\u91cf\u548c\u5ba2\u89c2\u7ea6\u675f\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u63d0\u5347\u4e3b\u89c2\u5199\u4f5c\u8d28\u91cf\u548c\u5ba2\u89c2\u7ea6\u675f\u9075\u5faa\u80fd\u529b\uff0c\u56e0\u6b64\u63d0\u51faRLMR\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u6df7\u5408\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5RLMR\uff0c\u8be5\u65b9\u6cd5\u6839\u636e\u5199\u4f5c\u8d28\u91cf\u52a8\u6001\u8c03\u6574\u7ea6\u675f\u9075\u5faa\u5956\u52b1\u6743\u91cd\uff0c\u4ece\u800c\u5e73\u8861\u4e3b\u89c2\u5199\u4f5c\u8d28\u91cf\u548c\u5ba2\u89c2\u7ea6\u675f\u3002", "result": "\u5728\u6307\u4ee4\u9075\u5faa\u548c\u5199\u4f5c\u8d28\u91cf\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5728WriteEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "RLMR\u65b9\u6cd5\u5728\u5e73\u8861\u4e3b\u89c2\u5199\u4f5c\u8d28\u91cf\u548c\u5ba2\u89c2\u7ea6\u675f\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5e76\u5728\u6307\u4ee4\u9075\u5faa\u548c\u5199\u4f5c\u8d28\u91cf\u4e0a\u5747\u6709\u63d0\u5347\u3002"}}
{"id": "2508.18646", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18646", "abs": "https://arxiv.org/abs/2508.18646", "authors": ["Jun Wang", "Ninglun Gu", "Kailai Zhang", "Zijiao Zhang", "Yelun Bao", "Jin Yang", "Xu Yin", "Liwei Liu", "Yihuan Liu", "Pengyong Li", "Gary G. Yen", "Junchi Yan"], "title": "Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap", "comment": "Preprint. Under review", "summary": "For Large Language Models (LLMs), a disconnect persists between benchmark\nperformance and real-world utility. Current evaluation frameworks remain\nfragmented, prioritizing technical metrics while neglecting holistic assessment\nfor deployment. This survey introduces an anthropomorphic evaluation paradigm\nthrough the lens of human intelligence, proposing a novel three-dimensional\ntaxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational\ncapacity, Emotional Quotient (EQ)-Alignment Ability for value-based\ninteractions, and Professional Quotient (PQ)-Professional Expertise for\nspecialized proficiency. For practical value, we pioneer a Value-oriented\nEvaluation (VQ) framework assessing economic viability, social impact, ethical\nalignment, and environmental sustainability. Our modular architecture\nintegrates six components with an implementation roadmap. Through analysis of\n200+ benchmarks, we identify key challenges including dynamic assessment needs\nand interpretability gaps. It provides actionable guidance for developing LLMs\nthat are technically proficient, contextually relevant, and ethically sound. We\nmaintain a curated repository of open-source evaluation resources at:\nhttps://github.com/onejune2018/Awesome-LLM-Eval.", "AI": {"tldr": "This paper proposes a new framework for evaluating LLMs considering intelligence, alignment, expertise, and societal impact.", "motivation": "Current evaluation frameworks for LLMs neglect holistic assessment for deployment.", "method": "Anthropomorphic evaluation paradigm, Value-oriented Evaluation (VQ) framework.", "result": "A three-dimensional taxonomy (IQ, EQ, PQ) and a VQ framework for evaluating LLMs, along with a curated repository of open-source evaluation resources.", "conclusion": "This survey proposes a novel three-dimensional taxonomy (IQ, EQ, PQ) and a Value-oriented Evaluation (VQ) framework for assessing LLMs, addressing the disconnect between benchmark performance and real-world utility."}}
{"id": "2508.18669", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18669", "abs": "https://arxiv.org/abs/2508.18669", "authors": ["Weikang Zhao", "Xili Wang", "Chengdi Ma", "Lingbin Kong", "Zhaohua Yang", "Mingxiang Tuo", "Xiaowei Shi", "Yitao Zhai", "Xunliang Cai"], "title": "MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use", "comment": null, "summary": "With the recent rapid advancement of Agentic Intelligence, agentic tool use\nin LLMs has become increasingly important. During multi-turn interactions\nbetween agents and users, the dynamic, uncertain, and stochastic nature of user\ndemands poses significant challenges to the agent's tool invocation\ncapabilities. Agents are no longer expected to simply call tools to deliver a\nresult; rather, they must iteratively refine their understanding of user needs\nthrough communication while simultaneously invoking tools to resolve user\nqueries. Existing reinforcement learning (RL) approaches for tool use lack the\nintegration of genuinely dynamic users during the RL training process. To\nbridge this gap, we introduce MUA-RL (Multi-turn User-interacting Agent\nReinforcement Learning for agentic tool use), a novel reinforcement learning\nframework that, for the first time in the field of agentic tool use, integrates\nLLM-simulated users into the reinforcement learning loop. MUA-RL aims to enable\nautonomous learning of models to communicate with users efficiently and use\nvarious tools to solve practical problems in dynamic multi-turn interactions.\nEvaluations are done on several multi-turn tool-using benchmarks (see Figure\n1). Specifically, MUA-RL-32B achieves 67.3 on TAU2 Retail, 45.4 on TAU2\nAirline, 28.3 on TAU2 Telecom, 28.4 on BFCL-V3 Multi Turn, and 82.5 on ACEBench\nAgent -- outperforming or matching the performance of larger open-source models\nsuch as DeepSeek-V3-0324 and Qwen3-235B-A22B in non-thinking settings.", "AI": {"tldr": "MUA-RL\u6846\u67b6\u901a\u8fc7\u5f15\u5165LLM\u6a21\u62df\u7528\u6237\uff0c\u63d0\u5347\u4e86\u4ee3\u7406\u5de5\u5177\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u4f7f\u7528\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4ee3\u7406\u5de5\u5177\u4f7f\u7528\u4e2d\u7f3a\u4e4f\u52a8\u6001\u7528\u6237\u4ea4\u4e92\uff0c\u96be\u4ee5\u5e94\u5bf9\u7528\u6237\u9700\u6c42\u7684\u52a8\u6001\u3001\u4e0d\u786e\u5b9a\u548c\u968f\u673a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6MUA-RL\uff0c\u8be5\u6846\u67b6\u5c06LLM\u6a21\u62df\u7528\u6237\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u5faa\u73af\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u81ea\u4e3b\u5b66\u4e60\u6a21\u578b\u9ad8\u6548\u5730\u4e0e\u7528\u6237\u6c9f\u901a\u5e76\u4f7f\u7528\u5404\u79cd\u5de5\u5177\u6765\u89e3\u51b3\u52a8\u6001\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u5b9e\u9645\u95ee\u9898\u3002", "result": "MUA-RL-32B\u5728TAU2 Retail\u4e0a\u53d6\u5f9767.3\u7684\u6210\u7ee9\uff0c\u5728TAU2 Airline\u4e0a\u53d6\u5f9745.4\u7684\u6210\u7ee9\uff0c\u5728TAU2 Telecom\u4e0a\u53d6\u5f9728.3\u7684\u6210\u7ee9\uff0c\u5728BFCL-V3 Multi Turn\u4e0a\u53d6\u5f9728.4\u7684\u6210\u7ee9\uff0c\u5728ACEBench Agent\u4e0a\u53d6\u5f9782.5\u7684\u6210\u7ee9\uff0c\u4f18\u4e8e\u6216\u5339\u914d\u4e86DeepSeek-V3-0324\u548cQwen3-235B-A22B\u7b49\u5927\u578b\u5f00\u6e90\u6a21\u578b\u5728\u975e\u601d\u8003\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "conclusion": "MUA-RL\u6846\u67b6\u901a\u8fc7\u5c06LLM\u6a21\u62df\u7528\u6237\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u5faa\u73af\u4e2d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4ee3\u7406\u5de5\u5177\u4f7f\u7528\u4e2d\u7f3a\u4e4f\u52a8\u6001\u7528\u6237\u4ea4\u4e92\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u7ee9\uff0c\u4f18\u4e8e\u6216\u5339\u914d\u4e86DeepSeek-V3-0324\u548cQwen3-235B-A22B\u7b49\u5927\u578b\u5f00\u6e90\u6a21\u578b\u3002"}}
{"id": "2508.18689", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18689", "abs": "https://arxiv.org/abs/2508.18689", "authors": ["Yuyang Zhao", "Wentao Shi", "Fuli Feng", "Xiangnan He"], "title": "AppAgent-Pro: A Proactive GUI Agent System for Multidomain Information Integration and User Assistance", "comment": "Accepted at CIKM 2025. 10 pages, 5 figures. Our code is available at:\n  https://github.com/LaoKuiZe/AppAgent-Pro. Our code is available at:\n  https://github.com/LaoKuiZe/AppAgent-Pro. The demonstration video could be\n  found at:\n  https://www.dropbox.com/scl/fi/hvzqo5vnusg66srydzixo/AppAgent-Pro-demo-video.mp4?rlkey=o2nlfqgq6ihl125mcqg7bpgqu&st=d29vrzii&dl=0", "summary": "Large language model (LLM)-based agents have demonstrated remarkable\ncapabilities in addressing complex tasks, thereby enabling more advanced\ninformation retrieval and supporting deeper, more sophisticated human\ninformation-seeking behaviors. However, most existing agents operate in a\npurely reactive manner, responding passively to user instructions, which\nsignificantly constrains their effectiveness and efficiency as general-purpose\nplatforms for information acquisition. To overcome this limitation, this paper\nproposes AppAgent-Pro, a proactive GUI agent system that actively integrates\nmulti-domain information based on user instructions. This approach enables the\nsystem to proactively anticipate users' underlying needs and conduct in-depth\nmulti-domain information mining, thereby facilitating the acquisition of more\ncomprehensive and intelligent information. AppAgent-Pro has the potential to\nfundamentally redefine information acquisition in daily life, leading to a\nprofound impact on human society. Our code is available at:\nhttps://github.com/LaoKuiZe/AppAgent-Pro. Our code is available at:\nhttps://github.com/LaoKuiZe/AppAgent-Pro. The demonstration video could be\nfound at:\nhttps://www.dropbox.com/scl/fi/hvzqo5vnusg66srydzixo/AppAgent-Pro-demo-video.mp4?rlkey=o2nlfqgq6ihl125mcqg7bpgqu&st=d29vrzii&dl=0.", "AI": {"tldr": "AppAgent-Pro \u662f\u4e00\u79cd\u4e3b\u52a8\u5f0f GUI \u4ee3\u7406\u7cfb\u7edf\uff0c\u80fd\u591f\u4e3b\u52a8\u6574\u5408\u591a\u9886\u57df\u4fe1\u606f\uff0c\u63d0\u9ad8\u4fe1\u606f\u83b7\u53d6\u6548\u7387\u3002", "motivation": "\u73b0\u6709 LLM \u4ee3\u7406\u7cfb\u7edf\u5927\u591a\u88ab\u52a8\u54cd\u5e94\u7528\u6237\u6307\u4ee4\uff0c\u6548\u7387\u548c\u6709\u6548\u6027\u6709\u9650\uff0cAppAgent-Pro\u65e8\u5728\u514b\u670d\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u8be5\u7cfb\u7edf\u91c7\u7528\u4e3b\u52a8\u5f0f\u4fe1\u606f\u68c0\u7d22\u65b9\u6cd5\uff0c\u80fd\u591f\u6839\u636e\u7528\u6237\u6307\u4ee4\u4e3b\u52a8\u6316\u6398\u591a\u9886\u57df\u4fe1\u606f\u3002", "result": "AppAgent-Pro \u7cfb\u7edf\u80fd\u591f\u66f4\u6709\u6548\u5730\u6ee1\u8db3\u7528\u6237\u7684\u4fe1\u606f\u9700\u6c42\uff0c\u5e76\u5bf9\u65e5\u5e38\u4fe1\u606f\u83b7\u53d6\u65b9\u5f0f\u4ea7\u751f\u6df1\u8fdc\u5f71\u54cd\u3002", "conclusion": "AppAgent-Pro \u662f\u4e00\u79cd\u4e3b\u52a8\u5f0f GUI \u4ee3\u7406\u7cfb\u7edf\uff0c\u80fd\u591f\u57fa\u4e8e\u7528\u6237\u6307\u4ee4\u4e3b\u52a8\u6574\u5408\u591a\u9886\u57df\u4fe1\u606f\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u6ee1\u8db3\u7528\u6237\u7684\u4fe1\u606f\u9700\u6c42\uff0c\u5e76\u5bf9\u65e5\u5e38\u4fe1\u606f\u83b7\u53d6\u65b9\u5f0f\u4ea7\u751f\u6df1\u8fdc\u5f71\u54cd\u3002"}}
{"id": "2508.18722", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18722", "abs": "https://arxiv.org/abs/2508.18722", "authors": ["Honghao Fu", "Junlong Ren", "Qi Chai", "Deheng Ye", "Yujun Cai", "Hao Wang"], "title": "VistaWise: Building Cost-Effective Agent with Cross-Modal Knowledge Graph for Minecraft", "comment": "Accepted by EMNLP 2025 main", "summary": "Large language models (LLMs) have shown significant promise in embodied\ndecision-making tasks within virtual open-world environments. Nonetheless,\ntheir performance is hindered by the absence of domain-specific knowledge.\nMethods that finetune on large-scale domain-specific data entail prohibitive\ndevelopment costs. This paper introduces VistaWise, a cost-effective agent\nframework that integrates cross-modal domain knowledge and finetunes a\ndedicated object detection model for visual analysis. It reduces the\nrequirement for domain-specific training data from millions of samples to a few\nhundred. VistaWise integrates visual information and textual dependencies into\na cross-modal knowledge graph (KG), enabling a comprehensive and accurate\nunderstanding of multimodal environments. We also equip the agent with a\nretrieval-based pooling strategy to extract task-related information from the\nKG, and a desktop-level skill library to support direct operation of the\nMinecraft desktop client via mouse and keyboard inputs. Experimental results\ndemonstrate that VistaWise achieves state-of-the-art performance across various\nopen-world tasks, highlighting its effectiveness in reducing development costs\nwhile enhancing agent performance.", "AI": {"tldr": "VistaWise\u662f\u4e00\u4e2a\u7ecf\u6d4e\u9ad8\u6548\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u8de8\u6a21\u6001\u77e5\u8bc6\u548c\u5fae\u8c03\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u5728\u5f00\u653e\u4e16\u754c\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u6781\u5927\u51cf\u5c11\u4e86\u6570\u636e\u9700\u6c42\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u865a\u62df\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u5177\u8eab\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u5176\u6027\u80fd\u53d7\u5230\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u7684\u9650\u5236\uff0c\u800c\u5927\u89c4\u6a21\u9886\u57df\u7279\u5b9a\u6570\u636e\u4e0a\u7684\u5fae\u8c03\u65b9\u6cd5\u5219\u9700\u8981\u9ad8\u6602\u7684\u5f00\u53d1\u6210\u672c\u3002", "method": "VistaWise\u6846\u67b6\u96c6\u6210\u4e86\u8de8\u6a21\u6001\u9886\u57df\u77e5\u8bc6\uff0c\u5fae\u8c03\u4e86\u4e13\u95e8\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u68c0\u7d22\u7684\u6c60\u5316\u7b56\u7565\u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u63d0\u53d6\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u3002", "result": "VistaWise\u5728\u591a\u4e2a\u5f00\u653e\u4e16\u754c\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c06\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u4ece\u6570\u767e\u4e07\u6837\u672c\u51cf\u5c11\u5230\u51e0\u767e\u4e2a\u3002", "conclusion": "VistaWise\u6846\u67b6\u5728\u5f00\u653e\u4e16\u754c\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u5f00\u53d1\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u667a\u80fd\u4f53\u6027\u80fd\u3002"}}
{"id": "2508.18724", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18724", "abs": "https://arxiv.org/abs/2508.18724", "authors": ["Karanbir Singh", "Deepak Muppiri", "William Ngu"], "title": "Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval", "comment": "Accepted at KDD'2025 Agent4IR workshop", "summary": "Large Language Models (LLMs) have transformed the field of artificial\nintelligence by unlocking the era of generative applications. Built on top of\ngenerative AI capabilities, Agentic AI represents a major shift toward\nautonomous, goal-driven systems that can reason, retrieve, and act. However,\nthey also inherit the bias present in both internal and external information\nsources. This significantly affects the fairness and balance of retrieved\ninformation, and hence reduces user trust. To address this critical challenge,\nwe introduce a novel Bias Mitigation Agent, a multi-agent system designed to\norchestrate the workflow of bias mitigation through specialized agents that\noptimize the selection of sources to ensure that the retrieved content is both\nhighly relevant and minimally biased to promote fair and balanced knowledge\ndissemination. The experimental results demonstrate an 81.82\\% reduction in\nbias compared to a baseline naive retrieval strategy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591aAgent\u7cfb\u7edf\u4ee5\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2dAgentic AI\u7cfb\u7edf\u7684\u504f\u5dee\uff0c\u5b9e\u9a8c\u8868\u660e\u504f\u5dee\u964d\u4f4e\u4e8681.82%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684Agentic AI\u7cfb\u7edf\u7ee7\u627f\u4e86\u5185\u90e8\u548c\u5916\u90e8\u4fe1\u606f\u6e90\u4e2d\u7684\u504f\u5dee\uff0c\u5f71\u54cd\u4e86\u68c0\u7d22\u4fe1\u606f\u7684\u516c\u5e73\u6027\u548c\u5e73\u8861\u6027\uff0c\u964d\u4f4e\u4e86\u7528\u6237\u4fe1\u4efb\u5ea6\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u591aAgent\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e13\u95e8\u7684Agent\u4f18\u5316\u4fe1\u606f\u6e90\u7684\u9009\u62e9\u6765\u51cf\u5c11\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u7b56\u7565\u76f8\u6bd4\uff0c\u504f\u5dee\u51cf\u5c11\u4e8681.82%\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u504f\u5dee\u7f13\u89e3Agent\uff0c\u8be5\u591aAgent\u7cfb\u7edf\u901a\u8fc7\u4e13\u95e8\u7684Agent\u4f18\u5316\u4fe1\u606f\u6e90\u7684\u9009\u62e9\uff0c\u4ee5\u786e\u4fdd\u68c0\u7d22\u5230\u7684\u5185\u5bb9\u9ad8\u5ea6\u76f8\u5173\u4e14\u504f\u5dee\u6700\u5c0f\uff0c\u4ece\u800c\u4fc3\u8fdb\u516c\u5e73\u3001\u5e73\u8861\u7684\u77e5\u8bc6\u4f20\u64ad\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u7b56\u7565\u76f8\u6bd4\uff0c\u504f\u5dee\u51cf\u5c11\u4e8681.82%\u3002"}}
