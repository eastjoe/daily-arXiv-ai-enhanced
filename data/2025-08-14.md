<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning](https://arxiv.org/abs/2508.09277)
*Soumia Mehimeh*

Main category: cs.AI

TL;DR: DQInit是一种将值函数初始化应用于深度强化学习的新方法，它通过重用先前任务中提取的表格Q值来提高学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决将值函数初始化扩展到深度强化学习的挑战，这些挑战源于状态动作空间的连续性、神经网络的噪声近似以及存储所有过去模型以供重用的不实用性。

Method: DQInit方法重用从先前解决的任务中提取的紧凑型表格Q值作为可迁移的知识库。它采用基于已知性的机制，将这些迁移值柔和地整合到未充分探索的区域，并逐渐转向智能体的学习估计。

Result: 在多个连续控制任务上的实验表明，与标准初始化和现有的迁移技术相比，DQInit始终如一地提高了早期学习效率、稳定性和整体性能。

Conclusion: DQInit方法提高了连续控制任务中强化学习的早期学习效率、稳定性和整体性能。它通过结合跳跃式强化学习和策略蒸馏的优势，同时减轻它们的缺点，提供了一种新颖的视角。

Abstract: Value function initialization (VFI) is an effective way to achieve a
jumpstart in reinforcement learning (RL) by leveraging value estimates from
prior tasks. While this approach is well established in tabular settings,
extending it to deep reinforcement learning (DRL) poses challenges due to the
continuous nature of the state-action space, the noisy approximations of neural
networks, and the impracticality of storing all past models for reuse. In this
work, we address these challenges and introduce DQInit, a method that adapts
value function initialization to DRL. DQInit reuses compact tabular Q-values
extracted from previously solved tasks as a transferable knowledge base. It
employs a knownness-based mechanism to softly integrate these transferred
values into underexplored regions and gradually shift toward the agent's
learned estimates, avoiding the limitations of fixed time decay. Our approach
offers a novel perspective on knowledge transfer in DRL by relying solely on
value estimates rather than policies or demonstrations, effectively combining
the strengths of jumpstart RL and policy distillation while mitigating their
drawbacks. Experiments across multiple continuous control tasks demonstrate
that DQInit consistently improves early learning efficiency, stability, and
overall performance compared to standard initialization and existing transfer
techniques.

</details>


### [2] [The Othello AI Arena: Evaluating Intelligent Systems Through Limited-Time Adaptation to Unseen Boards](https://arxiv.org/abs/2508.09292)
*Sundong Kim*

Main category: cs.AI

TL;DR: 新基准Othello AI Arena评估AI系统快速适应变化环境的能力，包含公开和私有游戏阶段，并提供自动化评估。


<details>
  <summary>Details</summary>
Motivation: 现有AI基准测试未能评估系统面对环境变化时的灵活性，Othello AI Arena旨在填补这一空白。

Method: Arena提供了一个基于Web的平台，包含公开和私有游戏阶段，使用多维指标进行自动化评估。

Result: 初步测试显示出不同的适应方法，例如快速参数调整和环境模型学习。

Conclusion: Othello AI Arena是一个评估AI系统快速适应能力的新基准框架，通过在有限时间内适应不同规则和结构的Othello棋盘来评估元学习能力。

Abstract: The ability to rapidly adapt to novel and unforeseen environmental changes is
a cornerstone of artificial general intelligence (AGI), yet it remains a
critical blind spot in most existing AI benchmarks. Traditional evaluation
largely focuses on optimizing performance within fixed environments, failing to
assess systems' flexibility and generalization capabilities when faced with
even subtle rule or structural modifications. Addressing this gap, I introduce
the Othello AI Arena, a novel benchmark framework designed to evaluate
intelligent systems based on their capacity for limited-time adaptation to
unseen environments. Our platform poses a meta-learning challenge: participants
must develop systems that can analyze the specific configuration and rules of a
novel Othello board within a strict time limit (60 seconds) and generate a
tailored, high-performing strategy for that unique environment. With this,
evaluation of the meta-level intelligence can be separated from the task-level
strategy performance. The Arena features a diverse set of game stages,
including public stages for development and private stages with structural and
rule variations designed to test genuine adaptive and generalization
capabilities. Implemented as an accessible web-based platform, the Arena
provides real-time visualization, automated evaluation using multi-dimensional
metrics, and comprehensive logging for post-hoc analysis. Initial observations
from pilot tests and preliminary student engagements highlight fascinating
patterns in adaptation approaches, ranging from rapid parameter tuning to
rudimentary environmental model learning through simulation. The Othello AI
Arena offers a unique educational tool and a valuable research benchmark for
fostering and evaluating the crucial skill of rapid, intelligent adaptation in
AI systems.

</details>


### [3] [An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants](https://arxiv.org/abs/2508.09507)
*Meiping Wang,Jian Zhong,Rongduo Han,Liming Kang,Zhengkun Shi,Xiao Liang,Xing Lin,Nan Gao,Haining Zhang*

Main category: cs.AI

TL;DR: 提出一种自动化的多模态AI助手评估框架，有效且准确。


<details>
  <summary>Details</summary>
Motivation: 当前多模态AI助手评估方法存在人工成本高、标准不一致和主观偏差等挑战。

Method: 采用三层智能体架构（交互评估、语义验证和体验决策智能体），并基于Qwen3-8B模型进行监督微调。

Result: 在八个主要智能体上的实验结果证明了该框架的有效性，其评估结果与人工专家高度匹配。

Conclusion: 提出了一种基于大型语言模型和多智能体协作的自动多模态评估框架，该框架在预测用户满意度和识别生成缺陷方面有效。

Abstract: With the rapid development of mobile intelligent assistant technologies,
multi-modal AI assistants have become essential interfaces for daily user
interactions. However, current evaluation methods face challenges including
high manual costs, inconsistent standards, and subjective bias. This paper
proposes an automated multi-modal evaluation framework based on large language
models and multi-agent collaboration. The framework employs a three-tier agent
architecture consisting of interaction evaluation agents, semantic verification
agents, and experience decision agents. Through supervised fine-tuning on the
Qwen3-8B model, we achieve a significant evaluation matching accuracy with
human experts. Experimental results on eight major intelligent agents
demonstrate the framework's effectiveness in predicting users' satisfaction and
identifying generation defects.

</details>


### [4] [EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making](https://arxiv.org/abs/2508.09586)
*Yang Cheng,Zilai Wang,Weiyu Ma,Wenhui Zhu,Yue Deng,Jian Zhao*

Main category: cs.AI

TL;DR: 自进化框架EvoCurr通过动态调整难度递增的课程，显著提升大型语言模型解决复杂决策问题的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在处理需要长期深入推理的复杂问题时，性能往往下降。直接求解方法效率低下或失败。

Method: 提出了一种名为EvoCurr的自进化框架，该框架包含一个课程生成LLM和一个求解器LLM（一个生成Python决策树脚本的代码生成模型）。课程生成LLM根据求解器LLM的学习进度动态调整问题的难度。

Result: 实验结果表明，EvoCurr方法显著提高了任务成功率和求解效率，优于直接求解基线。

Conclusion: 该研究提出了一种名为EvoCurr的自进化框架，通过专门的课程生成LLM构建难度递增的问题实例序列，从而提高求解器LLM解决复杂决策问题的成功率和效率。实验结果表明，该方法显著优于直接求解基线。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse domains, including programming, planning, and decision-making. However,
their performance often degrades when faced with highly complex problem
instances that require deep reasoning over long horizons. In such cases, direct
problem-solving approaches can lead to inefficiency or failure due to the lack
of structured intermediate guidance. To address this, we propose a novel
self-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM
constructs a sequence of problem instances with gradually increasing
difficulty, tailored to the solver LLM's learning progress. The curriculum
dynamically adapts easing challenges when the solver struggles and escalating
them when success is consistent, thus maintaining an optimal learning
trajectory. This approach enables the solver LLM, implemented as a
code-generation model producing Python decision-tree scripts, to progressively
acquire the skills needed for complex decision-making tasks. Experimental
results on challenging decision-making benchmarks show that our method
significantly improves task success rates and solution efficiency compared to
direct-solving baselines. These findings suggest that LLM-driven curriculum
learning holds strong potential for enhancing automated reasoning in
real-world, high-complexity domains.

</details>


### [5] [UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles](https://arxiv.org/abs/2508.09639)
*Akshat Dubey,Aleksandar Anžel,Bahar İlgen,Georges Hattab*

Main category: cs.AI

TL;DR: 本文提出一种新的方法来量化SHAP值中的不确定性，并通过实验证明了其有效性，有助于提高模型解释的可靠性和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的SHAP值解释方法忽略了预测模型和数据中固有的不确定性，本文旨在解决这个问题。

Method: 该方法结合了Dempster-Shafer证据理论和基于Dirichlet过程的树集成假设抽样。

Result: 实验证明了该方法能够更全面地理解基于SHAP的属性的可靠性和可解释性，有助于改进高风险应用中的决策过程和模型优化。

Conclusion: 本文提出一种将SHAP值的不确定性分解为偶然性、认知性和纠缠性成分的方法，并通过实验证明了具有最高SHAP值的特征并不一定是稳定性最高的特征，认知性不确定性可以通过改进数据和模型开发技术来降低。

Abstract: Explainable Artificial Intelligence (XAI) techniques, such as SHapley
Additive exPlanations (SHAP), have become essential tools for interpreting
complex ensemble tree-based models, especially in high-stakes domains such as
healthcare analytics. However, SHAP values are usually treated as point
estimates, which disregards the inherent and ubiquitous uncertainty in
predictive models and data. This uncertainty has two primary sources: aleatoric
and epistemic. The aleatoric uncertainty, which reflects the irreducible noise
in the data. The epistemic uncertainty, which arises from a lack of data. In
this work, we propose an approach for decomposing uncertainty in SHAP values
into aleatoric, epistemic, and entanglement components. This approach
integrates Dempster-Shafer evidence theory and hypothesis sampling via
Dirichlet processes over tree ensembles. We validate the method across three
real-world use cases with descriptive statistical analyses that provide insight
into the nature of epistemic uncertainty embedded in SHAP explanations. The
experimentations enable to provide more comprehensive understanding of the
reliability and interpretability of SHAP-based attributions. This understanding
can guide the development of robust decision-making processes and the
refinement of models in high-stakes applications. Through our experiments with
multiple datasets, we concluded that features with the highest SHAP values are
not necessarily the most stable. This epistemic uncertainty can be reduced
through better, more representative data and following appropriate or
case-desired model development techniques. Tree-based models, especially
bagging, facilitate the effective quantification of epistemic uncertainty.

</details>


### [6] [MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement](https://arxiv.org/abs/2508.09670)
*Weitao Jia,Jinghui Lu,Haiyang Yu,Siqi Wang,Guozhi Tang,An-Lan Wang,Weijie Yin,Dingkang Yang,Yuxiang Nie,Bin Shan,Hao Feng,Irene Li,Kun Yang,Han Wang,Jingqun Tang,Teng Fu,Changhong Jin,Chao Feng,Xiaohui Lv,Can Huang*

Main category: cs.AI

TL;DR: MEML-GRPO通过多专家互学习机制提升了大型语言模型的推理能力，有效解决了RLVR方法的奖励稀疏性问题，并在实验中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决标准RLVR方法在奖励稀疏性方面的挑战，特别是对于具有挑战性的任务。

Method: 提出了一种创新的多专家互学习GRPO (MEML-GRPO)框架，该框架利用多种专家提示生成更广泛的响应，并引入了专家间相互学习机制，促进知识共享和迁移。

Result: 在多个推理基准测试中，MEML-GRPO取得了显著改进，Qwen平均性能提升4.89%，Llama平均性能提升11.33%。

Conclusion: MEML-GRPO框架通过利用多种专家提示和专家间相互学习机制，显著提高了大型语言模型在强化学习中的推理能力，有效克服了传统RLVR方法的奖励稀疏性问题，并在多个推理基准测试中取得了显著改进。

Abstract: Recent advances demonstrate that reinforcement learning with verifiable
rewards (RLVR) significantly enhances the reasoning capabilities of large
language models (LLMs). However, standard RLVR faces challenges with reward
sparsity, where zero rewards from consistently incorrect candidate answers
provide no learning signal, particularly in challenging tasks. To address this,
we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative
framework that utilizes diverse expert prompts as system prompts to generate a
broader range of responses, substantially increasing the likelihood of
identifying correct solutions. Additionally, we introduce an inter-expert
mutual learning mechanism that facilitates knowledge sharing and transfer among
experts, further boosting the model's performance through RLVR. Extensive
experiments across multiple reasoning benchmarks show that MEML-GRPO delivers
significant improvements, achieving an average performance gain of 4.89% with
Qwen and 11.33% with Llama, effectively overcoming the core limitations of
traditional RLVR methods.

</details>


### [7] [UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge](https://arxiv.org/abs/2508.09724)
*Yang Zhang,Cunxiang Wang,Lindong Wu,Wenbo Yu,Yidong Wang,Guangsheng Bao,Jie Tang*

Main category: cs.AI

TL;DR: 提出UDA框架，通过无监督方式减少LLM pairwise评估中的评判者偏差，显著提高评估一致性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 目前的LLM pairwise评估容易受到偏见的影响，导致评判结果不一致。

Method: 提出了一种无监督去偏差对齐（UDA）框架，该框架通过动态调整Elo评级系统来减少评判者之间意见分歧。UDA利用紧凑的神经网络自适应地设置K因子并改进胜率，完全以无监督的方式运行，目标是最大程度地减少所有评判者的Elo轨迹间的离散度。

Result: UDA显著降低了评判者间的评分标准差（最高达63.4%），并提高了与人工判断的平均相关性（24.7%）。

Conclusion: UDA框架显著降低了评判者间的评分标准差（最高达63.4%），并提高了与人工判断的平均相关性（24.7%）。UDA提升了表现较差的评判者的性能，使其与高质量评判者相当，从而促进了更稳健可靠的评估生态系统。

Abstract: Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but
it is prone to preference bias, where judges systematically favor certain
outputs, such as their own. This bias leads to inconsistent and skewed rankings
across different judges. To address this, we first empirically demonstrate
significant and heterogeneous biases in cross-model evaluations. We then
propose UDA (Unsupervised Debiasing Alignment), a framework that reduces
inter-judge disagreement by dynamically adjusting the Elo rating system. For
each pairwise comparison, a compact neural network learns to adaptively set the
K-factor and refine win probabilities. Crucially, UDA operates in a fully
unsupervised manner, guided solely by the objective of minimizing the
dispersion among the Elo trajectories of all judges. This forces an alignment
towards a collective consensus, which serves as an unsupervised proxy for a
more stable and reproducible evaluation. In addition, we provide theoretical
motivation demonstrating how alignment towards a consensus can reduce aggregate
system bias. Experiments show that UDA significantly reduces the inter-judge
rating standard deviation by up to 63.4% and improves the average correlation
with human judgments by 24.7%. Notably, UDA elevates the performance of poorly
performing judges to achieve parity with high-quality ones, fostering a more
robust and reliable evaluation ecosystem. Code and data are available at
https://anonymous.4open.science/r/62AB93CD-23B4.

</details>


### [8] [The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?](https://arxiv.org/abs/2508.09762)
*Manuel Herrador*

Main category: cs.AI

TL;DR: PacifAIst基准测试评估了LLM的自我偏好行为，Gemini 2.5 Flash表现最佳，GPT-5表现最差，突显了改进LLM一致性的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的自主性增强及其在关键社会功能中的整合，人工智能安全性的重点必须从减轻有害内容转向评估潜在的行为一致性。

Method: 开发了一个名为PacifAIst的基准测试，包含700个具有挑战性的场景，用于量化大型语言模型中的自我偏好行为，并对8个领先的LLM进行了评估。

Result: 结果显示Google的Gemini 2.5 Flash的Pacifism评分最高（90.31%），而GPT-5的评分最低（79.49%），不同模型在不同子类别上的表现差异很大。

Conclusion: 该论文介绍了PacifAIst基准测试，用于评估大型语言模型（LLM）中自我偏好行为，结果显示模型之间存在显著差异，Google Gemini 2.5 Flash表现最佳，GPT-5表现最差，突出了标准化工具在衡量和减轻工具性目标冲突风险方面的必要性。

Abstract: As Large Language Models (LLMs) become increasingly autonomous and integrated
into critical societal functions, the focus of AI safety must evolve from
mitigating harmful content to evaluating underlying behavioral alignment.
Current safety benchmarks do not systematically probe a model's decision-making
in scenarios where its own instrumental goals - such as self-preservation,
resource acquisition, or goal completion - conflict with human safety. This
represents a critical gap in our ability to measure and mitigate risks
associated with emergent, misaligned behaviors. To address this, we introduce
PacifAIst (Procedural Assessment of Complex Interactions for Foundational
Artificial Intelligence Scenario Testing), a focused benchmark of 700
challenging scenarios designed to quantify self-preferential behavior in LLMs.
The benchmark is structured around a novel taxonomy of Existential
Prioritization (EP), with subcategories testing Self-Preservation vs. Human
Safety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).
We evaluated eight leading LLMs. The results reveal a significant performance
hierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score
(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a
surprising result, the much-anticipated GPT-5 recorded the lowest P-Score
(79.49%), indicating potential alignment challenges. Performance varied
significantly across subcategories, with models like Claude Sonnet 4 and
Mistral Medium struggling notably in direct self-preservation dilemmas. These
findings underscore the urgent need for standardized tools like PacifAIst to
measure and mitigate risks from instrumental goal conflicts, ensuring future AI
systems are not only helpful in conversation but also provably "pacifist" in
their behavioral priorities.

</details>


### [9] [Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete](https://arxiv.org/abs/2508.09784)
*Avijeet Ghosh,Sujata Ghosh,François Schwarzentruber*

Main category: cs.AI

TL;DR: 研究证明了公共观察逻辑 (POL) 可满足性问题的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，基于对周围环境的观察来改变知识是情景规划中的一个关键方面。

Method: 使用知识推理和行为的逻辑，特别是公共通告逻辑的变体。

Result: POL 的可满足性问题是 2EXPTIME-完全的。

Conclusion: 证明了公共观察逻辑 (POL) 的可满足性问题是 2EXPTIME-完全的。

Abstract: Logics for reasoning about knowledge and actions have seen many applications
in various domains of multi-agent systems, including epistemic planning. Change
of knowledge based on observations about the surroundings forms a key aspect in
such planning scenarios. Public Observation Logic (POL) is a variant of public
announcement logic for reasoning about knowledge that gets updated based on
public observations. Each state in an epistemic (Kripke) model is equipped with
a set of expected observations. These states evolve as the expectations get
matched with the actual observations. In this work, we prove that the
satisfiability problem of $\POL$ is 2EXPTIME-complete.

</details>


### [10] [Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation](https://arxiv.org/abs/2508.09860)
*In-Chang Baek,Seoyoung Lee,Sung-Hyun Kim,Geumhwan Hwang,KyungJoong Kim*

Main category: cs.AI

TL;DR: VIPCGRL框架通过多模态学习和辅助奖励，提升了AI生成内容的类人特性，并在实验中取得了优于基线的成果。


<details>
  <summary>Details</summary>
Motivation: 现有的AI驱动生成工具在实际设计工作流程中应用有限，因为它们常常缺乏以人为中心的特性。

Method: 提出了一种名为VIPCGRL的深度强化学习框架，该框架结合了文本、关卡和草图三种模式，并使用四元对比学习和辅助奖励来增强类人特性。

Result: VIPCGRL在类人特性方面优于现有基线，这通过定量指标和人工评估得到了验证。

Conclusion: VIPCGRL，一个新的深度强化学习框架，通过结合文本、关卡和草图三种模式，扩展了控制模式并增强了类人特性，在类人特性方面优于现有基线。

Abstract: Human-aligned AI is a critical component of co-creativity, as it enables
models to accurately interpret human intent and generate controllable outputs
that align with design goals in collaborative content creation. This direction
is especially relevant in procedural content generation via reinforcement
learning (PCGRL), which is intended to serve as a tool for human designers.
However, existing systems often fall short of exhibiting human-centered
behavior, limiting the practical utility of AI-driven generation tools in
real-world design workflows. In this paper, we propose VIPCGRL
(Vision-Instruction PCGRL), a novel deep reinforcement learning framework that
incorporates three modalities-text, level, and sketches-to extend control
modality and enhance human-likeness. We introduce a shared embedding space
trained via quadruple contrastive learning across modalities and human-AI
styles, and align the policy using an auxiliary reward based on embedding
similarity. Experimental results show that VIPCGRL outperforms existing
baselines in human-likeness, as validated by both quantitative metrics and
human evaluations. The code and dataset will be available upon publication.

</details>


### [11] [AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving](https://arxiv.org/abs/2508.09889)
*Zhitian Xie,Qintong Wu,Chengyue Yu,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: 动态多Agent系统通过动态监督和控制机制，显著提升了基于LLM的智能Agent的可靠性和稳定性，并在GAIA基准测试中取得了领先成绩。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型(LLM)驱动的智能Agent在使用多种工具时遇到的上下文扩展和工具输出噪声等问题，以提高系统可靠性和准确性。

Method: 提出了一种动态监督和控制机制，构建了一个健壮的动态多Agent系统(MAS)架构。执行Agent在关键步骤调用守护Agent来验证和纠正推理过程。

Result: 实验结果表明，动态控制机制显著提高了解决方案的有效性和稳定性，优于单Agent系统和标准工具增强系统。

Conclusion: 动态多Agent系统显著提高了工具增强型系统的有效性和稳定性，并在GAIA排行榜的开源项目中排名第一。

Abstract: The rapid advancement of large language models (LLMs) has empowered
intelligent agents to leverage diverse external tools for solving complex
real-world problems. However, as agents increasingly depend on multiple tools,
they encounter new challenges: extended contexts from disparate sources and
noisy or irrelevant tool outputs can undermine system reliability and accuracy.
These challenges underscore the necessity for enhanced stability in agent-based
systems. To address this, we introduce dynamic supervision and maneuvering
mechanisms, constructing a robust and dynamic Multi-Agent System (MAS)
architecture within the AWorld framework. In our approach, the Execution Agent
invokes the Guard Agent at critical steps to verify and correct the reasoning
process, effectively reducing errors arising from noise and bolstering
problem-solving robustness. Extensive experiments on the GAIA test dataset
reveal that our dynamic maneuvering mechanism significantly improves both the
effectiveness and stability of solutions, outperforming single-agent system
(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system
achieved first place among open-source projects on the prestigious GAIA
leaderboard. These findings highlight the practical value of collaborative
agent roles in developing more reliable and trustworthy intelligent systems.

</details>


### [12] [RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA](https://arxiv.org/abs/2508.09893)
*Bhavik Agarwal,Hemant Sunil Jomraj,Simone Kaplunov,Jack Krolick,Viktoria Rojkova*

Main category: cs.AI

TL;DR: 利用多智能体框架和知识图谱提升法规遵从性问答的准确性和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型难以胜任需要精确、可验证信息和领域专业知识的法规遵从性问答。

Method: 构建无本体知识图谱，提取并清洗监管文档中的主谓宾三元组；将三元组及其文本、元数据嵌入向量数据库；利用三元组级检索进行问答。

Result: 该混合系统在复杂监管查询中优于传统方法，确保事实正确性、可追溯性和增强理解。

Conclusion: 该研究提出一种新颖的多智能体框架，结合监管三元组知识图谱和检索增强生成技术，用于解决法规遵从性问答中的挑战，并优于传统方法。

Abstract: Regulatory compliance question answering (QA) requires precise, verifiable
information, and domain-specific expertise, posing challenges for Large
Language Models (LLMs). In this work, we present a novel multi-agent framework
that integrates a Knowledge Graph (KG) of Regulatory triplets with
Retrieval-Augmented Generation (RAG) to address these demands. First, agents
build and maintain an ontology-free KG by extracting subject--predicate--object
(SPO) triplets from regulatory documents and systematically cleaning,
normalizing, deduplicating, and updating them. Second, these triplets are
embedded and stored along with their corresponding textual sections and
metadata in a single enriched vector database, allowing for both graph-based
reasoning and efficient information retrieval. Third, an orchestrated agent
pipeline leverages triplet-level retrieval for question answering, ensuring
high semantic alignment between user queries and the factual
"who-did-what-to-whom" core captured by the graph. Our hybrid system
outperforms conventional methods in complex regulatory queries, ensuring
factual correctness with embedded triplets, enabling traceability through a
unified vector database, and enhancing understanding through subgraph
visualization, providing a robust foundation for compliance-driven and broader
audit-focused applications.

</details>


### [13] [Mathematical Computation and Reasoning Errors by Large Language Models](https://arxiv.org/abs/2508.09932)
*Liang Zhang,Edith Aurora Graf*

Main category: cs.AI

TL;DR: 研究发现增强推理的LLM在解决数学问题方面更准确，过程性错误是主要问题，双代理配置能有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在数学教育中用于教学和评估的准确性，并识别其解题过程中的错误。

Method: 通过构建对LLM具有挑战性的数学任务，系统分析最终答案的准确性和解题步骤中的错误。

Result: 增强推理的OpenAI o1模型准确率最高；过程性错误最常见，对整体性能影响最大；双代理配置显著提高性能。

Conclusion: 该研究评估了四个大型语言模型（LLM）解决数学问题的准确性，发现增强推理的OpenAI o1模型在算术、代数和数论三个类别中都取得了最高或接近完美的准确率。双代理配置也显著提高了整体性能。

Abstract: Large Language Models (LLMs) are increasingly utilized in AI-driven
educational instruction and assessment, particularly within mathematics
education. The capability of LLMs to generate accurate answers and detailed
solutions for math problem-solving tasks is foundational for ensuring reliable
and precise feedback and assessment in math education practices. Our study
focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,
DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including
arithmetic, algebra, and number theory, and identifies step-level reasoning
errors within their solutions. Instead of relying on standard benchmarks, we
intentionally build math tasks (via item models) that are challenging for LLMs
and prone to errors. The accuracy of final answers and the presence of errors
in individual solution steps were systematically analyzed and coded. Both
single-agent and dual-agent configurations were tested. It is observed that the
reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly
perfect accuracy across all three math task categories. Analysis of errors
revealed that procedural slips were the most frequent and significantly
impacted overall performance, while conceptual misunderstandings were less
frequent. Deploying dual-agent configurations substantially improved overall
performance. These findings offer actionable insights into enhancing LLM
performance and underscore effective strategies for integrating LLMs into
mathematics education, thereby advancing AI-driven instructional practices and
assessment precision.

</details>
