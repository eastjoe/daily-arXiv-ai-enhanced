{"id": "2509.10541", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10541", "abs": "https://arxiv.org/abs/2509.10541", "authors": ["V. Benes", "M. Svitek", "A. Michalikova", "M. Melicherik"], "title": "Situation Model of the Transport, Transport Emissions and Meteorological Conditions", "comment": null, "summary": "Air pollution in cities and the possibilities of reducing this pollution\nrepresents one of the most important factors that today's society has to deal\nwith. This paper focuses on a systemic approach to traffic emissions with their\nrelation to meteorological conditions, analyzing the effect of weather on the\nquantity and dispersion of traffic emissions in a city. Using fuzzy inference\nsystems (FIS) the model for prediction of changes in emissions depending on\nvarious conditions is developed. The proposed model is based on traffic,\nmeteorology and emission data measured in Prague, Czech Republic. The main\nobjective of the work is to provide insight into how urban planners and\npolicymakers can plan and manage urban transport more effectively with\nenvironmental protection in mind.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4ea4\u901a\u6392\u653e\u4e0e\u6c14\u8c61\u6761\u4ef6\u7684\u5173\u7cfb\uff0c\u5e76\u4f7f\u7528\u6a21\u7cca\u63a8\u7406\u7cfb\u7edf\u9884\u6d4b\u4e0d\u540c\u6761\u4ef6\u4e0b\u6392\u653e\u91cf\u7684\u53d8\u5316\uff0c\u65e8\u5728\u4e3a\u57ce\u5e02\u89c4\u5212\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u57ce\u5e02\u4ea4\u901a\u89c4\u5212\u548c\u7ba1\u7406\u65b9\u6cd5\u3002", "motivation": "\u57ce\u5e02\u7a7a\u6c14\u6c61\u67d3\u65e5\u76ca\u4e25\u91cd\uff0c\u4ea4\u901a\u6392\u653e\u662f\u91cd\u8981\u56e0\u7d20\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u65b9\u6cd5\u7814\u7a76\u5176\u4e0e\u6c14\u8c61\u6761\u4ef6\u7684\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u6a21\u7cca\u63a8\u7406\u7cfb\u7edf (FIS) \u5efa\u7acb\u4ea4\u901a\u6392\u653e\u9884\u6d4b\u6a21\u578b\uff0c\u57fa\u4e8e\u6377\u514b\u5e03\u62c9\u683c\u7684\u4ea4\u901a\u3001\u6c14\u8c61\u548c\u6392\u653e\u6570\u636e\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6a21\u7cca\u63a8\u7406\u7cfb\u7edf\u7684\u4ea4\u901a\u6392\u653e\u9884\u6d4b\u6a21\u578b\u3002", "conclusion": "\u8be5\u6a21\u578b\u53ef\u4ee5\u5e2e\u52a9\u57ce\u5e02\u89c4\u5212\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u66f4\u6709\u6548\u5730\u89c4\u5212\u548c\u7ba1\u7406\u57ce\u5e02\u4ea4\u901a\uff0c\u4ece\u800c\u66f4\u597d\u5730\u4fdd\u62a4\u73af\u5883\u3002"}}
{"id": "2509.10660", "categories": ["cs.AI", "cs.MA", "q-bio.CB"], "pdf": "https://arxiv.org/pdf/2509.10660", "abs": "https://arxiv.org/abs/2509.10660", "authors": ["Nam H. Le", "Patrick Erickson", "Yanbo Zhang", "Michael Levin", "Josh Bongard"], "title": "ZapGPT: Free-form Language Prompting for Simulated Cellular Control", "comment": null, "summary": "Human language is one of the most expressive tools for conveying intent, yet\nmost artificial or biological systems lack mechanisms to interpret or respond\nmeaningfully to it. Bridging this gap could enable more natural forms of\ncontrol over complex, decentralized systems. In AI and artificial life, recent\nwork explores how language can specify high-level goals, but most systems still\ndepend on engineered rewards, task-specific supervision, or rigid command sets,\nlimiting generalization to novel instructions. Similar constraints apply in\nsynthetic biology and bioengineering, where the locus of control is often\ngenomic rather than environmental perturbation.\n  A key open question is whether artificial or biological collectives can be\nguided by free-form natural language alone, without task-specific tuning or\ncarefully designed evaluation metrics. We provide one possible answer here by\nshowing, for the first time, that simple agents' collective behavior can be\nguided by free-form language prompts: one AI model transforms an imperative\nprompt into an intervention that is applied to simulated cells; a second AI\nmodel scores how well the prompt describes the resulting cellular dynamics; and\nthe former AI model is evolved to improve the scores generated by the latter.\n  Unlike previous work, our method does not require engineered fitness\nfunctions or domain-specific prompt design. We show that the evolved system\ngeneralizes to unseen prompts without retraining. By treating natural language\nas a control layer, the system suggests a future in which spoken or written\nprompts could direct computational, robotic, or biological systems to desired\nbehaviors. This work provides a concrete step toward this vision of AI-biology\npartnerships, in which language replaces mathematical objective functions,\nfixed rules, and domain-specific programming.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u5982\u4f55\u4ec5\u4f7f\u7528\u81ea\u7531\u683c\u5f0f\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6765\u5f15\u5bfc\u7b80\u5355\u667a\u80fd\u4f53\u7684\u96c6\u4f53\u884c\u4e3a\uff0c\u65e0\u9700\u7279\u5b9a\u4efb\u52a1\u7684\u8c03\u6574\u6216\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u5f25\u5408\u81ea\u7136\u8bed\u8a00\u4e0e\u4eba\u5de5/\u751f\u7269\u7cfb\u7edf\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u5bf9\u590d\u6742\u5206\u6563\u7cfb\u7edf\u7684\u66f4\u81ea\u7136\u63a7\u5236\u3002", "method": "\u4f7f\u7528\u4e24\u4e2aAI\u6a21\u578b\uff1a\u4e00\u4e2a\u5c06\u6307\u4ee4\u8f6c\u5316\u4e3a\u5bf9\u6a21\u62df\u7ec6\u80de\u7684\u5e72\u9884\uff0c\u53e6\u4e00\u4e2a\u8bc4\u4f30\u6307\u4ee4\u4e0e\u7ec6\u80de\u52a8\u6001\u7684\u5339\u914d\u7a0b\u5ea6\uff0c\u5e76\u901a\u8fc7\u8fdb\u5316\u6539\u8fdb\u7b2c\u4e00\u4e2a\u6a21\u578b\u4ee5\u63d0\u9ad8\u8bc4\u5206\u3002", "result": "\u8bc1\u660e\u8be5\u7cfb\u7edf\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u6307\u4ee4\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u672a\u6765\u5229\u7528\u8bed\u8a00\u6307\u5bfc\u8ba1\u7b97\u3001\u673a\u5668\u4eba\u6216\u751f\u7269\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\uff0c\u8bed\u8a00\u5c06\u53d6\u4ee3\u6570\u5b66\u76ee\u6807\u51fd\u6570\u3001\u56fa\u5b9a\u89c4\u5219\u548c\u7279\u5b9a\u9886\u57df\u7684\u7f16\u7a0b\u3002"}}
{"id": "2509.10704", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10704", "abs": "https://arxiv.org/abs/2509.10704", "authors": ["Xingchen Wan", "Han Zhou", "Ruoxi Sun", "Hootan Nakhost", "Ke Jiang", "Rajarishi Sinha", "Sercan \u00d6. Ar\u0131k"], "title": "Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration", "comment": "15 pages, 7 figures, 2 tables (22 pages, 9 figures and 3 tables\n  including references and appendices)", "summary": "Text-to-image (T2I) models, while offering immense creative potential, are\nhighly reliant on human intervention, posing significant usability challenges\nthat often necessitate manual, iterative prompt engineering over often\nunderspecified prompts. This paper introduces Maestro, a novel self-evolving\nimage generation system that enables T2I models to autonomously self-improve\ngenerated images through iterative evolution of prompts, using only an initial\nprompt. Maestro incorporates two key innovations: 1) self-critique, where\nspecialized multimodal LLM (MLLM) agents act as 'critics' to identify\nweaknesses in generated images, correct for under-specification, and provide\ninterpretable edit signals, which are then integrated by a 'verifier' agent\nwhile preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge\nfor head-to-head comparisons between iteratively generated images, eschewing\nproblematic images, and evolving creative prompt candidates that align with\nuser intents. Extensive experiments on complex T2I tasks using black-box models\ndemonstrate that Maestro significantly improves image quality over initial\nprompts and state-of-the-art automated methods, with effectiveness scaling with\nmore advanced MLLM components. This work presents a robust, interpretable, and\neffective pathway towards self-improving T2I generation.", "AI": {"tldr": "Maestro\u7cfb\u7edf\u901a\u8fc7\u8fed\u4ee3\u6539\u8fdb\u63d0\u793a\u8bcd\uff0c\u4f7f\u6587\u672c\u8f6c\u56fe\u50cf\u6a21\u578b\u80fd\u591f\u81ea\u4e3b\u6539\u8fdb\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u8f6c\u56fe\u50cf\u6a21\u578b\u4f9d\u8d56\u4eba\u5de5\u5e72\u9884\uff0cMaestro\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u81ea\u4e3b\u56fe\u50cf\u751f\u6210\u3002", "method": "Maestro\u7cfb\u7edf\u7ed3\u5408\u4e86\u81ea\u6211\u6279\u8bc4\u548c\u81ea\u6211\u8fdb\u5316\u673a\u5236\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLM)\u4f5c\u4e3a\u6279\u8bc4\u8005\u548c\u8bc4\u5224\u8005\uff0c\u8fed\u4ee3\u6539\u8fdb\u63d0\u793a\u8bcd\u5e76\u9009\u62e9\u6700\u4f73\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMaestro\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u50cf\u8d28\u91cf\uff0c\u4f18\u4e8e\u521d\u59cb\u63d0\u793a\u8bcd\u548c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "conclusion": "Maestro\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u4e14\u6709\u6548\u7684\u9014\u5f84\uff0c\u7528\u4e8e\u6539\u8fdb\u6587\u672c\u8f6c\u56fe\u50cf\u7684\u751f\u6210\u8fc7\u7a0b\u3002"}}
{"id": "2509.10707", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10707", "abs": "https://arxiv.org/abs/2509.10707", "authors": ["Sajjad Abdoli", "Rudi Cilibrasi", "Rima Al-Shikh"], "title": "Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions", "comment": null, "summary": "As AI systems increasingly evaluate other AI outputs, understanding their\nassessment behavior becomes crucial for preventing cascading biases. This study\nanalyzes vision-language descriptions generated by NVIDIA's Describe Anything\nModel and evaluated by three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to\nuncover distinct \"evaluation personalities\" the underlying assessment\nstrategies and biases each model demonstrates. GPT-4o-mini exhibits systematic\nconsistency with minimal variance, GPT-4o excels at error detection, while\nGPT-5 shows extreme conservatism with high variability. Controlled experiments\nusing Gemini 2.5 Pro as an independent question generator validate that these\npersonalities are inherent model properties rather than artifacts. Cross-family\nanalysis through semantic similarity of generated questions reveals significant\ndivergence: GPT models cluster together with high similarity while Gemini\nexhibits markedly different evaluation strategies. All GPT models demonstrate a\nconsistent 2:1 bias favoring negative assessment over positive confirmation,\nthough this pattern appears family-specific rather than universal across AI\narchitectures. These findings suggest that evaluation competence does not scale\nwith general capability and that robust AI assessment requires diverse\narchitectural perspectives.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u4e0d\u540cAI\u6a21\u578b\u5bf9\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u7684\u8bc4\u4f30\u7b56\u7565\u548c\u504f\u5dee\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5c55\u73b0\u51fa\u72ec\u7279\u7684\u201c\u8bc4\u4f30\u4e2a\u6027\u201d\uff0c\u8bc4\u4f30\u80fd\u529b\u5e76\u4e0d\u4e0e\u901a\u7528\u80fd\u529b\u6210\u6b63\u6bd4", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u65e5\u76ca\u4f9d\u8d56\u5176\u4ed6AI\u7cfb\u7edf\u8fdb\u884c\u8bc4\u4f30\uff0c\u7406\u89e3\u5176\u8bc4\u4f30\u884c\u4e3a\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u9632\u6b62\u504f\u5dee\u7d2f\u79ef", "method": "\u5206\u6790NVIDIA\u7684Describe Anything\u6a21\u578b\u751f\u6210\u7684\u89c6\u89c9\u8bed\u8a00\u63cf\u8ff0\uff0c\u5e76\u7528\u4e09\u4e2aGPT\u6a21\u578b(GPT-4o, GPT-4o-mini, GPT-5)\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u53d7\u63a7\u5b9e\u9a8c\u9a8c\u8bc1\u6a21\u578b\u7279\u6027", "result": "GPT-4o-mini\u8bc4\u4f30\u4e00\u81f4\u6027\u9ad8\uff0cGPT-4o\u64c5\u957f\u9519\u8bef\u68c0\u6d4b\uff0cGPT-5\u8bc4\u4f30\u4fdd\u5b88\u4e14\u53d8\u5f02\u6027\u9ad8\u3002GPT\u6a21\u578b\u95f4\u76f8\u4f3c\u5ea6\u9ad8\uff0c\u800cGemini\u8bc4\u4f30\u7b56\u7565\u663e\u8457\u4e0d\u540c\u3002\u6240\u6709GPT\u6a21\u578b\u90fd\u5b58\u5728\u8d1f\u9762\u8bc4\u4f30\u504f\u5411", "conclusion": "\u8bc4\u4f30\u80fd\u529b\u5e76\u4e0d\u4e0e\u901a\u7528\u80fd\u529b\u6210\u6b63\u6bd4\uff0c\u53ef\u9760\u7684AI\u8bc4\u4f30\u9700\u8981\u591a\u79cd\u67b6\u6784\u7684\u89c6\u89d2"}}
{"id": "2509.10762", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10762", "abs": "https://arxiv.org/abs/2509.10762", "authors": ["Arlen Kumar", "Leanid Palkhouski"], "title": "AI Answer Engine Citation Behavior An Empirical Analysis of the GEO16 Framework", "comment": null, "summary": "AI answer engines increasingly mediate access to domain knowledge by\ngenerating responses and citing web sources. We introduce GEO-16, a 16 pillar\nauditing framework that converts on page quality signals into banded pillar\nscores and a normalized GEO score G that ranges from 0 to 1. Using 70 product\nintent prompts, we collected 1,702 citations across three engines (Brave\nSummary, Google AI Overviews, and Perplexity) and audited 1,100 unique URLs. In\nour corpus, the engines differed in the GEO quality of the pages they cited,\nand pillars related to Metadata and Freshness, Semantic HTML, and Structured\nData showed the strongest associations with citation. Logistic models with\ndomain clustered standard errors indicate that overall page quality is a strong\npredictor of citation, and simple operating points (for example, G at least\n0.70 combined with at least 12 pillar hits) align with substantially higher\ncitation rates in our data. We report per engine contrasts, vertical effects,\nthreshold analysis, and diagnostics, then translate findings into a practical\nplaybook for publishers. The study is observational and focuses on English\nlanguage B2B SaaS pages; we discuss limitations, threats to validity, and\nreproducibility considerations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86GEO-16\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u5f15\u64ce\u5f15\u7528\u7684\u7f51\u9875\u8d28\u91cf\uff0c\u5e76\u53d1\u73b0\u7f51\u9875\u8d28\u91cf\u662fAI\u5f15\u64ce\u8fdb\u884c\u5f15\u7528\u7684\u5f3a\u9884\u6d4b\u6307\u6807\u3002", "motivation": "AI\u5f15\u64ce\u65e5\u76ca\u6210\u4e3a\u83b7\u53d6\u9886\u57df\u77e5\u8bc6\u7684\u4e3b\u8981\u9014\u5f84\uff0c\u4f46\u5176\u5f15\u7528\u7f51\u9875\u7684\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u6784\u5efa\u4e86GEO-16\u6846\u67b6\uff0c\u6536\u96c6\u4e86\u4e09\u4e2aAI\u5f15\u64ce(Brave Summary, Google AI Overviews, and Perplexity)\u5bf970\u4e2a\u4ea7\u54c1\u610f\u56fe\u63d0\u793a\u76841702\u4e2a\u5f15\u7528\uff0c\u5e76\u5ba1\u6838\u4e861100\u4e2aURL\uff0c\u4f7f\u7528\u903b\u8f91\u6a21\u578b\u5206\u6790\u7f51\u9875\u8d28\u91cf\u4e0e\u88ab\u5f15\u7528\u7387\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u5143\u6570\u636e\u3001\u65b0\u9c9c\u5ea6\u3001\u8bed\u4e49HTML\u548c\u7ed3\u6784\u5316\u6570\u636e\u7b49\u65b9\u9762\u4e0e\u88ab\u5f15\u7528\u7387\u9ad8\u5ea6\u76f8\u5173\uff1b\u7f51\u9875\u6574\u4f53\u8d28\u91cf\u662f\u5f15\u7528\u7387\u7684\u5f3a\u9884\u6d4b\u6307\u6807\uff1b\u8bbe\u5b9a\u7b80\u5355\u7684\u9608\u503c(\u4f8b\u5982G\u81f3\u5c110.70\uff0c\u4e14\u81f3\u5c1112\u4e2apillar hits)\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5f15\u7528\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u51fa\u7248\u5546\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u6307\u5357\uff0c\u4f46\u540c\u65f6\u4e5f\u6307\u51fa\u7814\u7a76\u7684\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u4ec5\u5173\u6ce8\u82f1\u8bedB2B SaaS\u9875\u9762\u3002"}}
{"id": "2509.10769", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.10769", "abs": "https://arxiv.org/abs/2509.10769", "authors": ["Tara Bogavelli", "Roshnee Sharma", "Hari Subramani"], "title": "AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise", "comment": null, "summary": "While individual components of agentic architectures have been studied in\nisolation, there remains limited empirical understanding of how different\ndesign dimensions interact within complex multi-agent systems. This study aims\nto address these gaps by providing a comprehensive enterprise-specific\nbenchmark evaluating 18 distinct agentic configurations across state-of-the-art\nlarge language models. We examine four critical agentic system dimensions:\norchestration strategy, agent prompt implementation (ReAct versus function\ncalling), memory architecture, and thinking tool integration. Our benchmark\nreveals significant model-specific architectural preferences that challenge the\nprevalent one-size-fits-all paradigm in agentic AI systems. It also reveals\nsignificant weaknesses in overall agentic performance on enterprise tasks with\nthe highest scoring models achieving a maximum of only 35.3\\% success on the\nmore complex task and 70.8\\% on the simpler task. We hope these findings inform\nthe design of future agentic systems by enabling more empirically backed\ndecisions regarding architectural components and model selection.", "AI": {"tldr": "\u7814\u7a76\u4e8618\u79cd\u4e0d\u540cagent\u914d\u7f6e\u5728\u590d\u6742\u591aagent\u7cfb\u7edf\u4e2d\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u53d1\u73b0\u6700\u4f73\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u4ec5\u8fbe\u523035.3%\u7684\u6210\u529f\u7387\uff0c\u6311\u6218\u4e86'\u4e00\u5200\u5207'\u7684agent AI\u7cfb\u7edf\u8bbe\u8ba1\u8303\u5f0f\u3002", "motivation": "\u7f3a\u4e4f\u5bf9\u590d\u6742\u591aagent\u7cfb\u7edf\u4e2d\u4e0d\u540c\u8bbe\u8ba1\u7ef4\u5ea6\u4ea4\u4e92\u7684\u7ecf\u9a8c\u6027\u7406\u89e3\u3002", "method": "\u5bf918\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684agent\u914d\u7f6e\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8003\u5bdf\u4e86\u7f16\u6392\u7b56\u7565\u3001agent\u63d0\u793a\u5b9e\u73b0\u3001\u5185\u5b58\u67b6\u6784\u548c\u601d\u7ef4\u5de5\u5177\u96c6\u6210\u56db\u4e2a\u7ef4\u5ea6\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u67b6\u6784\u504f\u597d\uff0c\u5e76\u4e14agent\u7cfb\u7edf\u5728\u4f01\u4e1a\u4efb\u52a1\u4e0a\u7684\u6574\u4f53\u6027\u80fd\u8f83\u5f31\uff0c\u6700\u9ad8\u6210\u529f\u7387\u4ec5\u4e3a35.3%\uff08\u590d\u6742\u4efb\u52a1\uff09\u548c70.8%\uff08\u7b80\u5355\u4efb\u52a1\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u53ef\u4e3a\u672a\u6765agent\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u66f4\u5177\u7ecf\u9a8c\u4f9d\u636e\u7684\u51b3\u7b56\uff0c\u5173\u4e8e\u67b6\u6784\u7ec4\u4ef6\u548c\u6a21\u578b\u9009\u62e9\u3002"}}
{"id": "2509.10818", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.10818", "abs": "https://arxiv.org/abs/2509.10818", "authors": ["Boris Kovalerchuk", "Brent D. Fegley"], "title": "LLM Enhancement with Domain Expert Mental Model to Reduce LLM Hallucination with Causal Prompt Engineering", "comment": "25 pages,4 figures, 2 tables", "summary": "Difficult decision-making problems abound in various disciplines and domains.\nThe proliferation of generative techniques, especially large language models\n(LLMs), has excited interest in using them for decision support. However, LLMs\ncannot yet resolve missingness in their training data, leading to\nhallucinations. Retrieval-Augmented Generation (RAG) enhances LLMs by\nincorporating external information retrieval, reducing hallucinations and\nimproving accuracy. Yet, RAG and related methods are only partial solutions, as\nthey may lack access to all necessary sources or key missing information. Even\neveryday issues often challenge LLMs' abilities. Submitting longer prompts with\ncontext and examples is one approach to address knowledge gaps, but designing\neffective prompts is non-trivial and may not capture complex mental models of\ndomain experts. For tasks with missing critical information, LLMs are\ninsufficient, as are many existing systems poorly represented in available\ndocuments. This paper explores how LLMs can make decision-making more\nefficient, using a running example of evaluating whether to respond to a call\nfor proposals. We propose a technology based on optimized human-machine\ndialogue and monotone Boolean and k-valued functions to discover a\ncomputationally tractable personal expert mental model (EMM) of\ndecision-making. Our EMM algorithm for LLM prompt engineering has four steps:\n(1) factor identification, (2) hierarchical structuring of factors, (3)\ngenerating a generalized expert mental model specification, and (4) generating\na detailed generalized expert mental model from that specification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u4eba\u673a\u5bf9\u8bdd\u548c\u5355\u8c03\u5e03\u5c14/k\u503c\u51fd\u6570\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u4e13\u5bb6\u51b3\u7b56\u6a21\u578b\uff0c\u4ece\u800c\u6539\u8fdbLLM\u5728\u51b3\u7b56\u652f\u6301\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3LLM\u5728\u5904\u7406\u4fe1\u606f\u7f3a\u5931\u95ee\u9898\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709LLM\u53caRAG\u65b9\u6cd5\u5728\u5904\u7406\u51b3\u7b56\u95ee\u9898\u65f6\uff0c\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u6216\u4fe1\u606f\u7f3a\u5931\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u51b3\u7b56\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u56db\u6b65\u6cd5\uff1a\u56e0\u7d20\u8bc6\u522b\u3001\u5206\u5c42\u7ed3\u6784\u5316\u3001\u751f\u6210\u901a\u7528\u4e13\u5bb6\u6a21\u578b\u89c4\u8303\u548c\u751f\u6210\u8be6\u7ec6\u7684\u901a\u7528\u4e13\u5bb6\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u4f18\u5316\u4eba\u673a\u5bf9\u8bdd\u548c\u5355\u8c03\u5e03\u5c14/k\u503c\u51fd\u6570\uff0c\u65e8\u5728\u6784\u5efa\u8ba1\u7b97\u53ef\u5904\u7406\u7684\u4e2a\u4eba\u4e13\u5bb6\u5fc3\u667a\u6a21\u578b\uff08EMM\uff09\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u6709\u6548\u5229\u7528LLM\u8fdb\u884c\u51b3\u7b56\u652f\u6301\u7684\u4e13\u5bb6\u5fc3\u667a\u6a21\u578b\uff08EMM\uff09\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u901a\u8fc7\u4f18\u5316\u4eba\u673a\u5bf9\u8bdd\u548c\u5229\u7528\u5355\u8c03\u5e03\u5c14/k\u503c\u51fd\u6570\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86LLM\u5728\u5904\u7406\u4fe1\u606f\u7f3a\u5931\u548c\u590d\u6742\u51b3\u7b56\u95ee\u9898\u4e0a\u7684\u4e0d\u8db3\uff0c\u4e3a\u63d0\u9ad8LLM\u5728\u51b3\u7b56\u652f\u6301\u4e2d\u7684\u6548\u7387\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.10837", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10837", "abs": "https://arxiv.org/abs/2509.10837", "authors": ["Yuyin Lu", "Hegang Chen", "Yanghui Rao"], "title": "From Grounding to Skolemization: A Logic-Constrained Vector Symbolic Architecture for Complex Query Answering", "comment": null, "summary": "Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs),\ntypically formalized as reasoning with Existential First-Order predicate logic\nwith one free variable (EFO$_1$), faces a fundamental trade-off between logical\nsoundness and computational efficiency. This work establishes the\nGrounding-Skolemization dichotomy for systematically analyzing CQA methods\nthrough the lens of formal logic. While Grounding-based methods inherently\nsuffer from combinatorial explosion, most Skolemization-based methods neglect\nto explicitly model Skolem functions and compromise logical consistency. To\naddress these limitations, we propose the Logic-constrained Vector Symbolic\nArchitecture (LVSA), a neuro-symbolic framework that unifies a differentiable\nSkolemization module and a neural negator, as well as a logical\nconstraint-driven optimization protocol to harmonize geometric and logical\nrequirements. Theoretically, LVSA guarantees universality for all EFO$_1$\nqueries. Empirically, it outperforms state-of-the-art Skolemization-based\nmethods and reduces inference costs by orders of magnitude compared to\nGrounding-based baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aLVSA\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4e0d\u5b8c\u6574\u77e5\u8bc6\u56fe\u8c31\u4e0a\u7684\u590d\u6742\u67e5\u8be2\u95ee\u9898\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u53ef\u5fae\u7684Skolem\u5316\u6a21\u5757\u548c\u795e\u7ecf\u5426\u5b9a\u5668\uff0c\u5e76\u5728\u7406\u8bba\u4e0a\u4fdd\u8bc1\u4e86\u5bf9\u6240\u6709EFO$_1$\u67e5\u8be2\u7684\u666e\u9002\u6027\uff0c\u540c\u65f6\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u903b\u8f91\u5b8c\u6574\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0cGrounding-based\u65b9\u6cd5\u9762\u4e34\u7ec4\u5408\u7206\u70b8\u95ee\u9898\uff0c\u800cSkolemization-based\u65b9\u6cd5\u5219\u53ef\u80fd\u727a\u7272\u903b\u8f91\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faLogic-constrained Vector Symbolic Architecture (LVSA)\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u5faeSkolem\u5316\u6a21\u5757\u3001\u795e\u7ecf\u5426\u5b9a\u5668\u548c\u903b\u8f91\u7ea6\u675f\u4f18\u5316\u534f\u8bae\u3002", "result": "LVSA\u5728\u7406\u8bba\u4e0a\u4fdd\u8bc1\u4e86\u5bf9\u6240\u6709EFO$_1$\u67e5\u8be2\u7684\u666e\u9002\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709Skolemization-based\u65b9\u6cd5\uff0c\u4e14\u63a8\u7406\u6210\u672c\u8fdc\u4f4e\u4e8eGrounding-based\u65b9\u6cd5\u3002", "conclusion": "LVSA\u6846\u67b6\u6709\u6548\u5730\u89e3\u51b3\u4e86\u4e0d\u5b8c\u6574\u77e5\u8bc6\u56fe\u8c31\u4e0a\u590d\u6742\u67e5\u8be2\u95ee\u9898\u7684\u6548\u7387\u548c\u903b\u8f91\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2509.10875", "categories": ["cs.AI", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2509.10875", "abs": "https://arxiv.org/abs/2509.10875", "authors": ["Jesse Gardner", "Vladimir A. Baulin"], "title": "Is the `Agent' Paradigm a Limiting Framework for Next-Generation Intelligent Systems?", "comment": null, "summary": "The concept of the 'agent' has profoundly shaped Artificial Intelligence (AI)\nresearch, guiding development from foundational theories to contemporary\napplications like Large Language Model (LLM)-based systems. This paper\ncritically re-evaluates the necessity and optimality of this agent-centric\nparadigm. We argue that its persistent conceptual ambiguities and inherent\nanthropocentric biases may represent a limiting framework. We distinguish\nbetween agentic systems (AI inspired by agency, often semi-autonomous, e.g.,\nLLM-based agents), agential systems (fully autonomous, self-producing systems,\ncurrently only biological), and non-agentic systems (tools without the\nimpression of agency). Our analysis, based on a systematic review of relevant\nliterature, deconstructs the agent paradigm across various AI frameworks,\nhighlighting challenges in defining and measuring properties like autonomy and\ngoal-directedness. We argue that the 'agentic' framing of many AI systems,\nwhile heuristically useful, can be misleading and may obscure the underlying\ncomputational mechanisms, particularly in Large Language Models (LLMs). As an\nalternative, we propose a shift in focus towards frameworks grounded in\nsystem-level dynamics, world modeling, and material intelligence. We conclude\nthat investigating non-agentic and systemic frameworks, inspired by complex\nsystems, biology, and unconventional computing, is essential for advancing\ntowards robust, scalable, and potentially non-anthropomorphic forms of general\nintelligence. This requires not only new architectures but also a fundamental\nreconsideration of our understanding of intelligence itself, moving beyond the\nagent metaphor.", "AI": {"tldr": "\u672c\u6587\u6279\u5224\u6027\u5730\u8bc4\u4f30\u4e86\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u4e2d\u4ee5\u4ee3\u7406\u4e3a\u4e2d\u5fc3\u7684\u8303\u5f0f\uff0c\u8ba4\u4e3a\u5176\u6982\u5ff5\u6a21\u7cca\u548c\u4eba\u7c7b\u4e2d\u5fc3\u4e3b\u4e49\u504f\u5dee\u53ef\u80fd\u5177\u6709\u5c40\u9650\u6027\uff0c\u5efa\u8bae\u8f6c\u5411\u57fa\u4e8e\u7cfb\u7edf\u52a8\u529b\u5b66\u3001\u4e16\u754c\u5efa\u6a21\u548c\u7269\u8d28\u667a\u80fd\u7684\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u4e2d\u666e\u904d\u5b58\u5728\u7684\u4ee5\u4ee3\u7406\u4e3a\u4e2d\u5fc3\u7684\u8303\u5f0f\u5b58\u5728\u6982\u5ff5\u6a21\u7cca\u548c\u4eba\u7c7b\u4e2d\u5fc3\u4e3b\u4e49\u504f\u5dee\u7b49\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u76f8\u5173\u6587\u732e\uff0c\u5206\u6790\u4e0d\u540c\u4eba\u5de5\u667a\u80fd\u6846\u67b6\u4e0b\u7684\u4ee3\u7406\u8303\u5f0f\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u7cfb\u7edf\u52a8\u529b\u5b66\u3001\u4e16\u754c\u5efa\u6a21\u548c\u7269\u8d28\u667a\u80fd\u7684\u66ff\u4ee3\u6846\u67b6\u3002", "result": "\u6307\u51fa\u4e86\u4ee5\u4ee3\u7406\u4e3a\u4e2d\u5fc3\u7684\u8303\u5f0f\u5728\u5b9a\u4e49\u548c\u8861\u91cf\u81ea\u4e3b\u6027\u548c\u76ee\u6807\u5bfc\u5411\u6027\u7b49\u5c5e\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u5e76\u5efa\u8bae\u8f6c\u5411\u975e\u4ee3\u7406\u548c\u7cfb\u7edf\u6846\u67b6\u3002", "conclusion": "\u4e3a\u4e86\u53d1\u5c55\u5065\u58ee\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u80fd\u975e\u4eba\u7c7b\u4e2d\u5fc3\u7684\u4e00\u822c\u667a\u80fd\u5f62\u5f0f\uff0c\u9700\u8981\u5bf9\u667a\u80fd\u7684\u7406\u89e3\u8fdb\u884c\u6839\u672c\u6027\u53cd\u601d\uff0c\u8d85\u8d8a\u4ee3\u7406\u9690\u55bb\u3002"}}
{"id": "2509.10931", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10931", "abs": "https://arxiv.org/abs/2509.10931", "authors": ["Seongho Joo", "Hyukhun Koh", "Kyomin Jung"], "title": "Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding", "comment": "EMNLP 2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, but their potential misuse for harmful purposes remains a\nsignificant concern. To strengthen defenses against such vulnerabilities, it is\nessential to investigate universal jailbreak attacks that exploit intrinsic\nweaknesses in the architecture and learning paradigms of LLMs. In response, we\npropose \\textbf{H}armful \\textbf{P}rompt \\textbf{La}undering (HaPLa), a novel\nand broadly applicable jailbreaking technique that requires only black-box\naccess to target models. HaPLa incorporates two primary strategies: 1)\n\\textit{abductive framing}, which instructs LLMs to infer plausible\nintermediate steps toward harmful activities, rather than directly responding\nto explicit harmful queries; and 2) \\textit{symbolic encoding}, a lightweight\nand flexible approach designed to obfuscate harmful content, given that current\nLLMs remain sensitive primarily to explicit harmful keywords. Experimental\nresults show that HaPLa achieves over 95% attack success rate on GPT-series\nmodels and 70% across all targets. Further analysis with diverse symbolic\nencoding rules also reveals a fundamental challenge: it remains difficult to\nsafely tune LLMs without significantly diminishing their helpfulness in\nresponding to benign queries.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHaPLa\u7684\u65b0\u578b\u653b\u51fb\u6280\u672f\uff0c\u53ef\u7ed5\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u673a\u5236\uff0c\u6210\u529f\u7387\u9ad8\u8fbe95%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u88ab\u6076\u610f\u5229\u7528\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u9632\u5fa1\u673a\u5236\u3002", "method": "HaPLa\u7ed3\u5408\u4e86\u6f14\u7ece\u6846\u67b6\u548c\u7b26\u53f7\u7f16\u7801\u4e24\u79cd\u7b56\u7565\uff0c\u524d\u8005\u5f15\u5bfc\u6a21\u578b\u63a8\u65ad\u6709\u5bb3\u884c\u4e3a\u7684\u4e2d\u95f4\u6b65\u9aa4\uff0c\u540e\u8005\u6a21\u7cca\u6709\u5bb3\u5185\u5bb9\u3002", "result": "HaPLa\u5728GPT\u7cfb\u5217\u6a21\u578b\u4e0a\u6210\u529f\u7387\u8d85\u8fc795%\uff0c\u5176\u4ed6\u6a21\u578b\u4e5f\u8fbe\u523070%\u3002", "conclusion": "\u96be\u4ee5\u5b89\u5168\u5730\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u5176\u65e2\u80fd\u62b5\u5fa1\u653b\u51fb\u53c8\u80fd\u4fdd\u6301\u6709\u7528\u6027\u3002"}}
{"id": "2509.10932", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10932", "abs": "https://arxiv.org/abs/2509.10932", "authors": ["Seongho Joo", "Hyukhun Koh", "Kyomin Jung"], "title": "Public Data Assisted Differentially Private In-Context Learning", "comment": "EMNLP 2025 Findings", "summary": "In-context learning (ICL) in Large Language Models (LLMs) has shown\nremarkable performance across various tasks without requiring fine-tuning.\nHowever, recent studies have highlighted the risk of private data leakage\nthrough the prompt in ICL, especially when LLMs are exposed to malicious\nattacks. While differential privacy (DP) provides strong privacy guarantees, it\noften significantly reduces the utility of in-context learning (ICL). To\naddress this challenge, we incorporate task-related public data into the ICL\nframework while maintaining the DP guarantee. Based on this approach, we\npropose a private in-context learning algorithm that effectively balances\nprivacy protection and model utility. Through experiments, we demonstrate that\nour approach significantly improves the utility of private ICL with the\nassistance of public data. Additionally, we show that our method is robust\nagainst membership inference attacks, demonstrating empirical privacy\nprotection.", "AI": {"tldr": "\u5229\u7528\u516c\u5171\u6570\u636e\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u79c1\u6709\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u548c\u6a21\u578b\u6548\u7528\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u79c1\u6709\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\uff0c\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6548\u7528\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "method": "\u5c06\u4efb\u52a1\u76f8\u5173\u7684\u516c\u5171\u6570\u636e\u6574\u5408\u5230\u79c1\u6709\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u79c1\u6709\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u79c1\u6709\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u6548\u7528\uff0c\u5e76\u5bf9\u6210\u5458\u63a8\u65ad\u653b\u51fb\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6709\u6548\u5e73\u8861\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u79c1\u6709\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u6548\u7528\u3002"}}
{"id": "2509.10972", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10972", "abs": "https://arxiv.org/abs/2509.10972", "authors": ["Ron Sun"], "title": "Enhancing Computational Cognitive Architectures with LLMs: A Case Study", "comment": null, "summary": "Computational cognitive architectures are broadly scoped models of the human\nmind that combine different psychological functionalities (as well as often\ndifferent computational methods for these different functionalities) into one\nunified framework. They structure them in a psychologically plausible and\nvalidated way. However, such models thus far have only limited computational\ncapabilities, mostly limited by the computational tools and techniques that\nwere adopted. More recently, LLMs have proved to be more capable\ncomputationally than any other tools. Thus, in order to deal with both\nreal-world complexity and psychological realism at the same time, incorporating\nLLMs into cognitive architectures naturally becomes an important task. In the\npresent article, a synergistic combination of the Clarion cognitive\narchitecture and LLMs is discussed as a case study. The implicit-explicit\ndichotomy that is fundamental to Clarion is leveraged for a seamless\nintegration of Clarion and LLMs. As a result, computational power of LLMs is\ncombined with psychological nicety of Clarion.", "AI": {"tldr": "\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6574\u5408\u5230\u8ba4\u77e5\u67b6\u6784\u4e2d\u4ee5\u63d0\u9ad8\u5176\u8ba1\u7b97\u80fd\u529b\u548c\u5fc3\u7406\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8ba4\u77e5\u67b6\u6784\u8ba1\u7b97\u80fd\u529b\u6709\u9650\uff0cLLM\u5177\u6709\u66f4\u9ad8\u7684\u8ba1\u7b97\u80fd\u529b\uff0c\u56e0\u6b64\u5c06\u4e24\u8005\u7ed3\u5408\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u6027\u548c\u5fc3\u7406\u771f\u5b9e\u6027\u3002", "method": "\u5c06Clarion\u8ba4\u77e5\u67b6\u6784\u4e0eLLM\u76f8\u7ed3\u5408\uff0c\u5229\u7528Clarion\u7684\u9690\u5f0f-\u663e\u5f0f\u4e8c\u5206\u6cd5\u5b9e\u73b0\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u7ed3\u5408\u4e86LLM\u7684\u8ba1\u7b97\u80fd\u529b\u548cClarion\u7684\u5fc3\u7406\u7cbe\u786e\u6027\u3002", "conclusion": "\u5c06LLM\u6574\u5408\u5230\u8ba4\u77e5\u67b6\u6784\u4e2d\u662f\u63d0\u9ad8\u5176\u80fd\u529b\u7684\u91cd\u8981\u9014\u5f84\uff0cClarion\u548cLLM\u7684\u7ed3\u5408\u662f\u4e00\u4e2a\u6210\u529f\u7684\u6848\u4f8b\u3002"}}
{"id": "2509.11026", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11026", "abs": "https://arxiv.org/abs/2509.11026", "authors": ["Ziang Li", "Manasi Ganti", "Zixian Ma", "Helena Vasconcelos", "Qijia He", "Ranjay Krishna"], "title": "Rethinking Human Preference Evaluation of LLM Rationales", "comment": "Published in the XLLM-Reason-Plan Workshop on the Application of LLM\n  Explainability to Reasoning and Planning at COLM 2025", "summary": "Large language models (LLMs) often generate natural language rationales --\nfree-form explanations that help improve performance on complex reasoning tasks\nand enhance interpretability for human users. However, evaluating these\nrationales remains challenging. While recent work has relied on binary\npreference judgments from humans or LLM judges, such evaluations are often\nopaque and coarse-grained, offering limited insight into what makes one\nrationale better than another. In this work, we rethink preference evaluation\nfor LLM-generated rationales by asking: (1) What attributes define good\nrationales? (2) Can human preferences be explained by these attributes? (3) Can\nattribute-based evaluation overcome the limitations of binary comparisons? We\nidentify a set of key rationale attributes from prior literature and assess\nthem using automatic metrics, LLM judgments, and human annotations. We then\nanalyze two standard human preference datasets MT Bench and Chatbot Arena using\nSHAP to identify which attributes best explain human preference outcomes.\nFinally, we re-evaluate model-generated rationales using attribute-specific ELO\nscores, revealing more nuanced model comparisons and insights. Our findings\nsuggest that fine-grained attribute evaluations can better characterize\nrationale quality and guide future research toward more interpretable and\nreliable evaluation practices.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u601d\u8003\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u751f\u6210\u7406\u7531\u7684\u8bc4\u4ef7\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u5c5e\u6027\u3001\u81ea\u52a8\u6307\u6807\u3001LLM\u5224\u65ad\u548c\u4eba\u5de5\u6807\u6ce8\u8bc4\u4f30\u5c5e\u6027\uff0c\u5e76\u4f7f\u7528SHAP\u5206\u6790\u4eba\u7c7b\u504f\u597d\uff0c\u6700\u7ec8\u7528\u5c5e\u6027\u7279\u5b9aELO\u8bc4\u5206\u91cd\u65b0\u8bc4\u4f30\u6a21\u578b\u751f\u6210\u7684\u7406\u7531\uff0c\u53d1\u73b0\u7ec6\u7c92\u5ea6\u5c5e\u6027\u8bc4\u4f30\u80fd\u66f4\u597d\u5730\u523b\u753b\u7406\u7531\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709LLM\u751f\u6210\u7406\u7531\u7684\u8bc4\u4ef7\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u96be\u4ee5\u6df1\u5165\u7406\u89e3\u4e0d\u540c\u7406\u7531\u7684\u4f18\u52a3\u3002", "method": "\u8bc6\u522b\u5173\u952e\u7406\u7531\u5c5e\u6027\uff0c\u4f7f\u7528\u81ea\u52a8\u6307\u6807\u3001LLM\u5224\u65ad\u548c\u4eba\u5de5\u6807\u6ce8\u8fdb\u884c\u8bc4\u4f30\uff0c\u5229\u7528SHAP\u5206\u6790\u4eba\u7c7b\u504f\u597d\uff0c\u5e76\u4f7f\u7528\u5c5e\u6027\u7279\u5b9aELO\u8bc4\u5206\u91cd\u65b0\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u7ec6\u7c92\u5ea6\u5c5e\u6027\u8bc4\u4f30\u80fd\u66f4\u597d\u5730\u523b\u753b\u7406\u7531\u8d28\u91cf\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u66f4\u5177\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u5c5e\u6027\u7684\u8bc4\u4f30\u4f18\u4e8e\u4e8c\u5143\u6bd4\u8f83\uff0c\u4e3aLLM\u751f\u6210\u7406\u7531\u7684\u8bc4\u4ef7\u63d0\u4f9b\u4e86\u66f4\u7ec6\u81f4\u7684\u5206\u6790\u548c\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2509.11035", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.11035", "abs": "https://arxiv.org/abs/2509.11035", "authors": ["Yu Cui", "Hang Fu", "Haibin Zhang", "Licheng Wang", "Cong Zuo"], "title": "Free-MAD: Consensus-Free Multi-Agent Debate", "comment": null, "summary": "Multi-agent debate (MAD) is an emerging approach to improving the reasoning\ncapabilities of large language models (LLMs). Existing MAD methods rely on\nmultiple rounds of interaction among agents to reach consensus, and the final\noutput is selected by majority voting in the last round. However, this\nconsensus-based design faces several limitations. First, multiple rounds of\ncommunication increases token overhead and limits scalability. Second, due to\nthe inherent conformity of LLMs, agents that initially produce correct\nresponses may be influenced by incorrect ones during the debate process,\ncausing error propagation. Third, majority voting introduces randomness and\nunfairness in the decision-making phase, and can degrade the reasoning\nperformance.\n  To address these issues, we propose \\textsc{Free-MAD}, a novel MAD framework\nthat eliminates the need for consensus among agents. \\textsc{Free-MAD}\nintroduces a novel score-based decision mechanism that evaluates the entire\ndebate trajectory rather than relying on the last round only. This mechanism\ntracks how each agent's reasoning evolves, enabling more accurate and fair\noutcomes. In addition, \\textsc{Free-MAD} reconstructs the debate phase by\nintroducing anti-conformity, a mechanism that enables agents to mitigate\nexcessive influence from the majority. Experiments on eight benchmark datasets\ndemonstrate that \\textsc{Free-MAD} significantly improves reasoning performance\nwhile requiring only a single-round debate and thus reducing token costs. We\nalso show that compared to existing MAD approaches, \\textsc{Free-MAD} exhibits\nimproved robustness in real-world attack scenarios.", "AI": {"tldr": "Free-MAD \u662f\u4e00\u79cd\u6539\u8fdb\u591a\u667a\u80fd\u4f53\u8fa9\u8bba (MAD) \u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u57fa\u4e8e\u8bc4\u5206\u7684\u51b3\u7b56\u673a\u5236\u548c\u6297\u4ece\u4f17\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684 MAD \u65b9\u6cd5\u5b58\u5728\u591a\u8f6e\u901a\u4fe1\u3001\u9519\u8bef\u4f20\u64ad\u548c\u6295\u7968\u4e0d\u516c\u5e73\u7b49\u95ee\u9898\u3002", "method": "Free-MAD \u6846\u67b6\u901a\u8fc7\u57fa\u4e8e\u8bc4\u5206\u7684\u51b3\u7b56\u673a\u5236\u8bc4\u4f30\u6574\u4e2a\u8fa9\u8bba\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165\u6297\u4ece\u4f17\u673a\u5236\u6765\u51cf\u5c11\u591a\u6570\u610f\u89c1\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u5b9e\u73b0\u5355\u8f6e\u8fa9\u8bba\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFree-MAD \u5728\u516b\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "Free-MAD \u662f\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.11067", "categories": ["cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.11067", "abs": "https://arxiv.org/abs/2509.11067", "authors": ["Liangxuan Guo", "Bin Zhu", "Qingqian Tao", "Kangning Liu", "Xun Zhao", "Xianzhe Qin", "Jin Gao", "Guangfu Hao"], "title": "Agentic Lybic: Multi-Agent Execution System with Tiered Reasoning and Orchestration", "comment": null, "summary": "Autonomous agents for desktop automation struggle with complex multi-step\ntasks due to poor coordination and inadequate quality control. We introduce\n\\textsc{Agentic Lybic}, a novel multi-agent system where the entire\narchitecture operates as a finite-state machine (FSM). This core innovation\nenables dynamic orchestration. Our system comprises four components: a\nController, a Manager, three Workers (Technician for code-based operations,\nOperator for GUI interactions, and Analyst for decision support), and an\nEvaluator. The critical mechanism is the FSM-based routing between these\ncomponents, which provides flexibility and generalization by dynamically\nselecting the optimal execution strategy for each subtask. This principled\norchestration, combined with robust quality gating, enables adaptive replanning\nand error recovery. Evaluated officially on the OSWorld benchmark,\n\\textsc{Agentic Lybic} achieves a state-of-the-art 57.07\\% success rate in 50\nsteps, substantially outperforming existing methods. Results demonstrate that\nprincipled multi-agent orchestration with continuous quality control provides\nsuperior reliability for generalized desktop automation in complex computing\nenvironments.", "AI": {"tldr": "Agentic Lybic\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6709\u9650\u72b6\u6001\u673a\u7684\u591aAgent\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u534f\u8c03\u548c\u8d28\u91cf\u63a7\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u684c\u9762\u81ea\u52a8\u5316\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u684c\u9762\u81ea\u52a8\u5316Agent\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u591a\u6b65\u4efb\u52a1\uff0c\u534f\u8c03\u6027\u5dee\uff0c\u8d28\u91cf\u63a7\u5236\u4e0d\u8db3\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u63a7\u5236\u5668\u3001\u7ba1\u7406\u5668\u3001\u4e09\u4e2a\u5de5\u4f5c\u5355\u5143\uff08\u6280\u672f\u5458\u3001\u64cd\u4f5c\u5458\u3001\u5206\u6790\u5458\uff09\u548c\u8bc4\u4f30\u5668\u7684\u591aAgent\u7cfb\u7edf\uff0c\u7cfb\u7edf\u6574\u4f53\u8fd0\u884c\u4e3a\u6709\u9650\u72b6\u6001\u673a\uff0c\u5b9e\u73b0\u52a8\u6001\u4efb\u52a1\u5206\u914d\u548c\u6267\u884c\u7b56\u7565\u9009\u62e9\u3002", "result": "\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgentic Lybic\u53d6\u5f97\u4e8657.07%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u6709\u9650\u72b6\u6001\u673a\u7684\u591aAgent\u534f\u540c\u548c\u6301\u7eed\u8d28\u91cf\u63a7\u5236\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u590d\u6742\u8ba1\u7b97\u73af\u5883\u4e0b\u684c\u9762\u81ea\u52a8\u5316\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.11068", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11068", "abs": "https://arxiv.org/abs/2509.11068", "authors": ["Zan-Kai Chong", "Hiroyuki Ohsaki", "Bryan Ng"], "title": "Tractable Asymmetric Verification for Large Language Models via Deterministic Replicability", "comment": null, "summary": "The landscape of Large Language Models (LLMs) shifts rapidly towards dynamic,\nmulti-agent systems. This introduces a fundamental challenge in establishing\ncomputational trust, specifically how one agent can verify that another's\noutput was genuinely produced by a claimed LLM, and not falsified or generated\nby a cheaper or inferior model. To address this challenge, this paper proposes\na verification framework that achieves tractable asymmetric effort, where the\ncost to verify a computation is substantially lower than the cost to perform\nit. Our approach is built upon the principle of deterministic replicability, a\nproperty inherent to autoregressive models that strictly necessitates a\ncomputationally homogeneous environment where all agents operate on identical\nhardware and software stacks. Within this defined context, our framework\nenables multiple validators to probabilistically audit small, random segments\nof an LLM's output and it distributes the verification workload effectively.\nThe simulations demonstrated that targeted verification can be over 12 times\nfaster than full regeneration, with tunable parameters to adjust the detection\nprobability. By establishing a tractable mechanism for auditable LLM systems,\nour work offers a foundational layer for responsible AI and serves as a\ncornerstone for future research into the more complex, heterogeneous\nmulti-agent systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u53ef\u884c\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9a8c\u8bc1\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u8f93\u51fa\u7684\u771f\u5b9e\u6027\uff0c\u8be5\u6846\u67b6\u6210\u672c\u4f4e\u4e14\u6548\u7387\u9ad8\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u8f93\u51fa\u771f\u5b9e\u6027\u7684\u6311\u6218\u3002", "method": "\u57fa\u4e8e\u786e\u5b9a\u6027\u53ef\u590d\u5236\u6027\u539f\u5219\uff0c\u901a\u8fc7\u591a\u4e2a\u9a8c\u8bc1\u5668\u6982\u7387\u5ba1\u8ba1LLM\u8f93\u51fa\u7684\u5c0f\u578b\u968f\u673a\u7247\u6bb5\uff0c\u6709\u6548\u5206\u914d\u9a8c\u8bc1\u5de5\u4f5c\u8d1f\u8f7d\u3002", "result": "\u6a21\u62df\u8868\u660e\uff0c\u5b9a\u5411\u9a8c\u8bc1\u901f\u5ea6\u6bd4\u5b8c\u5168\u518d\u751f\u5feb12\u500d\u4ee5\u4e0a\uff0c\u4e14\u53ef\u8c03\u53c2\u6570\u7528\u4e8e\u8c03\u6574\u68c0\u6d4b\u6982\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8d1f\u8d23\u4efb\u7684AI\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u672a\u6765\u66f4\u590d\u6742\u5f02\u6784\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u77f3\u3002"}}
{"id": "2509.11078", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11078", "abs": "https://arxiv.org/abs/2509.11078", "authors": ["Yunghwei Lai", "Weizhi Ma", "Yang Liu"], "title": "Patient-Zero: A Unified Framework for Real-Record-Free Patient Agent Generation", "comment": null, "summary": "Synthetic data generation using large language models (LLMs) has emerged as a\npromising solution across various domains, particularly in medical field, to\nmitigate data collection challenges. However, existing studies mainly utilize\nLLMs to rewrite and complete existing medical records, where the limitations in\ndata privacy, accuracy, and diversity sill exist, and additionally lack the\nability to interact like real patients. To address these issues, we propose a\nrealistic patient generation framework, Patient-Zero, which requires no real\nmedical records. Patient-Zero first introduces a medically-aligned multi-step\ngeneration architecture, which builds comprehensive patient records through\nhierarchical medical knowledge injection without real medical records. Then, to\noptimize the virtual patient's interaction abilities with humans, Patient-Zero\ndesigns a dynamic updating mechanism to improve the consistency and\nconversational performance. Our framework enables the generation of\ncontextually diverse patient records while maintaining strict medical\ncoherence, supported by adaptive dialogue strategies and real-time clinical\nplausibility verification. Experimental results demonstrate that our model\nachieves good performance in accuracy, diversity, and consistency. After\ntraining with our generated virtual patients, existing models show significant\nimprovements on the MedQA dataset.", "AI": {"tldr": "Patient-Zero\u6846\u67b6\u901a\u8fc7\u591a\u6b65\u9aa4\u751f\u6210\u67b6\u6784\u548c\u52a8\u6001\u66f4\u65b0\u673a\u5236\uff0c\u65e0\u9700\u771f\u5b9e\u75c5\u5386\u5373\u53ef\u751f\u6210\u903c\u771f\u3001\u4e00\u81f4\u4e14\u591a\u6837\u7684\u865a\u62df\u75c5\u4eba\u6570\u636e\uff0c\u5e76\u5728MedQA\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u5408\u6210\u533b\u7597\u6570\u636e\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u9690\u79c1\u3001\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u7b49\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u771f\u5b9e\u75c5\u4eba\u4ea4\u4e92\u80fd\u529b\u3002", "method": "\u63d0\u51faPatient-Zero\u6846\u67b6\uff0c\u91c7\u7528\u533b\u5b66\u77e5\u8bc6\u6ce8\u5165\u7684\u591a\u6b65\u9aa4\u751f\u6210\u67b6\u6784\u548c\u52a8\u6001\u66f4\u65b0\u673a\u5236\uff0c\u751f\u6210\u865a\u62df\u75c5\u4eba\u6570\u636e\u5e76\u4f18\u5316\u5176\u4ea4\u4e92\u80fd\u529b\u3002", "result": "\u751f\u6210\u7684\u865a\u62df\u75c5\u4eba\u6570\u636e\u5728\u51c6\u786e\u6027\u3001\u591a\u6837\u6027\u548c\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u7528\u4e8e\u8bad\u7ec3\u540e\u6539\u8fdb\u73b0\u6709\u6a21\u578b\u5728MedQA\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "Patient-Zero\u6846\u67b6\u4e3a\u89e3\u51b3\u533b\u7597\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6848\uff0c\u751f\u6210\u7684\u865a\u62df\u75c5\u4eba\u6570\u636e\u53ef\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u533b\u7597AI\u6a21\u578b\u3002"}}
