{"id": "2508.05731", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05731", "abs": "https://arxiv.org/abs/2508.05731", "authors": ["Yuhang Liu", "Zeyu Liu", "Shuanghe Zhu", "Pengxiang Li", "Congkai Xie", "Jiasheng Wang", "Xueyu Hu", "Xiaotian Han", "Jianbo Yuan", "Xinyao Wang", "Shengyu Zhang", "Hongxia Yang", "Fei Wu"], "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization", "comment": "11 pages, 3 figures", "summary": "The emergence of Multimodal Large Language Models (MLLMs) has propelled the\ndevelopment of autonomous agents that operate on Graphical User Interfaces\n(GUIs) using pure visual input. A fundamental challenge is robustly grounding\nnatural language instructions. This requires a precise spatial alignment, which\naccurately locates the coordinates of each element, and, more critically, a\ncorrect semantic alignment, which matches the instructions to the functionally\nappropriate UI element. Although Reinforcement Learning with Verifiable Rewards\n(RLVR) has proven to be effective at improving spatial alignment for these\nMLLMs, we find that inefficient exploration bottlenecks semantic alignment,\nwhich prevent models from learning difficult semantic associations. To address\nthis exploration problem, we present Adaptive Exploration Policy Optimization\n(AEPO), a new policy optimization framework. AEPO employs a multi-answer\ngeneration strategy to enforce broader exploration, which is then guided by a\ntheoretically grounded Adaptive Exploration Reward (AER) function derived from\nfirst principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B\nand InfiGUI-G1-7B, establish new state-of-the-art results across multiple\nchallenging GUI grounding benchmarks, achieving significant relative\nimprovements of up to 9.0% against the naive RLVR baseline on benchmarks\ndesigned to test generalization and semantic understanding. Resources are\navailable at https://github.com/InfiXAI/InfiGUI-G1.", "AI": {"tldr": "\u9488\u5bf9\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728GUI\u81ea\u4e3b\u4ee3\u7406\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u51faAEPO\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762\u81ea\u4e3b\u4ee3\u7406\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\u9650\u5236\u4e86\u6a21\u578b\u5b66\u4e60\u590d\u6742\u7684\u8bed\u4e49\u5173\u8054\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u4f18\u5316\u6846\u67b6AEPO\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u591a\u7b54\u6848\u751f\u6210\u7b56\u7565\u4ee5\u589e\u5f3a\u63a2\u7d22\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u6548\u7387\u539f\u7406\u7684\u9002\u5e94\u6027\u63a2\u7d22\u5956\u52b1\u51fd\u6570\u5f15\u5bfc\u63a2\u7d22\u3002", "result": "AEPO\u8bad\u7ec3\u7684\u6a21\u578bInfiGUI-G1-3B\u548cInfiGUI-G1-7B\u5728\u591a\u4e2aGUI grounding\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u76f8\u8f83\u4e8eRLVR\u57fa\u7ebf\uff0c\u76f8\u5bf9\u63d0\u5347\u9ad8\u8fbe9.0%\u3002", "conclusion": "AEPO\uff0c\u4e00\u4e2a\u65b0\u7684\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7b54\u6848\u751f\u6210\u7b56\u7565\u548c\u57fa\u4e8e\u6548\u7387\u539f\u7406\u7684\u9002\u5e94\u6027\u63a2\u7d22\u5956\u52b1\u51fd\u6570\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762\u81ea\u4e3b\u4ee3\u7406\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}}
{"id": "2508.05766", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2508.05766", "abs": "https://arxiv.org/abs/2508.05766", "authors": ["Bo Wen"], "title": "A Framework for Inherently Safer AGI through Language-Mediated Active Inference", "comment": null, "summary": "This paper proposes a novel framework for developing safe Artificial General\nIntelligence (AGI) by combining Active Inference principles with Large Language\nModels (LLMs). We argue that traditional approaches to AI safety, focused on\npost-hoc interpretability and reward engineering, have fundamental limitations.\nWe present an architecture where safety guarantees are integrated into the\nsystem's core design through transparent belief representations and\nhierarchical value alignment. Our framework leverages natural language as a\nmedium for representing and manipulating beliefs, enabling direct human\noversight while maintaining computational tractability. The architecture\nimplements a multi-agent system where agents self-organize according to Active\nInference principles, with preferences and safety constraints flowing through\nhierarchical Markov blankets. We outline specific mechanisms for ensuring\nsafety, including: (1) explicit separation of beliefs and preferences in\nnatural language, (2) bounded rationality through resource-aware free energy\nminimization, and (3) compositional safety through modular agent structures.\nThe paper concludes with a research agenda centered on the Abstraction and\nReasoning Corpus (ARC) benchmark, proposing experiments to validate our\nframework's safety properties. Our approach offers a path toward AGI\ndevelopment that is inherently safer, rather than retrofitted with safety\nmeasures.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u4e3b\u52a8\u63a8\u7406\u548cLLM\u5f00\u53d1\u5b89\u5168AGI\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7cfb\u7edf\u6838\u5fc3\u8bbe\u8ba1\u4e2d\u96c6\u6210\u5b89\u5168\u4fdd\u8bc1\u6765\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u5728ARC\u57fa\u51c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u7684\u5b9e\u9a8c\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u667a\u80fd\u5b89\u5168\u65b9\u6cd5\uff08\u4e8b\u540e\u89e3\u91ca\u6027\u548c\u5956\u52b1\u5de5\u7a0b\uff09\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u4ece\u7cfb\u7edf\u6838\u5fc3\u8bbe\u8ba1\u4e2d\u4fdd\u8bc1\u5b89\u5168\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u4e3b\u52a8\u63a8\u7406\u539f\u5219\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6784\u5efa\u591aAgent\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u5c42\u9a6c\u5c14\u53ef\u592b\u6bef\u4f20\u9012\u504f\u597d\u548c\u5b89\u5168\u7ea6\u675f\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u8868\u793a\u548c\u64cd\u4f5c\u4fe1\u5ff5\uff0c\u5177\u6709\u900f\u660e\u4fe1\u5ff5\u8868\u793a\u548c\u5206\u5c42\u4ef7\u503c\u5bf9\u9f50\u7684\u67b6\u6784\uff0c\u5e76\u5305\u542b\u5177\u4f53\u7684\u5b89\u5168\u673a\u5236\uff08\u4f8b\u5982\uff0c\u4fe1\u5ff5\u548c\u504f\u597d\u7684\u660e\u786e\u5206\u79bb\u3001\u8d44\u6e90\u611f\u77e5\u7684\u81ea\u7531\u80fd\u6700\u5c0f\u5316\u548c\u6a21\u5757\u5316Agent\u7ed3\u6784\uff09\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e3b\u52a8\u63a8\u7406\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u5b89\u5168\u4eba\u5de5\u667a\u80fd\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u5b89\u5168\u4fdd\u8bc1\u96c6\u6210\u5230\u7cfb\u7edf\u6838\u5fc3\u8bbe\u8ba1\u4e2d\u6765\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.05776", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05776", "abs": "https://arxiv.org/abs/2508.05776", "authors": ["Thomas L. Griffiths", "Brenden M. Lake", "R. Thomas McCoy", "Ellie Pavlick", "Taylor W. Webb"], "title": "Whither symbols in the era of advanced neural networks?", "comment": null, "summary": "Some of the strongest evidence that human minds should be thought about in\nterms of symbolic systems has been the way they combine ideas, produce novelty,\nand learn quickly. We argue that modern neural networks -- and the artificial\nintelligence systems built upon them -- exhibit similar abilities. This\nundermines the argument that the cognitive processes and representations used\nby human minds are symbolic, although the fact that these neural networks are\ntypically trained on data generated by symbolic systems illustrates that such\nsystems play an important role in characterizing the abstract problems that\nhuman minds have to solve. This argument leads us to offer a new agenda for\nresearch on the symbolic basis of human thought.", "AI": {"tldr": "\u795e\u7ecf\u7f51\u7edc\u4e0e\u4eba\u7c7b\u601d\u7ef4\u7684\u76f8\u4f3c\u6027\uff0c\u6311\u6218\u4e86\u7b26\u53f7\u7cfb\u7edf\u662f\u4eba\u7c7b\u601d\u7ef4\u57fa\u7840\u7684\u89c2\u70b9", "motivation": "\u6311\u6218\u5c06\u4eba\u7c7b\u601d\u7ef4\u89c6\u4e3a\u7b26\u53f7\u7cfb\u7edf\u7684\u4f20\u7edf\u89c2\u70b9", "method": "\u5bf9\u6bd4\u5206\u6790\u4eba\u7c7b\u8ba4\u77e5\u80fd\u529b\u548c\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u7684\u80fd\u529b", "result": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u5c55\u73b0\u51fa\u4e0e\u4eba\u7c7b\u601d\u7ef4\u7c7b\u4f3c\u7684\u7ec4\u5408\u601d\u60f3\u3001\u4ea7\u751f\u65b0\u9896\u6027\u548c\u5feb\u901f\u5b66\u4e60\u7684\u80fd\u529b\uff0c\u8fd9\u524a\u5f31\u4e86\u4eba\u7c7b\u8ba4\u77e5\u57fa\u4e8e\u7b26\u53f7\u7cfb\u7edf\u7684\u8bba\u70b9\u3002", "conclusion": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u7684\u76f8\u4f3c\u6027\u6311\u6218\u4e86\u4eba\u7c7b\u601d\u7ef4\u57fa\u4e8e\u7b26\u53f7\u7cfb\u7edf\u7684\u89c2\u70b9\uff0c\u4f46\u7b26\u53f7\u7cfb\u7edf\u4f9d\u7136\u5728\u523b\u753b\u4eba\u7c7b\u601d\u7ef4\u89e3\u51b3\u7684\u62bd\u8c61\u95ee\u9898\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\u3002"}}
{"id": "2508.05792", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05792", "abs": "https://arxiv.org/abs/2508.05792", "authors": ["Kausik Lakkaraju", "Siva Likitha Valluru", "Biplav Srivastava"], "title": "Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making", "comment": null, "summary": "Current eXplainable AI (XAI) methods largely serve developers, often focusing\non justifying model outputs rather than supporting diverse stakeholder needs. A\nrecent shift toward Evaluative AI reframes explanation as a tool for hypothesis\ntesting, but still focuses primarily on operational organizations. We introduce\nHolistic-XAI (H-XAI), a unified framework that integrates causal rating methods\nwith traditional XAI methods to support explanation as an interactive,\nmulti-method process. H-XAI allows stakeholders to ask a series of questions,\ntest hypotheses, and compare model behavior against automatically constructed\nrandom and biased baselines. It combines instance-level and global\nexplanations, adapting to each stakeholder's goals, whether understanding\nindividual decisions, assessing group-level bias, or evaluating robustness\nunder perturbations. We demonstrate the generality of our approach through two\ncase studies spanning six scenarios: binary credit risk classification and\nfinancial time-series forecasting. H-XAI fills critical gaps left by existing\nXAI methods by combining causal ratings and post-hoc explanations to answer\nstakeholder-specific questions at both the individual decision level and the\noverall model level.", "AI": {"tldr": "H-XAI\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u56e0\u679c\u8bc4\u7ea7\u548c\u4e8b\u540e\u89e3\u91ca\uff0c\u4ee5\u56de\u7b54\u5229\u76ca\u76f8\u5173\u8005\u5728\u4e2a\u4f53\u51b3\u7b56\u5c42\u9762\u548c\u6574\u4f53\u6a21\u578b\u5c42\u9762\u7684\u7279\u5b9a\u95ee\u9898\uff0c\u4ece\u800c\u6539\u8fdb\u73b0\u6709XAI\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709XAI\u65b9\u6cd5\u4e3b\u8981\u670d\u52a1\u4e8e\u5f00\u53d1\u8005\uff0c\u5173\u6ce8\u6a21\u578b\u8f93\u51fa\u7684\u5408\u7406\u6027\uff0c\u800c\u975e\u652f\u6301\u4e0d\u540c\u7684\u5229\u76ca\u76f8\u5173\u8005\u9700\u6c42\u3002", "method": "\u6574\u5408\u56e0\u679c\u8bc4\u7ea7\u65b9\u6cd5\u4e0e\u4f20\u7edfXAI\u65b9\u6cd5\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u591a\u65b9\u6cd5\u89e3\u91ca\u8fc7\u7a0b\u3002", "result": "H-XAI\u6846\u67b6\u80fd\u591f\u652f\u6301\u5229\u76ca\u76f8\u5173\u8005\u63d0\u51fa\u7cfb\u5217\u95ee\u9898\u3001\u68c0\u9a8c\u5047\u8bbe\uff0c\u5e76\u5c06\u6a21\u578b\u884c\u4e3a\u4e0e\u81ea\u52a8\u6784\u5efa\u7684\u968f\u673a\u548c\u504f\u5dee\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\uff0c\u7ed3\u5408\u5b9e\u4f8b\u7ea7\u548c\u5168\u5c40\u89e3\u91ca\uff0c\u9002\u5e94\u6bcf\u4e2a\u5229\u76ca\u76f8\u5173\u8005\u7684\u76ee\u6807\u3002", "conclusion": "Holistic-XAI (H-XAI)\u6846\u67b6\u6574\u5408\u56e0\u679c\u8bc4\u7ea7\u65b9\u6cd5\u548c\u4f20\u7edfXAI\u65b9\u6cd5\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u591a\u65b9\u6cd5\u89e3\u91ca\u8fc7\u7a0b\uff0c\u4ee5\u6ee1\u8db3\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\uff0c\u5f25\u8865\u73b0\u6709XAI\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2508.05855", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05855", "abs": "https://arxiv.org/abs/2508.05855", "authors": ["Zixia Wang", "Jia Hu", "Ronghui Mu"], "title": "Safety of Embodied Navigation: A Survey", "comment": null, "summary": "As large language models (LLMs) continue to advance and gain influence, the\ndevelopment of embodied AI has accelerated, drawing significant attention,\nparticularly in navigation scenarios. Embodied navigation requires an agent to\nperceive, interact with, and adapt to its environment while moving toward a\nspecified target in unfamiliar settings. However, the integration of embodied\nnavigation into critical applications raises substantial safety concerns. Given\ntheir deployment in dynamic, real-world environments, ensuring the safety of\nsuch systems is critical. This survey provides a comprehensive analysis of\nsafety in embodied navigation from multiple perspectives, encompassing attack\nstrategies, defense mechanisms, and evaluation methodologies. Beyond conducting\na comprehensive examination of existing safety challenges, mitigation\ntechnologies, and various datasets and metrics that assess effectiveness and\nrobustness, we explore unresolved issues and future research directions in\nembodied navigation safety. These include potential attack methods, mitigation\nstrategies, more reliable evaluation techniques, and the implementation of\nverification frameworks. By addressing these critical gaps, this survey aims to\nprovide valuable insights that can guide future research toward the development\nof safer and more reliable embodied navigation systems. Furthermore, the\nfindings of this study have broader implications for enhancing societal safety\nand increasing industrial efficiency.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u5177\u8eab\u5bfc\u822a\u5b89\u5168\u6027\u7684\u6311\u6218\u3001\u7f13\u89e3\u6280\u672f\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u4e0d\u65ad\u53d1\u5c55\u548c\u5f71\u54cd\u529b\u65e5\u76ca\u589e\u5f3a\uff0c\u5177\u8eabAI\u7684\u53d1\u5c55\u4e5f\u968f\u4e4b\u52a0\u5feb\uff0c\u5c24\u5176\u662f\u5728\u5bfc\u822a\u573a\u666f\u4e2d\u3002\u7136\u800c\uff0c\u5c06\u5177\u8eab\u5bfc\u822a\u96c6\u6210\u5230\u5173\u952e\u5e94\u7528\u4e2d\u4f1a\u5e26\u6765\u91cd\u5927\u7684\u5b89\u5168\u95ee\u9898\u3002", "method": "\u5bf9\u73b0\u6709\u5b89\u5168\u6311\u6218\u3001\u7f13\u89e3\u6280\u672f\u4ee5\u53ca\u8bc4\u4f30\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u7684\u5404\u79cd\u6570\u636e\u96c6\u548c\u6307\u6807\u8fdb\u884c\u4e86\u7efc\u5408\u8003\u5bdf\u3002", "result": "\u5bf9\u5177\u8eab\u5bfc\u822a\u5b89\u5168\u6027\u7684\u5168\u9762\u5206\u6790\uff0c\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u5f00\u53d1\u66f4\u5b89\u5168\u53ef\u9760\u7684\u5177\u8eab\u5bfc\u822a\u7cfb\u7edf\u3002", "conclusion": "\u5bf9\u5177\u8eab\u5bfc\u822a\u5b89\u5168\u6027\u7684\u5168\u9762\u5206\u6790\uff0c\u5305\u62ec\u653b\u51fb\u7b56\u7565\u3001\u9632\u5fa1\u673a\u5236\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4f8b\u5982\u6f5c\u5728\u7684\u653b\u51fb\u65b9\u6cd5\u3001\u7f13\u89e3\u7b56\u7565\u3001\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u6280\u672f\u548c\u9a8c\u8bc1\u6846\u67b6\u7684\u5b9e\u65bd\u3002"}}
{"id": "2508.05888", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.05888", "abs": "https://arxiv.org/abs/2508.05888", "authors": ["Sahil Bansal", "Sai Shruthi Sistla", "Aarti Arikatala", "Sebastian Schreiber"], "title": "Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning", "comment": null, "summary": "Effective tool retrieval is essential for AI agents to select from a vast\narray of tools when identifying and planning actions in the context of complex\nuser queries. Despite its central role in planning, this aspect remains\nunderexplored in the literature. Traditional approaches rely primarily on\nsimilarities between user queries and tool descriptions, which significantly\nlimits retrieval accuracy, specifically when handling multi-step user requests.\nTo address these limitations, we propose a Knowledge Graph (KG)-based tool\nretrieval framework that captures the semantic relationships between tools and\ntheir functional dependencies. Our retrieval algorithm leverages ensembles of\n1-hop ego tool graphs to model direct and indirect connections between tools,\nenabling more comprehensive and contextual tool selection for multi-step tasks.\nWe evaluate our approach on a synthetically generated internal dataset across\nsix defined user classes, extending previous work on coherent dialogue\nsynthesis and too retrieval benchmarks. Results demonstrate that our tool\ngraph-based method achieves 91.85% tool coverage on the micro-average Complete\nRecall metric, compared to 89.26% for re-ranked semantic-lexical hybrid\nretrieval, the strongest non-KG baseline in our experiments. These findings\nsupport our hypothesis that the structural information in the KG provides\ncomplementary signals to pure similarity matching, particularly for queries\nrequiring sequential tool composition.", "AI": {"tldr": "\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u5de5\u5177\u68c0\u7d22\u6846\u67b6\u63d0\u9ad8\u4e86AI\u667a\u80fd\u4f53\u5728\u590d\u6742\u7528\u6237\u67e5\u8be2\u4e2d\u9009\u62e9\u5de5\u5177\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u591a\u6b65\u9aa4\u4efb\u52a1\u4e2d\u6548\u679c\u663e\u8457\u3002", "motivation": "\u73b0\u6709\u7684\u5de5\u5177\u68c0\u7d22\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u7528\u6237\u67e5\u8be2\u548c\u5de5\u5177\u63cf\u8ff0\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u5728\u5904\u7406\u591a\u6b65\u9aa4\u7528\u6237\u8bf7\u6c42\u65f6\u51c6\u786e\u7387\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u5de5\u5177\u68c0\u7d22\u6846\u67b6\uff0c\u5229\u75281-hop ego\u5de5\u5177\u56fe\u7684\u96c6\u6210\u6765\u5efa\u6a21\u5de5\u5177\u4e4b\u95f4\u76f4\u63a5\u548c\u95f4\u63a5\u7684\u8054\u7cfb\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\uff0c\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u65b9\u6cd5\u5728Complete Recall\u6307\u6807\u4e0a\u53d6\u5f97\u4e8691.85%\u7684\u5de5\u5177\u8986\u76d6\u7387\uff0c\u8d85\u8fc7\u4e86\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u77e5\u8bc6\u56fe\u8c31(KG)\u65b9\u6cd5\u5728\u5de5\u5177\u68c0\u7d22\u4e2d\u4f18\u4e8e\u4f20\u7edf\u7684\u8bed\u4e49-\u8bcd\u6c47\u6df7\u5408\u68c0\u7d22\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u591a\u6b65\u9aa4\u4efb\u52a1\u4e2d\u6548\u679c\u663e\u8457\uff0c\u5b9e\u73b0\u4e8691.85%\u7684\u5de5\u5177\u8986\u76d6\u7387\u3002"}}
{"id": "2508.05996", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05996", "abs": "https://arxiv.org/abs/2508.05996", "authors": ["Kaitao Chen", "Mianxin Liu", "Daoming Zong", "Chaoyue Ding", "Shaohao Rui", "Yankai Jiang", "Mu Zhou", "Xiaosong Wang"], "title": "Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making", "comment": "14 pages, 4 figures", "summary": "Complex medical decision-making involves cooperative workflows operated by\ndifferent clinicians. Designing AI multi-agent systems can expedite and augment\nhuman-level clinical decision-making. Existing multi-agent researches primarily\nfocus on language-only tasks, yet their extension to multimodal scenarios\nremains challenging. A blind combination of diverse vision-language models\n(VLMs) can amplify an erroneous outcome interpretation. VLMs in general are\nless capable in instruction following and importantly self-reflection, compared\nto large language models (LLMs) of comparable sizes. This disparity largely\nconstrains VLMs' ability in cooperative workflows. In this study, we propose\nMedOrch, a mediator-guided multi-agent collaboration framework for medical\nmultimodal decision-making. MedOrch employs an LLM-based mediator agent that\nenables multiple VLM-based expert agents to exchange and reflect on their\noutputs towards collaboration. We utilize multiple open-source general-purpose\nand domain-specific VLMs instead of costly GPT-series models, revealing the\nstrength of heterogeneous models. We show that the collaboration within\ndistinct VLM-based agents can surpass the capabilities of any individual agent.\nWe validate our approach on five medical vision question answering benchmarks,\ndemonstrating superior collaboration performance without model training. Our\nfindings underscore the value of mediator-guided multi-agent collaboration in\nadvancing medical multimodal intelligence. Our code will be made publicly\navailable.", "AI": {"tldr": "MedOrch\u6846\u67b6\u5229\u7528LLM\u4e2d\u4ecb\u591a\u4e2aVLM\uff0c\u63d0\u5347\u533b\u7597\u591a\u6a21\u6001\u51b3\u7b56\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8bed\u8a00\u4efb\u52a1\uff0c\u5c06\u591aAgent\u7cfb\u7edf\u6269\u5c55\u5230\u591a\u6a21\u6001\u573a\u666f\u5177\u6709\u6311\u6218\u6027\uff0c\u4e14VLM\u5728\u6307\u4ee4\u9075\u5faa\u548c\u81ea\u6211\u53cd\u601d\u65b9\u9762\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u4e2d\u4ecb\u4ee3\u7406\u7684\u591aAgent\u534f\u4f5c\u6846\u67b6MedOrch\uff0c\u5229\u7528\u591a\u4e2a\u5f00\u6e90VLM\uff0c\u5b9e\u73b0\u533b\u7597\u591a\u6a21\u6001\u51b3\u7b56\u3002", "result": "\u5728\u4e94\u4e2a\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86MedOrch\u7684\u6709\u6548\u6027\uff0c\u5176\u534f\u4f5c\u6027\u80fd\u4f18\u4e8e\u4efb\u4f55\u5355\u4e2aAgent\u3002", "conclusion": "MedOrch\u6846\u67b6\u901a\u8fc7LLM\u4e2d\u4ecb\u4ee3\u7406\uff0c\u5b9e\u73b0\u591a\u4e2aVLM\u4e13\u5bb6\u4ee3\u7406\u7684\u534f\u4f5c\uff0c\u5728\u533b\u7597\u591a\u6a21\u6001\u51b3\u7b56\u4e2d\u53d6\u5f97\u4f18\u4e8e\u5355\u4e2a\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u65e0\u9700\u6a21\u578b\u8bad\u7ec3\u3002"}}
{"id": "2508.06042", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06042", "abs": "https://arxiv.org/abs/2508.06042", "authors": ["Daechul Ahn", "San Kim", "Jonghyun Choi"], "title": "Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning", "comment": "COLM 2025", "summary": "Large Language Models (LLMs) have recently demonstrated impressive action\nsequence prediction capabilities but often struggle with dynamic, long-horizon\ntasks such as real-time strategic games. In a game such as StarCraftII (SC2),\nagents need to manage resource constraints and adapt to evolving battlefield\nsituations in a partially observable environment. This often overwhelms\nexisiting LLM-based approaches. To address these challenges, we propose a\nhierarchical multi-agent framework that employs specialized imitation learning\nagents under a meta-controller called Strategic Planner (SP). By expert\ndemonstrations, each specialized agent learns a distinctive strategy, such as\naerial support or defensive maneuvers, and produces coherent, structured\nmultistep action sequences. The SP then orchestrates these proposals into a\nsingle, environmentally adaptive plan that ensures local decisions aligning\nwith long-term strategies. We call this HIMA (Hierarchical Imitation\nMulti-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that\nencompasses all race match combinations in SC2. Our empirical results show that\nHIMA outperforms state of the arts in strategic clarity, adaptability, and\ncomputational efficiency, underscoring the potential of combining specialized\nimitation modules with meta-level orchestration to develop more robust,\ngeneral-purpose AI agents.", "AI": {"tldr": "\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6HIMA\u901a\u8fc7\u7ed3\u5408\u4e13\u4e1a\u6a21\u4eff\u5b66\u4e60\u667a\u80fd\u4f53\u548c\u5143\u7ea7\u534f\u8c03\uff0c\u63d0\u9ad8\u4e86LLM\u5728\u661f\u9645\u4e89\u9738II\u7b49\u590d\u6742\u6218\u7565\u6e38\u620f\u4e2d\u8868\u73b0\u3002", "motivation": "\u73b0\u6709LLM\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u661f\u9645\u4e89\u9738II\u7b49\u52a8\u6001\u3001\u957f\u65f6\u5e8f\u4efb\u52a1\uff0c\u56e0\u4e3a\u9700\u8981\u7ba1\u7406\u8d44\u6e90\u9650\u5236\u5e76\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u6218\u573a\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6HIMA\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u5143\u63a7\u5236\u5668\u6218\u7565\u89c4\u5212\u5668(SP)\u4e0b\u7684\u4e13\u4e1a\u6a21\u4eff\u5b66\u4e60\u667a\u80fd\u4f53\u3002\u6bcf\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u5b66\u4e60\u4e0d\u540c\u7684\u7b56\u7565\uff08\u5982\u7a7a\u4e2d\u652f\u63f4\u6216\u9632\u5fa1\u6027\u7b56\u7565\uff09\uff0c\u4ea7\u751f\u8fde\u8d2f\u7684\u3001\u7ed3\u6784\u5316\u7684\u591a\u6b65\u52a8\u4f5c\u5e8f\u5217\u3002SP\u5c06\u8fd9\u4e9b\u65b9\u6848\u534f\u8c03\u6210\u4e00\u4e2a\u5355\u4e00\u7684\u3001\u9002\u5e94\u73af\u5883\u7684\u8ba1\u5212\u3002", "result": "HIMA\u5728\u6218\u7565\u6e05\u6670\u5ea6\u3001\u9002\u5e94\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "HIMA\u6846\u67b6\u5728\u7b56\u7565\u6e05\u6670\u5ea6\u3001\u9002\u5e94\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u8bc1\u660e\u5c06\u4e13\u4e1a\u6a21\u4eff\u6a21\u5757\u4e0e\u5143\u7ea7\u534f\u8c03\u76f8\u7ed3\u5408\u53ef\u4ee5\u5f00\u53d1\u51fa\u66f4\u5f3a\u5927\u3001\u66f4\u901a\u7528\u7684AI\u667a\u80fd\u4f53\u3002"}}
{"id": "2508.06060", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06060", "abs": "https://arxiv.org/abs/2508.06060", "authors": ["Sankarshan Damle", "Boi Faltings"], "title": "LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences", "comment": "Published in the Proceedings of the 28th European Conference on\n  Artificial Intelligence (ECAI 2025)", "summary": "Large Language Models (LLMs) are increasingly expected to handle complex\ndecision-making tasks, yet their ability to perform structured resource\nallocation remains underexplored. Evaluating their reasoning is also difficult\ndue to data contamination and the static nature of existing benchmarks. We\npresent a dual-purpose framework leveraging Participatory Budgeting (PB) both\nas (i) a practical setting for LLM-based resource allocation and (ii) an\nadaptive benchmark for evaluating their reasoning capabilities. We task LLMs\nwith selecting project subsets under feasibility (e.g., budget) constraints via\nthree prompting strategies: greedy selection, direct optimization, and a\nhill-climbing-inspired refinement. We benchmark LLMs' allocations against a\nutility-maximizing oracle. Interestingly, we also test whether LLMs can infer\nstructured preferences from natural-language voter input or metadata, without\nexplicit votes. By comparing allocations based on inferred preferences to those\nfrom ground-truth votes, we evaluate LLMs' ability to extract preferences from\nopen-ended input. Our results underscore the role of prompt design and show\nthat LLMs hold promise for mechanism design with unstructured inputs.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u53c2\u4e0e\u5f0f\u9884\u7b97\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u5206\u914d\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793aLLM\u5177\u6709\u6f5c\u529b\u4f46\u63d0\u793a\u8bbe\u8ba1\u5f88\u91cd\u8981\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5b58\u5728\u6570\u636e\u6c61\u67d3\u548c\u9759\u6001\u6027\u95ee\u9898\uff0c\u96be\u4ee5\u8bc4\u4f30LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u8be5\u6846\u67b6\u5229\u7528\u53c2\u4e0e\u5f0f\u9884\u7b97(PB)\u4f5c\u4e3aLLM\u8d44\u6e90\u5206\u914d\u7684\u5b9e\u9645\u573a\u666f\u548c\u81ea\u9002\u5e94\u57fa\u51c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff1a\u8d2a\u5a6a\u9009\u62e9\u3001\u76f4\u63a5\u4f18\u5316\u548c\u722c\u5c71\u5f0f\u6539\u8fdb\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u793a\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\uff0cLLM\u5728\u5904\u7406\u975e\u7ed3\u6784\u5316\u8f93\u5165\u7684\u673a\u5236\u8bbe\u8ba1\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u4ece\u81ea\u7136\u8bed\u8a00\u9009\u6c11\u8f93\u5165\u6216\u5143\u6570\u636e\u4e2d\u63a8\u65ad\u7ed3\u6784\u5316\u504f\u597d\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u8d44\u6e90\u5206\u914d\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u6709\u5f85\u63d0\u9ad8\uff0c\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u4e0e\u5f0f\u9884\u7b97\u7684\u53cc\u91cd\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u7684\u63a8\u7406\u80fd\u529b\u548c\u8fdb\u884c\u8d44\u6e90\u5206\u914d\u3002"}}
{"id": "2508.06062", "categories": ["cs.AI", "cs.LG", "cs.LO", "68T27, 68T30"], "pdf": "https://arxiv.org/pdf/2508.06062", "abs": "https://arxiv.org/abs/2508.06062", "authors": ["Evgenii E. Vityaev", "Andrei Mantsivoda"], "title": "Don't Forget Imagination!", "comment": "14 pages, 2 figures", "summary": "Cognitive imagination is a type of imagination that plays a key role in human\nthinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to\nmentally visualize coherent and holistic systems of concepts and causal links\nthat serve as semantic contexts for reasoning, decision making and prediction.\nOur position is that the role of cognitive imagination is still greatly\nunderestimated, and this creates numerous problems and diminishes the current\ncapabilities of AI. For instance, when reasoning, humans rely on imaginary\ncontexts to retrieve background info. They also constantly return to the\ncontext for semantic verification that their reasoning is still reasonable.\nThus, reasoning without imagination is blind. This paper is a call for greater\nattention to cognitive imagination as the next promising breakthrough in\nartificial intelligence. As an instrument for simulating cognitive imagination,\nwe propose semantic models -- a new approach to mathematical models that can\nlearn, like neural networks, and are based on probabilistic causal\nrelationships. Semantic models can simulate cognitive imagination because they\nensure the consistency of imaginary contexts and implement a glass-box approach\nthat allows the context to be manipulated as a holistic and coherent system of\ninterrelated facts glued together with causal relations.", "AI": {"tldr": "Cognitive imagination is key to advanced AI; semantic models offer a promising path to simulate it.", "motivation": "Current AI systems underestimate the role of cognitive imagination, limiting their capabilities in reasoning and decision-making.  Humans use cognitive imagination for context retrieval and semantic verification during reasoning.", "method": "Proposing semantic models as a tool to simulate cognitive imagination.", "result": "Semantic models offer a potential breakthrough by ensuring consistency of imaginary contexts and enabling manipulation of the context as a holistic system.", "conclusion": "Cognitive imagination is crucial for human thinking and AI advancement.  Semantic models, a new approach to mathematical models based on probabilistic causal relationships, are proposed to simulate cognitive imagination."}}
{"id": "2508.06064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06064", "abs": "https://arxiv.org/abs/2508.06064", "authors": ["Harold Silv\u00e8re Kiossou", "Siegfried Nijssen", "Pierre Schaus"], "title": "A Generic Complete Anytime Beam Search for Optimal Decision Tree", "comment": null, "summary": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u7cbe\u786e\u548canytime\u51b3\u7b56\u6811\u5b66\u4e60\u6846\u67b6CA-DL8.5\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7cbe\u786e\u65b9\u6cd5\u7684anytime\u6269\u5c55\uff08\u5982LDS-DL8.5\u3001Top-k-DL8.5\u548cBlossom\uff09\u7f3a\u4e4f\u7cfb\u7edf\u7684\u6bd4\u8f83\uff0c\u96be\u4ee5\u8bc4\u4f30\u5176\u76f8\u5bf9\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u3001\u5b8c\u6574\u7684\u3001anytime\u7684beam search\u7b97\u6cd5CA-DL8.5\uff0c\u8be5\u7b97\u6cd5\u6269\u5c55\u4e86DL8.5\u6846\u67b6\uff0c\u5e76\u7edf\u4e00\u4e86\u4e00\u4e9b\u73b0\u6709\u7684anytime\u7b56\u7565\u3002CA-DL8.5\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u96c6\u6210\u5404\u79cd\u542f\u53d1\u5f0f\u7b97\u6cd5\u548c\u677e\u5f1b\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4f7f\u7528LDS\u7684CA-DL8.5\u59cb\u7ec8\u63d0\u4f9b\u6700\u4f73\u7684anytime\u6027\u80fd\uff0c\u4f18\u4e8e\u5176\u4ed6CA-DL8.5\u53d8\u4f53\u548cBlossom\u7b97\u6cd5\u3002", "conclusion": "CA-DL8.5\u7b97\u6cd5\u5728\u6807\u51c6\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u63d0\u4f9b\u6700\u4f73\u7684 anytime \u6027\u80fd\uff0c\u4f18\u4e8e\u5176\u4ed6CA-DL8.5\u53d8\u4f53\u548cBlossom\u7b97\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u6574\u6027\u548c\u6700\u4f18\u6027\u4fdd\u8bc1\u3002"}}
{"id": "2508.06074", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06074", "abs": "https://arxiv.org/abs/2508.06074", "authors": ["Siyi Lu", "Run Liu", "Dongsheng Yang", "Lei He"], "title": "ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception", "comment": null, "summary": "Autonomous driving systems face significant challenges in perceiving complex\nenvironments and making real-time decisions. Traditional modular approaches,\nwhile offering interpretability, suffer from error propagation and coordination\nissues, whereas end-to-end learning systems can simplify the design but face\ncomputational bottlenecks. This paper presents a novel approach to autonomous\ndriving using deep reinforcement learning (DRL) that integrates bird's-eye view\n(BEV) perception for enhanced real-time decision-making. We introduce the\n\\texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction\nnetwork that combines BEV-based perception with the Mamba framework for\ntemporal feature modeling. This integration allows the system to encode vehicle\nsurroundings and road features in a unified coordinate system and accurately\nmodel long-range dependencies. Building on this, we propose the\n\\texttt{ME$^3$-BEV} framework, which utilizes the \\texttt{Mamba-BEV} model as a\nfeature input for end-to-end DRL, achieving superior performance in dynamic\nurban driving scenarios. We further enhance the interpretability of the model\nby visualizing high-dimensional features through semantic segmentation,\nproviding insight into the learned representations. Extensive experiments on\nthe CARLA simulator demonstrate that \\texttt{ME$^3$-BEV} outperforms existing\nmodels across multiple metrics, including collision rate and trajectory\naccuracy, offering a promising solution for real-time autonomous driving.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\uff0c\u7ed3\u5408\u9e1f\u77b0\u56fe\u611f\u77e5\u548c\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u5728CARLA\u6a21\u62df\u5668\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6a21\u5757\u5316\u65b9\u6cd5\u7684\u8bef\u5dee\u4f20\u64ad\u548c\u534f\u8c03\u95ee\u9898\u4ee5\u53ca\u7aef\u5230\u7aef\u5b66\u4e60\u65b9\u6cd5\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u7528Mamba-BEV\u6a21\u578b\u8fdb\u884c\u9ad8\u6548\u7684\u65f6\u7a7a\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u7ed3\u5408ME\u00b3-BEV\u6846\u67b6\u8fdb\u884c\u7aef\u5230\u7aef\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u540c\u65f6\u901a\u8fc7\u8bed\u4e49\u5206\u5272\u53ef\u89c6\u5316\u9ad8\u7ef4\u7279\u5f81\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "ME\u00b3-BEV\u6a21\u578b\u5728\u78b0\u649e\u7387\u548c\u8f68\u8ff9\u7cbe\u5ea6\u7b49\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u4e3a\u5b9e\u65f6\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u96c6\u6210\u4e86\u9e1f\u77b0\u56fe\uff08BEV\uff09\u611f\u77e5\uff0c\u63d0\u9ad8\u4e86\u5b9e\u65f6\u51b3\u7b56\u80fd\u529b\uff0c\u5e76\u5728CARLA\u6a21\u62df\u5668\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002"}}
{"id": "2508.06091", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06091", "abs": "https://arxiv.org/abs/2508.06091", "authors": ["Stan P Hauke", "Przemys\u0142aw Andrzej Wa\u0142\u0119ga"], "title": "Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2", "comment": "18 pages", "summary": "In recent years, there has been growing interest in understanding the\nexpressive power of graph neural networks (GNNs) by relating them to logical\nlanguages. This research has been been initialised by an influential result of\nBarcel\\'o et al. (2020), who showed that the graded modal logic (or a guarded\nfragment of the logic C2), characterises the logical expressiveness of\naggregate-combine GNNs. As a ``challenging open problem'' they left the\nquestion whether full C2 characterises the logical expressiveness of\naggregate-combine-readout GNNs. This question has remained unresolved despite\nseveral attempts. In this paper, we solve the above open problem by proving\nthat the logical expressiveness of aggregate-combine-readout GNNs strictly\nexceeds that of C2. This result holds over both undirected and directed graphs.\nBeyond its implications for GNNs, our work also leads to purely logical\ninsights on the expressive power of infinitary logics.", "AI": {"tldr": "\u89e3\u51b3\u4e86\u5173\u4e8e\u805a\u5408-\u7ec4\u5408-\u8bfb\u51faGNN\u903b\u8f91\u8868\u8fbe\u80fd\u529b\u7684\u5f00\u653e\u6027\u95ee\u9898\uff0c\u8bc1\u660e\u5176\u8868\u8fbe\u80fd\u529b\u8d85\u8fc7C2\u3002", "motivation": "\u7406\u89e3\u56fe\u795e\u7ecf\u7f51\u7edc (GNN) \u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u4e0e\u903b\u8f91\u8bed\u8a00\u8054\u7cfb\u8d77\u6765\u3002", "method": "\u8bc1\u660e", "result": "\u8bc1\u660e\u4e86\u805a\u5408-\u7ec4\u5408-\u8bfb\u51fa GNN \u7684\u903b\u8f91\u8868\u8fbe\u80fd\u529b\u4e25\u683c\u8d85\u8fc7 C2\u3002", "conclusion": "\u8bc1\u660e\u4e86\u805a\u5408-\u7ec4\u5408-\u8bfb\u51fa GNN \u7684\u903b\u8f91\u8868\u8fbe\u80fd\u529b\u4e25\u683c\u8d85\u8fc7 C2\uff0c\u89e3\u51b3\u4e86 Barcel\u00f3 \u7b49\u4eba\u63d0\u51fa\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002\u8be5\u7ed3\u679c\u9002\u7528\u4e8e\u65e0\u5411\u56fe\u548c\u6709\u5411\u56fe\u3002"}}
{"id": "2508.06110", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06110", "abs": "https://arxiv.org/abs/2508.06110", "authors": ["Yiran Rex Ma"], "title": "PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion", "comment": "Accepted at IJCNN 2025", "summary": "Table reasoning, including tabular QA and fact verification, often depends on\nannotated data or complex data augmentation, limiting flexibility and\ngeneralization. LLMs, despite their versatility, often underperform compared to\nsimple supervised models. To approach these issues, we introduce PanelTR, a\nframework utilizing LLM agent scientists for robust table reasoning through a\nstructured scientific approach. PanelTR's workflow involves agent scientists\nconducting individual investigations, engaging in self-review, and\nparticipating in collaborative peer-review discussions. This process, driven by\nfive scientist personas, enables semantic-level transfer without relying on\ndata augmentation or parametric optimization. Experiments across four\nbenchmarks show that PanelTR outperforms vanilla LLMs and rivals fully\nsupervised models, all while remaining independent of training data. Our\nfindings indicate that structured scientific methodology can effectively handle\ncomplex tasks beyond table reasoning with flexible semantic understanding in a\nzero-shot context.", "AI": {"tldr": "PanelTR\u5229\u7528LLM\u6a21\u62df\u79d1\u5b66\u5bb6\u8fdb\u884c\u7ed3\u6784\u5316\u7814\u7a76\uff0c\u5728\u8868\u683c\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u7684\u8868\u683c\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6807\u6ce8\u6570\u636e\u6216\u590d\u6742\u7684\u6570\u636e\u589e\u5f3a\uff0cLLM\u7684\u6027\u80fd\u4e5f\u900a\u8272\u4e8e\u7b80\u5355\u7684\u76d1\u7763\u6a21\u578b\u3002", "method": "\u5229\u7528LLM agent scientists\u8fdb\u884c\u7ed3\u6784\u5316\u79d1\u5b66\u7814\u7a76\uff0c\u5305\u62ec\u4e2a\u4f53\u8c03\u67e5\u3001\u81ea\u6211\u5ba1\u67e5\u548c\u540c\u884c\u8bc4\u5ba1\u3002", "result": "PanelTR\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u7ee9\uff0c\u8d85\u8d8a\u4e86\u666e\u901aLLM\uff0c\u4e0e\u5168\u76d1\u7763\u6a21\u578b\u6027\u80fd\u76f8\u5f53\uff0c\u4e14\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "PanelTR\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u79d1\u5b66\u5bb6\u8fdb\u884c\u7ed3\u6784\u5316\u7814\u7a76\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u666e\u901aLLM\uff0c\u4e0e\u5168\u76d1\u7763\u6a21\u578b\u4e0d\u76f8\u4e0a\u4e0b\uff0c\u4e14\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u3002"}}
{"id": "2508.06111", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06111", "abs": "https://arxiv.org/abs/2508.06111", "authors": ["Dewi S. W. Gould", "Bruno Mlodozeniec", "Samuel F. Brown"], "title": "SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges", "comment": "7 pages and appendices", "summary": "Evaluating the capabilities and risks of foundation models is paramount, yet\ncurrent methods demand extensive domain expertise, hindering their scalability\nas these models rapidly evolve. We introduce SKATE: a novel evaluation\nframework in which large language models (LLMs) compete by generating and\nsolving verifiable tasks for one another. Our core insight is to treat\nevaluation as a game: models act as both task-setters and solvers, incentivized\nto create questions which highlight their own strengths while exposing others'\nweaknesses. SKATE offers several key advantages, balancing scalability,\nopen-endedness, and objectivity. It is fully automated, data-free, and\nscalable, requiring no human input or domain expertise. By using verifiable\ntasks rather than LLM judges, scoring is objective. Unlike domain-limited\nprogrammatically-generated benchmarks (e.g. chess-playing or spatial\nreasoning), having LLMs creatively pose challenges enables open-ended and\nscalable evaluation. As a proof of concept, we introduce LLM-set\ncode-output-prediction (COP) challenges as a verifiable and extensible\nframework in which to test our approach. Using a TrueSkill-based ranking\nsystem, we evaluate six frontier LLMs and find that: (1) weaker models can\nreliably differentiate and score stronger ones, (2) LLM-based systems are\ncapable of self-preferencing behavior, generating questions that align with\ntheir own capabilities, and (3) SKATE automatically surfaces fine-grained\ncapability differences between models. Our findings are an important step\ntowards general, scalable evaluation frameworks which can keep pace with LLM\nprogress.", "AI": {"tldr": "SKATE\u6846\u67b6\u901a\u8fc7\u6a21\u578b\u95f4\u7684\u76f8\u4e92\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u548c\u98ce\u9669\u7684\u5ba2\u89c2\u3001\u53ef\u6269\u5c55\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u7684\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7684\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u96be\u4ee5\u968f\u7740\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u800c\u6269\u5c55\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSKATE\u7684\u65b0\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u8bc4\u4f30\u89c6\u4e3a\u4e00\u4e2a\u6e38\u620f\uff0c\u6a21\u578b\u65e2\u662f\u51fa\u9898\u8005\u53c8\u662f\u89e3\u9898\u8005\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a\uff081\uff09\u8f83\u5f31\u7684\u6a21\u578b\u80fd\u591f\u53ef\u9760\u5730\u533a\u5206\u548c\u8bc4\u4f30\u66f4\u5f3a\u7684\u6a21\u578b\uff1b\uff082\uff09\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u80fd\u591f\u8fdb\u884c\u81ea\u6211\u504f\u597d\u884c\u4e3a\uff0c\u751f\u6210\u4e0e\u5176\u81ea\u8eab\u80fd\u529b\u76f8\u7b26\u7684\u95ee\u9898\uff1b\uff083\uff09SKATE\u80fd\u591f\u81ea\u52a8\u53d1\u73b0\u6a21\u578b\u4e4b\u95f4\u7ec6\u5fae\u7684\u80fd\u529b\u5dee\u5f02\u3002", "conclusion": "SKATE\u6846\u67b6\u901a\u8fc7\u8ba9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f8\u4e92\u51fa\u9898\u89e3\u9898\u6765\u8bc4\u4f30\u5176\u80fd\u529b\u548c\u98ce\u9669\uff0c\u5177\u6709\u53ef\u6269\u5c55\u6027\u3001\u5ba2\u89c2\u6027\u548c\u5f00\u653e\u6027\u7b49\u4f18\u70b9\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u6709\u6548\u53ef\u884c\u3002"}}
{"id": "2508.06129", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06129", "abs": "https://arxiv.org/abs/2508.06129", "authors": ["Bachtiar Herdianto", "Romain Billot", "Flavien Lucas", "Marc Sevaux"], "title": "Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem", "comment": "22 pages, 14 figures", "summary": "The Vehicle Routing Problem (VRP) is a complex optimization problem with\nnumerous real-world applications, mostly solved using metaheuristic algorithms\ndue to its $\\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely\non human-crafted designs developed through empirical studies. However, recent\nresearch shows that machine learning methods can be used the structural\ncharacteristics of solutions in combinatorial optimization, thereby aiding in\ndesigning more efficient algorithms, particularly for solving VRP. Building on\nthis advancement, this study extends the previous research by conducting a\nsensitivity analysis using multiple classifier models that are capable of\npredicting the quality of VRP solutions. Hence, by leveraging explainable AI,\nthis research is able to extend the understanding of how these models make\ndecisions. Finally, our findings indicate that while feature importance varies,\ncertain features consistently emerge as strong predictors. Furthermore, we\npropose a unified framework able of ranking feature impact across different\nscenarios to illustrate this finding. These insights highlight the potential of\nfeature importance analysis as a foundation for developing a guidance mechanism\nof metaheuristic algorithms for solving the VRP.", "AI": {"tldr": "\u5229\u7528\u673a\u5668\u5b66\u4e60\u548c\u53ef\u89e3\u91caAI\u5206\u6790VRP\u89e3\u7684\u7279\u5f81\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u6539\u8fdb\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u6307\u5bfc\u673a\u5236\u3002", "motivation": "\u4f20\u7edf\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\uff0c\u800c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u5229\u7528\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u7ed3\u6784\u7279\u5f81\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5206\u6790\u7279\u5f81\u91cd\u8981\u6027\u6765\u6539\u8fdbVRP\u7684\u6c42\u89e3\u7b97\u6cd5\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u5206\u7c7b\u5668\u6a21\u578b\u8fdb\u884c\u7075\u654f\u5ea6\u5206\u6790\uff0c\u7ed3\u5408\u53ef\u89e3\u91caAI\u6280\u672f\u5206\u6790\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u63d0\u51fa\u7279\u5f81\u5f71\u54cd\u6392\u5e8f\u7684\u7edf\u4e00\u6846\u67b6\u3002", "result": "\u53d1\u73b0\u67d0\u4e9b\u7279\u5f81\u59cb\u7ec8\u662f\u5f3a\u9884\u6d4b\u6307\u6807\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u5bf9\u4e0d\u540c\u573a\u666f\u4e0b\u7279\u5f81\u5f71\u54cd\u8fdb\u884c\u6392\u5e8f\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4e3a\u5143\u542f\u53d1\u5f0fVRP\u7b97\u6cd5\u7684\u6307\u5bfc\u673a\u5236\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u6269\u5c55\u4e86\u5148\u524d\u5229\u7528\u673a\u5668\u5b66\u4e60\u9884\u6d4bVRP\u89e3\u8d28\u91cf\u7684\u7814\u7a76\uff0c\u901a\u8fc7\u7075\u654f\u5ea6\u5206\u6790\u548c\u53ef\u89e3\u91caAI\uff0c\u8bc6\u522b\u51fa\u4e00\u81f4\u6709\u6548\u7684\u9884\u6d4b\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u5bf9\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u7279\u5f81\u5f71\u54cd\u8fdb\u884c\u6392\u5e8f\uff0c\u4e3a\u6539\u8fdb\u5143\u542f\u53d1\u5f0fVRP\u7b97\u6cd5\u63d0\u4f9b\u6307\u5bfc\u3002"}}
