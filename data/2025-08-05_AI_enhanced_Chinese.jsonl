{"id": "2508.00844", "categories": ["cs.AI", "cs.ET", "cs.MA", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2508.00844", "abs": "https://arxiv.org/abs/2508.00844", "authors": ["Christopher Wissuchek", "Patrick Zschech"], "title": "Exploring Agentic Artificial Intelligence Systems: Towards a Typological Framework", "comment": "Preprint accepted for archival and presentation at the Pacific-Asia\n  Conference on Information Systems (PACIS) 2025, Kuala Lumpur, Malaysia", "summary": "Artificial intelligence (AI) systems are evolving beyond passive tools into\nautonomous agents capable of reasoning, adapting, and acting with minimal human\nintervention. Despite their growing presence, a structured framework is lacking\nto classify and compare these systems. This paper develops a typology of\nagentic AI systems, introducing eight dimensions that define their cognitive\nand environmental agency in an ordinal structure. Using a multi-phase\nmethodological approach, we construct and refine this typology, which is then\nevaluated through a human-AI hybrid approach and further distilled into\nconstructed types. The framework enables researchers and practitioners to\nanalyze varying levels of agency in AI systems. By offering a structured\nperspective on the progression of AI capabilities, the typology provides a\nfoundation for assessing current systems and anticipating future developments\nin agentic AI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AI\u7cfb\u7edf\u7c7b\u578b\u7cfb\u7edf\uff0c\u7528\u4e8e\u5206\u7c7b\u548c\u6bd4\u8f83\u81ea\u4e3bAI\u7cfb\u7edf\u3002", "motivation": "\u7f3a\u4e4f\u5bf9\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u7ed3\u6784\u5316\u5206\u7c7b\u548c\u6bd4\u8f83\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u591a\u9636\u6bb5\u65b9\u6cd5\u6784\u5efa\u548c\u5b8c\u5584\u7c7b\u578b\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u4eba\u673a\u6df7\u5408\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u516b\u4e2a\u7ef4\u5ea6\u7684\u81ea\u4e3bAI\u7cfb\u7edf\u7c7b\u578b\u7cfb\u7edf\uff0c\u80fd\u591f\u5206\u6790AI\u7cfb\u7edf\u4e2d\u4e0d\u540c\u7a0b\u5ea6\u7684\u81ea\u4e3b\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u81ea\u4e3bAI\u7cfb\u7edf\u8fdb\u884c\u5206\u7c7b\u548c\u6bd4\u8f83\u7684\u516b\u7ef4\u7c7b\u578b\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u591a\u9636\u6bb5\u65b9\u6cd5\u6784\u5efa\u548c\u5b8c\u5584\u4e86\u8be5\u7cfb\u7edf\uff0c\u6700\u7ec8\u5f62\u6210\u7ed3\u6784\u5316\u7684\u7c7b\u578b\uff0c\u6709\u52a9\u4e8e\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u5206\u6790AI\u7cfb\u7edf\u4e2d\u4e0d\u540c\u7a0b\u5ea6\u7684\u81ea\u4e3b\u6027\uff0c\u5e76\u9884\u6d4b\u672a\u6765\u81ea\u4e3bAI\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.00853", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.00853", "abs": "https://arxiv.org/abs/2508.00853", "authors": ["Kei Itoh"], "title": "A Formal Framework for the Definition of 'State': Hierarchical Representation and Meta-Universe Interpretation", "comment": "43 pages, 8 figures, 8 Tables, in English, in Japanese", "summary": "This study aims to reinforce the theoretical foundation for diverse\nsystems--including the axiomatic definition of intelligence--by introducing a\nmathematically rigorous and unified formal structure for the concept of\n'state,' which has long been used without consensus or formal clarity. First, a\n'hierarchical state grid' composed of two axes--state depth and mapping\nhierarchy--is proposed to provide a unified notational system applicable across\nmathematical, physical, and linguistic domains. Next, the 'Intermediate\nMeta-Universe (IMU)' is introduced to enable explicit descriptions of definers\n(ourselves) and the languages we use, thereby allowing conscious meta-level\noperations while avoiding self-reference and logical inconsistency. Building on\nthis meta-theoretical foundation, this study expands inter-universal theory\nbeyond mathematics to include linguistic translation and agent integration,\nintroducing the conceptual division between macrocosm-inter-universal and\nmicrocosm-inter-universal operations for broader expressivity. Through these\ncontributions, this paper presents a meta-formal logical framework--grounded in\nthe principle of definition = state--that spans time, language, agents, and\noperations, providing a mathematically robust foundation applicable to the\ndefinition of intelligence, formal logic, and scientific theory at large.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u7684\u5143\u5f62\u5f0f\u903b\u8f91\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u5bf9\u2018\u72b6\u6001\u2019\u6982\u5ff5\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u667a\u80fd\u7684\u5b9a\u4e49\u63d0\u4f9b\u4e86\u6570\u5b66\u57fa\u7840\u3002", "motivation": "\u4e3a\u4e86\u5f3a\u5316\u591a\u5143\u7cfb\u7edf\u7684\u7406\u8bba\u57fa\u7840\uff0c\u7279\u522b\u662f\u667a\u80fd\u7684\u516c\u7406\u5316\u5b9a\u4e49\uff0c\u5bf9\u957f\u671f\u4ee5\u6765\u7f3a\u4e4f\u5171\u8bc6\u548c\u5f62\u5f0f\u6e05\u6670\u5ea6\u7684'\u72b6\u6001'\u6982\u5ff5\u8fdb\u884c\u5f62\u5f0f\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u5206\u5c42\u72b6\u6001\u7f51\u683c\u548c\u4e2d\u95f4\u5143\u5b87\u5b99 (IMU) \u7b49\u6982\u5ff5\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u9002\u7528\u4e8e\u6570\u5b66\u3001\u7269\u7406\u548c\u8bed\u8a00\u9886\u57df\u7684\u7b26\u53f7\u7cfb\u7edf\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u8de8\u8d8a\u65f6\u95f4\u3001\u8bed\u8a00\u3001\u4e3b\u4f53\u548c\u64cd\u4f5c\u7684\u5143\u5f62\u5f0f\u903b\u8f91\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u8de8\u5b87\u5b99\u7406\u8bba\uff0c\u5e76\u589e\u5f3a\u4e86\u8868\u8fbe\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u201c\u5b9a\u4e49=\u72b6\u6001\u201d\u539f\u5219\u7684\u5143\u5f62\u5f0f\u903b\u8f91\u6846\u67b6\uff0c\u4e3a\u667a\u80fd\u3001\u5f62\u5f0f\u903b\u8f91\u548c\u79d1\u5b66\u7406\u8bba\u63d0\u4f9b\u4e86\u6570\u5b66\u57fa\u7840\u3002"}}
{"id": "2508.00890", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.00890", "abs": "https://arxiv.org/abs/2508.00890", "authors": ["Fali Wang", "Hui Liu", "Zhenwei Dai", "Jingying Zeng", "Zhiwei Zhang", "Zongyu Wu", "Chen Luo", "Zhen Li", "Xianfeng Tang", "Qi He", "Suhang Wang"], "title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks", "comment": "Under review", "summary": "Test-time scaling (TTS) enhances the performance of large language models\n(LLMs) by allocating additional compute resources during inference. However,\nexisting research primarily investigates TTS in single-stage tasks; while many\nreal-world problems are multi-stage complex tasks, composed of a sequence of\nheterogeneous subtasks with each subtask requires LLM of specific capability.\nTherefore, we study a novel problem: the test-time compute-optimal scaling in\nmulti-stage complex tasks, aiming to select suitable models and allocate\nbudgets per subtask to maximize overall performance. TTS in multi-stage tasks\nintroduces two fundamental challenges: (i) The combinatorial search space of\nmodel and budget allocations, combined with the high cost of inference, makes\nbrute-force search impractical. (ii) The optimal model and budget allocations\nacross subtasks are interdependent, increasing the complexity of the\ncompute-optimal search. To address this gap, we conduct extensive pilot\nexperiments on four tasks across six datasets, deriving three empirical\ninsights characterizing the behavior of LLMs in multi-stage complex tasks.\nInformed by these insights, we propose AgentTTS, an LLM-agent-based framework\nthat autonomously searches for compute-optimal allocations through iterative\nfeedback-driven interactions with the execution environment. Experimental\nresults demonstrate that AgentTTS significantly outperforms traditional and\nother LLM-based baselines in search efficiency, and shows improved robustness\nto varying training set sizes and enhanced interpretability.", "AI": {"tldr": "\u9488\u5bf9\u591a\u9636\u6bb5\u590d\u6742\u4efb\u52a1\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6700\u4f18\u7f29\u653e\u95ee\u9898\uff0c\u63d0\u51faAgentTTS\u6846\u67b6\uff0c\u901a\u8fc7LLM-agent\u81ea\u4e3b\u641c\u7d22\u8ba1\u7b97\u6700\u4f18\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5355\u9636\u6bb5\u4efb\u52a1\u7684TTS\uff0c\u800c\u5b9e\u9645\u95ee\u9898\u591a\u4e3a\u591a\u9636\u6bb5\u590d\u6742\u4efb\u52a1\uff0c\u6bcf\u4e2a\u5b50\u4efb\u52a1\u9700\u8981\u7279\u5b9a\u80fd\u529b\u7684LLM\uff0c\u56e0\u6b64\u7814\u7a76\u591a\u9636\u6bb5\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6700\u4f18\u7f29\u653e\u3002", "method": "\u63d0\u51faAgentTTS\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM-agent\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u53cd\u9988\u9a71\u52a8\u7684\u4ea4\u4e92\u81ea\u4e3b\u641c\u7d22\u8ba1\u7b97\u6700\u4f18\u5206\u914d\u3002", "result": "AgentTTS\u5728\u641c\u7d22\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u5176\u5b83\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u663e\u793a\u51fa\u5bf9\u4e0d\u540c\u8bad\u7ec3\u96c6\u5927\u5c0f\u7684\u589e\u5f3a\u9c81\u68d2\u6027\u548c\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "AgentTTS\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u5176\u5b83\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u641c\u7d22\u6548\u7387\u3001\u5bf9\u4e0d\u540c\u8bad\u7ec3\u96c6\u5927\u5c0f\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u6709\u6240\u63d0\u5347\u3002"}}
{"id": "2508.00899", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00899", "abs": "https://arxiv.org/abs/2508.00899", "authors": ["Abeer Dyoub", "Ivan Letteri", "Francesca A. Lisi"], "title": "ff4ERA: A new Fuzzy Framework for Ethical Risk Assessment in AI", "comment": null, "summary": "The emergence of Symbiotic AI (SAI) introduces new challenges to ethical\ndecision-making as it deepens human-AI collaboration. As symbiosis grows, AI\nsystems pose greater ethical risks, including harm to human rights and trust.\nEthical Risk Assessment (ERA) thus becomes crucial for guiding decisions that\nminimize such risks. However, ERA is hindered by uncertainty, vagueness, and\nincomplete information, and morality itself is context-dependent and imprecise.\nThis motivates the need for a flexible, transparent, yet robust framework for\nERA. Our work supports ethical decision-making by quantitatively assessing and\nprioritizing multiple ethical risks so that artificial agents can select\nactions aligned with human values and acceptable risk levels. We introduce\nff4ERA, a fuzzy framework that integrates Fuzzy Logic, the Fuzzy Analytic\nHierarchy Process (FAHP), and Certainty Factors (CF) to quantify ethical risks\nvia an Ethical Risk Score (ERS) for each risk type. The final ERS combines the\nFAHP-derived weight, propagated CF, and risk level. The framework offers a\nrobust mathematical approach for collaborative ERA modeling and systematic,\nstep-by-step analysis. A case study confirms that ff4ERA yields\ncontext-sensitive, ethically meaningful risk scores reflecting both expert\ninput and sensor-based evidence. Risk scores vary consistently with relevant\nfactors while remaining robust to unrelated inputs. Local sensitivity analysis\nshows predictable, mostly monotonic behavior across perturbations, and global\nSobol analysis highlights the dominant influence of expert-defined weights and\ncertainty factors, validating the model design. Overall, the results\ndemonstrate ff4ERA ability to produce interpretable, traceable, and risk-aware\nethical assessments, enabling what-if analyses and guiding designers in\ncalibrating membership functions and expert judgments for reliable ethical\ndecision support.", "AI": {"tldr": "\u63d0\u51faff4ERA\u6846\u67b6\uff0c\u5229\u7528\u6a21\u7cca\u903b\u8f91\u7b49\u65b9\u6cd5\u91cf\u5316\u4f26\u7406\u98ce\u9669\uff0c\u8f85\u52a9Symbiotic AI\u7684\u4f26\u7406\u51b3\u7b56\u3002", "motivation": "Symbiotic AI (SAI)\u7684\u51fa\u73b0\u7ed9\u4f26\u7406\u51b3\u7b56\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\uff0c\u4f26\u7406\u98ce\u9669\u8bc4\u4f30(ERA)\u5bf9\u4e8e\u6307\u5bfc\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u6b64\u7c7b\u98ce\u9669\u7684\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46ERA\u53d7\u5230\u4e0d\u786e\u5b9a\u6027\u3001\u6a21\u7cca\u6027\u548c\u4fe1\u606f\u4e0d\u5b8c\u6574\u6027\u7684\u963b\u788d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u7075\u6d3b\u3001\u900f\u660e\u4e14\u7a33\u5065\u7684ERA\u6846\u67b6\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aff4ERA\u7684\u6a21\u7cca\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u6a21\u7cca\u903b\u8f91\u3001\u6a21\u7cca\u5c42\u6b21\u5206\u6790\u6cd5(FAHP)\u548c\u786e\u5b9a\u6027\u56e0\u5b50(CF)\uff0c\u901a\u8fc7\u6bcf\u4e2a\u98ce\u9669\u7c7b\u578b\u7684\u4f26\u7406\u98ce\u9669\u8bc4\u5206(ERS)\u6765\u91cf\u5316\u4f26\u7406\u98ce\u9669\u3002\u6700\u7ec8\u7684ERS\u7ed3\u5408\u4e86FAHP\u5bfc\u51fa\u7684\u6743\u91cd\u3001\u4f20\u64ad\u7684CF\u548c\u98ce\u9669\u7b49\u7ea7\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8bc1\u5b9e\uff0cff4ERA\u4ea7\u751f\u4e86\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u3001\u5177\u6709\u4f26\u7406\u610f\u4e49\u7684\u98ce\u9669\u8bc4\u5206\uff0c\u53cd\u6620\u4e86\u4e13\u5bb6\u8f93\u5165\u548c\u57fa\u4e8e\u4f20\u611f\u5668\u7684\u8bc1\u636e\u3002\u98ce\u9669\u8bc4\u5206\u968f\u76f8\u5173\u56e0\u7d20\u4e00\u81f4\u53d8\u5316\uff0c\u540c\u65f6\u5bf9\u65e0\u5173\u8f93\u5165\u4fdd\u6301\u7a33\u5065\u3002\u5c40\u90e8\u654f\u611f\u6027\u5206\u6790\u663e\u793a\u51fa\u53ef\u9884\u6d4b\u7684\u3001\u5927\u90e8\u5206\u5355\u8c03\u7684\u884c\u4e3a\uff0c\u5168\u5c40Sobol\u5206\u6790\u7a81\u51fa\u4e86\u4e13\u5bb6\u5b9a\u4e49\u7684\u6743\u91cd\u548c\u786e\u5b9a\u6027\u56e0\u5b50\u7684\u4e3b\u5bfc\u5f71\u54cd\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u8bbe\u8ba1\u3002", "conclusion": "ff4ERA\u6846\u67b6\u80fd\u591f\u4ea7\u751f\u53ef\u89e3\u91ca\u3001\u53ef\u8ffd\u6eaf\u548c\u98ce\u9669\u611f\u77e5\u7684\u4f26\u7406\u8bc4\u4f30\uff0c\u652f\u6301\u5047\u8bbe\u5206\u6790\uff0c\u5e76\u6307\u5bfc\u8bbe\u8ba1\u8005\u6821\u51c6\u96b6\u5c5e\u51fd\u6570\u548c\u4e13\u5bb6\u5224\u65ad\uff0c\u4ee5\u5b9e\u73b0\u53ef\u9760\u7684\u4f26\u7406\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2508.00902", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.00902", "abs": "https://arxiv.org/abs/2508.00902", "authors": ["Kenneth Payne"], "title": "An analysis of AI Decision under Risk: Prospect theory emerges in Large Language Models", "comment": "26 pages, 2 figures, 9 tables, 2 appendices", "summary": "Judgment of risk is key to decision-making under uncertainty. As Daniel\nKahneman and Amos Tversky famously discovered, humans do so in a distinctive\nway that departs from mathematical rationalism. Specifically, they demonstrated\nexperimentally that humans accept more risk when they feel themselves at risk\nof losing something than when they might gain. I report the first tests of\nKahneman and Tversky's landmark 'prospect theory' with Large Language Models,\nincluding today's state of the art chain-of-thought 'reasoners'.\n  In common with humans, I find that prospect theory often anticipates how\nthese models approach risky decisions across a range of scenarios. I also\ndemonstrate that context is key to explaining much of the variance in risk\nappetite. The 'frame' through which risk is apprehended appears to be embedded\nwithin the language of the scenarios tackled by the models. Specifically, I\nfind that military scenarios generate far larger 'framing effects' than do\ncivilian settings, ceteris paribus. My research suggests, therefore, that\nlanguage models the world, capturing our human heuristics and biases. But also\nthat these biases are uneven - the idea of a 'frame' is richer than simple\ngains and losses. Wittgenstein's notion of 'language games' explains the\ncontingent, localised biases activated by these scenarios. Finally, I use my\nfindings to reframe the ongoing debate about reasoning and memorisation in\nLLMs.", "AI": {"tldr": "LLM\u7ee7\u627f\u4e86\u4eba\u7c7b\u7684\u98ce\u9669\u8ba4\u77e5\u504f\u5dee\uff0c\u4e14\u8fd9\u79cd\u504f\u5dee\u4e0e\u8bed\u8a00\u60c5\u5883\u5bc6\u5207\u76f8\u5173\u3002", "motivation": "\u68c0\u9a8c\u524d\u666f\u7406\u8bba\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u4e2d\u98ce\u9669\u504f\u597d\u7684\u6765\u6e90\u3002", "method": "\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u4e00\u7cfb\u5217\u98ce\u9669\u51b3\u7b56\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u4e86\u524d\u666f\u7406\u8bba\u3002", "result": "LLM\u5728\u98ce\u9669\u51b3\u7b56\u4e2d\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u7c7b\u4f3c\u7684\u524d\u666f\u7406\u8bba\u504f\u5dee\uff1b\u60c5\u5883\uff08\u5c24\u5176\u662f\u8bed\u8a00\u6846\u67b6\uff09\u5bf9\u98ce\u9669\u504f\u597d\u6709\u663e\u8457\u5f71\u54cd\uff1b\u519b\u4e8b\u573a\u666f\u7684\u6846\u67b6\u6548\u5e94\u5927\u4e8e\u6c11\u7528\u573a\u666f\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u98ce\u9669\u504f\u597d\u4e0e\u4eba\u7c7b\u76f8\u4f3c\uff0c\u53d7\u60c5\u5883\u5f71\u54cd\uff0c\u5c24\u5176\u519b\u4e8b\u573a\u666f\u6bd4\u6c11\u7528\u573a\u666f\u66f4\u5bb9\u6613\u4ea7\u751f\u6846\u67b6\u6548\u5e94\u3002"}}
{"id": "2508.00914", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00914", "abs": "https://arxiv.org/abs/2508.00914", "authors": ["Dominic Simon", "Rickard Ewetz"], "title": "Knowledge Editing for Multi-Hop Question Answering Using Semantic Analysis", "comment": "14 pages, 15 figures, pre-print of paper accepted to IJCAI 2025", "summary": "Large Language Models (LLMs) require lightweight avenues of updating stored\ninformation that has fallen out of date. Knowledge Editing (KE) approaches have\nbeen successful in updating model knowledge for simple factual queries but\nstruggle with handling tasks that require compositional reasoning such as\nmulti-hop question answering (MQA). We observe that existing knowledge editors\nleverage decompositional techniques that result in illogical reasoning\nprocesses. In this paper, we propose a knowledge editor for MQA based on\nsemantic analysis called CHECK. Our framework is based on insights from an\nanalogy between compilers and reasoning using LLMs. Similar to how source code\nis first compiled before being executed, we propose to semantically analyze\nreasoning chains before executing the chains to answer questions. Reasoning\nchains with semantic errors are revised to ensure consistency through logic\noptimization and re-prompting the LLM model at a higher temperature. We\nevaluate the effectiveness of CHECK against five state-of-the-art frameworks on\nfour datasets and achieve an average 22.8% improved MQA accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u77e5\u8bc6\u7f16\u8f91\u5668CHECK\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u6790\u63d0\u9ad8\u591a\u8df3\u95ee\u9898\u56de\u7b54\u7684\u51c6\u786e\u6027\uff0c\u5e73\u5747\u63d0\u534722.8%\u3002", "motivation": "\u73b0\u6709\u7684\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u9700\u8981\u7ec4\u5408\u63a8\u7406\u7684\u4efb\u52a1\uff0c\u4f8b\u5982\u591a\u8df3\u95ee\u9898\u56de\u7b54(MQA)\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u5206\u89e3\u6280\u672f\uff0c\u5bfc\u81f4\u975e\u903b\u8f91\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5206\u6790\u7684MQA\u77e5\u8bc6\u7f16\u8f91\u5668CHECK\uff0c\u8be5\u6846\u67b6\u5c06\u63a8\u7406\u94fe\u8fdb\u884c\u8bed\u4e49\u5206\u6790\uff0c\u7c7b\u4f3c\u4e8e\u7f16\u8bd1\u5668\u7f16\u8bd1\u6e90\u4ee3\u7801\uff0c\u4ee5\u786e\u4fdd\u63a8\u7406\u94fe\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u903b\u8f91\u4f18\u5316\u548c\u91cd\u65b0\u63d0\u793aLLM\u6a21\u578b\u6765\u4fee\u6b63\u8bed\u4e49\u9519\u8bef\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e86MQA\u51c6\u786e\u738722.8%\u3002", "conclusion": "CHECK\u6846\u67b6\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e86MQA\u51c6\u786e\u738722.8%\uff0c\u4f18\u4e8e\u5176\u4ed6\u4e94\u4e2a\u6700\u5148\u8fdb\u7684\u6846\u67b6\u3002"}}
{"id": "2508.00967", "categories": ["cs.AI", "cs.RO", "68T07, 68T45, 93C85", "I.2.6; I.2.9; I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2508.00967", "abs": "https://arxiv.org/abs/2508.00967", "authors": ["Massoud Pourmandi"], "title": "Cooperative Perception: A Resource-Efficient Framework for Multi-Drone 3D Scene Reconstruction Using Federated Diffusion and NeRF", "comment": "15 pages, 3 figures, 1 table, 1 algorithm. Preprint based on NeurIPS\n  2024 template", "summary": "The proposal introduces an innovative drone swarm perception system that aims\nto solve problems related to computational limitations and low-bandwidth\ncommunication, and real-time scene reconstruction. The framework enables\nefficient multi-agent 3D/4D scene synthesis through federated learning of\nshared diffusion model and YOLOv12 lightweight semantic extraction and local\nNeRF updates while maintaining privacy and scalability. The framework redesigns\ngenerative diffusion models for joint scene reconstruction, and improves\ncooperative scene understanding, while adding semantic-aware compression\nprotocols. The approach can be validated through simulations and potential\nreal-world deployment on drone testbeds, positioning it as a disruptive\nadvancement in multi-agent AI for autonomous systems.", "AI": {"tldr": "\u4e00\u79cd\u521b\u65b0\u7684\u65e0\u4eba\u673a\u96c6\u7fa4\u611f\u77e5\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u548c\u8f7b\u91cf\u7ea7\u8bed\u4e49\u63d0\u53d6\u5b9e\u73b0\u9ad8\u6548\u76843D/4D\u573a\u666f\u5408\u6210\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u9650\u5236\u548c\u4f4e\u5e26\u5bbd\u901a\u4fe1\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u96c6\u7fa4\u611f\u77e5\u7cfb\u7edf\u4e2d\u8ba1\u7b97\u9650\u5236\u3001\u4f4e\u5e26\u5bbd\u901a\u4fe1\u548c\u5b9e\u65f6\u573a\u666f\u91cd\u5efa\u7b49\u95ee\u9898\u3002", "method": "\u8be5\u6846\u67b6\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u5171\u4eab\u6269\u6563\u6a21\u578b\u548cYOLOv12\u8f7b\u91cf\u7ea7\u8bed\u4e49\u63d0\u53d6\u4ee5\u53ca\u5c40\u90e8NeRF\u66f4\u65b0\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f533D/4D\u573a\u666f\u5408\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u9690\u79c1\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u4eff\u771f\u548c\u6f5c\u5728\u7684\u65e0\u4eba\u673a\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u7684\u5b9e\u9645\u90e8\u7f72\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4f7f\u5176\u6210\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u591a\u667a\u80fd\u4f53AI\u9886\u57df\u7684\u4e00\u6b21\u7a81\u7834\u6027\u8fdb\u5c55\u3002", "conclusion": "\u8be5\u63d0\u6848\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65e0\u4eba\u673a\u96c6\u7fa4\u611f\u77e5\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u8ba1\u7b97\u9650\u5236\u3001\u4f4e\u5e26\u5bbd\u901a\u4fe1\u548c\u5b9e\u65f6\u573a\u666f\u91cd\u5efa\u7b49\u95ee\u9898\u3002"}}
{"id": "2508.01012", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01012", "abs": "https://arxiv.org/abs/2508.01012", "authors": ["Yiyi Lu", "Hoi Ian Au", "Junyao Zhang", "Jingyu Pan", "Yiting Wang", "Ang Li", "Jianyi Zhang", "Yiran Chen"], "title": "AutoEDA: Enabling EDA Flow Automation through Microservice-Based LLM Agents", "comment": null, "summary": "Modern Electronic Design Automation (EDA) workflows, especially the\nRTL-to-GDSII flow, require heavily manual scripting and demonstrate a multitude\nof tool-specific interactions which limits scalability and efficiency. While\nLLMs introduces strides for automation, existing LLM solutions require\nexpensive fine-tuning and do not contain standardized frameworks for\nintegration and evaluation. We introduce AutoEDA, a framework for EDA\nautomation that leverages paralleled learning through the Model Context\nProtocol (MCP) specific for standardized and scalable natural language\nexperience across the entire RTL-to-GDSII flow. AutoEDA limits fine-tuning\nthrough structured prompt engineering, implements intelligent parameter\nextraction and task decomposition, and provides an extended CodeBLEU metric to\nevaluate the quality of TCL scripts. Results from experiments over five\npreviously curated benchmarks show improvements in automation accuracy and\nefficiency, as well as script quality when compared to existing methods.\nAutoEDA is released open-sourced to support reproducibility and the EDA\ncommunity. Available at: https://github.com/AndyLu666/MCP-EDA-Server", "AI": {"tldr": "AutoEDA\u662f\u4e00\u4e2a\u5f00\u6e90\u7684EDA\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5229\u7528\u5e76\u884c\u5b66\u4e60\u548c\u7ed3\u6784\u5316\u63d0\u793a\u5de5\u7a0b\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u6539\u8fdb\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u7684\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316(EDA)\u6d41\u7a0b\u6548\u7387\u4f4e\u4e0b\uff0c\u4f9d\u8d56\u5927\u91cf\u624b\u52a8\u811a\u672c\u548c\u5de5\u5177\u7279\u5b9a\u7684\u4ea4\u4e92\u3002LLM\u867d\u7136\u80fd\u5b9e\u73b0\u81ea\u52a8\u5316\uff0c\u4f46\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u9700\u8981\u6602\u8d35\u7684\u5fae\u8c03\uff0c\u4e14\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u96c6\u6210\u548c\u8bc4\u4f30\u6846\u67b6\u3002", "method": "AutoEDA\u6846\u67b6\u5229\u7528\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae(MCP)\u5b9e\u73b0\u5e76\u884c\u5b66\u4e60\uff0c\u8fdb\u884c\u667a\u80fd\u53c2\u6570\u63d0\u53d6\u548c\u4efb\u52a1\u5206\u89e3\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u5de5\u7a0b\u51cf\u5c11\u5fae\u8c03\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cAutoEDA\u5728\u81ea\u52a8\u5316\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u811a\u672c\u8d28\u91cf\u65b9\u9762\u5747\u6709\u6240\u63d0\u9ad8\u3002", "conclusion": "AutoEDA\u6846\u67b6\u901a\u8fc7\u5e76\u884c\u5b66\u4e60\u548c\u7ed3\u6784\u5316\u63d0\u793a\u5de5\u7a0b\u63d0\u9ad8\u4e86EDA\u81ea\u52a8\u5316\u6d41\u7a0b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6269\u5c55\u7684CodeBLEU\u6307\u6807\u6765\u8bc4\u4f30TCL\u811a\u672c\u7684\u8d28\u91cf\u3002"}}
{"id": "2508.01031", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01031", "abs": "https://arxiv.org/abs/2508.01031", "authors": ["Jingzhe Ni", "Xiaolong Yin", "Xintong Li", "Xingyu Lu", "Ji Wei", "Ruofeng Tong", "Min Tang", "Peng Du"], "title": "CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent", "comment": null, "summary": "Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing\nbut typically requires a high level of expertise from designers. To lower the\nentry barrier and improve design efficiency, we present an agent for CAD\nconceptual design powered by large language models (LLMs). The agent accepts\nboth abstract textual descriptions and freehand sketches as input, engaging in\ninteractive dialogue with users to refine and clarify design requirements\nthrough comprehensive requirement analysis. Built upon a novel\nContext-Independent Imperative Paradigm (CIP), the agent generates high-quality\nCAD modeling code. During the generation process, the agent incorporates\niterative visual feedback to improve model quality. Generated design cases are\nstored in a structured knowledge base, enabling continuous improvement of the\nagent's code generation capabilities. Experimental results demonstrate that our\nmethod achieves state-of-the-art performance in CAD code generation.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u548c\u8fed\u4ee3\u89c6\u89c9\u53cd\u9988\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cfCAD\u4ee3\u7801\u81ea\u52a8\u751f\u6210\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u964d\u4f4eCAD\u8bbe\u8ba1\u7684\u51c6\u5165\u95e8\u69db\uff0c\u63d0\u9ad8\u8bbe\u8ba1\u6548\u7387\u3002", "method": "\u8be5\u667a\u80fd\u4f53\u57fa\u4e8e\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u65e0\u5173\u547d\u4ee4\u8303\u5f0f\uff08CIP\uff09\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884cCAD\u6982\u5ff5\u8bbe\u8ba1\uff0c\u5e76\u7ed3\u5408\u8fed\u4ee3\u89c6\u89c9\u53cd\u9988\u4ee5\u63d0\u9ad8\u6a21\u578b\u8d28\u91cf\u3002", "result": "\u8be5\u667a\u80fd\u4f53\u80fd\u591f\u6839\u636e\u62bd\u8c61\u6587\u672c\u63cf\u8ff0\u548c\u624b\u7ed8\u8349\u56fe\u751f\u6210\u9ad8\u8d28\u91cf\u7684CAD\u5efa\u6a21\u4ee3\u7801\uff0c\u5e76\u901a\u8fc7\u4e0e\u7528\u6237\u7684\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u6765\u6539\u8fdb\u548c\u9610\u660e\u8bbe\u8ba1\u9700\u6c42\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728CAD\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.01057", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.01057", "abs": "https://arxiv.org/abs/2508.01057", "authors": ["Fengze Yang", "Bo Yu", "Yang Zhou", "Xuewen Luo", "Zhengzhong Tu", "Chenxi Liu"], "title": "REACT: A Real-Time Edge-AI Based V2X Framework for Accident Avoidance in Autonomous Driving System", "comment": "24 pages, 6 tables, 7 figures", "summary": "Collisions caused by human error are the most common type of multi-vehicle\ncrash, highlighting the critical need for autonomous driving (AD) systems to\nleverage cooperative perception through Vehicle-to-Everything (V2X)\ncommunication. This capability extends situational awareness beyond the\nlimitations of onboard sensors. However, current transformer-based V2X\nframeworks suffer from limited generalization, shallow contextual reasoning,\nand reliance on mono-modal inputs. Vision-Language Models (VLMs) offer enhanced\nreasoning and multimodal integration but typically fall short of real-time\nperformance requirements in safety-critical applications. This paper presents\nREACT, a real-time, V2X-integrated trajectory optimization framework built upon\na fine-tuned lightweight VLM. REACT integrates a set of specialized modules\nthat process multimodal inputs into optimized, risk-aware trajectories. To\nensure real-time performance on edge devices, REACT incorporates edge\nadaptation strategies that reduce model complexity and accelerate inference.\nEvaluated on the DeepAccident benchmark, REACT achieves state-of-the-art\nperformance, a 77% collision rate reduction, a 48.2% Video Panoptic Quality\n(VPQ), and a 0.57-second inference latency on the Jetson AGX Orin. Ablation\nstudies validate the contribution of each input, module, and edge adaptation\nstrategy. These results demonstrate the feasibility of lightweight VLMs for\nreal-time edge-based cooperative planning and showcase the potential of\nlanguage-guided contextual reasoning to improve safety and responsiveness in\nautonomous driving.", "AI": {"tldr": "\u8f7b\u91cf\u7ea7VLM\u7528\u4e8e\u5b9e\u65f6\u8f66\u7aef\u534f\u540c\u89c4\u5212\uff0c\u8bed\u8a00\u5f15\u5bfc\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u63d0\u9ad8\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eTransformer\u7684V2X\u6846\u67b6\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3001\u4e0a\u4e0b\u6587\u63a8\u7406\u6d45\u663e\u4ee5\u53ca\u4f9d\u8d56\u5355\u6a21\u6001\u8f93\u5165\u7b49\u95ee\u9898\u3002\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u867d\u7136\u5177\u6709\u589e\u5f3a\u7684\u63a8\u7406\u548c\u591a\u6a21\u6001\u96c6\u6210\u80fd\u529b\uff0c\u4f46\u5728\u5b89\u5168\u5173\u952e\u578b\u5e94\u7528\u4e2d\u901a\u5e38\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u6027\u80fd\u8981\u6c42\u3002", "method": "REACT\u662f\u4e00\u4e2a\u57fa\u4e8e\u5fae\u8c03\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u7684\u5b9e\u65f6V2X\u96c6\u6210\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\u7684\u4e00\u7ec4\u4e13\u7528\u6a21\u5757\uff0c\u751f\u6210\u4f18\u5316\u7684\u98ce\u9669\u611f\u77e5\u8f68\u8ff9\uff0c\u5e76\u7ed3\u5408\u8fb9\u7f18\u9002\u5e94\u7b56\u7565\u4ee5\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u548c\u52a0\u5feb\u63a8\u7406\u901f\u5ea6\u3002", "result": "\u78b0\u649e\u7387\u964d\u4f4e77%\uff0cVPQ\u63d0\u9ad848.2%\uff0c\u63a8\u7406\u5ef6\u8fdf0.57\u79d2\u3002", "conclusion": "REACT\u6846\u67b6\u5728DeepAccident\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u78b0\u649e\u7387\u964d\u4f4e\u4e8677%\uff0cVPQ\u63d0\u9ad8\u4e8648.2%\uff0c\u63a8\u7406\u5ef6\u8fdf\u4ec5\u4e3a0.57\u79d2\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u6bcf\u4e2a\u8f93\u5165\u3001\u6a21\u5757\u548c\u8fb9\u7f18\u9002\u5e94\u7b56\u7565\u7684\u8d21\u732e\u3002"}}
{"id": "2508.01073", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01073", "abs": "https://arxiv.org/abs/2508.01073", "authors": ["Martin B\u00f6ckling", "Heiko Paulheim"], "title": "gpuRDF2vec -- Scalable GPU-based RDF2vec", "comment": "18 pages, ISWC 2025", "summary": "Generating Knowledge Graph (KG) embeddings at web scale remains challenging.\nAmong existing techniques, RDF2vec combines effectiveness with strong\nscalability. We present gpuRDF2vec, an open source library that harnesses\nmodern GPUs and supports multi-node execution to accelerate every stage of the\nRDF2vec pipeline. Extensive experiments on both synthetically generated graphs\nand real-world benchmarks show that gpuRDF2vec achieves up to a substantial\nspeedup over the currently fastest alternative, i.e., jRDF2vec. In a\nsingle-node setup, our walk-extraction phase alone outperforms pyRDF2vec,\nSparkKGML, and jRDF2vec by a substantial margin using random walks on large/\ndense graphs, and scales very well to longer walks, which typically lead to\nbetter quality embeddings. Our implementation of gpuRDF2vec enables\npractitioners and researchers to train high-quality KG embeddings on\nlarge-scale graphs within practical time budgets and builds on top of Pytorch\nLightning for the scalable word2vec implementation.", "AI": {"tldr": "gpuRDF2vec\u5e93\u5229\u7528GPU\u52a0\u901fRDF2vec\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u751f\u6210\u7684\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u751f\u6210web\u89c4\u6a21\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0cRDF2vec\u517c\u5177\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u4ecd\u9700\u52a0\u901f\u3002", "method": "\u5229\u7528GPU\u548c\u591a\u8282\u70b9\u6267\u884c\u52a0\u901fRDF2vec\u6d41\u7a0b\u7684\u6bcf\u4e2a\u9636\u6bb5\uff0c\u57fa\u4e8ePytorch Lightning\u5b9e\u73b0\u53ef\u6269\u5c55\u7684word2vec\u3002", "result": "gpuRDF2vec\u5728\u5408\u6210\u56fe\u548c\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\uff0c\u5355\u8282\u70b9\u8bbe\u7f6e\u4e0b\uff0c\u5176walk-extraction\u9636\u6bb5\u663e\u8457\u4f18\u4e8epyRDF2vec\u3001SparkKGML\u548cjRDF2vec\u3002", "conclusion": "gpuRDF2vec\u5e93\u901a\u8fc7\u5229\u7528\u73b0\u4ee3GPU\u548c\u652f\u6301\u591a\u8282\u70b9\u6267\u884c\uff0c\u663e\u8457\u52a0\u901f\u4e86RDF2vec\u6d41\u7a0b\u7684\u6bcf\u4e2a\u9636\u6bb5\uff0c\u5728\u5927\u578b\u56fe\u4e0a\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\uff0c\u5e76\u5728\u5355\u8282\u70b9\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.01097", "categories": ["cs.AI", "nlin.AO", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.01097", "abs": "https://arxiv.org/abs/2508.01097", "authors": ["Neil F. Johnson", "Frank Yingjie Huo"], "title": "Multispin Physics of AI Tipping Points and Hallucinations", "comment": null, "summary": "Output from generative AI such as ChatGPT, can be repetitive and biased. But\nmore worrying is that this output can mysteriously tip mid-response from good\n(correct) to bad (misleading or wrong) without the user noticing. In 2024\nalone, this reportedly caused $67 billion in losses and several deaths.\nEstablishing a mathematical mapping to a multispin thermal system, we reveal a\nhidden tipping instability at the scale of the AI's 'atom' (basic Attention\nhead). We derive a simple but essentially exact formula for this tipping point\nwhich shows directly the impact of a user's prompt choice and the AI's training\nbias. We then show how the output tipping can get amplified by the AI's\nmultilayer architecture. As well as helping improve AI transparency,\nexplainability and performance, our results open a path to quantifying users'\nAI risk and legal liabilities.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u5b58\u5728\u9690\u853d\u7684\u9519\u8bef\u6a21\u5f0f\uff0c\u8be5\u7814\u7a76\u901a\u8fc7\u5efa\u7acb\u6570\u5b66\u6a21\u578b\u89e3\u91ca\u4e86\u5176\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u91cf\u5316\u7528\u6237\u98ce\u9669\u7684\u65b9\u6cd5\u3002", "motivation": "\u751f\u6210\u5f0fAI\u8f93\u51fa\u5b58\u5728\u91cd\u590d\u3001\u504f\u5dee\u4ee5\u53ca\u65e0\u5bdf\u89c9\u7684\u4ece\u6b63\u786e\u5230\u9519\u8bef\u7684\u8f6c\u53d8\uff0c\u9020\u6210\u5de8\u5927\u7ecf\u6d4e\u635f\u5931\u548c\u4eba\u5458\u4f24\u4ea1\u3002", "method": "\u5efa\u7acb\u751f\u6210\u5f0fAI\u4e0e\u591a\u81ea\u65cb\u70ed\u529b\u5b66\u7cfb\u7edf\u7684\u6570\u5b66\u6620\u5c04\uff0c\u63a8\u5bfc\u516c\u5f0f\u3002", "result": "\u63ed\u793a\u4e86AI\u7684\u201c\u539f\u5b50\u201d\uff08\u57fa\u672c\u6ce8\u610f\u529b\u673a\u5236\uff09\u5c42\u9762\u7684\u503e\u7ffb\u4e0d\u7a33\u5b9a\u6027\uff0c\u5bfc\u51fa\u4e86\u9884\u6d4b\u503e\u7ffb\u70b9\u7684\u516c\u5f0f\uff0c\u89e3\u91ca\u4e86\u7528\u6237\u63d0\u793a\u3001AI\u8bad\u7ec3\u504f\u5dee\u548c\u591a\u5c42\u67b6\u6784\u7684\u5f71\u54cd\u3002", "conclusion": "\u63ed\u793a\u4e86\u751f\u6210\u5f0fAI\u8f93\u51fa\u4e2d\u4e00\u79cd\u9690\u85cf\u7684\u201c\u503e\u7ffb\u201d\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u5bfc\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u516c\u5f0f\u6765\u9884\u6d4b\u8be5\u70b9\uff0c\u89e3\u91ca\u4e86\u7528\u6237\u63d0\u793a\u548cAI\u8bad\u7ec3\u504f\u5dee\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u591a\u5c42\u67b6\u6784\u5982\u4f55\u653e\u5927\u8fd9\u79cd\u503e\u7ffb\u3002"}}
{"id": "2508.01109", "categories": ["cs.AI", "68T07", "I.2; J.4"], "pdf": "https://arxiv.org/pdf/2508.01109", "abs": "https://arxiv.org/abs/2508.01109", "authors": ["Satiyabooshan Murugaboopathy", "Connor T. Jerzak", "Adel Daoud"], "title": "Platonic Representations for Poverty Mapping: Unified Vision-Language Codes or Agent-Induced Novelty?", "comment": "7 figures", "summary": "We investigate whether socio-economic indicators like household wealth leave\nrecoverable imprints in satellite imagery (capturing physical features) and\nInternet-sourced text (reflecting historical/economic narratives). Using\nDemographic and Health Survey (DHS) data from African neighborhoods, we pair\nLandsat images with LLM-generated textual descriptions conditioned on\nlocation/year and text retrieved by an AI search agent from web sources. We\ndevelop a multimodal framework predicting household wealth (International\nWealth Index) through five pipelines: (i) vision model on satellite images,\n(ii) LLM using only location/year, (iii) AI agent searching/synthesizing web\ntext, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework\nyields three contributions. First, fusing vision and agent/LLM text outperforms\nvision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on\nout-of-sample splits), with LLM-internal knowledge proving more effective than\nagent-retrieved text, improving robustness to out-of-country and out-of-time\ngeneralization. Second, we find partial representational convergence: fused\nembeddings from vision/language modalities correlate moderately (median cosine\nsimilarity of 0.60 after alignment), suggesting a shared latent code of\nmaterial well-being while retaining complementary details, consistent with the\nPlatonic Representation Hypothesis. Although LLM-only text outperforms\nagent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest\ngains from combining agent data in some splits weakly support the notion that\nagent-gathered information introduces unique representational structures not\nfully captured by static LLM knowledge. Third, we release a large-scale\nmultimodal dataset comprising more than 60,000 DHS clusters linked to satellite\nimages, LLM-generated descriptions, and agent-retrieved texts.", "AI": {"tldr": "\u878d\u5408\u536b\u661f\u56fe\u50cf\u548c\u591a\u79cd\u6587\u672c\u6570\u636e\u80fd\u66f4\u51c6\u786e\u9884\u6d4b\u975e\u6d32\u5bb6\u5ead\u8d22\u5bcc\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6570\u636e\u4f18\u4e8eAI\u4ee3\u7406\u6570\u636e\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002", "motivation": "\u7814\u7a76\u793e\u4f1a\u7ecf\u6d4e\u6307\u6807\u5982\u4f55\u5728\u536b\u661f\u56fe\u50cf\u548c\u4e92\u8054\u7f51\u6587\u672c\u4e2d\u7559\u4e0b\u53ef\u6062\u590d\u7684\u75d5\u8ff9\uff0c\u5e76\u63d0\u9ad8\u5bb6\u5ead\u8d22\u5bcc\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7a33\u5065\u6027\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001\u6846\u67b6\uff0c\u878d\u5408\u536b\u661f\u56fe\u50cf\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u548cAI\u4ee3\u7406\u68c0\u7d22\u7684\u7f51\u7edc\u6587\u672c\uff0c\u9884\u6d4b\u5bb6\u5ead\u8d22\u5bcc\uff08\u56fd\u9645\u8d22\u5bcc\u6307\u6570\uff09\u3002", "result": "\u878d\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u63d0\u9ad8\u4e86\u8d22\u5bcc\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff08\u4f8b\u5982\uff0c\u6837\u672c\u5916\u5206\u5272\u7684R\u5e73\u65b9\u4ece0.63\u63d0\u9ad8\u52300.77\uff09\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u6bd4AI\u4ee3\u7406\u68c0\u7d22\u7684\u6587\u672c\u66f4\u6709\u6548\uff0c\u6a21\u578b\u5728\u4e0d\u540c\u56fd\u5bb6\u548c\u65f6\u95f4\u6bb5\u7684\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002\u90e8\u5206\u8868\u5f81\u6536\u655b\uff0c\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\u7684\u878d\u5408\u5d4c\u5165\u5177\u6709\u4e2d\u7b49\u76f8\u5173\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u878d\u5408\u536b\u661f\u56fe\u50cf\u548c\u6587\u672c\u6570\u636e\uff08\u5305\u62ec\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u548cAI\u4ee3\u7406\u68c0\u7d22\u7684\u7f51\u7edc\u6587\u672c\uff09\u9884\u6d4b\u975e\u6d32\u793e\u533a\u7684\u5bb6\u5ead\u8d22\u5bcc\uff0c\u53d1\u73b0\u878d\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u4f18\u4e8e\u4ec5\u4f7f\u7528\u89c6\u89c9\u6570\u636e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u6bd4AI\u4ee3\u7406\u68c0\u7d22\u7684\u6587\u672c\u66f4\u6709\u6548\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u8d85\u8fc760,000\u4e2a DHS \u96c6\u7fa4\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002"}}
{"id": "2508.01158", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01158", "abs": "https://arxiv.org/abs/2508.01158", "authors": ["Yunlong Lin", "Zirui Li", "Guodong Du", "Xiaocong Zhao", "Cheng Gong", "Xinwei Wang", "Chao Lu", "Jianwei Gong"], "title": "H2C: Hippocampal Circuit-inspired Continual Learning for Lifelong Trajectory Prediction in Autonomous Driving", "comment": "Open source code: https://github.com/BIT-Jack/H2C-lifelong", "summary": "Deep learning (DL) has shown state-of-the-art performance in trajectory\nprediction, which is critical to safe navigation in autonomous driving (AD).\nHowever, most DL-based methods suffer from catastrophic forgetting, where\nadapting to a new distribution may cause significant performance degradation in\npreviously learned ones. Such inability to retain learned knowledge limits\ntheir applicability in the real world, where AD systems need to operate across\nvarying scenarios with dynamic distributions. As revealed by neuroscience, the\nhippocampal circuit plays a crucial role in memory replay, effectively\nreconstructing learned knowledge based on limited resources. Inspired by this,\nwe propose a hippocampal circuit-inspired continual learning method (H2C) for\ntrajectory prediction across varying scenarios. H2C retains prior knowledge by\nselectively recalling a small subset of learned samples. First, two\ncomplementary strategies are developed to select the subset to represent\nlearned knowledge. Specifically, one strategy maximizes inter-sample diversity\nto represent the distinctive knowledge, and the other estimates the overall\nknowledge by equiprobable sampling. Then, H2C updates via a memory replay loss\nfunction calculated by these selected samples to retain knowledge while\nlearning new data. Experiments based on various scenarios from the INTERACTION\ndataset are designed to evaluate H2C. Experimental results show that H2C\nreduces catastrophic forgetting of DL baselines by 22.71% on average in a\ntask-free manner, without relying on manually informed distributional shifts.\nThe implementation is available at https://github.com/BIT-Jack/H2C-lifelong.", "AI": {"tldr": "\u53d7\u6d77\u9a6c\u4f53\u542f\u53d1\uff0c\u63d0\u51faH2C\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u89e3\u51b3\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u6709\u6548\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u8f68\u8ff9\u9884\u6d4b\u4e2d\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002\u53d7\u795e\u7ecf\u79d1\u5b66\u4e2d\u6d77\u9a6c\u4f53\u7535\u8def\u5728\u8bb0\u5fc6\u91cd\u653e\u4e2d\u7684\u4f5c\u7528\u542f\u53d1\uff0c\u63d0\u51faH2C\u65b9\u6cd5\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u6d77\u9a6c\u4f53\u7535\u8def\u542f\u53d1\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5H2C\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e24\u79cd\u4e92\u8865\u7b56\u7565\u9009\u62e9\u4ee3\u8868\u5df2\u5b66\u4e60\u77e5\u8bc6\u7684\u6837\u672c\u5b50\u96c6\uff0c\u5e76\u901a\u8fc7\u8bb0\u5fc6\u91cd\u653e\u635f\u5931\u51fd\u6570\u66f4\u65b0\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cH2C\u5728\u65e0\u4efb\u52a1\u7684\u60c5\u51b5\u4e0b\uff0c\u5e73\u5747\u51cf\u5c11\u4e86\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u6a21\u578b22.71%\u7684\u707e\u96be\u6027\u9057\u5fd8\u3002", "conclusion": "H2C\u65b9\u6cd5\u901a\u8fc7\u9009\u62e9\u6027\u5730\u56de\u5fc6\u5c11\u91cf\u5b66\u4e60\u6837\u672c\uff0c\u51cf\u5c11\u4e86\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u6a21\u578b\u5e73\u574722.71%\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u65e0\u9700\u624b\u52a8\u544a\u77e5\u5206\u5e03\u53d8\u5316\u3002"}}
{"id": "2508.01181", "categories": ["cs.AI", "cs.CV", "cs.MM", "cs.SD", "eess.AS", "68", "I.2.10"], "pdf": "https://arxiv.org/pdf/2508.01181", "abs": "https://arxiv.org/abs/2508.01181", "authors": ["Zhiyuan Han", "Beier Zhu", "Yanlong Xu", "Peipei Song", "Xun Yang"], "title": "Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion Reasoning", "comment": "ACM Multimedia 2025", "summary": "Despite their strong performance in multimodal emotion reasoning, existing\nMultimodal Large Language Models (MLLMs) often overlook the scenarios involving\nemotion conflicts, where emotional cues from different modalities are\ninconsistent. To fill this gap, we first introduce CA-MER, a new benchmark\ndesigned to examine MLLMs under realistic emotion conflicts. It consists of\nthree subsets: video-aligned, audio-aligned, and consistent, where only one or\nall modalities reflect the true emotion. However, evaluations on our CA-MER\nreveal that current state-of-the-art emotion MLLMs systematically over-rely on\naudio signal during emotion conflicts, neglecting critical cues from visual\nmodality. To mitigate this bias, we propose MoSEAR, a parameter-efficient\nframework that promotes balanced modality integration. MoSEAR consists of two\nmodules: (1)MoSE, modality-specific experts with a regularized gating mechanism\nthat reduces modality bias in the fine-tuning heads; and (2)AR, an attention\nreallocation mechanism that rebalances modality contributions in frozen\nbackbones during inference. Our framework offers two key advantages: it\nmitigates emotion conflicts and improves performance on consistent\nsamples-without incurring a trade-off between audio and visual modalities.\nExperiments on multiple benchmarks-including MER2023, EMER, DFEW, and our\nCA-MER-demonstrate that MoSEAR achieves state-of-the-art performance,\nparticularly under modality conflict conditions.", "AI": {"tldr": "\u9488\u5bf9\u591a\u6a21\u6001\u60c5\u611f\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u51faMoSEAR\u6846\u67b6\uff0c\u6709\u6548\u7f13\u89e3\u6a21\u6001\u504f\u5dee\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLMs)\u5728\u5904\u7406\u60c5\u611f\u51b2\u7a81\u573a\u666f\u65f6\uff0c\u5bb9\u6613\u5ffd\u7565\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7684\u60c5\u611f\u51b2\u7a81\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u6846\u67b6MoSEAR\uff0c\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a\u6a21\u6001\u7279\u5b9a\u4e13\u5bb6MoSE\u548c\u6ce8\u610f\u529b\u91cd\u65b0\u5206\u914d\u673a\u5236AR\uff0c\u4ee5\u4fc3\u8fdb\u5e73\u8861\u7684\u6a21\u6001\u6574\u5408\uff0c\u51cf\u8f7b\u6a21\u6001\u504f\u5dee\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5(\u5305\u62ecMER2023, EMER, DFEW\u548cCA-MER)\u4e0a\uff0cMoSEAR\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6a21\u6001\u51b2\u7a81\u6761\u4ef6\u4e0b\u3002", "conclusion": "MoSEAR\u6846\u67b6\u5728\u591a\u6a21\u6001\u60c5\u611f\u51b2\u7a81\u573a\u666f\u4e0b\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6a21\u6001\u51b2\u7a81\u6761\u4ef6\u4e0b\u3002"}}
