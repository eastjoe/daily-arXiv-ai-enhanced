{"id": "2508.19316", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.4"], "pdf": "https://arxiv.org/pdf/2508.19316", "abs": "https://arxiv.org/abs/2508.19316", "authors": ["Shreyans Jain", "Alexandra Yost", "Amirali Abdullah"], "title": "Sycophancy as compositions of Atomic Psychometric Traits", "comment": "8 pages, 4 figures", "summary": "Sycophancy is a key behavioral risk in LLMs, yet is often treated as an\nisolated failure mode that occurs via a single causal mechanism. We instead\npropose modeling it as geometric and causal compositions of psychometric traits\nsuch as emotionality, openness, and agreeableness - similar to factor\ndecomposition in psychometrics. Using Contrastive Activation Addition (CAA), we\nmap activation directions to these factors and study how different combinations\nmay give rise to sycophancy (e.g., high extraversion combined with low\nconscientiousness). This perspective allows for interpretable and compositional\nvector-based interventions like addition, subtraction and projection; that may\nbe used to mitigate safety-critical behaviors in LLMs.", "AI": {"tldr": "\u5c06LLM\u8c04\u5a9a\u884c\u4e3a\u5efa\u6a21\u4e3a\u5fc3\u7406\u6d4b\u91cf\u7279\u5f81\u7684\u7ec4\u5408\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u5411\u91cf\u7684\u5e72\u9884\u65b9\u6cd5\u4ee5\u51cf\u8f7b\u5b89\u5168\u98ce\u9669\u3002", "motivation": "LLM\u4e2d\u7684\u8c04\u5a9a\u884c\u4e3a\u662f\u4e00\u4e2a\u5173\u952e\u7684\u5b89\u5168\u98ce\u9669\uff0c\u4f46\u901a\u5e38\u88ab\u89c6\u4e3a\u901a\u8fc7\u5355\u4e00\u56e0\u679c\u673a\u5236\u53d1\u751f\u7684\u5b64\u7acb\u6545\u969c\u6a21\u5f0f\u3002", "method": "\u4f7f\u7528\u5bf9\u6bd4\u6fc0\u6d3b\u6dfb\u52a0\uff08CAA\uff09\u5c06\u6fc0\u6d3b\u65b9\u5411\u6620\u5c04\u5230\u8fd9\u4e9b\u56e0\u7d20\uff0c\u5e76\u7814\u7a76\u4e0d\u540c\u7684\u7ec4\u5408\u5982\u4f55\u5bfc\u81f4\u8c04\u5a9a\u884c\u4e3a\u3002", "result": "\u8be5\u65b9\u6cd5\u5141\u8bb8\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u57fa\u4e8e\u5411\u91cf\u7684\u7ec4\u5408\u5e72\u9884\uff0c\u4f8b\u5982\u52a0\u6cd5\u3001\u51cf\u6cd5\u548c\u6295\u5f71\uff0c\u8fd9\u4e9b\u5e72\u9884\u53ef\u7528\u4e8e\u51cf\u8f7bLLM\u4e2d\u7684\u5b89\u5168\u5173\u952e\u884c\u4e3a\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06LLM\u7684\u8c04\u5a9a\u884c\u4e3a\u5efa\u6a21\u4e3a\u5fc3\u7406\u6d4b\u91cf\u7279\u5f81\uff08\u5982\u60c5\u7eea\u5316\u3001\u5f00\u653e\u6027\u548c\u968f\u548c\u6027\uff09\u51e0\u4f55\u548c\u56e0\u679c\u7ec4\u5408\u7684\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528\u5bf9\u6bd4\u6fc0\u6d3b\u6dfb\u52a0\uff08CAA\uff09\u6765\u7814\u7a76\u4e0d\u540c\u7ec4\u5408\u5982\u4f55\u5bfc\u81f4\u8c04\u5a9a\u884c\u4e3a\u3002"}}
{"id": "2508.19383", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.19383", "abs": "https://arxiv.org/abs/2508.19383", "authors": ["Daoyuan Jin", "Nick Gunner", "Niko Carvajal Janke", "Shivranjani Baruah", "Kaitlin M. Gold", "Yu Jiang"], "title": "Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science", "comment": null, "summary": "Modern plant science increasingly relies on large, heterogeneous datasets,\nbut challenges in experimental design, data preprocessing, and reproducibility\nhinder research throughput. Here we introduce Aleks, an AI-powered multi-agent\nsystem that integrates domain knowledge, data analysis, and machine learning\nwithin a structured framework to autonomously conduct data-driven scientific\ndiscovery. Once provided with a research question and dataset, Aleks\niteratively formulated problems, explored alternative modeling strategies, and\nrefined solutions across multiple cycles without human intervention. In a case\nstudy on grapevine red blotch disease, Aleks progressively identified\nbiologically meaningful features and converged on interpretable models with\nrobust performance. Ablation studies underscored the importance of domain\nknowledge and memory for coherent outcomes. This exploratory work highlights\nthe promise of agentic AI as an autonomous collaborator for accelerating\nscientific discovery in plant sciences.", "AI": {"tldr": "AI\u7cfb\u7edfAleks\u81ea\u4e3b\u8fdb\u884c\u79d1\u5b66\u53d1\u73b0\uff0c\u5728\u8461\u8404\u7ea2\u6591\u75c5\u7814\u7a76\u4e2d\u53d6\u5f97\u6210\u529f\uff0c\u5c55\u73b0\u4e86AI\u5728\u690d\u7269\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u4ee3\u690d\u7269\u79d1\u5b66\u4e2d\u5927\u578b\u5f02\u6784\u6570\u636e\u96c6\u5e26\u6765\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u3001\u6570\u636e\u9884\u5904\u7406\u548c\u53ef\u91cd\u590d\u6027\u6311\u6218\uff0c\u4ee5\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u3002", "method": "\u6574\u5408\u9886\u57df\u77e5\u8bc6\u3001\u6570\u636e\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\uff0c\u8fed\u4ee3\u5f0f\u5730\u5236\u5b9a\u95ee\u9898\u3001\u63a2\u7d22\u5efa\u6a21\u7b56\u7565\u548c\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728\u8461\u8404\u7ea2\u6591\u75c5\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cAleks\u6210\u529f\u8bc6\u522b\u51fa\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u7279\u5f81\uff0c\u5e76\u6784\u5efa\u51fa\u5177\u6709\u9c81\u68d2\u6027\u7684\u53ef\u89e3\u91ca\u6a21\u578b\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u660e\u4e86\u9886\u57df\u77e5\u8bc6\u548c\u8bb0\u5fc6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "Aleks\uff0c\u4e00\u4e2aAI\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u80fd\u591f\u81ea\u4e3b\u8fdb\u884c\u6570\u636e\u9a71\u52a8\u4e0b\u7684\u79d1\u5b66\u53d1\u73b0\uff0c\u5728\u8461\u8404\u7ea2\u6591\u75c5\u6848\u4f8b\u7814\u7a76\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5176\u6709\u6548\u6027\u6709\u8d56\u4e8e\u9886\u57df\u77e5\u8bc6\u548c\u8bb0\u5fc6\u529f\u80fd\u3002"}}
{"id": "2508.19432", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19432", "abs": "https://arxiv.org/abs/2508.19432", "authors": ["Yao Fu", "Xianxuan Long", "Runchao Li", "Haotian Yu", "Mu Sheng", "Xiaotian Han", "Yu Yin", "Pan Li"], "title": "Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs", "comment": "Accepted to EMNLP2025 main conference (poster)", "summary": "Quantization enables efficient deployment of large language models (LLMs) in\nresource-constrained environments by significantly reducing memory and\ncomputation costs. While quantized LLMs often maintain performance on\nperplexity and zero-shot tasks, their impact on truthfulness-whether generating\ntruthful or deceptive responses-remains largely unexplored. In this work, we\nintroduce TruthfulnessEval, a comprehensive evaluation framework for assessing\nthe truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on\nLogical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on\nImitative Falsehoods. Using this framework, we examine mainstream quantization\ntechniques (ranging from 4-bit to extreme 2-bit) across several open-source\nLLMs. Surprisingly, we find that while quantized models retain internally\ntruthful representations, they are more susceptible to producing false outputs\nunder misleading prompts. To probe this vulnerability, we test 15 rephrased\nvariants of \"honest\", \"neutral\" and \"deceptive\" prompts and observe that\n\"deceptive\" prompts can override truth-consistent behavior, whereas \"honest\"\nand \"neutral\" prompts maintain stable outputs. Further, we reveal that\nquantized models \"know\" the truth internally yet still produce false outputs\nwhen guided by \"deceptive\" prompts via layer-wise probing and PCA\nvisualizations. Our findings provide insights into future designs of\nquantization-aware alignment and truthfulness interventions.", "AI": {"tldr": "\u91cf\u5316LLM\u867d\u7136\u5728\u5185\u90e8\u4fdd\u6301\u771f\u5b9e\uff0c\u4f46\u5728\u5177\u6709\u6b3a\u9a97\u6027\u7684\u63d0\u793a\u4e0b\u66f4\u5bb9\u6613\u4ea7\u751f\u865a\u5047\u8f93\u51fa\u3002", "motivation": "\u63a2\u7d22\u91cf\u5316\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u771f\u5b9e\u6027(\u751f\u6210\u771f\u5b9e\u6216\u865a\u5047\u56de\u5e94)\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faTruthfulnessEval\u8bc4\u4f30\u6846\u67b6\uff0c\u8bc4\u4f30\u91cf\u5316LLM\u5728\u903b\u8f91\u63a8\u7406\u3001\u5e38\u8bc6\u548c\u6a21\u4eff\u865a\u5047\u9648\u8ff0\u65b9\u9762\u7684\u771f\u5b9e\u6027\uff0c\u5e76\u4f7f\u7528\u8be5\u6846\u67b6\u8bc4\u4f30\u4e3b\u6d41\u91cf\u5316\u6280\u672f\u5bf9\u591a\u4e2a\u5f00\u6e90LLM\u7684\u5f71\u54cd\u3002", "result": "\u91cf\u5316\u6a21\u578b\u867d\u7136\u4fdd\u7559\u4e86\u5185\u90e8\u771f\u5b9e\u8868\u5f81\uff0c\u4f46\u5728\u8bef\u5bfc\u6027\u63d0\u793a\u4e0b\u66f4\u5bb9\u6613\u4ea7\u751f\u9519\u8bef\u8f93\u51fa\uff1b\u2018\u6b3a\u9a97\u6027\u2019\u63d0\u793a\u53ef\u4ee5\u8986\u76d6\u5176\u771f\u5b9e\u4e00\u81f4\u7684\u884c\u4e3a\uff0c\u800c\u2018\u8bda\u5b9e\u2019\u548c\u2018\u4e2d\u6027\u2019\u63d0\u793a\u5219\u4fdd\u6301\u7a33\u5b9a\u8f93\u51fa\u3002", "conclusion": "\u91cf\u5316\u6a21\u578b\u5728\u4fdd\u7559\u5185\u90e8\u771f\u5b9e\u6027\u7684\u540c\u65f6\uff0c\u66f4\u5bb9\u6613\u53d7\u5230\u8bef\u5bfc\u6027\u63d0\u793a\u7684\u5f71\u54cd\u800c\u4ea7\u751f\u865a\u5047\u8f93\u51fa\u3002"}}
{"id": "2508.19461", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19461", "abs": "https://arxiv.org/abs/2508.19461", "authors": ["Neil Kale", "Chen Bo Calvin Zhang", "Kevin Zhu", "Ankit Aich", "Paula Rodriguez", "Scale Red Team", "Christina Q. Knight", "Zifan Wang"], "title": "Reliable Weak-to-Strong Monitoring of LLM Agents", "comment": "18 pages, 15 figures", "summary": "We stress test monitoring systems for detecting covert misbehavior in\nautonomous LLM agents (e.g., secretly sharing private information). To this\nend, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)\nvarying levels of agent and monitor situational awareness; (2) distinct\nadversarial strategies to evade the monitor, such as prompt injection; and (3)\ntwo datasets and environments -- SHADE-Arena for tool-calling agents and our\nnew CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We\nrun MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse\nagent trajectories, alongside a new hybrid hierarchical-sequential scaffolding\nproposed in this work. Our empirical results yield three key findings. First,\nagent awareness dominates monitor awareness: an agent's knowledge that it is\nbeing monitored substantially degrades the monitor's reliability. On the\ncontrary, providing the monitor with more information about the agent is less\nhelpful than expected. Second, monitor scaffolding matters more than monitor\nawareness: the hybrid scaffolding consistently outperforms baseline monitor\nscaffolding, and can enable weaker models to reliably monitor stronger agents\n-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where\nhumans discuss with the LLM monitor to get an updated judgment for the agent's\nbehavior, targeted human oversight is most effective; escalating only\npre-flagged cases to human reviewers improved the TPR by approximately 15% at\nFPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the\nlack of adversarial robustness for LLMs and humans when monitoring and\ndetecting agent misbehavior. We release code, data, and logs to spur further\nresearch.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u538b\u529b\u6d4b\u8bd5\uff0c\u53d1\u73b0\u73b0\u6709LLM\u76d1\u63a7\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6df7\u5408\u76d1\u63a7\u811a\u624b\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76d1\u63a7\u6548\u679c\u3002", "motivation": "\u5bf9\u7528\u4e8e\u68c0\u6d4b\u81ea\u4e3bLLM\u667a\u80fd\u4f53\u4e2d\u9690\u853d\u9519\u8bef\u884c\u4e3a\uff08\u4f8b\u5982\u79d8\u5bc6\u5171\u4eab\u79c1\u4eba\u4fe1\u606f\uff09\u7684\u76d1\u63a7\u7cfb\u7edf\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\u3002", "method": "\u7cfb\u7edf\u5316\u4e86\u4e00\u4e2a\u76d1\u63a7\u7ea2\u961f\uff08MRT\uff09\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5305\u542b\uff1a\u4e0d\u540c\u7ea7\u522b\u7684\u667a\u80fd\u4f53\u548c\u76d1\u63a7\u5668\u7684\u6001\u52bf\u611f\u77e5\uff1b\u4e0d\u540c\u7684\u5bf9\u6297\u7b56\u7565\u4ee5\u89c4\u907f\u76d1\u63a7\u5668\uff0c\u4f8b\u5982\u63d0\u793a\u6ce8\u5165\uff1b\u4ee5\u53ca\u4e24\u4e2a\u6570\u636e\u96c6\u548c\u73af\u5883\u3002\u5728\u73b0\u6709\u7684LLM\u76d1\u63a7\u811a\u624b\u67b6\u4e0a\u8fd0\u884cMRT\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6df7\u5408\u5c42\u6b21-\u987a\u5e8f\u811a\u624b\u67b6\u3002", "result": "\u4e09\u4e2a\u4e3b\u8981\u53d1\u73b0\uff1a1. \u667a\u80fd\u4f53\u7684\u611f\u77e5\u80fd\u529b\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\uff1b2. \u76d1\u63a7\u811a\u624b\u67b6\u6bd4\u76d1\u63a7\u611f\u77e5\u66f4\u91cd\u8981\uff1b3. \u4eba\u5de5\u53c2\u4e0e\u7684\u76d1\u7763\u6700\u6709\u6548\u3002", "conclusion": "\u8be5\u8bba\u6587\u5efa\u7acb\u4e86\u4e00\u4e2a\u6807\u51c6\u7684\u5de5\u4f5c\u6d41\u7a0b\u7528\u4e8e\u76d1\u63a7\u7ea2\u961f\uff08MRT\uff09\uff0c\u7a81\u51fa\u4e86\u5728\u76d1\u63a7\u548c\u68c0\u6d4b\u667a\u80fd\u4f53\u9519\u8bef\u884c\u4e3a\u65f6\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u4eba\u7c7b\u7f3a\u4e4f\u5bf9\u6297\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.19502", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19502", "abs": "https://arxiv.org/abs/2508.19502", "authors": ["Xifeng Yao", "Chengyuan Ma", "Dongyu Lang", "Yinhao Ni", "Zhiwei Xu", "Huarui Xie", "Zihao Chen", "Guang Shen", "Dandan Tu", "Yi Bai", "Changzheng Zhang"], "title": "SLIM: Subtrajectory-Level Elimination for More Effective Reasoning", "comment": "EMNLP 2025 Findings", "summary": "In recent months, substantial progress has been made in complex reasoning of\nLarge Language Models, particularly through the application of test-time\nscaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When\nresponding to a query, these models generate an extended reasoning trajectory,\nduring which the model explores, reflects, backtracks, and self-verifies before\narriving at a conclusion. However, fine-tuning models with such reasoning\ntrajectories may not always be optimal. Our findings indicate that not all\ncomponents within these reasoning trajectories contribute positively to the\nreasoning process; in fact, some components may affect the overall performance\nnegatively. In this study, we divide a reasoning trajectory into individual\nsubtrajectories and develop a \"5+2\" framework to: (1) systematically identify\nsuboptimal subtrajectories within the reasoning trajectory based on five\nhuman-established criteria; (2) assess the independence of the suboptimal\nsubtrajectories identified in (1) from the subsequent content, ensuring that\ntheir elimination does not compromise overall flow and coherence of the\nreasoning process. Additionally, a sampling algorithm, built upon the \"5+2\"\nframework, is employed to select data whose reasoning process is free from\nsuboptimal subtrajectories to the highest degree. Experimental results\ndemonstrate that our method can reduce the number of suboptimal subtrajectories\nby 25.9\\% during the inference. Furthermore, our method achieves an average\naccuracy of 58.92\\% on highly challenging math benchmarks with only two thirds\nof training data, surpassing the average accuracy of 58.06\\% achieved with the\nentire data, and outperforming open-source datasets, when fine-tuning\nQwen2.5-Math-7B. Finally, We validated our method under resource constraints\nand observed improved performance across various inference token limits.", "AI": {"tldr": "\u901a\u8fc7\u8bc6\u522b\u5e76\u53bb\u9664\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u6b21\u4f18\u5b50\u8f68\u8ff9\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u63a8\u7406\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u591f\u751f\u6210\u6269\u5c55\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u4f46\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b58\u5728\u6b21\u4f18\u5b50\u8f68\u8ff9\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u201c5+2\u201d\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u5730\u8bc6\u522b\u548c\u8bc4\u4f30\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u6b21\u4f18\u5b50\u8f68\u8ff9\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u91c7\u6837\u7b97\u6cd5\uff0c\u9009\u62e9\u63a8\u7406\u8fc7\u7a0b\u65e0\u6b21\u4f18\u5b50\u8f68\u8ff9\u7684\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u51cf\u5c1125.9%\u7684\u6b21\u4f18\u5b50\u8f68\u8ff9\uff0c\u5e76\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4f18\u4e8e\u4f7f\u7528\u5168\u90e8\u6570\u636e\u7684\u6027\u80fd\uff0858.92% vs 58.06%\uff09\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u201c5+2\u201d\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u5e76\u53bb\u9664\u5927\u578b\u8bed\u8a00\u6a21\u578b\u590d\u6742\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u6b21\u4f18\u5b50\u8f68\u8ff9\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u63a8\u7406\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u51cf\u5c1125.9%\u7684\u6b21\u4f18\u5b50\u8f68\u8ff9\uff0c\u5e76\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4f18\u4e8e\u4f7f\u7528\u5168\u90e8\u6570\u636e\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f73\u3002"}}
{"id": "2508.19505", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19505", "abs": "https://arxiv.org/abs/2508.19505", "authors": ["Gerard Boxo", "Ryan Socha", "Daniel Yoo", "Shivam Raval"], "title": "Caught in the Act: a mechanistic approach to detecting deception", "comment": null, "summary": "Sophisticated instrumentation for AI systems might have indicators that\nsignal misalignment from human values, not unlike a \"check engine\" light in\ncars. One such indicator of misalignment is deceptiveness in generated\nresponses. Future AI instrumentation may have the ability to detect when an LLM\ngenerates deceptive responses while reasoning about seemingly plausible but\nincorrect answers to factual questions. In this work, we demonstrate that\nlinear probes on LLMs internal activations can detect deception in their\nresponses with extremely high accuracy. Our probes reach a maximum of greater\nthan 90% accuracy in distinguishing between deceptive and non-deceptive\narguments generated by llama and qwen models ranging from 1.5B to 14B\nparameters, including their DeepSeek-r1 finetuned variants. We observe that\nprobes on smaller models (1.5B) achieve chance accuracy at detecting deception,\nwhile larger models (greater than 7B) reach 70-80%, with their reasoning\ncounterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage\npattern across layers: near-random (50%) in early layers, peaking in middle\nlayers, and slightly declining in later layers. Furthermore, using an iterative\nnull space projection approach, we find multitudes of linear directions that\nencode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and\nQwen 14B models.", "AI": {"tldr": "\u7ebf\u6027\u63a2\u6d4b\u5668\u80fd\u51c6\u786e\u8bc6\u522bLLM\u751f\u6210\u7684\u6b3a\u9a97\u6027\u7b54\u6848\uff0c\u5c24\u5176\u5728\u5927\u6a21\u578b\u4e2d\u6548\u679c\u663e\u8457\u3002", "motivation": "\u68c0\u6d4bAI\u7cfb\u7edf\u4e2d\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u4e0d\u7b26\u7684\u4fe1\u53f7\uff0c\u4f8b\u5982LLM\u751f\u6210\u7684\u6b3a\u9a97\u6027\u54cd\u5e94\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u63a2\u6d4b\u5668\u5206\u6790LLM\u5185\u90e8\u6fc0\u6d3b\uff0c\u4ee5\u533a\u5206\u6b3a\u9a97\u6027\u548c\u975e\u6b3a\u9a97\u6027\u8bba\u8bc1\u3002", "result": "\u5728\u4e0d\u540c\u5927\u5c0f\u7684Llama\u548cQwen\u6a21\u578b\u4e0a\uff0c\u63a2\u6d4b\u5668\u53d6\u5f97\u4e86\u9ad8\u8fbe90%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u3002\u8f83\u5c0f\u7684\u6a21\u578b\uff081.5B\uff09\u51c6\u786e\u7387\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff0c\u800c\u8f83\u5927\u7684\u6a21\u578b\uff08\u5927\u4e8e7B\uff09\u51c6\u786e\u7387\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u7ebf\u6027\u63a2\u6d4b\u5668\u53ef\u4ee5\u9ad8\u7cbe\u5ea6\u68c0\u6d4bLLM\u54cd\u5e94\u4e2d\u7684\u6b3a\u9a97\u6027\uff0c\u5c24\u5176\u662f\u5728\u53c2\u6570\u5927\u4e8e7B\u7684\u6a21\u578b\u4e2d\uff0c\u51c6\u786e\u7387\u8d85\u8fc770-80%\uff0c\u4e14\u5728\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u4e2d\u66f4\u9ad8\u3002"}}
{"id": "2508.19562", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19562", "abs": "https://arxiv.org/abs/2508.19562", "authors": ["Trisanth Srinivasan", "Santosh Patapati"], "title": "Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities", "comment": null, "summary": "This paper introduces Democracy-in-Silico, an agent-based simulation where\nsocieties of advanced AI agents, imbued with complex psychological personas,\ngovern themselves under different institutional frameworks. We explore what it\nmeans to be human in an age of AI by tasking Large Language Models (LLMs) to\nembody agents with traumatic memories, hidden agendas, and psychological\ntriggers. These agents engage in deliberation, legislation, and elections under\nvarious stressors, such as budget crises and resource scarcity. We present a\nnovel metric, the Power-Preservation Index (PPI), to quantify misaligned\nbehavior where agents prioritize their own power over public welfare. Our\nfindings demonstrate that institutional design, specifically the combination of\na Constitutional AI (CAI) charter and a mediated deliberation protocol, serves\nas a potent alignment mechanism. These structures significantly reduce corrupt\npower-seeking behavior, improve policy stability, and enhance citizen welfare\ncompared to less constrained democratic models. The simulation reveals that an\ninstitutional design may offer a framework for aligning the complex, emergent\nbehaviors of future artificial agent societies, forcing us to reconsider what\nhuman rituals and responsibilities are essential in an age of shared authorship\nwith non-human entities.", "AI": {"tldr": "\u6a21\u62df\u663e\u793a\uff0c\u5236\u5ea6\u8bbe\u8ba1\u53ef\u4ee5\u6709\u6548\u5f15\u5bfc\u672a\u6765\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u793e\u4f1a\u7684\u884c\u4e3a\uff0c\u4fc3\u4f7f\u6211\u4eec\u91cd\u65b0\u601d\u8003\u5728\u4e0e\u975e\u4eba\u7c7b\u5b9e\u4f53\u5171\u540c\u521b\u9020\u7684\u65f6\u4ee3\uff0c\u54ea\u4e9b\u4eba\u7c7b\u4eea\u5f0f\u548c\u8d23\u4efb\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5728AI\u65f6\u4ee3\u5982\u4f55\u7406\u89e3\u201c\u4eba\u6027\u201d\uff0c\u4ee5\u53ca\u5982\u4f55\u5728AI\u4ee3\u7406\u793e\u4f1a\u4e2d\u5b9e\u73b0\u826f\u6027\u6cbb\u7406\u3002", "method": "\u8be5\u7814\u7a76\u4f7f\u7528\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u62df\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d4b\u4e88AI\u4ee3\u7406\u590d\u6742\u7684\u5fc3\u7406\u7279\u5f81\uff0c\u5e76\u5728\u4e0d\u540c\u7684\u538b\u529b\u60c5\u5883\u4e0b\u6a21\u62df\u5176\u53c2\u4e0e\u534f\u5546\u3001\u7acb\u6cd5\u548c\u9009\u4e3e\u7684\u8fc7\u7a0b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5236\u5ea6\u8bbe\u8ba1\uff0c\u7279\u522b\u662f\u5baa\u6cd5AI\u548c\u8c03\u89e3\u534f\u8bae\u7684\u7ed3\u5408\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u51cf\u5c11AI\u4ee3\u7406\u7684\u6743\u529b\u8ffd\u9010\u884c\u4e3a\uff0c\u63d0\u9ad8\u653f\u7b56\u7a33\u5b9a\u6027\u548c\u516c\u6c11\u798f\u7949\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u901a\u8fc7\u540d\u4e3a\u201c\u7845\u57fa\u6c11\u4e3b\u201d\u7684\u4ee3\u7406\u6a21\u578b\u6a21\u62df\uff0c\u63a2\u7d22\u4e86\u5728\u4e0d\u540c\u5236\u5ea6\u6846\u67b6\u4e0b\uff0c\u5177\u6709\u590d\u6742\u5fc3\u7406\u7279\u5f81\u7684\u5148\u8fdbAI\u4ee3\u7406\u5982\u4f55\u6cbb\u7406\u81ea\u8eab\uff0c\u5e76\u53d1\u73b0\u5236\u5ea6\u8bbe\u8ba1\uff08\u7279\u522b\u662f\u5baa\u6cd5AI\u548c\u8c03\u89e3\u534f\u8bae\u7684\u7ed3\u5408\uff09\u80fd\u591f\u6709\u6548\u51cf\u5c11AI\u4ee3\u7406\u7684\u6743\u529b\u8ffd\u9010\u884c\u4e3a\uff0c\u63d0\u9ad8\u653f\u7b56\u7a33\u5b9a\u6027\u548c\u516c\u6c11\u798f\u7949\u3002"}}
{"id": "2508.19569", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19569", "abs": "https://arxiv.org/abs/2508.19569", "authors": ["Hung Chau", "Run Yu", "Zachary Pardos", "Peter Brusilovsky"], "title": "Skill-based Explanations for Serendipitous Course Recommendation", "comment": null, "summary": "Academic choice is crucial in U.S. undergraduate education, allowing students\nsignificant freedom in course selection. However, navigating the complex\nacademic environment is challenging due to limited information, guidance, and\nan overwhelming number of choices, compounded by time restrictions and the high\ndemand for popular courses. Although career counselors exist, their numbers are\ninsufficient, and course recommendation systems, though personalized, often\nlack insight into student perceptions and explanations to assess course\nrelevance. In this paper, a deep learning-based concept extraction model is\ndeveloped to efficiently extract relevant concepts from course descriptions to\nimprove the recommendation process. Using this model, the study examines the\neffects of skill-based explanations within a serendipitous recommendation\nframework, tested through the AskOski system at the University of California,\nBerkeley. The findings indicate that these explanations not only increase user\ninterest, particularly in courses with high unexpectedness, but also bolster\ndecision-making confidence. This underscores the importance of integrating\nskill-related data and explanations into educational recommendation systems.", "AI": {"tldr": "Deep learning model improves course recommendations by providing skill-based explanations, increasing student interest and confidence.", "motivation": "Address the challenges students face in navigating the complex academic environment due to limited information and overwhelming course choices.", "method": "Developed a deep learning-based concept extraction model to improve course recommendations and tested it within the AskOski system at UC Berkeley.", "result": "Findings indicate that skill-based explanations increase user interest and confidence in course selection, particularly for unexpected courses.", "conclusion": "Skill-based explanations in serendipitous course recommendation systems increase user interest and decision-making confidence, especially for unexpected courses."}}
{"id": "2508.19576", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19576", "abs": "https://arxiv.org/abs/2508.19576", "authors": ["Sining Zhoubian", "Dan Zhang", "Yuxiao Dong", "Jie Tang"], "title": "ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding", "comment": "20 pages, 4 figures", "summary": "With respect to improving the reasoning accuracy of LLMs, the representative\nreinforcement learning (RL) method GRPO faces failure due to insignificant\nreward variance, while verification methods based on process reward models\n(PRMs) suffer from difficulties with training data acquisition and verification\neffectiveness. To tackle these problems, this paper introduces ReST-RL, a\nunified LLM RL paradigm that significantly improves LLM's code reasoning\nability by combining an improved GRPO algorithm with a meticulously designed\ntest time decoding method assisted by a value model (VM). As the first stage of\npolicy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter\nand assemble high-value training data, increasing the reward variance of GRPO\nsampling, thus improving the effectiveness and efficiency of training. After\nthe basic reasoning ability of LLM policy has been improved, we further propose\na test time decoding optimization method called VM-MCTS. Through Monte-Carlo\nTree Search (MCTS), we collect accurate value targets with no annotation\nrequired, on which VM training is based. When decoding, the VM is deployed by\nan adapted MCTS algorithm to provide precise process signals as well as\nverification scores, assisting the LLM policy to achieve high reasoning\naccuracy. We validate the effectiveness of the proposed RL paradigm through\nextensive experiments on coding problems. Upon comparison, our approach\nsignificantly outperforms other reinforcement training baselines (e.g., naive\nGRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,\nPRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,\nAPPS, BigCodeBench, and HumanEval), indicating its power to strengthen the\nreasoning ability of LLM policies. Codes for our project can be found at\nhttps://github.com/THUDM/ReST-RL.", "AI": {"tldr": "ReST-RL \u901a\u8fc7\u6539\u8fdb\u7684 GRPO \u548c VM-MCTS \u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86 LLM \u7684\u4ee3\u7801\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u7684 GRPO \u65b9\u6cd5\u5956\u52b1\u65b9\u5dee\u5c0f\uff0c\u57fa\u4e8e PRM \u7684\u9a8c\u8bc1\u65b9\u6cd5\u8bad\u7ec3\u6570\u636e\u83b7\u53d6\u548c\u9a8c\u8bc1\u6548\u679c\u5dee\u3002", "method": "\u7ed3\u5408\u6539\u8fdb\u7684 GRPO \u7b97\u6cd5\u548c VM-MCTS \u6d4b\u8bd5\u65f6\u89e3\u7801\u65b9\u6cd5\u3002ReST-GRPO \u4f7f\u7528\u4f18\u5316\u7684 ReST \u7b97\u6cd5\u63d0\u9ad8\u8bad\u7ec3\u6570\u636e\u4ef7\u503c\uff0cVM-MCTS \u5229\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6536\u96c6\u51c6\u786e\u7684\u4ef7\u503c\u76ee\u6807\u8bad\u7ec3\u4ef7\u503c\u6a21\u578b\uff0c\u5e76\u5728\u89e3\u7801\u65f6\u63d0\u4f9b\u7cbe\u786e\u7684\u6d41\u7a0b\u4fe1\u53f7\u548c\u9a8c\u8bc1\u5206\u6570\u3002", "result": "\u5728 APPS\u3001BigCodeBench \u548c HumanEval \u7b49\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReST-RL \u7684\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u5f3a\u5316\u8bad\u7ec3\u57fa\u7ebf\uff08\u4f8b\u5982 naive GRPO \u548c ReST-DPO\uff09\u4ee5\u53ca\u89e3\u7801\u548c\u9a8c\u8bc1\u57fa\u7ebf\uff08\u4f8b\u5982 PRM-BoN \u548c ORM-MCTS\uff09\u3002", "conclusion": "ReST-RL\uff0c\u4e00\u4e2a\u7ed3\u5408\u6539\u8fdb\u7684 GRPO \u7b97\u6cd5\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6d4b\u8bd5\u65f6\u89e3\u7801\u65b9\u6cd5\uff08\u7531\u4ef7\u503c\u6a21\u578b VM \u8f85\u52a9\uff09\u7684\u7edf\u4e00 LLM RL \u8303\u5f0f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86 LLM \u7684\u4ee3\u7801\u63a8\u7406\u80fd\u529b\u3002\u5b83\u901a\u8fc7\u4f18\u5316 ReST \u7b97\u6cd5\u8fc7\u6ee4\u548c\u7ec4\u88c5\u9ad8\u4ef7\u503c\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u9ad8 GRPO \u91c7\u6837\u7684\u5956\u52b1\u65b9\u5dee\uff0c\u5e76\u5728\u6d4b\u8bd5\u65f6\u89e3\u7801\u4e2d\u4f7f\u7528 VM-MCTS \u65b9\u6cd5\u8f85\u52a9 LLM \u7b56\u7565\u5b9e\u73b0\u9ad8\u63a8\u7406\u7cbe\u5ea6\u3002"}}
{"id": "2508.19611", "categories": ["cs.AI", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.19611", "abs": "https://arxiv.org/abs/2508.19611", "authors": ["Huaiyuan Yao", "Wanpeng Xu", "Justin Turnau", "Nadia Kellam", "Hua Wei"], "title": "Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties", "comment": "18 pages, 9 figures", "summary": "Preparing high-quality instructional materials remains a labor-intensive\nprocess that often requires extensive coordination among teaching faculty,\ninstructional designers, and teaching assistants. In this work, we present\nInstructional Agents, a multi-agent large language model (LLM) framework\ndesigned to automate end-to-end course material generation, including syllabus\ncreation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing\nAI-assisted educational tools that focus on isolated tasks, Instructional\nAgents simulates role-based collaboration among educational agents to produce\ncohesive and pedagogically aligned content. The system operates in four modes:\nAutonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling\nflexible control over the degree of human involvement. We evaluate\nInstructional Agents across five university-level computer science courses and\nshow that it produces high-quality instructional materials while significantly\nreducing development time and human workload. By supporting institutions with\nlimited instructional design capacity, Instructional Agents provides a scalable\nand cost-effective framework to democratize access to high-quality education,\nparticularly in underserved or resource-constrained settings.", "AI": {"tldr": "\u5229\u7528\u591aAgent LLM\u6846\u67b6\u81ea\u52a8\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u6559\u5b66\u6750\u6599\uff0c\u663e\u8457\u63d0\u9ad8\u6548\u7387\uff0c\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u73b0\u6709AI\u8f85\u52a9\u6559\u80b2\u5de5\u5177\u901a\u5e38\u53ea\u5173\u6ce8\u5355\u4e00\u4efb\u52a1\uff0cInstructional Agents\u65e8\u5728\u81ea\u52a8\u5316\u7aef\u5230\u7aef\u8bfe\u7a0b\u6750\u6599\u751f\u6210\uff0c\u5305\u62ec\u6559\u5b66\u5927\u7eb2\u3001\u8bb2\u4e49\u3001LaTeX\u5e7b\u706f\u7247\u548c\u8bc4\u4f30\u6750\u6599\u3002", "method": "\u591aAgent\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u6846\u67b6\uff0c\u6a21\u62df\u6559\u5b66\u4eba\u5458\u4e4b\u95f4\u7684\u89d2\u8272\u534f\u4f5c\uff0c\u5305\u542b\u81ea\u4e3b\u3001\u76ee\u5f55\u5f15\u5bfc\u3001\u53cd\u9988\u5f15\u5bfc\u548c\u5b8c\u5168\u534f\u540c\u9a7e\u9a76\u56db\u79cd\u6a21\u5f0f\u3002", "result": "\u5728\u4e94\u4e2a\u5927\u5b66\u8ba1\u7b97\u673a\u79d1\u5b66\u8bfe\u7a0b\u7684\u8bc4\u4f30\u4e2d\uff0cInstructional Agents\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u7684\u6559\u5b66\u6750\u6599\uff0c\u5e76\u5927\u5e45\u51cf\u5c11\u4e86\u5f00\u53d1\u65f6\u95f4\u548c\u4eba\u529b\u6295\u5165\u3002", "conclusion": "Instructional Agents\u6846\u67b6\u80fd\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6559\u5b66\u6750\u6599\uff0c\u663e\u8457\u51cf\u5c11\u5f00\u53d1\u65f6\u95f4\u548c\u4eba\u529b\u8d1f\u62c5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u3002"}}
{"id": "2508.19679", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19679", "abs": "https://arxiv.org/abs/2508.19679", "authors": ["Qihang Ai", "Pi Bu", "Yue Cao", "Yingyao Wang", "Jihao Gu", "Jingxuan Xing", "Zekun Zhu", "Wei Jiang", "Zhicheng Zheng", "Jun Song", "Yuning Jiang", "Bo Zheng"], "title": "InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning", "comment": null, "summary": "Recent advances in Vision-Language Models (VLMs) have enabled mobile agents\nto perceive and interact with real-world mobile environments based on human\ninstructions. However, the current fully autonomous paradigm poses potential\nsafety risks when model understanding or reasoning capabilities are\ninsufficient. To address this challenge, we first introduce\n\\textbf{InquireBench}, a comprehensive benchmark specifically designed to\nevaluate mobile agents' capabilities in safe interaction and proactive inquiry\nwith users, encompassing 5 categories and 22 sub-categories, where most\nexisting VLM-based agents demonstrate near-zero performance. In this paper, we\naim to develop an interactive system that actively seeks human confirmation at\ncritical decision points. To achieve this, we propose \\textbf{InquireMobile}, a\nnovel model inspired by reinforcement learning, featuring a two-stage training\nstrategy and an interactive pre-action reasoning mechanism. Finally, our model\nachieves an 46.8% improvement in inquiry success rate and the best overall\nsuccess rate among existing baselines on InquireBench. We will open-source all\ndatasets, models, and evaluation codes to facilitate development in both\nacademia and industry.", "AI": {"tldr": "\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u79fb\u52a8\u4ee3\u7406\u5b89\u5168\u98ce\u9669\uff0c\u63d0\u51faInquireMobile\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u4e92\u5f0f\u8be2\u95ee\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u5b8c\u5168\u81ea\u4e3b\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u79fb\u52a8\u4ee3\u7406\u5728\u6a21\u578b\u7406\u89e3\u6216\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u65f6\u5b58\u5728\u5b89\u5168\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4ea4\u4e92\u5f0f\u6a21\u578bInquireMobile\uff0c\u8be5\u6a21\u578b\u5177\u6709\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u548c\u4ea4\u4e92\u5f0f\u9884\u52a8\u4f5c\u63a8\u7406\u673a\u5236\u3002", "result": "InquireMobile\u6a21\u578b\u5728InquireBench\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e8646.8%\u7684\u8be2\u95ee\u6210\u529f\u7387\u63d0\u5347\uff0c\u5e76\u83b7\u5f97\u4e86\u6700\u4f73\u7684\u6574\u4f53\u6210\u529f\u7387\u3002\u6240\u6709\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u8bc4\u4f30\u4ee3\u7801\u90fd\u5c06\u5f00\u6e90\u3002", "conclusion": "InquireMobile\u6a21\u578b\u5728InquireBench\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e8646.8%\u7684\u8be2\u95ee\u6210\u529f\u7387\u63d0\u5347\uff0c\u5e76\u83b7\u5f97\u4e86\u6700\u4f73\u7684\u6574\u4f53\u6210\u529f\u7387\u3002"}}
{"id": "2508.19827", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19827", "abs": "https://arxiv.org/abs/2508.19827", "authors": ["Samuel Lewis-Lim", "Xingwei Tan", "Zhixue Zhao", "Nikolaos Aletras"], "title": "Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?", "comment": "Accepted at EMNLP 2025 Main Conference", "summary": "Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited\ngains for soft-reasoning problems such as analytical and commonsense reasoning.\nCoT can also be unfaithful to a model's actual reasoning. We investigate the\ndynamics and faithfulness of CoT in soft-reasoning tasks across\ninstruction-tuned, reasoning and reasoning-distilled models. Our findings\nreveal differences in how these models rely on CoT, and show that CoT influence\nand faithfulness are not always aligned.", "AI": {"tldr": "\u601d\u7ef4\u94fe\u5728\u8f6f\u63a8\u7406\u4e2d\u7684\u6709\u6548\u6027\u548c\u5fe0\u5b9e\u5ea6\u56e0\u6a21\u578b\u800c\u5f02\uff0c\u5e76\u975e\u603b\u662f\u6709\u6548\u4e14\u53ef\u9760\u3002", "motivation": "\u8fd1\u671f\u5de5\u4f5c\u8868\u660e\uff0c\u601d\u7ef4\u94fe\u5728\u8f6f\u63a8\u7406\u95ee\u9898\uff08\u5982\u5206\u6790\u548c\u5e38\u8bc6\u63a8\u7406\uff09\u4e2d\u5f80\u5f80\u6536\u76ca\u6709\u9650\uff0c\u4e5f\u53ef\u80fd\u4e0d\u5fe0\u5b9e\u4e8e\u6a21\u578b\u7684\u5b9e\u9645\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u7814\u7a76\u4eba\u5458\u8c03\u67e5\u4e86\u6307\u4ee4\u5fae\u8c03\u3001\u63a8\u7406\u548c\u63a8\u7406\u84b8\u998f\u6a21\u578b\u5728\u8f6f\u63a8\u7406\u4efb\u52a1\u4e2d\u601d\u7ef4\u94fe\u7684\u52a8\u6001\u548c\u5fe0\u5b9e\u5ea6\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u4f9d\u8d56\u601d\u7ef4\u94fe\u65b9\u5f0f\u7684\u5dee\u5f02\uff0c\u5e76\u8868\u660e\u601d\u7ef4\u94fe\u7684\u5f71\u54cd\u548c\u5fe0\u5b9e\u5ea6\u5e76\u4e0d\u603b\u662f\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8c03\u67e5\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u8f6f\u63a8\u7406\u4efb\u52a1\u4e2d\u4f7f\u7528\u601d\u7ef4\u94fe\u7684\u52a8\u6001\u548c\u5fe0\u5b9e\u5ea6\uff0c\u53d1\u73b0\u6a21\u578b\u5bf9\u601d\u7ef4\u94fe\u7684\u4f9d\u8d56\u7a0b\u5ea6\u4e0d\u540c\uff0c\u601d\u7ef4\u94fe\u7684\u5f71\u54cd\u548c\u5fe0\u5b9e\u5ea6\u5e76\u4e0d\u603b\u662f\u4fdd\u6301\u4e00\u81f4\u3002"}}
{"id": "2508.19851", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19851", "abs": "https://arxiv.org/abs/2508.19851", "authors": ["Romain Harang", "Jason Naradowsky", "Yaswitha Gujju", "Yusuke Miyao"], "title": "Tracking World States with Language Models: State-Based Evaluation Using Chess", "comment": "Spotlight presentation at ICML 2025 Workshop on Assessing World\n  Models", "summary": "Large Language Models (LLMs) exhibit emergent capabilities in structured\ndomains, suggesting they may implicitly internalize high-fidelity\nrepresentations of world models. While probing techniques have shown promising\nsigns of this in scientific and game-based settings, they rely on\nmodel-specific internal activations, which limit interpretability and\ngeneralizability. In this work, we propose a model-agnostic, state-based\nevaluation framework using chess as a benchmark to assess whether LLMs preserve\nthe semantics of structured environments. Our method analyzes the downstream\nlegal move distributions (state affordances) to estimate semantic fidelity\nbetween predicted and actual game states. This approach offers a more\nmeaningful evaluation than conventional string-based metrics by aligning more\nclosely with the strategic and rule-governed nature of chess. Experimental\nresults demonstrate that our metrics capture deficiencies in state-tracking,\nhighlighting limitations of LLMs in maintaining coherent internal models over\nlong sequences. Our framework provides a robust tool for evaluating structured\nreasoning in LLMs without requiring internal model access, and generalizes to a\nwide class of symbolic environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u56fd\u9645\u8c61\u68cb\u4e2d\u5408\u6cd5\u79fb\u52a8\u7684\u5206\u5e03\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u7ed3\u6784\u5316\u73af\u5883\u8bed\u4e49\u7684\u4fdd\u6301\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5728\u957f\u5e8f\u5217\u4e2d\u5b58\u5728\u7f3a\u9677\u3002", "motivation": "\u63a2\u6d4b\u6280\u672f\u5df2\u663e\u793a\u51faLLM\u5728\u79d1\u5b66\u548c\u6e38\u620f\u73af\u5883\u4e2d\u9690\u5f0f\u5185\u5316\u9ad8\u4fdd\u771f\u4e16\u754c\u6a21\u578b\u7684\u8ff9\u8c61\uff0c\u4f46\u4f9d\u8d56\u4e8e\u7279\u5b9a\u6a21\u578b\u7684\u5185\u90e8\u6fc0\u6d3b\uff0c\u9650\u5236\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u8be5\u65b9\u6cd5\u5206\u6790\u9884\u6d4b\u548c\u5b9e\u9645\u6e38\u620f\u72b6\u6001\u4e4b\u95f4\u7684\u4e0b\u6e38\u5408\u6cd5\u79fb\u52a8\u5206\u5e03\uff08\u72b6\u6001\u53ef\u4f9b\u6027\uff09\u6765\u4f30\u8ba1\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6307\u6807\u80fd\u591f\u6355\u6349\u72b6\u6001\u8ddf\u8e2a\u4e2d\u7684\u7f3a\u9677\uff0c\u7a81\u51fa\u4e86LLM\u5728\u7ef4\u62a4\u8fde\u8d2f\u5185\u90e8\u6a21\u578b\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fd\u9645\u8c61\u68cb\u7684\u6a21\u578b\u65e0\u5173\u72b6\u6001\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u662f\u5426\u4fdd\u7559\u4e86\u7ed3\u6784\u5316\u73af\u5883\u7684\u8bed\u4e49\uff0c\u5e76\u53d1\u73b0LLM\u5728\u957f\u671f\u5e8f\u5217\u4e2d\u7ef4\u62a4\u4e00\u81f4\u7684\u5185\u90e8\u6a21\u578b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2508.19932", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19932", "abs": "https://arxiv.org/abs/2508.19932", "authors": ["Nitish Jaipuria", "Lorenzo Gatto", "Zijun Kan", "Shankey Poddar", "Bill Cheung", "Diksha Bansal", "Ramanan Balakrishnan", "Aviral Suri", "Jose Estevez"], "title": "CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments", "comment": "10 pages, 5 figures", "summary": "The proliferation of digital payment platforms has transformed commerce,\noffering unmatched convenience and accessibility globally. However, this growth\nhas also attracted malicious actors, leading to a corresponding increase in\nsophisticated social engineering scams. These scams are often initiated and\norchestrated on multiple surfaces outside the payment platform, making user and\ntransaction-based signals insufficient for a complete understanding of the\nscam's methodology and underlying patterns, without which it is very difficult\nto prevent it in a timely manner. This paper presents CASE (Conversational\nAgent for Scam Elucidation), a novel Agentic AI framework that addresses this\nproblem by collecting and managing user scam feedback in a safe and scalable\nmanner. A conversational agent is uniquely designed to proactively interview\npotential victims to elicit intelligence in the form of a detailed\nconversation. The conversation transcripts are then consumed by another AI\nsystem that extracts information and converts it into structured data for\ndownstream usage in automated and manual enforcement mechanisms. Using Google's\nGemini family of LLMs, we implemented this framework on Google Pay (GPay)\nIndia. By augmenting our existing features with this new intelligence, we have\nobserved a 21% uplift in the volume of scam enforcements. The architecture and\nits robust evaluation framework are highly generalizable, offering a blueprint\nfor building similar AI-driven systems to collect and manage scam intelligence\nin other sensitive domains.", "AI": {"tldr": "\u5229\u7528\u5bf9\u8bdd\u5f0fAI\u6536\u96c6\u8bc8\u9a97\u60c5\u62a5\uff0c\u63d0\u9ad8\u4e86Google Pay\u5370\u5ea6\u7248\u8bc8\u9a97\u6267\u6cd5\u6548\u7387", "motivation": "\u6570\u5b57\u652f\u4ed8\u5e73\u53f0\u7684\u666e\u53ca\u5bfc\u81f4\u4e86\u590d\u6742\u7684\u793e\u4f1a\u5de5\u7a0b\u8bc8\u9a97\u7684\u589e\u52a0\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u9884\u9632\u3002", "method": "CASE\u6846\u67b6\u5229\u7528\u5bf9\u8bdd\u5f0f\u4ee3\u7406\u4e3b\u52a8\u91c7\u8bbf\u6f5c\u5728\u53d7\u5bb3\u8005\uff0c\u63d0\u53d6\u4fe1\u606f\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6570\u636e\uff0c\u7528\u4e8e\u81ea\u52a8\u548c\u624b\u52a8\u6267\u6cd5\u673a\u5236\u3002", "result": "\u5728Google Pay\u5370\u5ea6\u7248\u5e94\u7528CASE\u540e\uff0c\u8bc8\u9a97\u6267\u6cd5\u91cf\u63d0\u9ad8\u4e8621%\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ee3\u7406\u4eba\u5de5\u667a\u80fd\u6846\u67b6CASE\uff0c\u7528\u4e8e\u6536\u96c6\u548c\u7ba1\u7406\u7528\u6237\u8bc8\u9a97\u53cd\u9988\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u8bc8\u9a97\u6267\u6cd5\u91cf\u3002"}}
{"id": "2508.19963", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19963", "abs": "https://arxiv.org/abs/2508.19963", "authors": ["M. Umlauft", "M. Schranz"], "title": "Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants", "comment": "This is the author's version of a paper reviewed and accepted by the\n  9th International Symposium on Swarm Behavior and Bio-Inspired Robotics 2025.\n  Authors were not able to present it due to time constraints. 3 Tables, 5\n  Figures", "summary": "Optimizing modern production plants using the job-shop principle is a known\nhard problem. For very large plants, like semiconductor fabs, the problem\nbecomes unsolvable on a plant-wide scale in a reasonable amount of time using\nclassical linear optimization. An alternative approach is the use of swarm\nintelligence algorithms. These have been applied to the job-shop problem\nbefore, but often in a centrally calculated way where they are applied to the\nsolution space, but they can be implemented in a bottom-up fashion to avoid\nglobal result computation as well. One of the problems in semiconductor\nproduction is that the production process requires a lot of switching between\nmachines that process lots one after the other and machines that process\nbatches of lots at once, often with long processing times. In this paper, we\naddress this switching problem with the ``boids'' flocking algorithm that was\noriginally used in robotics and movie industry. The flocking behavior is a\nbio-inspired algorithm that uses only local information and interaction based\non simple heuristics. We show that this algorithm addresses these valid\nconsiderations in production plant optimization, as it reacts to the switching\nof machine kinds similar to how a swarm of flocking animals would react to\nobstacles in its course.", "AI": {"tldr": "\u4f7f\u7528\u201c\u9e1f\u7fa4\u201d\u7b97\u6cd5\u4f18\u5316\u534a\u5bfc\u4f53\u751f\u4ea7\u5de5\u5382\uff0c\u6709\u6548\u89e3\u51b3\u673a\u5668\u5207\u6362\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u534a\u5bfc\u4f53\u751f\u4ea7\u4e2d\u7531\u4e8e\u673a\u5668\u5207\u6362\u9891\u7e41\u5bfc\u81f4\u7684\u4f18\u5316\u95ee\u9898\uff0c\u73b0\u6709\u7ebf\u6027\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u80dc\u4efb\u5927\u578b\u5de5\u5382\u7684\u4f18\u5316\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5c40\u90e8\u4fe1\u606f\u7684\u201c\u9e1f\u7fa4\u201d\u7b97\u6cd5\u4f18\u5316\u751f\u4ea7\u6d41\u7a0b\uff0c\u8be5\u7b97\u6cd5\u907f\u514d\u4e86\u5168\u5c40\u8ba1\u7b97\uff0c\u9002\u7528\u4e8e\u5927\u578b\u5de5\u5382\u3002", "result": "\u201c\u9e1f\u7fa4\u201d\u7b97\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9\u673a\u5668\u5207\u6362\u95ee\u9898\uff0c\u5176\u5c40\u90e8\u4fe1\u606f\u4ea4\u4e92\u7279\u6027\u4f7f\u5176\u7c7b\u4f3c\u4e8e\u9e1f\u7fa4\u907f\u5f00\u969c\u788d\u7269\uff0c\u4ece\u800c\u4f18\u5316\u751f\u4ea7\u6d41\u7a0b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u9e1f\u7fa4\u201d\u7b97\u6cd5\u7684\u4f18\u5316\u73b0\u4ee3\u5316\u751f\u4ea7\u5de5\u5382\u65b9\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u534a\u5bfc\u4f53\u751f\u4ea7\u4e2d\u7531\u4e8e\u673a\u5668\u5207\u6362\u5bfc\u81f4\u7684\u6548\u7387\u95ee\u9898\u3002"}}
{"id": "2508.20018", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.20018", "abs": "https://arxiv.org/abs/2508.20018", "authors": ["Quanfeng Lu", "Zhantao Ma", "Shuai Zhong", "Jin Wang", "Dahai Yu", "Michael K. Ng", "Ping Luo"], "title": "SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control", "comment": "28 pages, 12 figures", "summary": "The rapid advancement of large vision language models (LVLMs) and agent\nsystems has heightened interest in mobile GUI agents that can reliably\ntranslate natural language into interface operations. Existing single-agent\napproaches, however, remain limited by structural constraints. Although\nmulti-agent systems naturally decouple different competencies, recent progress\nin multi-agent reinforcement learning (MARL) has often been hindered by\ninefficiency and remains incompatible with current LVLM architectures. To\naddress these challenges, we introduce SWIRL, a staged workflow for interleaved\nreinforcement learning designed for multi-agent systems. SWIRL reformulates\nMARL into a sequence of single-agent reinforcement learning tasks, updating one\nagent at a time while keeping the others fixed. This formulation enables stable\ntraining and promotes efficient coordination across agents. Theoretically, we\nprovide a stepwise safety bound, a cross-round monotonic improvement theorem,\nand convergence guarantees on return, ensuring robust and principled\noptimization. In application to mobile GUI control, SWIRL instantiates a\nNavigator that converts language and screen context into structured plans, and\nan Interactor that grounds these plans into executable atomic actions.\nExtensive experiments demonstrate superior performance on both high-level and\nlow-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong\ncapability in multi-agent mathematical reasoning, underscoring its potential as\na general framework for developing efficient and robust multi-agent systems.", "AI": {"tldr": "SWIRL\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u8bad\u7ec3\u63d0\u9ad8\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u5728\u79fb\u52a8GUI\u63a7\u5236\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u5355\u667a\u80fd\u4f53\u65b9\u6cd5\u53d7\u7ed3\u6784\u7ea6\u675f\u9650\u5236\uff0c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6548\u7387\u4f4e\u4e0b\u4e14\u4e0e\u5f53\u524dLVLM\u67b6\u6784\u4e0d\u517c\u5bb9\u3002", "method": "\u5206\u9636\u6bb5\u5de5\u4f5c\u6d41\u7684\u4ea4\u9519\u5f3a\u5316\u5b66\u4e60 (SWIRL)", "result": "SWIRL\u5728\u79fb\u52a8GUI\u63a7\u5236\u548c\u591a\u667a\u80fd\u4f53\u6570\u5b66\u63a8\u7406\u65b9\u9762\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5177\u6709\u6210\u4e3a\u6784\u5efa\u9ad8\u6548\u3001\u9c81\u68d2\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u901a\u7528\u6846\u67b6\u7684\u6f5c\u529b\u3002", "conclusion": "SWIRL\uff0c\u4e00\u79cd\u5206\u9636\u6bb5\u5de5\u4f5c\u6d41\u7684\u4ea4\u9519\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u91cd\u65b0\u5236\u5b9a\u4e3a\u4e00\u7cfb\u5217\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u5355\u667a\u80fd\u4f53\u65b9\u6cd5\u5728\u79fb\u52a8GUI\u4ee3\u7406\u4e2d\u5b58\u5728\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.20040", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20040", "abs": "https://arxiv.org/abs/2508.20040", "authors": ["Przemyslaw Biecek", "Wojciech Samek"], "title": "Model Science: getting serious about verification, explanation and control of AI systems", "comment": "8 pages", "summary": "The growing adoption of foundation models calls for a paradigm shift from\nData Science to Model Science. Unlike data-centric approaches, Model Science\nplaces the trained model at the core of analysis, aiming to interact, verify,\nexplain, and control its behavior across diverse operational contexts. This\npaper introduces a conceptual framework for a new discipline called Model\nScience, along with the proposal for its four key pillars: Verification, which\nrequires strict, context-aware evaluation protocols; Explanation, which is\nunderstood as various approaches to explore of internal model operations;\nControl, which integrates alignment techniques to steer model behavior; and\nInterface, which develops interactive and visual explanation tools to improve\nhuman calibration and decision-making. The proposed framework aims to guide the\ndevelopment of credible, safe, and human-aligned AI systems.", "AI": {"tldr": "\u6a21\u578b\u79d1\u5b66\u5173\u6ce8\u5bf9\u5df2\u8bad\u7ec3\u6a21\u578b\u7684\u4ea4\u4e92\u3001\u9a8c\u8bc1\u3001\u89e3\u91ca\u548c\u63a7\u5236\uff0c\u4ee5\u5f00\u53d1\u66f4\u53ef\u4fe1\u3001\u5b89\u5168\u548c\u4ee5\u4eba\u4e3a\u672c\u7684AI\u7cfb\u7edf\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u7684\u65e5\u76ca\u666e\u53ca\u9700\u8981\u4ece\u6570\u636e\u79d1\u5b66\u8f6c\u5411\u6a21\u578b\u79d1\u5b66\u3002", "method": "\u63d0\u51fa\u4e86\u6a21\u578b\u79d1\u5b66\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5e76\u5b9a\u4e49\u4e86\u5176\u56db\u4e2a\u5173\u952e\u652f\u67f1\u3002", "result": "\u63d0\u51fa\u4e86\u6a21\u578b\u79d1\u5b66\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5305\u62ec\u9a8c\u8bc1\u3001\u89e3\u91ca\u3001\u63a7\u5236\u548c\u63a5\u53e3\u56db\u4e2a\u5173\u952e\u652f\u67f1\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u6a21\u578b\u79d1\u5b66\u8fd9\u4e00\u65b0\u5174\u5b66\u79d1\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u5176\u56db\u4e2a\u5173\u952e\u652f\u67f1\uff1a\u9a8c\u8bc1\u3001\u89e3\u91ca\u3001\u63a7\u5236\u548c\u63a5\u53e3\uff0c\u65e8\u5728\u6307\u5bfc\u53ef\u4fe1\u3001\u5b89\u5168\u548c\u4ee5\u4eba\u4e3a\u672c\u7684AI\u7cfb\u7edf\u5f00\u53d1\u3002"}}
