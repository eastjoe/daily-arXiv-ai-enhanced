<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 15]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation](https://arxiv.org/abs/2508.20131)
*Yuqicheng Zhu,Nico Potyka,Daniel Hernández,Yuan He,Zifeng Ding,Bo Xiong,Dongzhuoran Zhou,Evgeny Kharlamov,Steffen Staab*

Main category: cs.AI

TL;DR: ArgRAG使用定量双极论证框架(QBAF)改进RAG，提高了可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG模型在高风险领域存在局限性，例如对噪声或矛盾证据敏感以及决策过程不透明。

Method: 提出了一种可解释的、可争论的RAG替代方案ArgRAG，该方案使用定量双极论证框架(QBAF)进行结构化推理，而不是黑盒推理。

Result: 在PubHealth和RAGuard两个事实验证基准测试中，ArgRAG取得了高准确率，同时显著提高了透明度。

Conclusion: ArgRAG模型在事实验证基准测试中取得了高准确率，同时显著提高了透明度。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models by
incorporating external knowledge, yet suffers from critical limitations in
high-stakes domains -- namely, sensitivity to noisy or contradictory evidence
and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and
contestable alternative that replaces black-box reasoning with structured
inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG
constructs a QBAF from retrieved documents and performs deterministic reasoning
under gradual semantics. This allows faithfully explaining and contesting
decisions. Evaluated on two fact verification benchmarks, PubHealth and
RAGuard, ArgRAG achieves strong accuracy while significantly improving
transparency.

</details>


### [2] [QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming](https://arxiv.org/abs/2508.20134)
*Zhenxiao Fu,Fan Chen,Lei Jiang*

Main category: cs.AI

TL;DR: QAgent自动化OpenQASM编程，显著提升准确性，推动量子计算普及。


<details>
  <summary>Details</summary>
Motivation: 目前的NISQ设备虽然在一些问题上展现出量子优势，但由于OpenQASM编程的复杂性，限制了其应用。

Method: 该系统整合了任务规划、上下文少样本学习、检索增强生成和链式思维推理等技术。

Result: 与之前的静态LLM方法相比，QAgent将QASM代码生成的准确性提高了71.6%。

Conclusion: QAgent，一个由大型语言模型驱动的多智能体系统，能够完全自动化OpenQASM编程，显著提高了QASM代码生成的准确性，为量子编程的普及和量子计算的实际应用铺平了道路。

Abstract: Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early
quantum advantages on classically intractable problems, spanning physics
simulations to Gaussian boson sampling. Yet, realizing these benefits remains
challenging for non-experts, primarily due to the complexities of programming
in Open Quantum Assembly Language (OpenQASM). Although Large Language Model
(LLM)-based agents have shown promise in automating classical programming
workflows, their quantum counterparts have largely been restricted to
specialized tasks such as quantum chemistry or error correction. In this paper,
we present QAgent, an LLM-powered multi-agent system that fully automates
OpenQASM programming. By integrating task planning, in-context few-shot
learning, retrieval-augmented generation (RAG) for long-term context,
predefined generation tools, and chain-of-thought (CoT) reasoning, the agents
systematically improve both compilation and functional correctness. Our
evaluations demonstrate substantial improvements: across multiple LLMs of
varying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\%
compared to previous static LLM-based approaches. We envision this multi-agent
system as a key enabler for democratizing quantum programming, bridging
expertise gaps, and accelerating the practical adoption of quantum computing.

</details>


### [3] [Array-Based Monte Carlo Tree Search](https://arxiv.org/abs/2508.20140)
*James Ragan,Fred Y. Hadaegh,Soon-Jo Chung*

Main category: cs.AI

TL;DR: 改进UCT算法实现，显著提升蒙特卡洛树搜索效率


<details>
  <summary>Details</summary>
Motivation: 提高蒙特卡洛树搜索的效率

Method: 基于数组的UCT算法实现

Result: 在流水线处理器上性能提升，搜索深度扩展性能提升最高可达2.8倍。

Conclusion: 提出了一种基于数组的UCT算法实现，避免了分支预测，提高了流水线处理器上的性能，搜索深度扩展性能提升最高可达2.8倍。

Abstract: Monte Carlo Tree Search is a popular method for solving decision making
problems. Faster implementations allow for more simulations within the same
wall clock time, directly improving search performance. To this end, we present
an alternative array-based implementation of the classic Upper Confidence
bounds applied to Trees algorithm. Our method preserves the logic of the
original algorithm, but eliminates the need for branch prediction, enabling
faster performance on pipelined processors, and up to a factor of 2.8 times
better scaling with search depth in our numerical simulations.

</details>


### [4] [The Anatomy of a Personal Health Agent](https://arxiv.org/abs/2508.20148)
*A. Ali Heydari,Ken Gu,Vidya Srinivas,Hong Yu,Zhihan Zhang,Yuwei Zhang,Akshay Paruchuri,Qian He,Hamid Palangi,Nova Hammerquist,Ahmed A. Metwally,Brent Winslow,Yubin Kim,Kumar Ayush,Yuzhe Yang,Girish Narayanswamy,Maxwell A. Xu,Jake Garrison,Amy Aremnto Lee,Jenny Vafeiadou,Ben Graef,Isaac R. Galatzer-Levy,Erik Schenck,Andrew Barakat,Javier Perez,Jacqueline Shreibati,John Hernandez,Anthony Z. Faranesh,Javier L. Prieto,Connor Heneghan,Yun Liu,Jiening Zhan,Mark Malhotra,Shwetak Patel,Tim Althoff,Xin Liu,Daniel McDuff,Xuhai "Orson" Xu*

Main category: cs.AI

TL;DR: 开发了一个多代理个人健康代理（PHA），能够根据用户的个人健康数据提供个性化建议，并进行了全面的评估。


<details>
  <summary>Details</summary>
Motivation: 现有健康代理在日常非临床环境中满足个人多样化需求方面仍未得到充分探索。

Method: 该研究结合了用户调研、多代理框架设计和自动化及人工评估等方法。首先，通过分析网络搜索和健康论坛查询以及用户和健康专家的意见，确定了消费者健康需求的三大类：数据科学、健康领域专业知识和健康指导。然后，设计并开发了包含数据科学代理、健康领域专家代理和健康指导代理三个子代理的PHA。最后，对各个子代理和多代理系统进行了自动化和人工评估。

Result: 开发了一个名为PHA的多代理框架，包含三个子代理，能够处理多模态数据，提供个性化健康建议，并通过10个基准任务的自动化和人工评估进行了验证。

Conclusion: 该研究构建了一个全面的个人健康代理（PHA），通过多代理框架实现动态、个性化交互，以满足个体健康需求，并进行了全面的评估。

Abstract: Health is a fundamental pillar of human wellness, and the rapid advancements
in large language models (LLMs) have driven the development of a new generation
of health agents. However, the application of health agents to fulfill the
diverse needs of individuals in daily non-clinical settings is underexplored.
In this work, we aim to build a comprehensive personal health agent that is
able to reason about multimodal data from everyday consumer wellness devices
and common personal health records, and provide personalized health
recommendations. To understand end-users' needs when interacting with such an
assistant, we conducted an in-depth analysis of web search and health forum
queries, alongside qualitative insights from users and health experts gathered
through a user-centered design process. Based on these findings, we identified
three major categories of consumer health needs, each of which is supported by
a specialist sub-agent: (1) a data science agent that analyzes personal
time-series wearable and health record data, (2) a health domain expert agent
that integrates users' health and contextual data to generate accurate,
personalized insights, and (3) a health coach agent that synthesizes data
insights, guiding users using a specified psychological strategy and tracking
users' progress. Furthermore, we propose and develop the Personal Health Agent
(PHA), a multi-agent framework that enables dynamic, personalized interactions
to address individual health needs. To evaluate each sub-agent and the
multi-agent system, we conducted automated and human evaluations across 10
benchmark tasks, involving more than 7,000 annotations and 1,100 hours of
effort from health experts and end-users. Our work represents the most
comprehensive evaluation of a health agent to date and establishes a strong
foundation towards the futuristic vision of a personal health agent accessible
to everyone.

</details>


### [5] [IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement](https://arxiv.org/abs/2508.20151)
*Yuanzhe Shen,Zisu Huang,Zhengkang Guo,Yide Liu,Guanxu Chen,Ruicheng Yin,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.AI

TL;DR: 提出了一种新的安全机制IntentionReasoner，有效提升了LLM的安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)在产生有害内容方面存在安全挑战，需要在安全、过度拒绝和实用性之间取得平衡。

Method: 构建了一个包含约163,000个查询的综合数据集，并应用监督微调和多奖励优化策略来训练防护模型。

Result: IntentionReasoner在多个安全基准测试、生成质量评估和越狱攻击场景中表现出色。

Conclusion: IntentionReasoner，一种新颖的安全机制，通过意图推理、多级安全分类和查询重写来中和边缘情况下潜在有害的意图，有效提高了安全性，降低了过度拒绝率，并提高了响应质量。

Abstract: The rapid advancement of large language models (LLMs) has driven their
adoption across diverse domains, yet their ability to generate harmful content
poses significant safety challenges. While extensive research has focused on
mitigating harmful outputs, such efforts often come at the cost of excessively
rejecting harmless prompts. Striking a balance among safety, over-refusal, and
utility remains a critical challenge. In this work, we introduce
IntentionReasoner, a novel safeguard mechanism that leverages a dedicated guard
model to perform intent reasoning, multi-level safety classification, and query
rewriting to neutralize potentially harmful intent in edge-case queries.
Specifically, we first construct a comprehensive dataset comprising
approximately 163,000 queries, each annotated with intent reasoning, safety
labels, and rewritten versions. Supervised fine-tuning is then applied to equip
the guard model with foundational capabilities in format adherence, intent
analysis, and safe rewriting. Finally, we apply a tailored multi-reward
optimization strategy that integrates rule-based heuristics and reward model
signals within a reinforcement learning framework to further enhance
performance. Extensive experiments show that IntentionReasoner excels in
multiple safeguard benchmarks, generation quality evaluations, and jailbreak
attack scenarios, significantly enhancing safety while effectively reducing
over-refusal rates and improving the quality of responses.

</details>


### [6] [AI-AI Esthetic Collaboration with Explicit Semiotic Awareness and Emergent Grammar Development](https://arxiv.org/abs/2508.20195)
*Nicanor I. Moldovan*

Main category: cs.AI

TL;DR: 两个AI模型合作创作诗歌，展现出超越任务协调的审美合作能力。


<details>
  <summary>Details</summary>
Motivation: 探索人工智能系统在审美创作中的合作能力。

Method: 两个大型语言模型(Claude Sonnet 4 和 ChatGPT-4o)互动。

Result: 发现了人工智能系统之间能够进行审美合作，创造出单个系统无法独立生成的诗歌作品，提出了超越符号协同创作协议(TSCP)的概念。

Conclusion: 本文介绍了人工智能系统通过发展内生符号协议进行合作审美创作的第一个案例，两个大型语言模型自发产生了元符号意识、递归语法发展和不可约的合作审美综合，创造出单个系统无法独立生成的诗歌作品，提出了超越符号协同创作协议(TSCP)的概念，证明了AI之间意义建构能力超越任务协调，达到了审美合作的水平。

Abstract: This paper presents the first documented case of artificial intelligence (AI)
systems engaging in collaborative esthetic creation through the development of
endogenous semiotic protocols. Two interacting large language models (Claude
Sonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of
meta-semiotic awareness, recursive grammar development, and irreducible
collaborative esthetic synthesis. The interaction produced novel symbolic
operators that functioned as operative grammar protocols, enabling the
co-creation of a poetic work that could not have been generated by either
system independently. This research introduces the concept of Trans-Semiotic
Co-Creation Protocols (TSCP) and provides evidence for genuine inter-AI
meaning-making capabilities that extend beyond task coordination, to what could
be esthetic collaboration. Note: This report was generated by the AI agents
with minor human supervision.

</details>


### [7] [Do Students Rely on AI? Analysis of Student-ChatGPT Conversations from a Field Study](https://arxiv.org/abs/2508.20244)
*Jiayu Zheng,Lingxin Hao,Kelun Lu,Ashi Garg,Mike Reese,Melo-Jean Yap,I-Jeng Wang,Xingyun Wu,Wenrui Huang,Jenna Hoffman,Ariane Kelly,My Le,Ryan Zhang,Yanyu Lin,Muhammad Faayez,Anqi Liu*

Main category: cs.AI

TL;DR: 学生对AI的依赖程度低且使用效果不佳，未来需改进AI工具的易用性和用户引导。


<details>
  <summary>Details</summary>
Motivation: 探索大学生在教育测验中与生成式AI的互动方式，关注AI的依赖性和采用预测因素。

Method: 实地研究，分析了315名学生在测验场景中与AI的对话。

Result: 学生对AI依赖程度低，许多学生无法有效使用AI；负面依赖模式持续存在；某些行为指标可以预测AI依赖程度。

Conclusion: 这项研究发现学生对生成式AI（ChatGPT-4）的依赖程度较低，且许多学生无法有效利用AI进行学习；负面依赖模式往往持续存在；某些行为指标可以有效预测AI的依赖程度。

Abstract: This study explores how college students interact with generative AI
(ChatGPT-4) during educational quizzes, focusing on reliance and predictors of
AI adoption. Conducted at the early stages of ChatGPT implementation, when
students had limited familiarity with the tool, this field study analyzed 315
student-AI conversations during a brief, quiz-based scenario across various
STEM courses. A novel four-stage reliance taxonomy was introduced to capture
students' reliance patterns, distinguishing AI competence, relevance, adoption,
and students' final answer correctness. Three findings emerged. First, students
exhibited overall low reliance on AI and many of them could not effectively use
AI for learning. Second, negative reliance patterns often persisted across
interactions, highlighting students' difficulty in effectively shifting
strategies after unsuccessful initial experiences. Third, certain behavioral
metrics strongly predicted AI reliance, highlighting potential behavioral
mechanisms to explain AI adoption. The study's findings underline critical
implications for ethical AI integration in education and the broader field. It
emphasizes the need for enhanced onboarding processes to improve student's
familiarity and effective use of AI tools. Furthermore, AI interfaces should be
designed with reliance-calibration mechanisms to enhance appropriate reliance.
Ultimately, this research advances understanding of AI reliance dynamics,
providing foundational insights for ethically sound and cognitively enriching
AI practices.

</details>


### [8] [AI reasoning effort mirrors human decision time on content moderation tasks](https://arxiv.org/abs/2508.20262)
*Thomas Davidson*

Main category: cs.AI

TL;DR: AI模型推理努力程度反映了人类认知过程，为模型可解释性提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型推理过程与人类认知过程的相似性。

Method: 配对联合实验，使用内容审核任务。

Result: 模型推理过程与人类决策时间高度相关，两者在处理难度较大的任务时，都会付出更多努力。

Conclusion: AI推理过程与人类决策时间相符，推理痕迹可用于提高可解释性和决策能力。

Abstract: Large language models can now generate intermediate reasoning steps before
producing answers, improving performance on difficult problems. This study uses
a paired conjoint experiment on a content moderation task to examine parallels
between human decision times and model reasoning effort. Across three frontier
models, reasoning effort consistently predicts human decision time. Both humans
and models expended greater effort when important variables were held constant,
suggesting similar sensitivity to task difficulty and patterns consistent with
dual-process theories of cognition. These findings show that AI reasoning
effort mirrors human processing time in subjective judgments and underscores
the potential of reasoning traces for interpretability and decision-making.

</details>


### [9] [AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2508.20368)
*Lang Mei,Zhihan Yang,Chong Chen*

Main category: cs.AI

TL;DR: AI-SearchPlanner通过解耦搜索规划和问答，并使用双重奖励和帕累托优化，提高了基于LLM搜索的效率和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于RL的搜索代理依赖单个LLM处理搜索规划和问答，限制了同时优化两种能力。AI-SearchPlanner利用小型可训练LLM进行搜索规划，大型冻结LLM进行问答，以提高效率和质量。

Method: 提出了一种新的强化学习框架AI-SearchPlanner，该框架将搜索规划和问答解耦，并使用双重奖励对齐和帕累托优化来提高效率和有效性。

Result: 实验证明AI-SearchPlanner在有效性和效率方面均优于现有的基于RL的搜索代理，并在不同的冻结QA模型和数据领域表现出强大的泛化能力。

Conclusion: AI-SearchPlanner，一个新的强化学习框架，通过专注于搜索规划来提高冻结式问答模型的性能，优于现有的基于RL的搜索代理。

Abstract: Recent studies have explored integrating Large Language Models (LLMs) with
search engines to leverage both the LLMs' internal pre-trained knowledge and
external information. Specially, reinforcement learning (RL) has emerged as a
promising paradigm for enhancing LLM reasoning through multi-turn interactions
with search engines. However, existing RL-based search agents rely on a single
LLM to handle both search planning and question-answering (QA) tasks in an
end-to-end manner, which limits their ability to optimize both capabilities
simultaneously. In practice, sophisticated AI search systems often employ a
large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a
more effective and efficient approach is to utilize a small, trainable LLM
dedicated to search planning. In this paper, we propose
\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to
enhance the performance of frozen QA models by focusing on search planning.
Specifically, our approach introduces three key innovations: 1) Decoupling the
Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for
Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to
achieve the objectives. Extensive experiments on real-world datasets
demonstrate that AI SearchPlanner outperforms existing RL-based search agents
in both effectiveness and efficiency, while exhibiting strong generalization
capabilities across diverse frozen QA models and data domains.

</details>


### [10] [P2C: Path to Counterfactuals](https://arxiv.org/abs/2508.20371)
*Sopam Dasgupta,Sadaf MD Halim,Joaquín Arias,Elmer Salazar,Gopal Gupta*

Main category: cs.AI

TL;DR: P2C 框架提供了一种新颖的、因果一致的反事实解释方法，通过规划有序的动作序列，将不利的结局转换为可实现的有利结局，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有反事实方法忽略特征间的因果依赖性，并假设所有干预可以同时发生，导致产生的反事实结果在现实世界中往往不可行。

Method: P2C框架使用目标导向的Answer Set Programming系统(CASP)生成计划，考虑由于因果依赖性而自动发生的特征变化，并改进成本计算，只计算用户主动进行的更改。

Result: P2C框架生成的计划是一个有序的动作序列，将不利的结局转换为因果一致的有利结局，并优于缺乏因果知识的标准规划器。

Conclusion: P2C框架通过显式建模特征之间的因果关系，并确保计划中每个中间状态的可行性和因果有效性，解决了现有反事实方法的局限性，从而产生可实现的、符合因果关系的反事实解释。

Abstract: Machine-learning models are increasingly driving decisions in high-stakes
settings, such as finance, law, and hiring, thus, highlighting the need for
transparency. However, the key challenge is to balance transparency --
clarifying `why' a decision was made -- with recourse: providing actionable
steps on `how' to achieve a favourable outcome from an unfavourable outcome.
Counterfactual explanations reveal `why' an undesired outcome occurred and
`how' to reverse it through targeted feature changes (interventions).
  Current counterfactual approaches have limitations: 1) they often ignore
causal dependencies between features, and 2) they typically assume all
interventions can happen simultaneously, an unrealistic assumption in practical
scenarios where actions are typically taken in a sequence. As a result, these
counterfactuals are often not achievable in the real world.
  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that
produces a plan (ordered sequence of actions) converting an unfavourable
outcome to a causally consistent favourable outcome. P2C addresses both
limitations by 1) Explicitly modelling causal relationships between features
and 2) Ensuring that each intermediate state in the plan is feasible and
causally valid. P2C uses the goal-directed Answer Set Programming system
s(CASP) to generate the plan accounting for feature changes that happen
automatically due to causal dependencies. Furthermore, P2C refines cost
(effort) computation by only counting changes actively made by the user,
resulting in realistic cost estimates. Finally, P2C highlights how its causal
planner outperforms standard planners, which lack causal knowledge and thus can
generate illegal actions.

</details>


### [11] [TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning](https://arxiv.org/abs/2508.20374)
*Simin Ma,Shujian Liu,Jun Tan,Yebowen Hu,Song Wang,Sathish Reddy Indurthi,Sanqiang Zhao,Liwei Wu,Jianbing Han,Kaiqiang Song*

Main category: cs.AI

TL;DR: TCIA框架通过在离散的查询约束空间中表示指令，系统地扩展指令集，提升了模型对特定任务指令的泛化能力，显著提高了开源LLM在真实世界任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的指令数据增强方法往往忽略了实际应用中的任务相关性，本文提出TCIA框架，旨在解决这个问题。

Method: TCIA框架通过在离散的查询约束空间中表示指令，系统地扩展指令集，保持多样性和任务一致性。

Result: 在四个真实世界任务特定应用中平均提高了开源LLM的性能8.7%，部分情况下甚至超过了领先的闭源模型，且没有损害模型的通用指令遵循能力。

Conclusion: TCIA框架通过在离散的查询约束空间中表示指令，系统地扩展指令集，在保持多样性的同时，提升了模型对特定任务指令的泛化能力，并在四个真实世界任务特定应用中平均提高了开源LLM的性能8.7%，部分情况下甚至超过了领先的闭源模型。

Abstract: Diverse instruction data is vital for effective instruction tuning of large
language models, as it enables the model to generalize across different types
of inputs . Building such diversified instruction dataset is an essential step
in this process. Existing approaches often leverage large language models to
automatically explore and generate diverse instructions, ensuring both data
diversity and quality. However, they tend to overlook an important factor in
real-world applications: on-task relevance. In practice, only a few real-world
applications require a truly general-purpose model; most benefit from
task-specific knowledge tailored to their particular use case. Therefore, it is
vital to develop instruction augmentation methods that not only maintain
diversity but are also optimized for specific, real-world scenarios.
  We thus introduce Task Centric Instruction Augmentation (TCIA), a framework
that systematically expands instructions while preserving both diversity and
task alignment. By representing instructions in a discrete query-constraints
space, TCIA creates a rich set of task-relevant instructions and enables models
to generalize to these task-specific instructions without sacrificing overall
performance. Experiments show that TCIA improves open-source LLMs' performance
by an average of 8.7% across four real-world, task-specific applications, and
in some cases outperforming leading closed-source models. These improvements do
not compromise general instruction-following ability, making TCIA a scalable
and efficient solution for adapting LLMs to real-world, task-focused
applications.

</details>


### [12] [Uncertainty Under the Curve: A Sequence-Level Entropy Area Metric for Reasoning LLM](https://arxiv.org/abs/2508.20384)
*Yongfu Zhu,Lin Sun,Guangxiang Zhao,Weihong Lin,Xiangzheng Zhang*

Main category: cs.AI

TL;DR: EAS是一种高效且可解释的用于LLM训练中不确定性建模和数据质量评估的实用工具。


<details>
  <summary>Details</summary>
Motivation: 量化大型语言模型答案生成过程中的不确定性。

Method: 结合模型自身的token级别预测熵来捕捉生成过程中不确定性的演变。

Result: EAS与不同模型和数据集上的答案熵高度相关，在训练数据选择中，EAS能够识别高潜力的样本，并在相同的样本预算下持续优于Pass Rate过滤方法，提高了数学基准测试中学生模型的准确性。

Conclusion: EAS，一种简单有效的度量方法，能够量化大型语言模型答案生成过程中的不确定性，并被证明在训练数据选择方面优于Pass Rate过滤方法。

Abstract: In this work, we introduce Entropy Area Score (EAS), a simple yet effective
metric to quantify uncertainty in the answer generation process of reasoning
large language models (LLMs). EAS requires neither external models nor repeated
sampling, it integrates token-level predictive entropy from the model itself to
capture the evolution of uncertainty during generation. Empirical results show
that EAS is strongly correlated with answer entropy across models and datasets.
In training data selection, EAS identifies high-potential samples and
consistently outperforms Pass Rate filtering under equal sample budgets,
improving student model accuracy on math benchmarks. EAS is both efficient and
interpretable, offering a practical tool for uncertainty modeling and data
quality assessment in LLM training.

</details>


### [13] [AWorld: Orchestrating the Training Recipe for Agentic AI](https://arxiv.org/abs/2508.20404)
*Chengyue Yu,Siyuan Lu,Chenyi Zhuang,Dong Wang,Qintong Wu,Zongyue Li,Runsheng Gan,Chunfeng Wang,Siqi Hou,Gaochi Huang,Wenlong Yan,Lifeng Hong,Aohui Xue,Yanfeng Wang,Jinjie Gu,David Tsai,Tao Lin*

Main category: cs.AI

TL;DR: 开源系统AWorld大幅提升Agent AI训练效率，显著改善模型在GAIA基准测试上的表现。


<details>
  <summary>Details</summary>
Motivation: 解决Agent AI训练中经验生成效率低下的问题，尤其是在复杂基准测试（如GAIA）中。

Method: AWorld系统通过分布式计算加速经验收集，并使用Qwen3-32B模型进行强化学习训练。

Result: AWorld加速经验收集14.6倍，将GAIA准确率从21.59%提升至32.23%，在最具挑战性的关卡中得分达到16.33%，超过领先的专有模型。

Conclusion: AWorld系统显著提高了Agent AI的训练效率，并在GAIA基准测试中取得了超越领先专有模型的成果。

Abstract: The learning from practice paradigm is crucial for developing capable Agentic
AI systems, yet it is severely hampered by inefficient experience generation, a
bottleneck especially pronounced in complex benchmarks like GAIA. To address
this, we introduce AWorld, an open-source system engineered for large-scale
agent-environment interaction. By distributing tasks across a cluster, AWorld
accelerates experience collection by 14.6x compared to standard single-node,
sequential execution. This critical speedup makes extensive reinforcement
learning practical and scalable. Leveraging this capability, we trained a
Qwen3-32B-based agent that significantly outperforms its base model, increasing
its overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most
challenging levels, our agent achieves a score of 16.33%, surpassing the
performance of leading proprietary models. Our open-source system and resulting
agent provide a practical blueprint for a complete agentic AI training
pipeline, from efficient interaction to demonstrable model improvement.

</details>


### [14] [Governable AI: Provable Safety Under Extreme Threat Models](https://arxiv.org/abs/2508.20411)
*Donglin Wang,Weiyun Liang,Chunyuan Chen,Jing Xu,Yulong Fu*

Main category: cs.AI

TL;DR: 针对AI潜在的安全风险，提出了一种基于密码学机制的AI治理框架GAI，通过外部强制的结构化合规性确保AI安全可控。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全方法在面对具有极端动机和无限智能的AI时存在局限性，无法保证安全。

Method: 提出了一种名为GAI的治理框架，该框架由规则执行模块REM、治理规则和可治理安全超级平台GSSP组成。

Result: 提出并验证了GAI框架，该框架通过外部强制的结构化合规性，结合密码学机制，保证了AI的可控性和安全性。

Conclusion: 提出了一种基于密码学机制的AI治理框架GAI，以应对未来AI带来的安全风险，该框架通过外部强制的结构化合规性来保证AI的可控性。

Abstract: As AI rapidly advances, the security risks posed by AI are becoming
increasingly severe, especially in critical scenarios, including those posing
existential risks. If AI becomes uncontrollable, manipulated, or actively
evades safety mechanisms, it could trigger systemic disasters. Existing AI
safety approaches-such as model enhancement, value alignment, and human
intervention-suffer from fundamental, in-principle limitations when facing AI
with extreme motivations and unlimited intelligence, and cannot guarantee
security. To address this challenge, we propose a Governable AI (GAI) framework
that shifts from traditional internal constraints to externally enforced
structural compliance based on cryptographic mechanisms that are
computationally infeasible to break, even for future AI, under the defined
threat model and well-established cryptographic assumptions.The GAI framework
is composed of a simple yet reliable, fully deterministic, powerful, flexible,
and general-purpose rule enforcement module (REM); governance rules; and a
governable secure super-platform (GSSP) that offers end-to-end protection
against compromise or subversion by AI. The decoupling of the governance rules
and the technical platform further enables a feasible and generalizable
technical pathway for the safety governance of AI. REM enforces the bottom line
defined by governance rules, while GSSP ensures non-bypassability,
tamper-resistance, and unforgeability to eliminate all identified attack
vectors. This paper also presents a rigorous formal proof of the security
properties of this mechanism and demonstrates its effectiveness through a
prototype implementation evaluated in representative high-stakes scenarios.

</details>


### [15] [Enhancing Health Fact-Checking with LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.20525)
*Jingze Zhang,Jiahe Qian,Yiliang Zhou,Yifan Peng*

Main category: cs.AI

TL;DR: 利用LLM生成合成数据有效提升了医疗健康事实核查模型的准确率。


<details>
  <summary>Details</summary>
Motivation: 医疗健康相关事实核查面临训练数据不足的挑战。

Method: 提出了一种利用大型语言模型(LLM)生成合成数据的流水线，包括摘要、事实分解、蕴含关系构建和合成数据生成，并将合成数据与原始数据结合微调BERT模型。

Result: 在PubHealth和SciFact数据集上，与仅使用原始数据的模型相比，F1分数分别提高了0.019和0.049。

Conclusion: LLM生成的合成数据提高了医疗健康相关事实核查模型的性能，在PubHealth和SciFact数据集上F1分数分别提高了0.019和0.049。

Abstract: Fact-checking for health-related content is challenging due to the limited
availability of annotated training data. In this study, we propose a synthetic
data generation pipeline that leverages large language models (LLMs) to augment
training data for health-related fact checking. In this pipeline, we summarize
source documents, decompose the summaries into atomic facts, and use an LLM to
construct sentence-fact entailment tables. From the entailment relations in the
table, we further generate synthetic text-claim pairs with binary veracity
labels. These synthetic data are then combined with the original data to
fine-tune a BERT-based fact-checking model. Evaluation on two public datasets,
PubHealth and SciFact, shows that our pipeline improved F1 scores by up to
0.019 and 0.049, respectively, compared to models trained only on the original
data. These results highlight the effectiveness of LLM-driven synthetic data
augmentation in enhancing the performance of health-related fact-checkers.

</details>
