<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 15]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Human Creativity and AI](https://arxiv.org/abs/2507.08001)
*Shengyi Xie*

Main category: cs.AI

TL;DR: 本文探討人工智慧是否具有創造力，並回顧了創造力哲學和相關科學研究的進展。


<details>
  <summary>Details</summary>
Motivation: 探討人工智慧與創造力的關係，並重新詮釋創造力的哲學。

Method: 回顧文獻、分析

Result: 探討了創造力的定義，以及自然主義和認知神經科學對創造力的回應。

Conclusion: 本文探討了人工智慧是否能展現創造力，回顧了創造力哲學的歷史觀點，並探討了心理學和認知神經科學的進展對創造力研究的影響。

Abstract: With the advancement of science and technology, the philosophy of creativity
has undergone significant reinterpretation. This paper investigates
contemporary research in the fields of psychology, cognitive neuroscience, and
the philosophy of creativity, particularly in the context of the development of
artificial intelligence (AI) techniques. It aims to address the central
question: Can AI exhibit creativity? The paper reviews the historical
perspectives on the philosophy of creativity and explores the influence of
psychological advancements on the study of creativity. Furthermore, it analyzes
various definitions of creativity and examines the responses of naturalism and
cognitive neuroscience to the concept of creativity.

</details>


### [2] [TableReasoner: Advancing Table Reasoning Framework with Large Language Models](https://arxiv.org/abs/2507.08046)
*Sishi Xiong,Dakai Wang,Yu Zhao,Jie Zhang,Changzai Pan,Haowei He,Xiangyu Li,Wenhan Chang,Zhongjiang He,Shuangyong Song,Yongxiang Li*

Main category: cs.AI

TL;DR: 基于LLM和编程的TableReasoner系统在TQA任务中取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界表格数据带来的挑战，例如大尺寸、不完整的列语义和实体歧义。

Method: 提出一个基于大型语言模型（LLM）和编程的表格推理框架TableReasoner，该框架使用结合结构和语义表示的模式对表格建模，并设计了一个多步骤模式链接计划来导出仅包含查询相关信息的聚焦表格模式。

Result: 在SemEval-2025 Task 8的两个子任务中获得第一名。

Conclusion: TableReasoner系统在SemEval-2025 Task 8的两个子任务中均获得第一名。

Abstract: The paper presents our system developed for table question answering (TQA).
TQA tasks face challenges due to the characteristics of real-world tabular
data, such as large size, incomplete column semantics, and entity ambiguity. To
address these issues, we propose a large language model (LLM)-powered and
programming-based table reasoning framework, named TableReasoner. It models a
table using the schema that combines structural and semantic representations,
enabling holistic understanding and efficient processing of large tables. We
design a multi-step schema linking plan to derive a focused table schema that
retains only query-relevant information, eliminating ambiguity and alleviating
hallucinations. This focused table schema provides precise and sufficient table
details for query refinement and programming. Furthermore, we integrate the
reasoning workflow into an iterative thinking architecture, allowing
incremental cycles of thinking, reasoning and reflection. Our system achieves
first place in both subtasks of SemEval-2025 Task 8.

</details>


### [3] [A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM Jailbreaking](https://arxiv.org/abs/2507.08207)
*Zhengye Han,Quanyan Zhu*

Main category: cs.AI

TL;DR: 利用博弈论和RRT算法提出Purple Agent，有效防御大型语言模型越狱。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)的越狱问题日益突出，需要一种有效的防御机制。

Method: 动态Stackelberg博弈框架，快速探索随机树（RRT）

Result: 提出了一种新的代理AI解决方案Purple Agent，用于主动防御LLM越狱攻击。

Conclusion: 本文提出了一种动态Stackelberg博弈框架来模拟攻击者和防御者在大型语言模型越狱场景下的交互，并提出了一种新的代理AI解决方案——Purple Agent，该方案使用快速探索随机树（RRT）集成对抗性探索和防御策略，主动模拟潜在攻击轨迹并积极干预以防止有害输出。

Abstract: As large language models (LLMs) are increasingly deployed in critical
applications, the challenge of jailbreaking, where adversaries manipulate the
models to bypass safety mechanisms, has become a significant concern. This
paper presents a dynamic Stackelberg game framework to model the interactions
between attackers and defenders in the context of LLM jailbreaking. The
framework treats the prompt-response dynamics as a sequential extensive-form
game, where the defender, as the leader, commits to a strategy while
anticipating the attacker's optimal responses. We propose a novel agentic AI
solution, the "Purple Agent," which integrates adversarial exploration and
defensive strategies using Rapidly-exploring Random Trees (RRT). The Purple
Agent actively simulates potential attack trajectories and intervenes
proactively to prevent harmful outputs. This approach offers a principled
method for analyzing adversarial dynamics and provides a foundation for
mitigating the risk of jailbreaking.

</details>


### [4] [Reasoning and Behavioral Equilibria in LLM-Nash Games: From Mindsets to Actions](https://arxiv.org/abs/2507.08208)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: 提出LLM-Nash框架，建模LLM驱动的策略互动，考虑有限理性，并揭示了与经典博弈论不同的均衡结果。


<details>
  <summary>Details</summary>
Motivation: 探索LLM驱动决策系统中的策略互动，考虑认知约束、思维表达和认知学习。

Method: 博弈论模型，在提示空间上定义均衡，LLM推理作为行为输出。

Result: 展示了推理均衡与经典纳什均衡的差异，为LLM驱动系统中的策略互动提供了新的基础。

Conclusion: LLM-Nash框架为LLM驱动的决策系统中策略互动提供了新的基础，通过显式建模推理过程来捕捉有限理性，并展现出与经典纳什均衡不同的推理均衡。

Abstract: We introduce the LLM-Nash framework, a game-theoretic model where agents
select reasoning prompts to guide decision-making via Large Language Models
(LLMs). Unlike classical games that assume utility-maximizing agents with full
rationality, this framework captures bounded rationality by modeling the
reasoning process explicitly. Equilibrium is defined over the prompt space,
with actions emerging as the behavioral output of LLM inference. This approach
enables the study of cognitive constraints, mindset expressiveness, and
epistemic learning. Through illustrative examples, we show how reasoning
equilibria can diverge from classical Nash outcomes, offering a new foundation
for strategic interaction in LLM-enabled systems.

</details>


### [5] [From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration](https://arxiv.org/abs/2507.08210)
*Fryderyk Mantiuk,Hanqi Zhou,Charley M. Wu*

Main category: cs.AI

TL;DR: 好奇心与能力共同驱动高效探索，两者平衡是关键。


<details>
  <summary>Details</summary>
Motivation: 探究驱使智能体探索世界同时保持环境控制力的因素，即好奇心（寻求知识的驱动力）和能力（掌握和控制环境的驱动力）之间的平衡。

Method: 比较了使用手工状态抽象（Tabular）或学习内部世界模型（Dreamer）的两个基于模型的智能体。

Result: Tabular智能体显示好奇心和能力指导探索的不同模式，同时优先考虑两者能改进探索；Dreamer智能体揭示了探索和表征学习之间的双向交互作用，反映了好奇心和能力的发展性共同进化。

Conclusion: 好奇心和能力在探索中起着关键作用，两者平衡才能实现高效探索。基于模型的智能体实验表明，好奇心和能力分别引导探索，但同时优先考虑两者能改进探索效果。

Abstract: What drives an agent to explore the world while also maintaining control over
the environment? From a child at play to scientists in the lab, intelligent
agents must balance curiosity (the drive to seek knowledge) with competence
(the drive to master and control the environment). Bridging cognitive theories
of intrinsic motivation with reinforcement learning, we ask how evolving
internal representations mediate the trade-off between curiosity (novelty or
information gain) and competence (empowerment). We compare two model-based
agents using handcrafted state abstractions (Tabular) or learning an internal
world model (Dreamer). The Tabular agent shows curiosity and competence guide
exploration in distinct patterns, while prioritizing both improves exploration.
The Dreamer agent reveals a two-way interaction between exploration and
representation learning, mirroring the developmental co-evolution of curiosity
and competence. Our findings formalize adaptive exploration as a balance
between pursuing the unknown and the controllable, offering insights for
cognitive theories and efficient reinforcement learning.

</details>


### [6] [Grounding Methods for Neural-Symbolic AI](https://arxiv.org/abs/2507.08216)
*Rodrigo Castellano Ontiveros,Francesco Giannini,Marco Gori,Giuseppe Marra,Michelangelo Diligenti*

Main category: cs.AI

TL;DR: 提出一种参数化逻辑接地方法族，平衡表达性和可扩展性，实验表明接地准则选择很重要


<details>
  <summary>Details</summary>
Motivation: 现有的神经符号方法在逻辑接地过程中面临组合爆炸或缺乏信息保证的问题。

Method: 提出了一种参数化的逻辑接地方法族，该方法概括了经典的逆向链接方法，并在表达性和可扩展性之间取得平衡。

Result: 实验结果表明，接地准则的选择与NeSy方法本身同等重要。

Conclusion: 本文提出了一种参数化的逻辑接地方法族，该方法族概括了经典的逆向链接方法，并在表达性和可扩展性之间取得平衡。实验结果表明，接地准则的选择与NeSy方法本身同等重要。

Abstract: A large class of Neural-Symbolic (NeSy) methods employs a machine learner to
process the input entities, while relying on a reasoner based on First-Order
Logic to represent and process more complex relationships among the entities. A
fundamental role for these methods is played by the process of logic grounding,
which determines the relevant substitutions for the logic rules using a
(sub)set of entities. Some NeSy methods use an exhaustive derivation of all
possible substitutions, preserving the full expressive power of the logic
knowledge. This leads to a combinatorial explosion in the number of ground
formulas to consider and, therefore, strongly limits their scalability. Other
methods rely on heuristic-based selective derivations, which are generally more
computationally efficient, but lack a justification and provide no guarantees
of preserving the information provided to and returned by the reasoner. Taking
inspiration from multi-hop symbolic reasoning, this paper proposes a
parametrized family of grounding methods generalizing classic Backward
Chaining. Different selections within this family allow us to obtain commonly
employed grounding methods as special cases, and to control the trade-off
between expressiveness and scalability of the reasoner. The experimental
results show that the selection of the grounding criterion is often as
important as the NeSy method itself.

</details>


### [7] [Quantum Federated Learning for Multimodal Data: A Modality-Agnostic Approach](https://arxiv.org/abs/2507.08217)
*Atit Pokharel,Ratun Rahman,Thomas Morris,Dinh C. Nguyen*

Main category: cs.AI

TL;DR: 提出一种多模态量子联邦学习方法，并通过MMA机制解决了缺失模态问题，显著提高了准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的QFL框架主要集中在单模态系统上，限制了其在涉及多种模态的现实任务中的应用。

Method: 采用基于量子纠缠的中间融合方法，并设计了MMA机制来处理训练过程中缺失模态的问题。

Result: 模拟结果表明，该方法在IID和non-IID数据分布下，与现有技术相比，准确率分别提高了6.84%和7.25%。

Conclusion: 提出了一种新颖的多模态量子联邦学习(QFL)方法，并引入了缺失模态不可知(MMA)机制，以提高在独立同分布(IID)和非独立同分布(non-IID)数据分布下的准确性。

Abstract: Quantum federated learning (QFL) has been recently introduced to enable a
distributed privacy-preserving quantum machine learning (QML) model training
across quantum processors (clients). Despite recent research efforts, existing
QFL frameworks predominantly focus on unimodal systems, limiting their
applicability to real-world tasks that often naturally involve multiple
modalities. To fill this significant gap, we present for the first time a novel
multimodal approach specifically tailored for the QFL setting with the
intermediate fusion using quantum entanglement. Furthermore, to address a major
bottleneck in multimodal QFL, where the absence of certain modalities during
training can degrade model performance, we introduce a Missing Modality
Agnostic (MMA) mechanism that isolates untrained quantum circuits, ensuring
stable training without corrupted states. Simulation results demonstrate that
the proposed multimodal QFL method with MMA yields an improvement in accuracy
of 6.84% in independent and identically distributed (IID) and 7.25% in non-IID
data distributions compared to the state-of-the-art methods.

</details>


### [8] [Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates New Vectors of AI Harm](https://arxiv.org/abs/2507.08249)
*Bill Marino,Ari Juels*

Main category: cs.AI

TL;DR: 赋予AI加密货币和智能合约访问权限可能带来重大风险，需要进一步研究以确保安全。


<details>
  <summary>Details</summary>
Motivation: 对赋予AI智能体加密货币和智能合约访问权限日益增长的兴趣，以及由此可能产生的AI风险。

Method: 首先分析加密货币和智能合约的特性，然后详细描述这些新风险向量，最后呼吁进行更多技术研究。

Result: 识别出赋予AI智能体加密货币和智能合约访问权限可能导致的新的AI风险向量。

Conclusion: 赋予AI智能体加密货币和智能合约访问权限可能导致新的AI风险，需要更多技术研究来预防和减轻这些风险。

Abstract: There is growing interest in giving AI agents access to cryptocurrencies as
well as to the smart contracts that transact them. But doing so, this position
paper argues, could lead to formidable new vectors of AI harm. To support this
argument, we first examine the unique properties of cryptocurrencies and smart
contracts that could lead to these new vectors of harm. Next, we describe each
of these new vectors of harm in detail. Finally, we conclude with a call for
more technical research aimed at preventing and mitigating these harms and,
thereby making it safer to endow AI agents with cryptocurrencies and smart
contracts.

</details>


### [9] [Abductive Computational Systems: Creative Abduction and Future Directions](https://arxiv.org/abs/2507.08264)
*Abhinav Sood,Kazjon Grace,Stephen Wan,Cecile Paris*

Main category: cs.AI

TL;DR: 现有对溯因推理的理论和计算实现都不能充分处理创造性假设生成。


<details>
  <summary>Details</summary>
Motivation: 溯因推理在科学、设计和艺术领域都有提及，但其理解各不相同。

Method: 回顾性分析，比较了不同领域对溯因推理的理解，并分析了各种计算系统如何使用溯因推理。

Result: 理论框架没有提供生成创造性溯因假设的直接模型，计算系统大多实现了溯因推理的syllogistic形式。

Conclusion: 现有理论框架和计算系统都不能充分解决创造性假设生成的问题，需要进一步研究才能改进计算系统中创造性溯因推理的现状。

Abstract: Abductive reasoning, reasoning for inferring explanations for observations,
is often mentioned in scientific, design-related and artistic contexts, but its
understanding varies across these domains. This paper reviews how abductive
reasoning is discussed in epistemology, science and design, and then analyses
how various computational systems use abductive reasoning. Our analysis shows
that neither theoretical accounts nor computational implementations of
abductive reasoning adequately address generating creative hypotheses.
Theoretical frameworks do not provide a straightforward model for generating
creative abductive hypotheses, computational systems largely implement
syllogistic forms of abductive reasoning. We break down abductive computational
systems into components and conclude by identifying specific directions for
future research that could advance the state of creative abductive reasoning in
computational systems.

</details>


### [10] [Agent Safety Alignment via Reinforcement Learning](https://arxiv.org/abs/2507.08270)
*Zeyang Sha,Hanling Tian,Zhuoer Xu,Shiwen Cui,Changhua Meng,Weiqiang Wang*

Main category: cs.AI

TL;DR: 研究提出一个框架，增强工具型LLM代理的安全性和有效性，实验表明该框架有效


<details>
  <summary>Details</summary>
Motivation: 随着自主 LLM 代理能够使用工具，出现了超出传统对话滥用的新安全风险，该研究旨在解决这些新风险。

Method: 该框架采用三模态分类法，结合结构化推理和沙盒强化学习，设计了一个自定义沙盒环境来模拟真实世界的工具执行，并进行奖励塑造。

Result: 实验结果表明，安全对齐的代理显著提高了对安全威胁的抵抗力，同时保留了良性任务上的强大效用，证明了安全性和有效性可以共同优化。

Conclusion: 这项研究提出了一个用于工具使用型自主大型语言模型 (LLM) 代理的统一安全对齐框架，以应对用户和工具引发的安全威胁，并通过实验验证了该框架在提高安全性和保持效用方面的有效性。

Abstract: The emergence of autonomous Large Language Model (LLM) agents capable of tool
usage has introduced new safety risks that go beyond traditional conversational
misuse. These agents, empowered to execute external functions, are vulnerable
to both user-initiated threats (e.g., adversarial prompts) and tool-initiated
threats (e.g., malicious outputs from compromised tools). In this paper, we
propose the first unified safety-alignment framework for tool-using agents,
enabling models to handle both channels of threat via structured reasoning and
sandboxed reinforcement learning. We introduce a tri-modal taxonomy, including
benign, malicious, and sensitive for both user prompts and tool responses, and
define a policy-driven decision model. Our framework employs a custom-designed
sandbox environment that simulates real-world tool execution and allows
fine-grained reward shaping. Through extensive evaluations on public and
self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we
demonstrate that our safety-aligned agents significantly improve resistance to
security threats while preserving strong utility on benign tasks. Our results
show that safety and effectiveness can be jointly optimized, laying the
groundwork for trustworthy deployment of autonomous LLM agents.

</details>


### [11] [M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning](https://arxiv.org/abs/2507.08306)
*Inclusion AI,:,Fudong Wang,Jiajia Liu,Jingdong Chen,Jun Zhou,Kaixiang Ji,Lixiang Ru,Qingpei Guo,Ruobing Zheng,Tianqi Li,Yi Yuan,Yifan Mao,Yuting Xiao,Ziping Ma*

Main category: cs.AI

TL;DR: M2-Reasoning-7B模型通过改进数据和训练策略，显著提升了多模态大语言模型在一般和空间推理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 弥合现有MLLMs模型在动态空间交互方面的不足，提升其在实际应用中的推理能力。

Method: 该模型整合了新颖的数据管道和动态多任务训练策略，包含高质量数据样本的生成以及逐步优化的策略。

Result: 在8个基准测试中取得了最先进的结果，优于现有模型。

Conclusion: M2-Reasoning-7B模型在8个基准测试中取得了最先进的结果，在一般推理和空间推理领域都表现出优越的性能。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs), particularly
through Reinforcement Learning with Verifiable Rewards (RLVR), have
significantly enhanced their reasoning abilities. However, a critical gap
persists: these models struggle with dynamic spatial interactions, a capability
essential for real-world applications. To bridge this gap, we introduce
M2-Reasoning-7B, a model designed to excel in both general and spatial
reasoning. Our approach integrates two key innovations: (1) a novel data
pipeline that generates 294.2K high-quality data samples (168K for cold-start
fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning
trajectories and have undergone comprehensive assessment; and (2) a dynamic
multi-task training strategy with step-wise optimization to mitigate conflicts
between data, and task-specific rewards for delivering tailored incentive
signals. This combination of curated data and advanced training allows
M2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,
showcasing superior performance in both general and spatial reasoning domains.

</details>


### [12] [Multi-Agent LLMs as Ethics Advocates in AI-Based Systems](https://arxiv.org/abs/2507.08392)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.AI

TL;DR: 利用多智能体LLM和伦理倡导者智能体自动生成伦理需求，提高效率，但需人工校验


<details>
  <summary>Details</summary>
Motivation: 传统的伦理需求获取方法成本高、效率低，难以在需求工程中得到重视。

Method: 提出了一种基于多智能体LLM框架的伦理需求生成方法，引入伦理倡导者智能体进行伦理审查。

Result: 该框架能够有效生成伦理需求草案，但可靠性有待提高，需要结合人工反馈。

Conclusion: 这项研究提出了一种利用多智能体LLM框架生成伦理需求草案的方法，该方法通过引入伦理倡导者智能体来评价和提供伦理问题的反馈。案例研究表明该框架能够捕捉到大部分伦理需求，但也存在可靠性问题，需要人工反馈。

Abstract: Incorporating ethics into the requirement elicitation process is essential
for creating ethically aligned systems. Although eliciting manual ethics
requirements is effective, it requires diverse input from multiple
stakeholders, which can be challenging due to time and resource constraints.
Moreover, it is often given a low priority in the requirements elicitation
process. This study proposes a framework for generating ethics requirements
drafts by introducing an ethics advocate agent in a multi-agent LLM setting.
This agent critiques and provides input on ethical issues based on the system
description. The proposed framework is evaluated through two case studies from
different contexts, demonstrating that it captures the majority of ethics
requirements identified by researchers during 30-minute interviews and
introduces several additional relevant requirements. However, it also
highlights reliability issues in generating ethics requirements, emphasizing
the need for human feedback in this sensitive domain. We believe this work can
facilitate the broader adoption of ethics in the requirements engineering
process, ultimately leading to more ethically aligned products.

</details>


### [13] [Why this and not that? A Logic-based Framework for Contrastive Explanations](https://arxiv.org/abs/2507.08454)
*Tobias Geibinger,Reijo Jaakkola,Antti Kuusisto,Xinghan Liu,Miikka Vilander*

Main category: cs.AI

TL;DR: 该论文定义了一系列对比解释问题，分析了其性质和复杂度，并用Answer Set Programming实现了这些问题。


<details>
  <summary>Details</summary>
Motivation: 回答“为什么是P而不是Q？”形式的问题，计算P和Q的原因，明确比较它们的差异，捕获文献中现有对比解释的基数最小版本。

Method: 在命题逻辑的设置下，研究了对比解释问题的基本属性，并使用Answer Set Programming实现了CNF公式的问题。

Result: 定义了几个与对比解释相关的规范问题，分析了它们的计算复杂性，并使用Answer Set Programming实现了CNF公式的问题。

Conclusion: 定义了几个与对比解释相关的规范问题，每个问题都回答“为什么是P而不是Q？”形式的问题，计算P和Q的原因，明确比较它们的差异。研究了命题逻辑中定义的基本属性，表明该框架捕获了文献中现有对比解释的基数最小版本。此外，对问题的计算复杂性进行了广泛的分析，并使用Answer Set Programming实现了CNF公式的问题，并给出了一些例子来说明它们在实践中的工作方式。

Abstract: We define several canonical problems related to contrastive explanations,
each answering a question of the form ''Why P but not Q?''. The problems
compute causes for both P and Q, explicitly comparing their differences. We
investigate the basic properties of our definitions in the setting of
propositional logic. We show, inter alia, that our framework captures a
cardinality-minimal version of existing contrastive explanations in the
literature. Furthermore, we provide an extensive analysis of the computational
complexities of the problems. We also implement the problems for CNF-formulas
using answer set programming and present several examples demonstrating how
they work in practice.

</details>


### [14] [From Language to Logic: A Bi-Level Framework for Structured Reasoning](https://arxiv.org/abs/2507.08501)
*Keying Yang,Hao Wang,Kai Yang*

Main category: cs.AI

TL;DR: 提出一种双层框架，利用LLM将自然语言转化为可执行推理程序，在多个基准测试中准确率提高高达40%。


<details>
  <summary>Details</summary>
Motivation: 解决将非结构化语言表达与形式逻辑表示之间架起桥梁的挑战，实现对自然语言输入的结构化推理。

Method: 提出了一种新颖的双层框架，将自然语言映射到逻辑，分为高层任务抽象和低层逻辑生成两个阶段。高层使用大型语言模型（LLM）将自然语言查询解析为中间结构化表示，低层使用这些表示生成符号工作流或可执行推理程序。

Result: 在多个真实的推理基准测试中，该方法在准确性方面显著优于现有基线，准确率提高高达40%。双层设计增强了透明度和错误追溯性。

Conclusion: 该双层框架显著优于现有基线，准确率提高高达40%，并增强了透明度和错误追溯性，为构建可信赖和系统的基于LLM的推理系统提供了有前景的途径。

Abstract: Structured reasoning over natural language inputs remains a core challenge in
artificial intelligence, as it requires bridging the gap between unstructured
linguistic expressions and formal logical representations. In this paper, we
propose a novel \textbf{bi-level framework} that maps language to logic through
a two-stage process: high-level task abstraction and low-level logic
generation. At the upper level, a large language model (LLM) parses natural
language queries into intermediate structured representations specifying the
problem type, objectives, decision variables, and symbolic constraints. At the
lower level, the LLM uses these representations to generate symbolic workflows
or executable reasoning programs for accurate and interpretable decision
making. The framework supports modular reasoning, enforces explicit
constraints, and generalizes across domains such as mathematical problem
solving, question answering, and logical inference. We further optimize the
framework with an end-to-end {bi-level} optimization approach that jointly
refines both the high-level abstraction and low-level logic generation stages.
Experiments on multiple realistic reasoning benchmarks demonstrate that our
approach significantly outperforms existing baselines in accuracy, with
accuracy gains reaching as high as 40\%. Moreover, the bi-level design enhances
transparency and error traceability, offering a promising step toward
trustworthy and systematic reasoning with LLMs.

</details>


### [15] [A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis](https://arxiv.org/abs/2507.08529)
*Mingda Zhang,Na Zhao,Jianglong Qin,Guoyu Ye,Ruixiang Tang*

Main category: cs.AI

TL;DR: 该研究提出一个新框架，通过结合多粒度稀疏激活的医学概念和分层知识图谱，显著提高了罕见病诊断的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的医疗大型语言模型在罕见病诊断方面存在知识表达深度不足、概念理解有限和临床推理受限等问题。

Method: 该框架结合了多粒度稀疏激活的医学概念和分层知识图谱，使用了四种互补匹配算法、多样性控制和五级回退策略，以及三层知识图谱（分类法、临床特征、实例）。

Result: 在BioASQ罕见病问答集上的实验结果表明，该方法在BLEU、ROUGE和准确率上分别提高了0.09、0.05和0.12，最高准确率达到0.89，接近0.90的临床阈值。

Conclusion: 该框架提高了罕见病诊断的准确性，缩短了诊断时间。实验结果表明，该方法在BLEU、ROUGE和准确率上均有显著提高，接近临床阈值。专家评估也证实了该方法在信息质量、推理和专业表达方面的改进。

Abstract: Despite advances from medical large language models in healthcare,
rare-disease diagnosis remains hampered by insufficient
knowledge-representation depth, limited concept understanding, and constrained
clinical reasoning. We propose a framework that couples multi-granularity
sparse activation of medical concepts with a hierarchical knowledge graph. Four
complementary matching algorithms, diversity control, and a five-level fallback
strategy enable precise concept activation, while a three-layer knowledge graph
(taxonomy, clinical features, instances) provides structured, up-to-date
context. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09,
ROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89
approaching the 0.90 clinical threshold. Expert evaluation confirms
improvements in information quality, reasoning, and professional expression,
suggesting our approach shortens the "diagnostic odyssey" for rare-disease
patients.

</details>
