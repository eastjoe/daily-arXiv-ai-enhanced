<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 15]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Unified Crew Planning and Replanning Optimization in Multi-Line Metro Systems Considering Workforce Heterogeneity](https://arxiv.org/abs/2509.14251)
*Qihang Chen*

Main category: cs.AI

TL;DR: 本文提出了一种统一的优化框架，用于解决多线路地铁人员计划和重新计划问题，该框架考虑了异构员工队伍，并利用列生成和最短路径调整算法，在上海和北京地铁的实际数据实验中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单线路地铁人员计划，缺乏跨线路协调和快速重规划机制，尤其在突发事件下效率低下。

Method: 提出了一种分层时空网络模型，并设计了高效的约束和公式，用于表示统一的乘务员行动空间，以及考虑员工异质资质和偏好的算法。

Result: 实验结果表明，该方法优于基准启发式算法，降低了成本，提高了任务完成率，尤其是在处理突发事件时效率更高。

Conclusion: 该研究强调了全局优化和跨线路协调在多线路地铁系统运行中的重要作用，为智能城市公共交通的高效可靠运行提供了见解。

Abstract: Metro crew planning is a key component of smart city development as it
directly impacts the operational efficiency and service reliability of public
transportation. With the rapid expansion of metro networks, effective
multi-line scheduling and emergency management have become essential for
large-scale seamless operations. However, current research focuses primarily on
individual metro lines,with insufficient attention on cross-line coordination
and rapid replanning during disruptions. Here, a unified optimization framework
is presented for multi-line metro crew planning and replanning with
heterogeneous workforce. Specifically, a hierarchical time-space network model
is proposed to represent the unified crew action space, and computationally
efficient constraints and formulations are derived for the crew's heterogeneous
qualifications and preferences. Solution algorithms based on column generation
and shortest path adjustment are further developed, utilizing the proposed
network model. Experiments with real data from Shanghai and Beijing Metro
demonstrate that the proposed methods outperform benchmark heuristics in both
cost reduction and task completion,and achieve notable efficiency gains by
incorporating cross-line operations, particularly for urgent tasks during
disruptions. This work highlights the role of global optimization and
cross-line coordination in multi-line metro system operations, providing
insights into the efficient and reliable functioning of public transportation
in smart cities.

</details>


### [2] [From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing](https://arxiv.org/abs/2509.14289)
*Lanxiao Huang,Daksh Dave,Ming Jin,Tyler Cody,Peter Beling*

Main category: cs.AI

TL;DR: 该论文评估了大型语言模型 (LLM) 在渗透测试中的有效性和可靠性，并通过五个核心功能增强（全局上下文记忆、代理间消息传递、上下文条件调用、自适应规划和实时监控）提高了模块化代理的性能。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在渗透测试各个阶段的有效性和可靠性，目前尚不清楚。

Method: 对多个基于LLM的代理（从单代理到模块化设计）进行了全面的评估，测量了其实际性能和反复出现的故障模式，并通过有针对性的增强来隔离五个核心功能的影响。

Result: 结果表明，虽然某些架构天生就具有这些属性的子集，但有针对性的增强大大提高了模块化代理的性能，尤其是在复杂、多步骤和实时渗透测试任务中。

Conclusion: 有针对性的增强，例如全局上下文记忆、代理间消息传递等，能显著提高基于LLM的渗透测试代理的性能。

Abstract: Large language models (LLMs) are increasingly used to automate or augment
penetration testing, but their effectiveness and reliability across attack
phases remain unclear. We present a comprehensive evaluation of multiple
LLM-based agents, from single-agent to modular designs, across realistic
penetration testing scenarios, measuring empirical performance and recurring
failure patterns. We also isolate the impact of five core functional
capabilities via targeted augmentations: Global Context Memory (GCM),
Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive
Planning (AP), and Real-Time Monitoring (RTM). These interventions support,
respectively: (i) context coherence and retention, (ii) inter-component
coordination and state management, (iii) tool use accuracy and selective
execution, (iv) multi-step strategic planning, error detection, and recovery,
and (v) real-time dynamic responsiveness. Our results show that while some
architectures natively exhibit subsets of these properties, targeted
augmentations substantially improve modular agent performance, especially in
complex, multi-step, and real-time penetration testing tasks.

</details>


### [3] [Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents](https://arxiv.org/abs/2509.14382)
*Daniel Röder,Akhil Juneja,Roland Roller,Sven Schmeier*

Main category: cs.AI

TL;DR: 现有评估框架忽略了大型语言模型(LLM)驱动Web智能体的中间错误，限制了对失败模式的理解。本文提出了一种模块化评估框架，通过将智能体流程分解成可解释的阶段来进行详细的错误分析，从而改进Web智能体的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM驱动Web智能体的评估主要关注整体成功率，忽略中间错误，这阻碍了对失败模式的深入理解和系统改进。

Method: 提出一个模块化评估框架，将智能体流程分解成可解释的阶段，以便进行详细的错误分析。以SeeAct框架和Mind2Web数据集为例进行案例研究。

Result: 该方法揭示了标准指标无法发现的实际缺陷，为构建更健壮和更通用的Web智能体铺平了道路。

Conclusion: 模块化评估框架有助于改进LLM驱动Web智能体的鲁棒性和泛化能力。

Abstract: Web agents powered by large language models (LLMs) can autonomously perform
complex, multistep tasks in dynamic web environments. However, current
evaluations mostly focus on the overall success while overlooking intermediate
errors. This limits insight into failure modes and hinders systematic
improvement. This work analyzes existing benchmarks and highlights the lack of
fine-grained diagnostic tools. To address this gap, we propose a modular
evaluation framework that decomposes agent pipelines into interpretable stages
for detailed error analysis. Using the SeeAct framework and the Mind2Web
dataset as a case study, we show how this approach reveals actionable
weaknesses missed by standard metrics - paving the way for more robust and
generalizable web agents.

</details>


### [4] [VCBench: Benchmarking LLMs in Venture Capital](https://arxiv.org/abs/2509.14448)
*Rick Chen,Joseph Ternasky,Afriyie Samuel Kwesi,Ben Griffin,Aaron Ontoyin Yin,Zakari Salifu,Kelvin Amoaba,Xianling Mu,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: VCBench，首个预测风险投资创始人成功的基准测试，利用9000个匿名创始人资料，评估了九种LLM模型，其中DeepSeek-V3的精度超过基线六倍，GPT-4o的F0.5最高，大多数模型超过人类基准。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试加速了人工智能通用领域发展，但缺乏针对风险投资领域稀疏信号和不确定结果的基准。

Method: 构建VCBench数据集（9000个匿名创始人资料），并用其评估九种LLM模型的预测能力。

Result: DeepSeek-V3精度超过基线六倍，GPT-4o的F0.5最高，大多数模型超过人类基准。

Conclusion: VCBench为风险投资预测领域建立了可重复和保护隐私的AGI评估标准。

Abstract: Benchmarks such as SWE-bench and ARC-AGI demonstrate how shared datasets
accelerate progress toward artificial general intelligence (AGI). We introduce
VCBench, the first benchmark for predicting founder success in venture capital
(VC), a domain where signals are sparse, outcomes are uncertain, and even top
investors perform modestly. At inception, the market index achieves a precision
of 1.9%. Y Combinator outperforms the index by a factor of 1.7x, while tier-1
firms are 2.9x better. VCBench provides 9,000 anonymized founder profiles,
standardized to preserve predictive features while resisting identity leakage,
with adversarial tests showing more than 90% reduction in re-identification
risk. We evaluate nine state-of-the-art large language models (LLMs).
DeepSeek-V3 delivers over six times the baseline precision, GPT-4o achieves the
highest F0.5, and most models surpass human benchmarks. Designed as a public
and evolving resource available at vcbench.com, VCBench establishes a
community-driven standard for reproducible and privacy-preserving evaluation of
AGI in early-stage venture forecasting.

</details>


### [5] [From Mimicry to True Intelligence (TI) -- A New Paradigm for Artificial General Intelligence](https://arxiv.org/abs/2509.14474)
*Meltem Subasioglu,Nevzat Subasioglu*

Main category: cs.AI

TL;DR: 本文提出了一种基于人类大脑认知架构的真智能(TI)定义，并构建了一个五级AGI分类体系，为AGI研究提供了明确的路线图。


<details>
  <summary>Details</summary>
Motivation: 当前基于性能的AGI定义不足以指导研究，本文旨在提出一个基于认知机制的AGI定义。

Method: 通过借鉴人类大脑的结构和功能，定义了真智能的六个核心组成部分，并构建了一个五级AGI分类体系。

Result: 提出了一个新的真智能定义和一个五级AGI分类体系，为AGI研究提供了清晰的路径和发展里程碑。

Conclusion: 五级AGI在功能上与真智能等效，其区别仅在于哲学层面。

Abstract: The debate around Artificial General Intelligence (AGI) remains open due to
two fundamentally different goals: replicating human-like performance versus
replicating human-like cognitive processes. We argue that current
performance-based definitions are inadequate because they provide no clear,
mechanism-focused roadmap for research, and they fail to properly define the
qualitative nature of genuine intelligence. Drawing inspiration from the human
brain, we propose a new paradigm that shifts the focus from external mimicry to
the development of foundational cognitive architectures. We define True
Intelligence (TI) as a system characterized by six core components: embodied
sensory fusion, core directives, dynamic schemata creation, a
highly-interconnected multi-expert architecture, an orchestration layer, and
lastly, the unmeasurable quality of Interconnectedness, which we hypothesize
results in consciousness and a subjective experience. We propose a practical,
five-level taxonomy of AGI based on the number of the first five measurable
components a system exhibits. This framework provides a clear path forward with
developmental milestones that directly address the challenge of building
genuinely intelligent systems. We contend that once a system achieves Level-5
AGI by implementing all five measurable components, the difference between it
and TI remains as a purely philosophical debate. For practical purposes - and
given theories indicate consciousness is an emergent byproduct of integrated,
higher-order cognition - we conclude that a fifth-level AGI is functionally and
practically equivalent to TI. This work synthesizes diverse insights from
analytical psychology, schema theory, metacognition, modern brain architectures
and latest works in AI to provide the first holistic, mechanism-based
definition of AGI that offers a clear and actionable path for the research
community.

</details>


### [6] [Beyond the high score: Prosocial ability profiles of multi-agent populations](https://arxiv.org/abs/2509.14485)
*Marko Tesic,Yue Zhao,Joel Z. Leibo,Rakshit S. Trivedi,Jose Hernandez-Orallo*

Main category: cs.AI

TL;DR: 本文利用贝叶斯方法评估了Melting Pot竞赛中多智能体系统的合作能力，发现高社会性能力与高性能之间并非总是正相关，顶级参赛队伍可能利用了评估框架的局限性。


<details>
  <summary>Details</summary>
Motivation: 评估AI智能体的社会能力需要复杂的竞争与合作环境，而Melting Pot竞赛旨在评估AI系统的合作能力。

Method: 采用贝叶斯方法（Measurement Layouts）推断多智能体系统在Melting Pot竞赛中的能力特征。

Result: 高社会性能力与高性能并非总是正相关；顶级参赛队伍可能针对无需合作的场景进行了优化，利用了评估框架的局限性。

Conclusion: Measurement Layouts方法具有较强的预测准确性和可操作性，有助于更透明、更普遍地评估复杂社会环境下的AI系统。建议改进合作需求的标注，并考虑不同测试环境引入的偏差。

Abstract: The development and evaluation of social capabilities in AI agents require
complex environments where competitive and cooperative behaviours naturally
emerge. While game-theoretic properties can explain why certain teams or agent
populations outperform others, more abstract behaviours, such as convention
following, are harder to control in training and evaluation settings. The
Melting Pot contest is a social AI evaluation suite designed to assess the
cooperation capabilities of AI systems. In this paper, we apply a Bayesian
approach known as Measurement Layouts to infer the capability profiles of
multi-agent systems in the Melting Pot contest. We show that these capability
profiles not only predict future performance within the Melting Pot suite but
also reveal the underlying prosocial abilities of agents. Our analysis
indicates that while higher prosocial capabilities sometimes correlate with
better performance, this is not a universal trend-some lower-scoring agents
exhibit stronger cooperation abilities. Furthermore, we find that
top-performing contest submissions are more likely to achieve high scores in
scenarios where prosocial capabilities are not required. These findings,
together with reports that the contest winner used a hard-coded solution
tailored to specific environments, suggest that at least one top-performing
team may have optimised for conditions where cooperation was not necessary,
potentially exploiting limitations in the evaluation framework. We provide
recommendations for improving the annotation of cooperation demands and propose
future research directions to account for biases introduced by different
testing environments. Our results demonstrate that Measurement Layouts offer
both strong predictive accuracy and actionable insights, contributing to a more
transparent and generalisable approach to evaluating AI systems in complex
social settings.

</details>


### [7] [DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction](https://arxiv.org/abs/2509.14507)
*Jian Chen,Zhenyan Chen,Xuming Hu,Peilin Zhou,Yining Hua,Han Fang,Cissy Hing Yee Choy,Xinmei Ke,Jingfeng Luo,Zixuan Yuan*

Main category: cs.AI

TL;DR: DeKeyNLU数据集和DeKeySQL模型提升了NL2SQL任务的准确率


<details>
  <summary>Details</summary>
Motivation: 现有的NL2SQL模型在任务分解和关键词提取方面存在不足，导致SQL生成错误率较高

Method: 构建DeKeyNLU数据集，并基于RAG框架提出DeKeySQL模型，包含三个模块：用户问题理解、实体检索和SQL生成

Result: 在BIRD和Spider数据集上取得了显著的性能提升(BIRD: 62.31% -> 69.10%, Spider: 84.2% -> 88.7%)

Conclusion: DeKeyNLU和DeKeySQL有效地改进了NL2SQL任务的准确性

Abstract: Natural Language to SQL (NL2SQL) provides a new model-centric paradigm that
simplifies database access for non-technical users by converting natural
language queries into SQL commands. Recent advancements, particularly those
integrating Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)
reasoning, have made significant strides in enhancing NL2SQL performance.
However, challenges such as inaccurate task decomposition and keyword
extraction by LLMs remain major bottlenecks, often leading to errors in SQL
generation. While existing datasets aim to mitigate these issues by fine-tuning
models, they struggle with over-fragmentation of tasks and lack of
domain-specific keyword annotations, limiting their effectiveness. To address
these limitations, we present DeKeyNLU, a novel dataset which contains 1,500
meticulously annotated QA pairs aimed at refining task decomposition and
enhancing keyword extraction precision for the RAG pipeline. Fine-tuned with
DeKeyNLU, we propose DeKeySQL, a RAG-based NL2SQL pipeline that employs three
distinct modules for user question understanding, entity retrieval, and
generation to improve SQL generation accuracy. We benchmarked multiple model
configurations within DeKeySQL RAG pipeline. Experimental results demonstrate
that fine-tuning with DeKeyNLU significantly improves SQL generation accuracy
on both BIRD (62.31% to 69.10%) and Spider (84.2% to 88.7%) dev datasets.

</details>


### [8] [Rationality Check! Benchmarking the Rationality of Large Language Models](https://arxiv.org/abs/2509.14546)
*Zhilun Zhou,Jing Yi Wang,Nicholas Sukiennik,Chen Gao,Fengli Xu,Yong Li,James Evans*

Main category: cs.AI

TL;DR: 该论文提出了一种评估大型语言模型（LLM）理性性的基准，涵盖多个领域和模型，并提供工具包和实验结果。


<details>
  <summary>Details</summary>
Motivation: LLM展现出类人的能力，引发了其是否具有理性行为的担忧，该基准旨在评估LLM的理性程度。

Method: 构建了一个易于使用的工具包，进行了广泛的实验，评估LLM在不同领域的理性程度。

Result: 对LLM的理性能力进行了评估，并分析了其与理想化人类理性的差距。

Conclusion: 该基准可作为开发人员和用户评估LLM理性性的基础工具。

Abstract: Large language models (LLMs), a recent advance in deep learning and machine
intelligence, have manifested astonishing capacities, now considered among the
most promising for artificial general intelligence. With human-like
capabilities, LLMs have been used to simulate humans and serve as AI assistants
across many applications. As a result, great concern has arisen about whether
and under what circumstances LLMs think and behave like real human agents.
Rationality is among the most important concepts in assessing human behavior,
both in thinking (i.e., theoretical rationality) and in taking action (i.e.,
practical rationality). In this work, we propose the first benchmark for
evaluating the omnibus rationality of LLMs, covering a wide range of domains
and LLMs. The benchmark includes an easy-to-use toolkit, extensive experimental
results, and analysis that illuminates where LLMs converge and diverge from
idealized human rationality. We believe the benchmark can serve as a
foundational tool for both developers and users of LLMs.

</details>


### [9] [(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration](https://arxiv.org/abs/2509.14547)
*Yi Lin,Lujin Zhao,Yijie Shi*

Main category: cs.AI

TL;DR: 该论文提出了一种用于自动化工作流构建的动态框架，该框架结合Q-table学习和对当前任务的评估，以提高效率和适应性，在四个基准数据集上取得了优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖历史经验构建工作流，效率和适应性有限，该论文旨在构建一个能灵活响应不同任务特点的框架。

Method: 该框架利用Q-table学习优化决策空间，并根据当前任务进展进行先验决策，选择合适的执行agent和工作流结构，同时包含冷启动初始化、提前停止和剪枝等机制。

Result: 在四个基准数据集上，该方法平均提升了4.05%，并且工作流构建和推理成本仅为现有方法的30.68%-48.31%。

Conclusion: 该论文提出的动态框架有效地提高了工作流构建的效率和适应性，并在实验中取得了显著效果。

Abstract: Recent studies have shown that carefully designed workflows coordinating
large language models(LLMs) significantly enhance task-solving capabilities
compared to using a single model. While an increasing number of works focus on
autonomous workflow construction, most existing approaches rely solely on
historical experience, leading to limitations in efficiency and adaptability.
We argue that while historical experience is valuable, workflow construction
should also flexibly respond to the unique characteristics of each task. To
this end, we propose an a priori dynamic framework for automated workflow
construction. Our framework first leverages Q-table learning to optimize the
decision space, guiding agent decisions and enabling effective use of
historical experience. At the same time, agents evaluate the current task
progress and make a priori decisions regarding the next executing agent,
allowing the system to proactively select the more suitable workflow structure
for each given task. Additionally, we incorporate mechanisms such as cold-start
initialization, early stopping, and pruning to further improve system
efficiency. Experimental evaluations on four benchmark datasets demonstrate the
feasibility and effectiveness of our approach. Compared to state-of-the-art
baselines, our method achieves an average improvement of 4.05%, while reducing
workflow construction and inference costs to only 30.68%-48.31% of those
required by existing methods.

</details>


### [10] [SynBench: A Benchmark for Differentially Private Text Generation](https://arxiv.org/abs/2509.14594)
*Yidan Sun,Viktor Schlegel,Srinivasan Nandakumar,Iqra Zahid,Yuping Wu,Yulong Wu,Hao Li,Jie Zhang,Warren Del-Pinto,Goran Nenadic,Siew Kei Lam,Anil Anthony Bharath*

Main category: cs.AI

TL;DR: 本文探究了在医疗和金融等高风险领域，使用差分隐私生成合成数据以解决数据共享难题。


<details>
  <summary>Details</summary>
Motivation: 现有数据匿名化方法不足，差分隐私提供了一种生成具有隐私保证的合成数据的替代方案。

Method: 构建了一个综合评估框架，基准测试了最先进的差分隐私文本生成方法和不同规模的LLM，并开发了一种针对合成文本的成员推理攻击方法。

Result: 研究发现，在差分隐私约束下生成高质量的特定领域合成数据仍然是一个挑战，并且使用公共数据集可能会使隐私保证失效。

Conclusion: 需要对生成式AI进行严格的隐私审计，并弥合开放领域和专业评估之间的差距，以确保其在隐私敏感领域的安全部署。

Abstract: Data-driven decision support in high-stakes domains like healthcare and
finance faces significant barriers to data sharing due to regulatory,
institutional, and privacy concerns. While recent generative AI models, such as
large language models, have shown impressive performance in open-domain tasks,
their adoption in sensitive environments remains limited by unpredictable
behaviors and insufficient privacy-preserving datasets for benchmarking.
Existing anonymization methods are often inadequate, especially for
unstructured text, as redaction and masking can still allow re-identification.
Differential Privacy (DP) offers a principled alternative, enabling the
generation of synthetic data with formal privacy assurances. In this work, we
address these challenges through three key contributions. First, we introduce a
comprehensive evaluation framework with standardized utility and fidelity
metrics, encompassing nine curated datasets that capture domain-specific
complexities such as technical jargon, long-context dependencies, and
specialized document structures. Second, we conduct a large-scale empirical
study benchmarking state-of-the-art DP text generation methods and LLMs of
varying sizes and different fine-tuning strategies, revealing that high-quality
domain-specific synthetic data generation under DP constraints remains an
unsolved challenge, with performance degrading as domain complexity increases.
Third, we develop a membership inference attack (MIA) methodology tailored for
synthetic text, providing first empirical evidence that the use of public
datasets - potentially present in pre-training corpora - can invalidate claimed
privacy guarantees. Our findings underscore the urgent need for rigorous
privacy auditing and highlight persistent gaps between open-domain and
specialist evaluations, informing responsible deployment of generative AI in
privacy-sensitive, high-stakes settings.

</details>


### [11] [AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production](https://arxiv.org/abs/2509.14647)
*NVJK Kartik,Garvit Sapra,Rishav Hada,Nikhil Pareek*

Main category: cs.AI

TL;DR: AgentCompass框架用于监控和调试大型语言模型驱动的多智能体工作流，通过多阶段分析流程和双重记忆系统实现持续学习，并在真实场景和TRAIL基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法无法捕捉大型语言模型驱动的多智能体工作流中错误、涌现行为和系统性故障的风险。

Method: AgentCompass框架通过错误识别和分类、主题聚类、定量评分和战略总结等多阶段分析流程，结合情景记忆和语义记忆的双重记忆系统实现持续学习。

Result: 在真实场景部署和TRAIL基准测试中取得了最先进的结果，发现了人工标注中遗漏的关键问题。

Conclusion: AgentCompass是一个强大的、以开发者为中心的工具，能够可靠地监控和改进生产环境中的多智能体系统。

Abstract: With the growing adoption of Large Language Models (LLMs) in automating
complex, multi-agent workflows, organizations face mounting risks from errors,
emergent behaviors, and systemic failures that current evaluation methods fail
to capture. We present AgentCompass, the first evaluation framework designed
specifically for post-deployment monitoring and debugging of agentic workflows.
AgentCompass models the reasoning process of expert debuggers through a
structured, multi-stage analytical pipeline: error identification and
categorization, thematic clustering, quantitative scoring, and strategic
summarization. The framework is further enhanced with a dual memory
system-episodic and semantic-that enables continual learning across executions.
Through collaborations with design partners, we demonstrate the framework's
practical utility on real-world deployments, before establishing its efficacy
against the publicly available TRAIL benchmark. AgentCompass achieves
state-of-the-art results on key metrics, while uncovering critical issues
missed in human annotations, underscoring its role as a robust,
developer-centric tool for reliable monitoring and improvement of agentic
systems in production.

</details>


### [12] [Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory](https://arxiv.org/abs/2509.14662)
*Ming Li,Nan Zhang,Chenrui Fan,Hong Jiao,Yanbin Fu,Sydney Peters,Qingshu Xu,Robert Lissitz,Tianyi Zhou*

Main category: cs.AI

TL;DR: 用Schoenfeld的事件理论分析大型推理模型的推理过程，构建了一个公开的、用于细粒度分析机器推理的基准，包括大型标注语料库和详细的标注指南。


<details>
  <summary>Details</summary>
Motivation: 缺乏理解大型推理模型推理结构的原理框架。

Method: 应用Schoenfeld的事件理论，对模型生成的数学问题解答中的句子和段落进行标注分析，使用七个认知标签（例如，计划、执行、验证）。

Result: 构建了首个公开的、用于细粒度分析机器推理的基准，包括大型标注语料库和详细的标注指南。发现了大型推理模型推理中的独特模式，例如认知状态之间的转换动态。

Conclusion: 该框架为解释大型推理模型认知提供了理论基础，并为未来构建更可控、更透明的推理系统奠定了基础。

Abstract: While Large Reasoning Models (LRMs) generate extensive chain-of-thought
reasoning, we lack a principled framework for understanding how these thoughts
are structured. In this paper, we introduce a novel approach by applying
Schoenfeld's Episode Theory, a classic cognitive framework for human
mathematical problem-solving, to analyze the reasoning traces of LRMs. We
annotated thousands of sentences and paragraphs from model-generated solutions
to math problems using seven cognitive labels (e.g., Plan, Implement, Verify).
The result is the first publicly available benchmark for the fine-grained
analysis of machine reasoning, including a large annotated corpus and detailed
annotation guidebooks. Our preliminary analysis reveals distinct patterns in
LRM reasoning, such as the transition dynamics between cognitive states. This
framework provides a theoretically grounded methodology for interpreting LRM
cognition and enables future work on more controllable and transparent
reasoning systems.

</details>


### [13] [RationAnomaly: Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning](https://arxiv.org/abs/2509.14693)
*Song Xu,Yilun Liu,Minggui He,Mingchen Dai,Ziang Chen,Chunguang Zhao,Jingzhou Du,Shimin Tao,Weibin Meng,Shenglin Zhang,Yongqian Sun,Boxing Chen,Daimeng Wei*

Main category: cs.AI

TL;DR: RationAnomaly框架结合思维链(CoT)微调和强化学习，提高了日志异常检测的准确性和可解释性，并克服了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法缺乏可解释性和泛化能力，或存在不可靠和事实错误的问题。

Method: 该方法首先使用高质量数据集和CoT指导的监督微调，然后通过强化学习和多方面奖励函数优化准确性和逻辑一致性，减少幻觉。

Result: RationAnomaly在关键基准测试中取得了优越的F1分数，并提供透明的步骤式分析输出。

Conclusion: RationAnomaly框架有效提高了日志异常检测的性能和可解释性，相关代码和数据集已公开发布。

Abstract: Logs constitute a form of evidence signaling the operational status of
software systems. Automated log anomaly detection is crucial for ensuring the
reliability of modern software systems. However, existing approaches face
significant limitations: traditional deep learning models lack interpretability
and generalization, while methods leveraging Large Language Models are often
hindered by unreliability and factual inaccuracies. To address these issues, we
propose RationAnomaly, a novel framework that enhances log anomaly detection by
synergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our
approach first instills expert-like reasoning patterns using CoT-guided
supervised fine-tuning, grounded in a high-quality dataset corrected through a
rigorous expert-driven process. Subsequently, a reinforcement learning phase
with a multi-faceted reward function optimizes for accuracy and logical
consistency, effectively mitigating hallucinations. Experimentally,
RationAnomaly outperforms state-of-the-art baselines, achieving superior
F1-scores on key benchmarks while providing transparent, step-by-step
analytical outputs. We have released the corresponding resources, including
code and datasets.

</details>


### [14] [The NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs](https://arxiv.org/abs/2509.14704)
*Masaharu Mizumoto,Dat Nguyen,Zhiheng Han,Jiyuan Fang,Heyuan Guan,Xingfu Li,Naoya Shiraishi,Xuyang Tian,Yo Nakawake,Le Minh Nguyen*

Main category: cs.AI

TL;DR: Nazonazo基准测试使用日本儿童谜语评估LLM的洞察力推理能力，发现只有GPT-5的表现与人类相当，模型大小与准确性之间无可靠关联，并揭示了模型验证失败的常见问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM评估基准存在饱和和污染问题，导致对评估结果的信心不足。

Method: 构建Nazonazo基准测试，使用日本儿童谜语评估38个前沿模型和126个成年人的洞察力推理能力，并分析模型的思考过程。

Result: 只有GPT-5的表现与人类相当（52.9%的平均准确率），推理模型显著优于非推理模型，模型大小与准确性之间无可靠关联，发现模型常出现验证失败：产生正确答案却未选择。

Conclusion: Nazonazo基准测试提供了一种经济高效、可扩展且易于更新的基准测试格式，解决了当前的评估危机，并指出了模型的元认知弱点，为未来的控制和校准方法提供了明确的目标。

Abstract: Benchmark saturation and contamination undermine confidence in LLM
evaluation. We present Nazonazo, a cost-effective and extensible benchmark
built from Japanese children's riddles to test insight-based reasoning. Items
are short (mostly one sentence), require no specialized domain knowledge, and
can be generated at scale, enabling rapid refresh of blind sets when leakage is
suspected. We evaluate 38 frontier models and 126 adults on 120 riddles. No
model except for GPT-5 is comparable to human performance, which achieves a
52.9% mean accuracy. Model comparison on extended 201 items shows that
reasoning models significantly outperform non-reasoning peers, while model size
shows no reliable association with accuracy. Beyond aggregate accuracy, an
informal candidate-tracking analysis of thought logs reveals many cases of
verification failure: models often produce the correct solution among
intermediate candidates yet fail to select it as the final answer, which we
illustrate with representative examples observed in multiple models. Nazonazo
thus offers a cost-effective, scalable, and easily renewable benchmark format
that addresses the current evaluation crisis while also suggesting a recurrent
meta-cognitive weakness, providing clear targets for future control and
calibration methods.

</details>


### [15] [Enhancing Retrieval Augmentation via Adversarial Collaboration](https://arxiv.org/abs/2509.14750)
*Letian Zhang,Guanghao Meng,Xudong Ren,Yiming Wang,Shu-Tao Xia*

Main category: cs.AI

TL;DR: 该论文提出了一种对抗性协作检索增强生成（AC-RAG）框架，以解决现有RAG模型中存在的“检索幻觉”问题，显著提高了检索精度。


<details>
  <summary>Details</summary>
Motivation: 现有RAG模型容易出现“检索幻觉”，即模型无法识别和处理低质量检索文档，影响性能。

Method: 提出AC-RAG框架，包含一个通用的检测器和一个领域相关的解决器，两者在协调者的引导下进行对抗性协作，迭代式地改进知识检索。

Result: 实验表明，AC-RAG显著提高了检索精度，优于现有最先进的RAG方法。

Conclusion: AC-RAG框架有效解决了RAG模型的“检索幻觉”问题，为领域特定LLM的构建提供了新的思路。

Abstract: Retrieval-augmented Generation (RAG) is a prevalent approach for
domain-specific LLMs, yet it is often plagued by "Retrieval Hallucinations"--a
phenomenon where fine-tuned models fail to recognize and act upon poor-quality
retrieved documents, thus undermining performance. To address this, we propose
the Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two
heterogeneous agents: a generalist Detector that identifies knowledge gaps, and
a domain-specialized Resolver that provides precise solutions. Guided by a
moderator, these agents engage in an adversarial collaboration, where the
Detector's persistent questioning challenges the Resolver's expertise. This
dynamic process allows for iterative problem dissection and refined knowledge
retrieval. Extensive experiments show that AC-RAG significantly improves
retrieval accuracy and outperforms state-of-the-art RAG methods across various
vertical domains.

</details>
