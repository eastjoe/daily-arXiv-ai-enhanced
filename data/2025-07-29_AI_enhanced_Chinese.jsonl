{"id": "2507.19489", "categories": ["cs.AI", "cs.CV", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.19489", "abs": "https://arxiv.org/abs/2507.19489", "authors": ["Simone Bendazzoli", "Sanna Persson", "Mehdi Astaraki", "Sebastian Pettersson", "Vitali Grozman", "Rodrigo Moreno"], "title": "MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation", "comment": "26 pages, 12 figures", "summary": "The integration of Artificial Intelligence (AI) into clinical workflows\nrequires robust collaborative platforms that are able to bridge the gap between\ntechnical innovation and practical healthcare applications. This paper\nintroduces MAIA (Medical Artificial Intelligence Assistant), an open-source\nplatform designed to facilitate interdisciplinary collaboration among\nclinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a\nmodular, scalable environment with integrated tools for data management, model\ndevelopment, annotation, deployment, and clinical feedback. Key features\ninclude project isolation, CI/CD automation, integration with high-computing\ninfrastructures and in clinical workflows. MAIA supports real-world use cases\nin medical imaging AI, with deployments in both academic and clinical\nenvironments. By promoting collaborations and interoperability, MAIA aims to\naccelerate the translation of AI research into impactful clinical solutions\nwhile promoting reproducibility, transparency, and user-centered design. We\nshowcase the use of MAIA with different projects, both at KTH Royal Institute\nof Technology and Karolinska University Hospital.", "AI": {"tldr": "MAIA\u662f\u4e00\u4e2a\u5f00\u6e90\u5e73\u53f0\uff0c\u4fc3\u8fdb\u533b\u7597AI\u7684\u8de8\u5b66\u79d1\u5408\u4f5c\u548c\u4e34\u5e8a\u8f6c\u5316\u3002", "motivation": "\u5f25\u5408AI\u6280\u672f\u521b\u65b0\u4e0e\u5b9e\u9645\u533b\u7597\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4fc3\u8fdb\u4e34\u5e8a\u533b\u751f\u3001\u7814\u7a76\u4eba\u5458\u548cAI\u5f00\u53d1\u4eba\u5458\u4e4b\u95f4\u7684\u8de8\u5b66\u79d1\u5408\u4f5c\u3002", "method": "\u6784\u5efa\u4e86\u57fa\u4e8eKubernetes\u7684\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684MAIA\u5e73\u53f0\uff0c\u96c6\u6210\u6570\u636e\u7ba1\u7406\u3001\u6a21\u578b\u5f00\u53d1\u3001\u6807\u6ce8\u3001\u90e8\u7f72\u548c\u4e34\u5e8a\u53cd\u9988\u5de5\u5177\u3002", "result": "MAIA\u5df2\u5728\u5b66\u672f\u548c\u4e34\u5e8a\u73af\u5883\u4e2d\u6210\u529f\u90e8\u7f72\uff0c\u652f\u6301\u533b\u5b66\u5f71\u50cfAI\u7b49\u771f\u5b9e\u4e16\u754c\u7528\u4f8b\u3002", "conclusion": "MAIA\u5e73\u53f0\u65e8\u5728\u52a0\u901fAI\u7814\u7a76\u8f6c\u5316\u4e3a\u4e34\u5e8a\u5e94\u7528\uff0c\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u3001\u900f\u660e\u5ea6\u548c\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2507.19543", "categories": ["cs.AI", "cs.MA", "I.2.11; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.19543", "abs": "https://arxiv.org/abs/2507.19543", "authors": ["Maria Emilia Mazzolenis", "Ruirui Zhang"], "title": "Agent WARPP: Workflow Adherence via Runtime Parallel Personalization", "comment": "Accepted at the ICML 2025 Workshop on Multi-Agent Systems in the Era\n  of Foundation Models: Opportunities, Challenges, and Futures. Code repo:\n  https://github.com/emiliamazzo/WARPP/", "summary": "Large language models (LLMs) are increasingly applied in task-oriented\ndialogue (TOD) systems but often struggle with long, conditional workflows that\ninvolve external tool calls and depend on user-specific information. We present\nWorkflow Adherence via Runtime Parallel Personalization, or WARPP, a\ntraining-free, modular framework that combines multi-agent orchestration with\nruntime personalization to improve workflow adherence in LLM-based systems. By\ndynamically pruning conditional branches based on user attributes, the\nframework reduces reasoning overhead and narrows tool selection at runtime.\nWARPP deploys a parallelized architecture where a dedicated Personalizer agent\noperates alongside modular, domain-specific agents to dynamically tailor\nexecution paths in real time. The framework is evaluated across five\nrepresentative user intents of varying complexity within three domains:\nbanking, flights, and healthcare. Our evaluation leverages synthetic datasets\nand LLM-powered simulated users to test scenarios with conditional\ndependencies. Our results demonstrate that WARPP outperforms both the\nnon-personalized method and the ReAct baseline, achieving increasingly larger\ngains in parameter fidelity and tool accuracy as intent complexity grows, while\nalso reducing average token usage, without any additional training.", "AI": {"tldr": "WARPP\u6846\u67b6\u901a\u8fc7\u8fd0\u884c\u65f6\u4e2a\u6027\u5316\u548c\u591aAgent\u534f\u540c\uff0c\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u5de5\u4f5c\u6d41\u4e2d\u7684\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "LLM\u5728\u9762\u5411\u4efb\u52a1\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u96be\u4ee5\u5904\u7406\u6d89\u53ca\u5916\u90e8\u5de5\u5177\u8c03\u7528\u548c\u4f9d\u8d56\u7528\u6237\u7279\u5b9a\u4fe1\u606f\u7684\u957f\u6761\u4ef6\u5de5\u4f5c\u6d41\u3002", "method": "WARPP\u662f\u4e00\u4e2a\u514d\u8bad\u7ec3\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u591aAgent\u7f16\u6392\u548c\u8fd0\u884c\u65f6\u4e2a\u6027\u5316\uff0c\u52a8\u6001\u4fee\u526a\u6761\u4ef6\u5206\u652f\uff0c\u51cf\u5c11\u63a8\u7406\u5f00\u9500\uff0c\u7f29\u5c0f\u8fd0\u884c\u65f6\u5de5\u5177\u9009\u62e9\u8303\u56f4\u3002\u91c7\u7528\u5e76\u884c\u67b6\u6784\uff0c\u4e2a\u6027\u5316Agent\u4e0e\u7279\u5b9a\u9886\u57dfAgent\u534f\u540c\u5de5\u4f5c\uff0c\u52a8\u6001\u8c03\u6574\u6267\u884c\u8def\u5f84\u3002", "result": "\u5728\u4e09\u4e2a\u9886\u57df\uff08\u94f6\u884c\u3001\u822a\u73ed\u3001\u533b\u7597\uff09\u7684\u4e94\u4e2a\u7528\u6237\u610f\u56fe\u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660eWARPP\u4f18\u4e8e\u975e\u4e2a\u6027\u5316\u65b9\u6cd5\u548cReAct\u57fa\u7ebf\uff0c\u968f\u7740\u610f\u56fe\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u53c2\u6570\u4fdd\u771f\u5ea6\u548c\u5de5\u5177\u51c6\u786e\u6027\u63d0\u5347\u8d8a\u5927\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5e73\u5747token\u4f7f\u7528\u91cf\u3002", "conclusion": "WARPP\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u591aAgent\u7f16\u6392\u548c\u8fd0\u884c\u65f6\u4e2a\u6027\u5316\uff0c\u63d0\u9ad8\u4e86\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u4e2d\u5de5\u4f5c\u6d41\u7684\u9075\u5faa\u6027\uff0c\u5728\u53c2\u6570\u4fdd\u771f\u5ea6\u548c\u5de5\u5177\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u975e\u4e2a\u6027\u5316\u65b9\u6cd5\u548cReAct\u57fa\u7ebf\uff0c\u5e76\u51cf\u5c11\u4e86\u5e73\u5747token\u4f7f\u7528\u91cf\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2507.19593", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.19593", "abs": "https://arxiv.org/abs/2507.19593", "authors": ["Vince Trencsenyi", "Agnieszka Mensfelt", "Kostas Stathis"], "title": "Hypergames: Modeling Misaligned Perceptions and Nested Beliefs for Multi-agent Systems", "comment": null, "summary": "Classical game-theoretic models typically assume rational agents, complete\ninformation, and common knowledge of payoffs - assumptions that are often\nviolated in real-world MAS characterized by uncertainty, misaligned\nperceptions, and nested beliefs. To overcome these limitations, researchers\nhave proposed extensions that incorporate models of cognitive constraints,\nsubjective beliefs, and heterogeneous reasoning. Among these, hypergame theory\nextends the classical paradigm by explicitly modeling agents' subjective\nperceptions of the strategic scenario, known as perceptual games, in which\nagents may hold divergent beliefs about the structure, payoffs, or available\nactions. We present a systematic review of agent-compatible applications of\nhypergame theory, examining how its descriptive capabilities have been adapted\nto dynamic and interactive MAS contexts. We analyze 44 selected studies from\ncybersecurity, robotics, social simulation, communications, and general\ngame-theoretic modeling. Building on a formal introduction to hypergame theory\nand its two major extensions - hierarchical hypergames and HNF - we develop\nagent-compatibility criteria and an agent-based classification framework to\nassess integration patterns and practical applicability. Our analysis reveals\nprevailing tendencies, including the prevalence of hierarchical and graph-based\nmodels in deceptive reasoning and the simplification of extensive theoretical\nframeworks in practical applications. We identify structural gaps, including\nthe limited adoption of HNF-based models, the lack of formal hypergame\nlanguages, and unexplored opportunities for modeling human-agent and\nagent-agent misalignment. By synthesizing trends, challenges, and open research\ndirections, this review provides a new roadmap for applying hypergame theory to\nenhance the realism and effectiveness of strategic modeling in dynamic\nmulti-agent environments.", "AI": {"tldr": "A systematic review shows hypergame theory enhances MAS realism but faces limitations in practical application, suggesting future research directions.", "motivation": "To overcome limitations of classical game-theoretic models in real-world MAS characterized by uncertainty and misaligned perceptions, the authors explore the applicability of hypergame theory, which explicitly models agents' subjective perceptions of the strategic scenario.", "method": "Systematic review of 44 studies on agent-compatible applications of hypergame theory, using an agent-based classification framework and agent-compatibility criteria.", "result": "The review reveals prevailing tendencies and structural gaps in the application of hypergame theory to MAS, providing a roadmap for future research.", "conclusion": "This paper systematically reviews agent-compatible applications of hypergame theory in multi-agent systems (MAS), analyzing 44 studies across various domains.  It identifies prevailing trends, such as the use of hierarchical and graph-based models, and highlights structural gaps, including limited adoption of HNF-based models and a lack of formal hypergame languages."}}
{"id": "2507.19608", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.19608", "abs": "https://arxiv.org/abs/2507.19608", "authors": ["Jiawen Qi", "Chang Gao", "Zhaochun Ren", "Qinyu Chen"], "title": "DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference", "comment": null, "summary": "Deploying Large Language Models (LLMs) on edge devices remains challenging\ndue to their quadratically increasing computations with the sequence length.\nExisting studies for dynamic attention pruning are designed for hardware with\nmassively parallel computation capabilities, such as GPUs or TPUs, and aim at\nlong context lengths (e.g., 64K), making them unsuitable for edge scenarios. We\npresent DeltaLLM, a training-free framework that exploits temporal sparsity in\nattention patterns to enable efficient LLM inference across both the prefilling\nand decoding stages, on resource-constrained edge devices. DeltaLLM introduces\nan accuracy- and memory-aware delta matrix construction strategy that\nintroduces temporal sparsity, and a context-aware hybrid attention mechanism\nthat combines full attention in a local context window with delta approximation\noutside it to increase accuracy. We evaluate our framework on the\nedge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model\nacross diverse language tasks. The results show that on BitNet, our framework\nincreases the attention sparsity from 0% to 60% during the prefilling stage\nwith slight accuracy improvement on the WG task, and 0% to 57% across both the\nprefilling and decoding stages, with even higher F1 score from 29.63 to 30.97\non SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity\nduring the prefilling stage and around 57% across both stages with negligible\naccuracy drop. These results demonstrate that DeltaLLM offers a promising\nsolution for efficient edge deployment, requiring no fine-tuning and seamlessly\nintegrating with existing inference pipelines.", "AI": {"tldr": "DeltaLLM\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u7a00\u758f\u6027\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u63a8\u7406\u6548\u7387\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u52a8\u6001\u6ce8\u610f\u529b\u526a\u679d\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\uff0cDeltaLLM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DeltaLLM\u6846\u67b6\u5229\u7528\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u65f6\u95f4\u7a00\u758f\u6027\uff0c\u901a\u8fc7\u7cbe\u5ea6\u548c\u5185\u5b58\u611f\u77e5\u7684\u589e\u91cf\u77e9\u9635\u6784\u5efa\u7b56\u7565\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u63a8\u7406\u6548\u7387\u3002", "result": "\u5728BitNet\u548cLlama\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDeltaLLM\u80fd\u591f\u5728\u9884\u586b\u5145\u9636\u6bb5\u5c06\u6ce8\u610f\u529b\u7a00\u758f\u6027\u63d0\u9ad8\u523060%\u5de6\u53f3\uff0c\u5728\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u63d0\u9ad8\u523057%\u5de6\u53f3\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u7565\u5fae\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\u548cF1\u5206\u6570\u3002", "conclusion": "DeltaLLM\u6846\u67b6\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u4e0e\u73b0\u6709\u63a8\u7406\u6d41\u7a0b\u65e0\u7f1d\u96c6\u6210\u3002"}}
{"id": "2507.19672", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.19672", "abs": "https://arxiv.org/abs/2507.19672", "authors": ["Haoran Lu", "Luyang Fang", "Ruidong Zhang", "Xinliang Li", "Jiazhang Cai", "Huimin Cheng", "Lin Tang", "Ziyu Liu", "Zeliang Sun", "Tao Wang", "Yingchuan Zhang", "Arif Hassan Zidan", "Jinwen Xu", "Jincheng Yu", "Meizhi Yu", "Hanqi Jiang", "Xilin Gong", "Weidi Luo", "Bolun Sun", "Yongkai Chen", "Terry Ma", "Shushan Wu", "Yifan Zhou", "Junhao Chen", "Haotian Xiang", "Jing Zhang", "Afrar Jahin", "Wei Ruan", "Ke Deng", "Yi Pan", "Peilong Wang", "Jiahui Li", "Zhengliang Liu", "Lu Zhang", "Lin Zhao", "Wei Liu", "Dajiang Zhu", "Xin Xing", "Fei Dou", "Wei Zhang", "Chao Huang", "Rongjie Liu", "Mengrui Zhang", "Yiwen Liu", "Xiaoxiao Sun", "Qin Lu", "Zhen Xiang", "Wenxuan Zhong", "Tianming Liu", "Ping Ma"], "title": "Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges", "comment": "119 pages, 10 figures, 7 tables", "summary": "Due to the remarkable capabilities and growing impact of large language\nmodels (LLMs), they have been deeply integrated into many aspects of society.\nThus, ensuring their alignment with human values and intentions has emerged as\na critical challenge. This survey provides a comprehensive overview of\npractical alignment techniques, training protocols, and empirical findings in\nLLM alignment. We analyze the development of alignment methods across diverse\nparadigms, characterizing the fundamental trade-offs between core alignment\nobjectives. Our analysis shows that while supervised fine-tuning enables basic\ninstruction-following, preference-based methods offer more flexibility for\naligning with nuanced human intent. We discuss state-of-the-art techniques,\nincluding Direct Preference Optimization (DPO), Constitutional AI,\nbrain-inspired methods, and alignment uncertainty quantification (AUQ),\nhighlighting their approaches to balancing quality and efficiency. We review\nexisting evaluation frameworks and benchmarking datasets, emphasizing\nlimitations such as reward misspecification, distributional robustness, and\nscalable oversight. We summarize strategies adopted by leading AI labs to\nillustrate the current state of practice. We conclude by outlining open\nproblems in oversight, value pluralism, robustness, and continuous alignment.\nThis survey aims to inform both researchers and practitioners navigating the\nevolving landscape of LLM alignment.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u5168\u9762\u6982\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u6280\u672f\uff0c\u5e76\u6307\u51fa\u4e86\u8be5\u9886\u57df\u672a\u6765\u7684\u6311\u6218\u3002", "motivation": "\u786e\u4fdd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u548c\u610f\u56fe\u4fdd\u6301\u4e00\u81f4\u662f\u4e00\u9879\u5173\u952e\u6311\u6218\u3002", "method": "\u5206\u6790\u4e86\u4e0d\u540c\u8303\u4f8b\u4e2d\u5bf9\u9f50\u65b9\u6cd5\u7684\u53d1\u5c55\uff0c\u5bf9\u6838\u5fc3\u5bf9\u9f50\u76ee\u6807\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u8fdb\u884c\u4e86\u8868\u5f81\uff0c\u8ba8\u8bba\u4e86\u5305\u62ec\u76f4\u63a5\u504f\u597d\u4f18\u5316 (DPO)\u3001\u5baa\u6cd5 AI\u3001\u8111\u542f\u53d1\u65b9\u6cd5\u548c\u5bf9\u9f50\u4e0d\u786e\u5b9a\u6027\u91cf\u5316 (AUQ) \u7b49\u73b0\u6709\u6280\u672f\u3002", "result": "\u7efc\u8ff0\u8868\u660e\uff0c\u867d\u7136\u76d1\u7763\u5fae\u8c03\u80fd\u591f\u5b9e\u73b0\u57fa\u672c\u7684\u6307\u4ee4\u9075\u5faa\uff0c\u4f46\u57fa\u4e8e\u504f\u597d\u7684\u65b9\u6cd5\u4e3a\u4e0e\u7ec6\u5fae\u7684\u4eba\u7c7b\u610f\u56fe\u4fdd\u6301\u4e00\u81f4\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u7075\u6d3b\u6027\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u603b\u7ed3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u9f50\u7684\u6700\u65b0\u6280\u672f\u3001\u8bad\u7ec3\u534f\u8bae\u548c\u7ecf\u9a8c\u53d1\u73b0\uff0c\u6307\u51fa\u4e86\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u548c\u57fa\u51c6\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6982\u8ff0\u4e86\u5bf9\u9f50\u9886\u57df\u4e2d\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002"}}
{"id": "2507.19703", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19703", "abs": "https://arxiv.org/abs/2507.19703", "authors": ["Peter V. Coveney", "Sauro Succi"], "title": "The wall confronting large language models", "comment": null, "summary": "We show that the scaling laws which determine the performance of large\nlanguage models (LLMs) severely limit their ability to improve the uncertainty\nof their predictions. As a result, raising their reliability to meet the\nstandards of scientific inquiry is intractable by any reasonable measure. We\nargue that the very mechanism which fuels much of the learning power of LLMs,\nnamely the ability to generate non-Gaussian output distributions from Gaussian\ninput ones, might well be at the roots of their propensity to produce error\npileup, ensuing information catastrophes and degenerative AI behaviour. This\ntension between learning and accuracy is a likely candidate mechanism\nunderlying the observed low values of the scaling components. It is\nsubstantially compounded by the deluge of spurious correlations pointed out by\nCalude and Longo which rapidly increase in any data set merely as a function of\nits size, regardless of its nature. The fact that a degenerative AI pathway is\na very probable feature of the LLM landscape does not mean that it must\ninevitably arise in all future AI research. Its avoidance, which we also\ndiscuss in this paper, necessitates putting a much higher premium on insight\nand understanding of the structural characteristics of the problems being\ninvestigated.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u63d0\u9ad8\u9884\u6d4b\u53ef\u9760\u6027\uff0c\u5176\u5b66\u4e60\u80fd\u529b\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\uff0c\u8fd9\u53ef\u80fd\u662f\u5176\u56fa\u6709\u7f3a\u9677\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4ee5\u53ca\u5176\u5b66\u4e60\u80fd\u529b\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "method": "\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6269\u5c55\u5b9a\u5f8b\u53ca\u5176\u5bf9\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "result": "LLM\u7684\u6269\u5c55\u5b9a\u5f8b\u9650\u5236\u4e86\u5176\u63d0\u9ad8\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u9519\u8bef\u7d2f\u79ef\u3001\u4fe1\u606f\u707e\u96be\u548c\u9000\u5316\u6027AI\u884c\u4e3a\u3002\u8fd9\u79cd\u5b66\u4e60\u80fd\u529b\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u77db\u76fe\u53ef\u80fd\u662f\u5bfc\u81f4\u6269\u5c55\u7ec4\u4ef6\u503c\u4f4e\u7684\u539f\u56e0\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6269\u5c55\u5b9a\u5f8b\u4e25\u91cd\u9650\u5236\u4e86\u5176\u63d0\u9ad8\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u8fbe\u5230\u79d1\u5b66\u7814\u7a76\u6807\u51c6\u7684\u53ef\u9760\u6027\u662f\u96be\u4ee5\u5b9e\u73b0\u7684\u3002"}}
{"id": "2507.19725", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19725", "abs": "https://arxiv.org/abs/2507.19725", "authors": ["Leonardo Villalobos-Arias", "Grant Forbes", "Jianxun Wang", "David L Roberts", "Arnav Jhala"], "title": "Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors", "comment": "11 pages, 7 figures, 3 tables", "summary": "Games are challenging for Reinforcement Learning~(RL) agents due to their\nreward-sparsity, as rewards are only obtainable after long sequences of\ndeliberate actions. Intrinsic Motivation~(IM) methods -- which introduce\nexploration rewards -- are an effective solution to reward-sparsity. However,\nIM also causes an issue known as `reward hacking' where the agent optimizes for\nthe new reward at the expense of properly playing the game. The larger problem\nis that reward hacking itself is largely unknown; there is no answer to\nwhether, and to what extent, IM rewards change the behavior of RL agents. This\nstudy takes a first step by empirically evaluating the impact on behavior of\nthree IM techniques on the MiniGrid game-like environment. We compare these IM\nmodels with Generalized Reward Matching~(GRM), a method that can be used with\nany intrinsic reward function to guarantee optimality. Our results suggest that\nIM causes noticeable change by increasing the initial rewards, but also\naltering the way the agent plays; and that GRM mitigated reward hacking in some\nscenarios.", "AI": {"tldr": "\u5185\u5728\u52a8\u673a\u65b9\u6cd5\u4f1a\u6539\u53d8\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u884c\u4e3a\uff0c\u4f46\u5e7f\u4e49\u5956\u52b1\u5339\u914d\u65b9\u6cd5\u6709\u52a9\u4e8e\u51cf\u8f7b\u8fd9\u4e2a\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60 (RL) \u667a\u80fd\u4f53\u5728\u5956\u52b1\u7a00\u758f\u6e38\u620f\u4e2d\u96be\u4ee5\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u5e76\u7814\u7a76\u5185\u5728\u52a8\u673a\u65b9\u6cd5\u662f\u5426\u4ee5\u53ca\u5982\u4f55\u5728\u4f55\u79cd\u7a0b\u5ea6\u4e0a\u6539\u53d8\u667a\u80fd\u4f53\u884c\u4e3a\u3002", "method": "\u5b9e\u8bc1\u8bc4\u4f30\u4e09\u79cd\u5185\u5728\u52a8\u673a\u6280\u672f\u5bf9 MiniGrid \u6e38\u620f\u73af\u5883\u7684\u5f71\u54cd\uff0c\u5e76\u4e0e\u5e7f\u4e49\u5956\u52b1\u5339\u914d (GRM) \u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "IM \u4f1a\u589e\u52a0\u521d\u59cb\u5956\u52b1\u5e76\u6539\u53d8\u667a\u80fd\u4f53\u884c\u4e3a\uff0cGRM \u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u4ee5\u51cf\u8f7b\u5956\u52b1\u4f5c\u5f0a\u95ee\u9898\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u5b9e\u8bc1\u8bc4\u4f30\u4e86\u4e09\u79cd\u5185\u5728\u52a8\u673a (IM) \u6280\u672f\u5bf9 MiniGrid \u6e38\u620f\u73af\u5883\u4e2d\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5e76\u4f7f\u7528\u5e7f\u4e49\u5956\u52b1\u5339\u914d (GRM) \u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660e IM \u4f1a\u589e\u52a0\u521d\u59cb\u5956\u52b1\u5e76\u6539\u53d8\u667a\u80fd\u4f53\u884c\u4e3a\uff0c\u800c GRM \u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u4ee5\u51cf\u8f7b\u5956\u52b1\u4f5c\u5f0a\u95ee\u9898\u3002"}}
{"id": "2507.19726", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19726", "abs": "https://arxiv.org/abs/2507.19726", "authors": ["Yuzhang Xie", "Xu Han", "Ran Xu", "Xiao Hu", "Jiaying Lu", "Carl Yang"], "title": "HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision Healthcare", "comment": "Extended version of paper accepted at the 24th International Semantic\n  Web Conference (ISWC 2025), Main Tracks, Research Track, Oral", "summary": "Knowledge graphs (KGs) are important products of the semantic web, which are\nwidely used in various application domains. Healthcare is one of such domains\nwhere KGs are intensively used, due to the high requirement for knowledge\naccuracy and interconnected nature of healthcare data. However, KGs storing\ngeneral factual information often lack the ability to account for important\ncontexts of the knowledge such as the status of specific patients, which are\ncrucial in precision healthcare. Meanwhile, electronic health records (EHRs)\nprovide rich personal data, including various diagnoses and medications, which\nprovide natural contexts for general KGs. In this paper, we propose HypKG, a\nframework that integrates patient information from EHRs into KGs to generate\ncontextualized knowledge representations for accurate healthcare predictions.\nUsing advanced entity-linking techniques, we connect relevant knowledge from\ngeneral KGs with patient information from EHRs, and then utilize a hypergraph\nmodel to \"contextualize\" the knowledge with the patient information. Finally,\nwe employ hypergraph transformers guided by downstream prediction tasks to\njointly learn proper contextualized representations for both KGs and patients,\nfully leveraging existing knowledge in KGs and patient contexts in EHRs. In\nexperiments using a large biomedical KG and two real-world EHR datasets, HypKG\ndemonstrates significant improvements in healthcare prediction tasks across\nmultiple evaluation metrics. Additionally, by integrating external contexts,\nHypKG can learn to adjust the representations of entities and relations in KG,\npotentially improving the quality and real-world utility of knowledge.", "AI": {"tldr": "HypKG\u6846\u67b6\u6574\u5408\u7535\u5b50\u75c5\u5386\u4fe1\u606f\u5230\u77e5\u8bc6\u56fe\u8c31\uff0c\u751f\u6210\u60c5\u5883\u5316\u77e5\u8bc6\u8868\u793a\uff0c\u63d0\u5347\u533b\u7597\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u77e5\u8bc6\u56fe\u8c31\u7f3a\u4e4f\u5bf9\u60a3\u8005\u5177\u4f53\u72b6\u6001\u7b49\u91cd\u8981\u60c5\u5883\u4fe1\u606f\u7684\u8003\u8651\uff0c\u800c\u7535\u5b50\u75c5\u5386\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u4e2a\u4eba\u6570\u636e\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u8865\u5145\u3002", "method": "\u8be5\u6846\u67b6\u4f7f\u7528\u5148\u8fdb\u7684\u5b9e\u4f53\u94fe\u63a5\u6280\u672f\u8fde\u63a5\u901a\u7528\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u76f8\u5173\u77e5\u8bc6\u548c\u7535\u5b50\u75c5\u5386\u4e2d\u7684\u60a3\u8005\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u8d85\u56fe\u6a21\u578b\u5bf9\u77e5\u8bc6\u8fdb\u884c\u60c5\u5883\u5316\u5904\u7406\uff0c\u6700\u7ec8\u4f7f\u7528\u8d85\u56fe\u8f6c\u6362\u5668\u8fdb\u884c\u8054\u5408\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHypKG\u5728\u533b\u7597\u9884\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "HypKG\u6846\u67b6\u901a\u8fc7\u6574\u5408\u7535\u5b50\u75c5\u5386\u4e2d\u7684\u60a3\u8005\u4fe1\u606f\u5230\u77e5\u8bc6\u56fe\u8c31\u4e2d\uff0c\u751f\u6210\u60c5\u5883\u5316\u7684\u77e5\u8bc6\u8868\u793a\uff0c\u4ece\u800c\u63d0\u9ad8\u533b\u7597\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2507.19733", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.19733", "abs": "https://arxiv.org/abs/2507.19733", "authors": ["Alec Scully", "Cameron Stockton", "Forrest Hare"], "title": "Integrating Activity Predictions in Knowledge Graphs", "comment": "7 pages. 18 figures. Semantic Technology for Intelligence, Defense,\n  and Security (STIDS 2024)", "summary": "We argue that ontology-structured knowledge graphs can play a crucial role in\ngenerating predictions about future events. By leveraging the semantic\nframework provided by Basic Formal Ontology (BFO) and Common Core Ontologies\n(CCO), we demonstrate how data such as the movements of a fishing vessel can be\norganized in and retrieved from a knowledge graph. These query results are then\nused to create Markov chain models, allowing us to predict future states based\non the vessel's history. To fully support this process, we introduce the term\n`spatiotemporal instant' to complete the necessary structural semantics.\nAdditionally, we critique the prevailing ontological model of probability,\nwhich conflates probability with likelihood and relies on the problematic\nconcept of modal measurements: measurements of future entities. We propose an\nalternative view, where probabilities are treated as being about process\nprofiles, which better captures the dynamics of real world phenomena. Finally,\nwe demonstrate how our Markov chain based probability calculations can be\nseamlessly integrated back into the knowledge graph, enabling further analysis\nand decision-making. Keywords: predictive analytics, ontology, Markov chains,\nprobability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL.", "AI": {"tldr": "\u5229\u7528\u672c\u4f53\u7ed3\u6784\u77e5\u8bc6\u56fe\u8c31\u548c\u9a6c\u5c14\u79d1\u592b\u94fe\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\uff0c\u5e76\u6539\u8fdb\u6982\u7387\u8bba\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u9884\u6d4b\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f5c\u8005\u65e8\u5728\u5229\u7528\u672c\u4f53\u7ed3\u6784\u77e5\u8bc6\u56fe\u8c31\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "method": "\u5229\u7528BFO\u548cCCO\u672c\u4f53\u6846\u67b6\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff0c\u7ec4\u7ec7\u548c\u68c0\u7d22\u6570\u636e\uff08\u4f8b\u5982\u6e14\u8239\u7684\u79fb\u52a8\u8f68\u8ff9\uff09\uff0c\u6784\u5efa\u9a6c\u5c14\u79d1\u592b\u94fe\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u7387\u89c2\uff0c\u5c06\u6982\u7387\u89c6\u4e3a\u5173\u4e8e\u8fc7\u7a0b\u6982\u8981\u7684\u9648\u8ff0\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u672c\u4f53\u7ed3\u6784\u77e5\u8bc6\u56fe\u8c31\u548c\u9a6c\u5c14\u79d1\u592b\u94fe\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u5e76\u5bf9\u6982\u7387\u8bba\u7684\u672c\u4f53\u6a21\u578b\u8fdb\u884c\u4e86\u6539\u8fdb\u3002", "conclusion": "\u672c\u6587\u8bba\u8bc1\u4e86\u672c\u4f53\u7ed3\u6784\u77e5\u8bc6\u56fe\u8c31\u5728\u751f\u6210\u672a\u6765\u4e8b\u4ef6\u9884\u6d4b\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9a6c\u5c14\u79d1\u592b\u94fe\u548c\u6539\u8fdb\u6982\u7387\u8bba\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u6700\u7ec8\u5c06\u8ba1\u7b97\u7ed3\u679c\u96c6\u6210\u56de\u77e5\u8bc6\u56fe\u8c31\u3002"}}
{"id": "2507.19749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19749", "abs": "https://arxiv.org/abs/2507.19749", "authors": ["Lin Ren", "Guohui Xiao", "Guilin Qi", "Yishuai Geng", "Haohan Xue"], "title": "Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)", "comment": "Accepted for publication at the 22nd International Conference on\n  Principles of Knowledge Representation and Reasoning (KR 2025). The code is\n  available at https://github.com/HomuraT/ASPBench", "summary": "Answer Set Programming (ASP) is a powerful paradigm for non-monotonic\nreasoning. Recently, large language models (LLMs) have demonstrated promising\ncapabilities in logical reasoning. Despite this potential, current evaluations\nof LLM capabilities in ASP are often limited. Existing works normally employ\noverly simplified ASP programs, do not support negation, disjunction, or\nmultiple answer sets. Furthermore, there is a lack of benchmarks that introduce\ntasks specifically designed for ASP solving. To bridge this gap, we introduce\nASPBench, a comprehensive ASP benchmark, including three ASP specific tasks:\nASP entailment, answer set verification, and answer set computation. Our\nextensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs,\nincluding \\emph{deepseek-r1}, \\emph{o4-mini}, and\n\\emph{gemini-2.5-flash-thinking}, perform relatively well on the first two\nsimpler tasks, they struggle with answer set computation, which is the core of\nASP solving. These findings offer insights into the current limitations of LLMs\nin ASP solving. This highlights the need for new approaches that integrate\nsymbolic reasoning capabilities more effectively. The code and dataset are\navailable at https://github.com/HomuraT/ASPBench.", "AI": {"tldr": "LLM\u5728ASP\u6c42\u89e3\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u5c24\u5176\u5728\u56de\u7b54\u96c6\u8ba1\u7b97\u65b9\u9762\uff0c\u9700\u8981\u6539\u8fdb\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30LLM\u5728ASP\u4e2d\u7684\u80fd\u529b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7f3a\u4e4f\u4e13\u95e8\u4e3aASP\u6c42\u89e3\u8bbe\u8ba1\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684ASP\u57fa\u51c6\u6d4b\u8bd5ASPBench\uff0c\u5305\u62ec\u4e09\u4e2a\u7279\u5b9a\u4efb\u52a1\uff1aASP\u8574\u6db5\u3001\u56de\u7b54\u96c6\u9a8c\u8bc1\u548c\u56de\u7b54\u96c6\u8ba1\u7b97\uff0c\u5e76\u5bf914\u4e2a\u6700\u5148\u8fdb\u7684LLM\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cLLM\u5728\u8f83\u7b80\u5355\u7684ASP\u8574\u6db5\u548c\u56de\u7b54\u96c6\u9a8c\u8bc1\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u56de\u7b54\u96c6\u8ba1\u7b97\u65b9\u9762\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u56de\u7b54\u96c6\u7f16\u7a0b(ASP)\u6c42\u89e3\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u5c24\u5176\u5728\u6838\u5fc3\u4efb\u52a1\u2014\u2014\u56de\u7b54\u96c6\u8ba1\u7b97\u65b9\u9762\u8868\u73b0\u8f83\u5dee\uff0c\u8fd9\u8868\u660e\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u6574\u5408\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.19788", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19788", "abs": "https://arxiv.org/abs/2507.19788", "authors": ["Rifny Rachman", "Josh Tingey", "Richard Allmendinger", "Pradyumn Shukla", "Wei Pan"], "title": "Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation", "comment": null, "summary": "This study develops a generalised multi-objective, multi-echelon supply chain\noptimisation model with non-stationary markets based on a Markov decision\nprocess, incorporating economic, environmental, and social considerations. The\nmodel is evaluated using a multi-objective reinforcement learning (RL) method,\nbenchmarked against an originally single-objective RL algorithm modified with\nweighted sum using predefined weights, and a multi-objective evolutionary\nalgorithm (MOEA)-based approach. We conduct experiments on varying network\ncomplexities, mimicking typical real-world challenges using a customisable\nsimulator. The model determines production and delivery quantities across\nsupply chain routes to achieve near-optimal trade-offs between competing\nobjectives, approximating Pareto front sets. The results demonstrate that the\nprimary approach provides the most balanced trade-off between optimality,\ndiversity, and density, further enhanced with a shared experience buffer that\nallows knowledge transfer among policies. In complex settings, it achieves up\nto 75\\% higher hypervolume than the MOEA-based method and generates solutions\nthat are approximately eleven times denser, signifying better robustness, than\nthose produced by the modified single-objective RL method. Moreover, it ensures\nstable production and inventory levels while minimising demand loss.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u591a\u76ee\u6807\u591a\u7ea7\u4f9b\u5e94\u94fe\u4f18\u5316\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e0b\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5728\u975e\u5e73\u7a33\u5e02\u573a\u4e2d\uff0c\u8003\u8651\u7ecf\u6d4e\u3001\u73af\u5883\u548c\u793e\u4f1a\u56e0\u7d20\uff0c\u4f18\u5316\u591a\u76ee\u6807\u3001\u591a\u7ea7\u4f9b\u5e94\u94fe\u7684\u6a21\u578b\u3002", "method": "\u8be5\u7814\u7a76\u4f7f\u7528\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60 (RL) \u65b9\u6cd5\u8bc4\u4f30\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u4e0e\u6539\u8fdb\u7684\u5355\u76ee\u6807 RL \u7b97\u6cd5\u548c\u57fa\u4e8e\u591a\u76ee\u6807\u8fdb\u5316\u7b97\u6cd5 (MOEA) \u7684\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u8be5\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u6bd4\u57fa\u4e8eMOEA\u7684\u65b9\u6cd5\u9ad875%\u7684\u8d85\u4f53\u79ef\uff0c\u5e76\u4ea7\u751f\u6bd4\u6539\u8fdb\u7684\u5355\u76ee\u6807RL\u65b9\u6cd5\u5bc6\u5ea6\u9ad8\u7ea611\u500d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u786e\u4fdd\u4e86\u7a33\u5b9a\u7684\u751f\u4ea7\u548c\u5e93\u5b58\u6c34\u5e73\uff0c\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e86\u9700\u6c42\u635f\u5931\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u901a\u7528\u591a\u76ee\u6807\u3001\u591a\u7ea7\u4f9b\u5e94\u94fe\u4f18\u5316\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u8003\u8651\u4e86\u7ecf\u6d4e\u3001\u73af\u5883\u548c\u793e\u4f1a\u56e0\u7d20\uff0c\u5e76\u5728\u975e\u5e73\u7a33\u5e02\u573a\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u4f18\u5316\u6027\u3001\u591a\u6837\u6027\u548c\u5bc6\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u5747\u8861\u7684\u6743\u8861\u3002"}}
{"id": "2507.19882", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19882", "abs": "https://arxiv.org/abs/2507.19882", "authors": ["Xinshu Li", "Ruoyu Wang", "Erdun Gao", "Mingming Gong", "Lina Yao"], "title": "Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation", "comment": null, "summary": "Prompt learning has garnered attention for its efficiency over traditional\nmodel training and fine-tuning. However, existing methods, constrained by\ninadequate theoretical foundations, encounter difficulties in achieving\ncausally invariant prompts, ultimately falling short of capturing robust\nfeatures that generalize effectively across categories. To address these\nchallenges, we introduce the $\\textit{\\textbf{DiCap}}$ model, a theoretically\ngrounded $\\textbf{Di}$ffusion-based $\\textbf{C}$ounterf$\\textbf{a}$ctual\n$\\textbf{p}$rompt learning framework, which leverages a diffusion process to\niteratively sample gradients from the marginal and conditional distributions of\nthe causal model, guiding the generation of counterfactuals that satisfy the\nminimal sufficiency criterion. Grounded in rigorous theoretical derivations,\nthis approach guarantees the identifiability of counterfactual outcomes while\nimposing strict bounds on estimation errors. We further employ a contrastive\nlearning framework that leverages the generated counterfactuals, thereby\nenabling the refined extraction of prompts that are precisely aligned with the\ncausal features of the data. Extensive experimental results demonstrate that\nour method performs excellently across tasks such as image classification,\nimage-text retrieval, and visual question answering, with particularly strong\nadvantages in unseen categories.", "AI": {"tldr": "DiCap\u6a21\u578b\u5229\u7528\u6269\u6563\u8fc7\u7a0b\u751f\u6210\u56e0\u679c\u4e0d\u53d8\u63d0\u793a\uff0c\u63d0\u5347\u4e86\u63d0\u793a\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\uff0c\u96be\u4ee5\u83b7\u5f97\u56e0\u679c\u4e0d\u53d8\u7684\u63d0\u793a\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u6269\u6563\u8fc7\u7a0b\u7684\u9006\u53cd\u4e8b\u5b9e\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u8fed\u4ee3\u91c7\u6837\u56e0\u679c\u6a21\u578b\u7684\u8fb9\u9645\u5206\u5e03\u548c\u6761\u4ef6\u5206\u5e03\u7684\u68af\u5ea6\uff0c\u751f\u6210\u6ee1\u8db3\u6700\u5c0f\u5145\u5206\u6027\u51c6\u5219\u7684\u9006\u53cd\u4e8b\u5b9e\u3002", "result": "DiCap\u6a21\u578b\u901a\u8fc7\u751f\u6210\u6ee1\u8db3\u6700\u5c0f\u5145\u5206\u6027\u51c6\u5219\u7684\u9006\u53cd\u4e8b\u5b9e\uff0c\u5e76\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u53d6\u4e0e\u6570\u636e\u56e0\u679c\u7279\u5f81\u7cbe\u786e\u5bf9\u9f50\u7684\u63d0\u793a\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DiCap\u6a21\u578b\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u56fe\u6587\u68c0\u7d22\u548c\u89c6\u89c9\u95ee\u7b54\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u672a\u89c1\u7c7b\u522b\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2507.19960", "categories": ["cs.AI", "I.2.0; K.2; K.4.0"], "pdf": "https://arxiv.org/pdf/2507.19960", "abs": "https://arxiv.org/abs/2507.19960", "authors": ["Olivia Guest"], "title": "What Does 'Human-Centred AI' Mean?", "comment": null, "summary": "While it seems sensible that human-centred artificial intelligence (AI) means\ncentring \"human behaviour and experience,\" it cannot be any other way. AI, I\nargue, is usefully seen as a relationship between technology and humans where\nit appears that artifacts can perform, to a greater or lesser extent, human\ncognitive labour. This is evinced using examples that juxtapose technology with\ncognition, inter alia: abacus versus mental arithmetic; alarm clock versus\nknocker-upper; camera versus vision; and sweatshop versus tailor. Using novel\ndefinitions and analyses, sociotechnical relationships can be analysed into\nvarying types of: displacement (harmful), enhancement (beneficial), and/or\nreplacement (neutral) of human cognitive labour. Ultimately, all AI implicates\nhuman cognition; no matter what. Obfuscation of cognition in the AI context --\nfrom clocks to artificial neural networks -- results in distortion, in slowing\ncritical engagement, perverting cognitive science, and indeed in limiting our\nability to truly centre humans and humanity in the engineering of AI systems.\nTo even begin to de-fetishise AI, we must look the human-in-the-loop in the\neyes.", "AI": {"tldr": "AI\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u606f\u606f\u76f8\u5173\uff0c\u5ffd\u89c6\u8ba4\u77e5\u4f1a\u963b\u788d\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684AI\u53d1\u5c55\u3002", "motivation": "\u6f84\u6e05\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684AI\u7684\u771f\u6b63\u542b\u4e49\uff0c\u907f\u514d\u5bf9AI\u7684\u8ff7\u4fe1\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u6280\u672f\u4e0e\u8ba4\u77e5\u7684\u4f8b\u5b50\uff08\u7b97\u76d8\u4e0e\u5fc3\u7b97\u3001\u95f9\u949f\u4e0e\u53eb\u9192\u670d\u52a1\u3001\u76f8\u673a\u4e0e\u89c6\u89c9\u3001\u8840\u6c57\u5de5\u5382\u4e0e\u88c1\u7f1d\uff09\u5206\u6790\u793e\u4f1a\u6280\u672f\u5173\u7cfb\uff0c\u5c06\u5176\u5206\u4e3a\u4e09\u79cd\u7c7b\u578b\uff1a\u7f6e\u6362\uff08\u6709\u5bb3\uff09\u3001\u589e\u5f3a\uff08\u6709\u76ca\uff09\u548c\u66ff\u4ee3\uff08\u4e2d\u6027\uff09\u3002", "result": "\u5c06AI\u5b9a\u4e49\u4e3a\u6280\u672f\u4e0e\u4eba\u7c7b\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5176\u4e2d\u4eba\u5de5\u5236\u54c1\u53ef\u5728\u4e0d\u540c\u7a0b\u5ea6\u4e0a\u6267\u884c\u4eba\u7c7b\u8ba4\u77e5\u52b3\u52a8\u3002", "conclusion": "\u6240\u6709AI\u90fd\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u6709\u5173\uff0c\u5ffd\u89c6\u8ba4\u77e5\u4f1a\u5bfc\u81f4\u626d\u66f2\u548c\u5bf9\u4eba\u7c7b\u4e2d\u5fc3AI\u5de5\u7a0b\u7684\u9650\u5236\u3002"}}
{"id": "2507.19973", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.19973", "abs": "https://arxiv.org/abs/2507.19973", "authors": ["Ebrahim Rasromani", "Stella K. Kang", "Yanqi Xu", "Beisong Liu", "Garvit Luhadia", "Wan Fung Chui", "Felicia L. Pasadyn", "Yu Chih Hung", "Julie Y. An", "Edwin Mathieu", "Zehui Gu", "Carlos Fernandez-Granda", "Ammar A. Javed", "Greg D. Sacks", "Tamas Gonda", "Chenchan Huang", "Yiqiu Shen"], "title": "Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization", "comment": null, "summary": "Background: Manual extraction of pancreatic cystic lesion (PCL) features from\nradiology reports is labor-intensive, limiting large-scale studies needed to\nadvance PCL research. Purpose: To develop and evaluate large language models\n(LLMs) that automatically extract PCL features from MRI/CT reports and assign\nrisk categories based on guidelines. Materials and Methods: We curated a\ntraining dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134\npatients that described PCLs. Labels were generated by GPT-4o using\nchain-of-thought (CoT) prompting to extract PCL and main pancreatic duct\nfeatures. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated\nCoT data. Features were mapped to risk categories per institutional guideline\nbased on the 2017 ACR White Paper. Evaluation was performed on 285 held-out\nhuman-annotated reports. Model outputs for 100 cases were independently\nreviewed by three radiologists. Feature extraction was evaluated using exact\nmatch accuracy, risk categorization with macro-averaged F1 score, and\nradiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning\nimproved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79%\nto 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved\n(LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no\nstatistically significant differences. Radiologist inter-reader agreement was\nhigh (Fleiss' Kappa = 0.888) and showed no statistically significant difference\nwith the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT\n(Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels\non par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT\nsupervision enable accurate, interpretable, and efficient phenotyping for\nlarge-scale PCL research, achieving performance comparable to GPT-4o.", "AI": {"tldr": "\u5f00\u6e90LLM\u6a21\u578b\u5728\u8f85\u52a9\u80f0\u817a\u56ca\u6027\u75c5\u53d8(PCL)\u7279\u5f81\u63d0\u53d6\u548c\u98ce\u9669\u5206\u7ea7\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u8fbe\u5230\u4e0eGPT-4o\u76f8\u5f53\u7684\u6c34\u5e73\uff0c\u6781\u5927\u63d0\u9ad8\u4e86\u79d1\u7814\u6548\u7387\u3002", "motivation": "\u4eba\u5de5\u63d0\u53d6\u653e\u5c04\u5b66\u62a5\u544a\u4e2d\u7684\u80f0\u817a\u56ca\u6027\u75c5\u53d8(PCL)\u7279\u5f81\u8d39\u65f6\u8d39\u529b\uff0c\u9650\u5236\u4e86\u63a8\u8fdbPCL\u7814\u7a76\u6240\u9700\u7684\u5927\u89c4\u6a21\u7814\u7a76\u3002", "method": "\u4f7f\u7528QLoRA\u5fae\u8c03\u4e24\u4e2a\u5f00\u6e90LLM\uff0c\u5e76\u4f7f\u7528GPT-4o\u751f\u6210\u7684\u601d\u7ef4\u94fe\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u5c06\u63d0\u53d6\u7684\u7279\u5f81\u6839\u636e2017\u5e74ACR\u767d\u76ae\u4e66\u4e2d\u7684\u673a\u6784\u6307\u5357\u6620\u5c04\u5230\u98ce\u9669\u7c7b\u522b\u3002", "result": "\u5fae\u8c03\u540e\u7684LLaMA\u548cDeepSeek\u6a21\u578b\u5728\u7279\u5f81\u63d0\u53d6\u51c6\u786e\u7387\u548c\u98ce\u9669\u5206\u7c7bF1\u5206\u6570\u4e0a\u5747\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u4e0eGPT-4o\u7684\u7ed3\u679c\u975e\u5e38\u63a5\u8fd1\u3002\u653e\u5c04\u79d1\u533b\u5e08\u4e0e\u6a21\u578b\u7684\u4e00\u81f4\u6027\u4e5f\u8fbe\u5230\u4e86\u5f88\u9ad8\u7684\u6c34\u5e73\u3002", "conclusion": "\u5fae\u8c03\u540e\u7684\u5f00\u6e90LLM\u7ed3\u5408\u601d\u7ef4\u94fe\u76d1\u7763\u80fd\u591f\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u5730\u8fdb\u884c\u8868\u578b\u5206\u6790\uff0c\u4ece\u800c\u652f\u6301\u5927\u89c4\u6a21PCL\u7814\u7a76\uff0c\u5176\u6027\u80fd\u53ef\u4e0eGPT-4o\u5ab2\u7f8e\u3002"}}
{"id": "2507.19974", "categories": ["cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.19974", "abs": "https://arxiv.org/abs/2507.19974", "authors": ["Tongjie Li", "Jianhua Zhang", "Li Yu", "Yuxiang Zhang", "Yunlong Cai", "Fan Xu", "Guangyi Liu"], "title": "Digital Twin Channel-Enabled Online Resource Allocation for 6G: Principle, Architecture and Application", "comment": null, "summary": "Emerging applications such as holographic communication, autonomous driving,\nand the industrial Internet of Things impose stringent requirements on\nflexible, low-latency, and reliable resource allocation in 6G networks.\nConventional methods, which rely on statistical modeling, have proven effective\nin general contexts but may fail to achieve optimal performance in specific and\ndynamic environments. Furthermore, acquiring real-time channel state\ninformation (CSI) typically requires excessive pilot overhead. To address these\nchallenges, a digital twin channel (DTC)-enabled online optimization framework\nis proposed, in which DTC is employed to predict CSI based on environmental\nsensing. The predicted CSI is then utilized by lightweight game-theoretic\nalgorithms to perform online resource allocation in a timely and efficient\nmanner. Simulation results based on a digital replica of a realistic industrial\nworkshop demonstrate that the proposed method achieves throughput improvements\nof up to 11.5\\% compared with pilot-based ideal CSI schemes, validating its\neffectiveness for scalable, low-overhead, and environment-aware communication\nin future 6G networks.", "AI": {"tldr": "\u5229\u7528\u6570\u5b57\u5b6a\u751f\u4fe1\u9053\u9884\u6d4b CSI\uff0c\u5e76\u7528\u535a\u5f08\u8bba\u7b97\u6cd5\u8fdb\u884c\u8d44\u6e90\u5206\u914d\uff0c\u63d0\u9ad8\u4e86 6G \u7f51\u7edc\u7684\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u7edf\u8ba1\u5efa\u6a21\u7684\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e0b\u96be\u4ee5\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u4e14\u83b7\u53d6\u5b9e\u65f6 CSI \u9700\u8981\u5927\u91cf\u7684\u5bfc\u9891\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u4fe1\u9053 (DTC) \u7684\u5728\u7ebf\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528 DTC \u9884\u6d4b CSI\uff0c\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7\u535a\u5f08\u8bba\u7b97\u6cd5\u8fdb\u884c\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u5bfc\u9891\u7684\u7406\u60f3 CSI \u65b9\u6848\u541e\u5410\u91cf\u63d0\u9ad8\u4e86 11.5%\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u4fe1\u9053 (DTC) \u7684\u5728\u7ebf\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3 6G \u7f51\u7edc\u4e2d\u8d44\u6e90\u5206\u914d\u7684\u6311\u6218\uff0c\u8be5\u6846\u67b6\u5229\u7528 DTC \u9884\u6d4b CSI\uff0c\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7\u535a\u5f08\u8bba\u7b97\u6cd5\u8fdb\u884c\u5728\u7ebf\u8d44\u6e90\u5206\u914d\uff0c\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u5bfc\u9891\u7684\u7406\u60f3 CSI \u65b9\u6848\u541e\u5410\u91cf\u63d0\u9ad8\u4e86 11.5%\u3002"}}
{"id": "2507.20000", "categories": ["cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2507.20000", "abs": "https://arxiv.org/abs/2507.20000", "authors": ["Renaud Fabre", "Daniel Egret", "Patrice Bellot"], "title": "Matching Game Preferences Through Dialogical Large Language Models: A Perspective", "comment": "28 pages, 1 figure. Published in Applied Sciences", "summary": "This perspective paper explores the future potential of \"conversational\nintelligence\" by examining how Large Language Models (LLMs) could be combined\nwith GRAPHYP's network system to better understand human conversations and\npreferences. Using recent research and case studies, we propose a conceptual\nframework that could make AI rea-soning transparent and traceable, allowing\nhumans to see and understand how AI reaches its conclusions. We present the\nconceptual perspective of \"Matching Game Preferences through Dialogical Large\nLanguage Models (D-LLMs),\" a proposed system that would allow multiple users to\nshare their different preferences through structured conversations. This\napproach envisions personalizing LLMs by embedding individual user preferences\ndirectly into how the model makes decisions. The proposed D-LLM framework would\nrequire three main components: (1) reasoning processes that could analyze\ndifferent search experiences and guide performance, (2) classification systems\nthat would identify user preference patterns, and (3) dialogue approaches that\ncould help humans resolve conflicting information. This perspective framework\naims to create an interpretable AI system where users could examine,\nunderstand, and combine the different human preferences that influence AI\nresponses, detected through GRAPHYP's search experience networks. The goal of\nthis perspective is to envision AI systems that would not only provide answers\nbut also show users how those answers were reached, making artificial\nintelligence more transparent and trustworthy for human decision-making.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u7f51\u7edc\u7cfb\u7edf\uff0c\u521b\u5efa\u66f4\u900f\u660e\u548c\u53ef\u4fe1\u8d56\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u548c\u6ee1\u8db3\u4eba\u7c7b\u9700\u6c42\u3002", "motivation": "\u4e3a\u4e86\u4f7f\u4eba\u5de5\u667a\u80fd\u63a8\u7406\u8fc7\u7a0b\u900f\u660e\u548c\u53ef\u8ffd\u6eaf\uff0c\u8ba9\u4eba\u4eec\u80fd\u591f\u770b\u5230\u5e76\u7406\u89e3\u4eba\u5de5\u667a\u80fd\u5982\u4f55\u5f97\u51fa\u7ed3\u8bba\uff0c\u4ece\u800c\u63d0\u9ad8\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6982\u5ff5\u6846\u67b6\u201c\u901a\u8fc7\u5bf9\u8bdd\u5f0f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5339\u914d\u6e38\u620f\u504f\u597d (D-LLM)\u201d\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u5206\u6790\u4e0d\u540c\u641c\u7d22\u4f53\u9a8c\u5e76\u6307\u5bfc\u6027\u80fd\u7684\u63a8\u7406\u8fc7\u7a0b\uff1b\u8bc6\u522b\u7528\u6237\u504f\u597d\u6a21\u5f0f\u7684\u5206\u7c7b\u7cfb\u7edf\uff1b\u4ee5\u53ca\u5e2e\u52a9\u4eba\u7c7b\u89e3\u51b3\u51b2\u7a81\u4fe1\u606f\u7684\u5bf9\u8bdd\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u4f7f\u4eba\u5de5\u667a\u80fd\u63a8\u7406\u8fc7\u7a0b\u900f\u660e\u548c\u53ef\u8ffd\u6eaf\uff0c\u5e76\u5141\u8bb8\u4e2a\u6027\u5316LLM\u3002", "conclusion": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e0e GRAPHYP \u7f51\u7edc\u7cfb\u7edf\u76f8\u7ed3\u5408\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u4eba\u7c7b\u5bf9\u8bdd\u548c\u504f\u597d\u7684\u6f5c\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u901a\u8fc7\u5bf9\u8bdd\u5f0f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5339\u914d\u6e38\u620f\u504f\u597d (D-LLM)\u201d\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u65e8\u5728\u521b\u5efa\u53ef\u89e3\u91ca\u7684 AI \u7cfb\u7edf\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u68c0\u67e5\u3001\u7406\u89e3\u548c\u7ed3\u5408\u5f71\u54cd AI \u54cd\u5e94\u7684\u4e0d\u540c\u4eba\u7c7b\u504f\u597d\u3002"}}
{"id": "2507.20010", "categories": ["cs.AI", "cs.GT", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.20010", "abs": "https://arxiv.org/abs/2507.20010", "authors": ["M\u00fcge Fidan", "Esra Erdem"], "title": "Finding Personalized Good-Enough Solutions to Unsatisfiable Stable Roommates Problems", "comment": null, "summary": "The Stable Roommates problems are characterized by the preferences of agents\nover other agents as roommates. A solution is a partition of the agents into\npairs that are acceptable to each other (i.e., they are in the preference lists\nof each other), and the matching is stable (i.e., there do not exist any two\nagents who prefer each other to their roommates, and thus block the matching).\nMotivated by real-world applications, and considering that stable roommates\nproblems do not always have solutions, we continue our studies to compute\n\"good-enough\" matchings. In addition to the agents' habits and habitual\npreferences, we consider their networks of preferred friends, and introduce a\nmethod to generate personalized solutions to stable roommates problems. We\nillustrate the usefulness of our method with examples and empirical\nevaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8003\u8651\u793e\u4ea4\u7f51\u7edc\u7684\u4e2a\u6027\u5316\u7a33\u5b9a\u5ba4\u53cb\u5339\u914d\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u7a33\u5b9a\u5ba4\u53cb\u95ee\u9898\u6c42\u89e3\u56f0\u96be\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u7a33\u5b9a\u5ba4\u53cb\u95ee\u9898\u5e76\u4e0d\u603b\u662f\u6709\u89e3\uff0c\u56e0\u6b64\u7814\u7a76\u5982\u4f55\u8ba1\u7b97\u201c\u8db3\u591f\u597d\u201d\u7684\u5339\u914d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u4ee3\u7406\u504f\u597d\u548c\u793e\u4ea4\u7f51\u7edc\u7684\u4e2a\u6027\u5316\u5339\u914d\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u7a33\u5b9a\u5ba4\u53cb\u95ee\u9898\u4e2a\u6027\u5316\u89e3\u51b3\u65b9\u6848\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u548c\u5b9e\u8bc1\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u7814\u7a76\u4e86\u7a33\u5b9a\u5ba4\u53cb\u95ee\u9898\u7684\u4e2a\u6027\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u8003\u8651\u4e86\u4ee3\u7406\u7684\u504f\u597d\u548c\u793e\u4ea4\u7f51\u7edc\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u548c\u5b9e\u8bc1\u8bc4\u4f30\u8bf4\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
