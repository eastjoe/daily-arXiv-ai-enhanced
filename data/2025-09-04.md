<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 15]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Can Media Act as a Soft Regulator of Safe AI Development? A Game Theoretical Analysis](https://arxiv.org/abs/2509.02650)
*Henrique Correia da Fonseca,António Fernandes,Zhao Song,Theodor Cimpeanu,Nataliya Balabanova,Adeela Bashir,Paolo Bova,Alessio Buscemi,Alessandro Di Stefano,Manh Hong Duong,Elias Fernandez Domingos,Ndidi Bianca Ogbo,Simon T. Powers,Daniele Proverbio,Zia Ush Shamszaman,Fernando P. Santos,The Anh Han,Marcus Krellner*

Main category: cs.AI

TL;DR: 媒体报道能促进AI开发者生产安全产品，但信息可靠性及成本是关键因素。


<details>
  <summary>Details</summary>
Motivation: 探究媒体报道对AI安全的影响，以及其能否促使开发者优先考虑用户安全而非利润。

Method: 构建基于进化博弈论的自利开发者和用户人工种群模型进行研究。

Result: 媒体报道能够促进开发者与用户间的合作，从而提升AI产品安全性，但信息可靠性不足或成本过高会影响其效果。

Conclusion: 媒体作为一种强大的软性监管者，即使在缺乏政府监管的情况下，也能通过塑造公众认知和追究开发者责任来引导AI安全。

Abstract: When developers of artificial intelligence (AI) products need to decide
between profit and safety for the users, they likely choose profit.
Untrustworthy AI technology must come packaged with tangible negative
consequences. Here, we envisage those consequences as the loss of reputation
caused by media coverage of their misdeeds, disseminated to the public. We
explore whether media coverage has the potential to push AI creators into the
production of safe products, enabling widespread adoption of AI technology. We
created artificial populations of self-interested creators and users and
studied them through the lens of evolutionary game theory. Our results reveal
that media is indeed able to foster cooperation between creators and users, but
not always. Cooperation does not evolve if the quality of the information
provided by the media is not reliable enough, or if the costs of either
accessing media or ensuring safety are too high. By shaping public perception
and holding developers accountable, media emerges as a powerful soft regulator
-- guiding AI safety even in the absence of formal government oversight.

</details>


### [2] [The Future of Artificial Intelligence and the Mathematical and Physical Sciences (AI+MPS)](https://arxiv.org/abs/2509.02661)
*Andrew Ferguson,Marisa LaFleur,Lars Ruthotto,Jesse Thaler,Yuan-Sen Ting,Pratyush Tiwary,Soledad Villar,E. Paulo Alves,Jeremy Avigad,Simon Billinge,Camille Bilodeau,Keith Brown,Emmanuel Candes,Arghya Chattopadhyay,Bingqing Cheng,Jonathan Clausen,Connor Coley,Andrew Connolly,Fred Daum,Sijia Dong,Chrisy Xiyu Du,Cora Dvorkin,Cristiano Fanelli,Eric B. Ford,Luis Manuel Frutos,Nicolás García Trillos,Cecilia Garraffo,Robert Ghrist,Rafael Gomez-Bombarelli,Gianluca Guadagni,Sreelekha Guggilam,Sergei Gukov,Juan B. Gutiérrez,Salman Habib,Johannes Hachmann,Boris Hanin,Philip Harris,Murray Holland,Elizabeth Holm,Hsin-Yuan Huang,Shih-Chieh Hsu,Nick Jackson,Olexandr Isayev,Heng Ji,Aggelos Katsaggelos,Jeremy Kepner,Yannis Kevrekidis,Michelle Kuchera,J. Nathan Kutz,Branislava Lalic,Ann Lee,Matt LeBlanc,Josiah Lim,Rebecca Lindsey,Yongmin Liu,Peter Y. Lu,Sudhir Malik,Vuk Mandic,Vidya Manian,Emeka P. Mazi,Pankaj Mehta,Peter Melchior,Brice Ménard,Jennifer Ngadiuba,Stella Offner,Elsa Olivetti,Shyue Ping Ong,Christopher Rackauckas,Philippe Rigollet,Chad Risko,Philip Romero,Grant Rotskoff,Brett Savoie,Uros Seljak,David Shih,Gary Shiu,Dima Shlyakhtenko,Eva Silverstein,Taylor Sparks,Thomas Strohmer,Christopher Stubbs,Stephen Thomas,Suriyanarayanan Vaikuntanathan,Rene Vidal,Francisco Villaescusa-Navarro,Gregory Voth,Benjamin Wandelt,Rachel Ward,Melanie Weber,Risa Wechsler,Stephen Whitelam,Olaf Wiest,Mike Williams,Zhuoran Yang,Yaroslava G. Yingling,Bin Yu,Shuwen Yue,Ann Zabludoff,Huimin Zhao,Tong Zhang*

Main category: cs.AI

TL;DR: 该论文总结了2025年NSF研讨会关于人工智能与数学物理科学未来发展的观点，强调了AI与MPS学科交叉融合的重要性，并提出促进AI+MPS研究、社区建设和人才培养的战略重点。


<details>
  <summary>Details</summary>
Motivation: 探讨数学物理科学领域如何利用并促进人工智能发展，以及如何反过来利用基础科学知识来优化人工智能发展。

Method: 对NSF研讨会结果进行总结和归纳，提出促进AI+MPS发展的战略建议。

Result: 提出三项战略重点：促进AI+MPS双向研究；建设AI+MPS跨学科研究社区；加强AI相关教育和人才培养。

Conclusion: 建议资助机构、教育机构和研究人员采取相应措施，使数学物理科学领域能够在AI+MPS变革中发挥主导作用并充分利用其潜力。

Abstract: This community paper developed out of the NSF Workshop on the Future of
Artificial Intelligence (AI) and the Mathematical and Physics Sciences (MPS),
which was held in March 2025 with the goal of understanding how the MPS domains
(Astronomy, Chemistry, Materials Research, Mathematical Sciences, and Physics)
can best capitalize on, and contribute to, the future of AI. We present here a
summary and snapshot of the MPS community's perspective, as of Spring/Summer
2025, in a rapidly developing field. The link between AI and MPS is becoming
increasingly inextricable; now is a crucial moment to strengthen the link
between AI and Science by pursuing a strategy that proactively and thoughtfully
leverages the potential of AI for scientific discovery and optimizes
opportunities to impact the development of AI by applying concepts from
fundamental science. To achieve this, we propose activities and strategic
priorities that: (1) enable AI+MPS research in both directions; (2) build up an
interdisciplinary community of AI+MPS researchers; and (3) foster education and
workforce development in AI for MPS researchers and students. We conclude with
a summary of suggested priorities for funding agencies, educational
institutions, and individual researchers to help position the MPS community to
be a leader in, and take full advantage of, the transformative potential of
AI+MPS.

</details>


### [3] [Planning with Reasoning using Vision Language World Model](https://arxiv.org/abs/2509.02722)
*Delong Chen,Theo Moutakanni,Willy Chung,Yejin Bang,Ziwei Ji,Allen Bolourchi,Pascale Fung*

Main category: cs.AI

TL;DR: VLWM是一个基于自然视频的视觉语言世界模型，能够理解和推理具有语义和时间抽象的动作，在视觉规划方面取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有高层次世界模型难以理解和推理具有语义和时间抽象的动作。

Method: VLWM利用LLM Self-Refine和Tree of Captions对视频进行处理，学习动作策略和动态模型，并通过代价最小化进行规划，其中代价由自监督训练的critic模型评估。

Result: VLWM在VPA基准测试和PlannerArena人工评估中取得了最先进的性能，系统2比系统1的Elo评分提高了27%。在RoboVQA和WorldPrediction基准测试中也优于强基线。

Conclusion: VLWM为视觉语言世界建模提供了一种有效的方法，并在视觉规划任务中展现出巨大的潜力。

Abstract: Effective planning requires strong world models, but high-level world models
that can understand and reason about actions with semantic and temporal
abstraction remain largely underdeveloped. We introduce the Vision Language
World Model (VLWM), a foundation model trained for language-based world
modeling on natural videos. Given visual observations, the VLWM first infers
the overall goal achievements then predicts a trajectory composed of
interleaved actions and world state changes. Those targets are extracted by
iterative LLM Self-Refine conditioned on compressed future observations
represented by Tree of Captions. The VLWM learns both an action policy and a
dynamics model, which respectively facilitates reactive system-1 plan decoding
and reflective system-2 planning via cost minimization. The cost evaluates the
semantic distance between the hypothetical future states given by VLWM
roll-outs and the expected goal state, and is measured by a critic model that
we trained in a self-supervised manner. The VLWM achieves state-of-the-art
Visual Planning for Assistance (VPA) performance on both benchmark evaluations
and our proposed PlannerArena human evaluations, where system-2 improves the
Elo score by +27% upon system-1. The VLWM models also outperforms strong VLM
baselines on RoboVQA and WorldPrediction benchmark.

</details>


### [4] [Deep Research is the New Analytics System: Towards Building the Runtime for AI-Driven Analytics](https://arxiv.org/abs/2509.02751)
*Matthew Russo,Tim Kraska*

Main category: cs.AI

TL;DR: 该论文提出了一种结合语义算子和深度研究系统优点的AI驱动分析运行时原型，在两个基本查询上取得了显著的性能提升，F1分数最高提升1.95倍，成本和运行时间最多分别节省76.8%和72.7%。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义算子的AI驱动分析系统执行效率低，而深度研究系统缺乏查询计划优化，该论文旨在结合两者的优势。

Method: 构建了一个原型系统，使深度研究代理能够编写和执行优化的语义算子程序。

Result: 该原型系统在两个基本查询上优于手工编写的语义算子程序和开放的深度研究系统，F1分数最高提升1.95倍，成本和运行时间最多分别节省76.8%和72.7%。

Conclusion: 该原型系统为构建高效的AI驱动分析运行时迈出了第一步，未来可进一步扩展和优化。

Abstract: With advances in large language models (LLMs), researchers are creating new
systems that can perform AI-driven analytics over large unstructured datasets.
Recent work has explored executing such analytics queries using semantic
operators -- a declarative set of AI-powered data transformations with natural
language specifications. However, even when optimized, these operators can be
expensive to execute on millions of records and their iterator execution
semantics make them ill-suited for interactive data analytics tasks. In another
line of work, Deep Research systems have demonstrated an ability to answer
natural language question(s) over large datasets. These systems use one or more
LLM agent(s) to plan their execution, process the dataset(s), and iteratively
refine their answer. However, these systems do not explicitly optimize their
query plans which can lead to poor plan execution. In order for AI-driven
analytics to excel, we need a runtime which combines the optimized execution of
semantic operators with the flexibility and more dynamic execution of Deep
Research systems. As a first step towards this vision, we build a prototype
which enables Deep Research agents to write and execute optimized semantic
operator programs. We evaluate our prototype and demonstrate that it can
outperform a handcrafted semantic operator program and open Deep Research
systems on two basic queries. Compared to a standard open Deep Research agent,
our prototype achieves up to 1.95x better F1-score. Furthermore, even if we
give the agent access to semantic operators as tools, our prototype still
achieves cost and runtime savings of up to 76.8% and 72.7% thanks to its
optimized execution.

</details>


### [5] [Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving](https://arxiv.org/abs/2509.02754)
*Mingyi Wang,Jingke Wang,Tengju Ye,Junbo Chen,Kaicheng Yu*

Main category: cs.AI

TL;DR: 本文评估了大型语言模型(LLM)的五个关键模块在自动驾驶运动生成中的可迁移性，并在Waymo Sim Agents基准测试上取得了有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型组件在自动驾驶运动生成中的可迁移性，以提高性能。

Method: 对五个LLM模块（分词器设计、位置嵌入、预训练范式、后训练策略和测试时计算）进行了全面评估，并在Waymo Sim Agents基准测试上进行了实验。

Result: 实验证明，适当调整后，这些模块可以显著提高自动驾驶运动生成的性能。研究者识别了哪些技术可以有效迁移，分析了其他技术失败的潜在原因，并讨论了自动驾驶场景所需的具体调整。

Conclusion: 部分LLM模块可有效迁移到自动驾驶运动生成，但需要针对特定场景进行调整。

Abstract: Recent breakthroughs in large language models (LLMs) have not only advanced
natural language processing but also inspired their application in domains with
structurally similar problems--most notably, autonomous driving motion
generation. Both domains involve autoregressive sequence modeling, token-based
representations, and context-aware decision making, making the transfer of LLM
components a natural and increasingly common practice. However, despite
promising early attempts, a systematic understanding of which LLM modules are
truly transferable remains lacking. In this paper, we present a comprehensive
evaluation of five key LLM modules--tokenizer design, positional embedding,
pre-training paradigms, post-training strategies, and test-time
computation--within the context of motion generation for autonomous driving.
Through extensive experiments on the Waymo Sim Agents benchmark, we demonstrate
that, when appropriately adapted, these modules can significantly improve
performance for autonomous driving motion generation. In addition, we identify
which techniques can be effectively transferred, analyze the potential reasons
for the failure of others, and discuss the specific adaptations needed for
autonomous driving scenarios. We evaluate our method on the Sim Agents task and
achieve competitive results.

</details>


### [6] [Plan Verification for LLM-Based Embodied Task Completion Agents](https://arxiv.org/abs/2509.02761)
*Ananth Hariharan,Vardhan Dongre,Dilek Hakkani-Tür,Gokhan Tur*

Main category: cs.AI

TL;DR: 该论文提出一种迭代式验证框架，利用大型语言模型（LLM）改进具身AI的任务计划，提高轨迹质量。


<details>
  <summary>Details</summary>
Motivation: LLM生成的具身AI任务计划可能存在噪声，例如冗余动作和逻辑错误，降低策略质量。

Method: 该框架使用一个“法官”LLM批判动作序列，另一个“规划者”LLM应用修改，迭代改进轨迹。

Result: 在TEACh数据集上，该框架在四个LLM上实现了高达90%的召回率和100%的精确率，并能快速收敛。

Conclusion: 该方法为具身AI的模仿学习提供了高质量训练数据，并保留了人类的错误恢复模式。

Abstract: Large language model (LLM) based task plans and corresponding human
demonstrations for embodied AI may be noisy, with unnecessary actions,
redundant navigation, and logical errors that reduce policy quality. We propose
an iterative verification framework in which a Judge LLM critiques action
sequences and a Planner LLM applies the revisions, yielding progressively
cleaner and more spatially coherent trajectories. Unlike rule-based approaches,
our method relies on natural language prompting, enabling broad generalization
across error types including irrelevant actions, contradictions, and missing
steps. On a set of manually annotated actions from the TEACh embodied AI
dataset, our framework achieves up to 90% recall and 100% precision across four
state-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout).
The refinement loop converges quickly, with 96.5% of sequences requiring at
most three iterations, while improving both temporal efficiency and spatial
action organization. Crucially, the method preserves human error-recovery
patterns rather than collapsing them, supporting future work on robust
corrective behavior. By establishing plan verification as a reliable LLM
capability for spatial planning and action refinement, we provide a scalable
path to higher-quality training data for imitation learning in embodied AI.

</details>


### [7] [Key Principles in Cross-Domain Hyper-Heuristic Performance](https://arxiv.org/abs/2509.02782)
*Václav Sobotka,Lucas Kletzander,Nysret Musliu,Hana Rudová*

Main category: cs.AI

TL;DR: 本文提出一种改进的跨域选择超启发式算法，通过对低层次启发式算法集合进行战略性变换，提升了算法性能，在多个实际问题中取得了优于现有算法的结果，并发现了11个新的最佳已知解。


<details>
  <summary>Details</summary>
Motivation: 现有的选择超启发式算法主要关注低层次启发式算法的适应性选择，本文则关注该集合的构成及其战略性变换。

Method: 系统分析了基于解接受、LLH重复和扰动强度的三种关键原则的变换，并在简单的随机选择机制上进行了验证。

Result: 在三个具有挑战性的实际领域中，该方法优于所有现有的最先进的超启发式算法，并发现了11个新的最佳已知解；在CHeSC基准测试中也具有竞争力。

Conclusion: 通过战略性变换，可以改进现有的超启发式算法，并在实际问题和基准测试中取得更好的结果，同时简化算法设计。

Abstract: Cross-domain selection hyper-heuristics aim to distill decades of research on
problem-specific heuristic search algorithms into adaptable general-purpose
search strategies. In this respect, existing selection hyper-heuristics
primarily focus on an adaptive selection of low-level heuristics (LLHs) from a
predefined set. In contrast, we concentrate on the composition of this set and
its strategic transformations. We systematically analyze transformations based
on three key principles: solution acceptance, LLH repetitions, and perturbation
intensity, i.e., the proportion of a solution affected by a perturbative LLH.
We demonstrate the raw effects of our transformations on a trivial unbiased
random selection mechanism. With an appropriately constructed transformation,
this trivial method outperforms all available state-of-the-art hyper-heuristics
on three challenging real-world domains and finds 11 new best-known solutions.
The same method is competitive with the winner of the CHeSC competition,
commonly used as the standard cross-domain benchmark. Moreover, we accompany
several recent hyper-heuristics with such strategic transformations. Using this
approach, we outperform the current state-of-the-art methods on both the CHeSC
benchmark and real-world domains while often simplifying their designs.

</details>


### [8] [Learning General Policies From Examples](https://arxiv.org/abs/2509.02794)
*Blai Bonet,Hector Geffner*

Main category: cs.AI

TL;DR: 一种新的基于采样计划泛化的符号策略学习方法，能够处理百万级状态和数十万级特征的问题。


<details>
  <summary>Details</summary>
Motivation: 现有组合方法难以扩展到大型规划问题，本文提出一种新的可扩展的符号方法。

Method: 基于Hitting Set算法的采样计划泛化方法，保证策略的结构终止和非循环性。

Result: 在多个基准测试中验证了该方法的可扩展性，能够有效处理大规模问题。

Conclusion: 该方法克服了现有符号方法的局限性，为学习大型规划问题的通用策略提供了一种新的有效途径。

Abstract: Combinatorial methods for learning general policies that solve large
collections of planning problems have been recently developed. One of their
strengths, in relation to deep learning approaches, is that the resulting
policies can be understood and shown to be correct. A weakness is that the
methods do not scale up and learn only from small training instances and
feature pools that contain a few hundreds of states and features at most. In
this work, we propose a new symbolic method for learning policies based on the
generalization of sampled plans that ensures structural termination and hence
acyclicity. The proposed learning approach is not based on SAT/ASP, as previous
symbolic methods, but on a hitting set algorithm that can effectively handle
problems with millions of states, and pools with hundreds of thousands of
features. The formal properties of the approach are analyzed, and its
scalability is tested on a number of benchmarks.

</details>


### [9] [Uncertainty-driven Adaptive Exploration](https://arxiv.org/abs/2509.03219)
*Leonidas Bakopoulos,Georgios Chalkiadakis*

Main category: cs.AI

TL;DR: 本文提出了一种通用的自适应探索框架，该框架利用不确定性来决定探索和利用之间的切换时机，并在多个MuJoCo环境中取得了优于标准方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自适应探索方法难以确定探索和利用之间的最佳切换时机，尤其是在需要学习长而复杂的动作序列的领域。

Method: 提出一个通用的自适应探索框架，该框架利用不确定性来决定探索和利用之间的切换时机。该框架包含了之前的自适应探索方法作为特例，并且可以结合任何不确定性测量机制。

Result: 实验结果表明，该框架产生的自适应探索策略在多个MuJoCo环境中优于标准方法。

Conclusion: 该框架提供了一种原则性的方法来解决自适应探索中的关键问题，为学习复杂策略提供了新的途径。

Abstract: Adaptive exploration methods propose ways to learn complex policies via
alternating between exploration and exploitation. An important question for
such methods is to determine the appropriate moment to switch between
exploration and exploitation and vice versa. This is critical in domains that
require the learning of long and complex sequences of actions. In this work, we
present a generic adaptive exploration framework that employs uncertainty to
address this important issue in a principled manner. Our framework includes
previous adaptive exploration approaches as special cases. Moreover, we can
incorporate in our framework any uncertainty-measuring mechanism of choice, for
instance mechanisms used in intrinsic motivation or epistemic uncertainty-based
exploration methods. We experimentally demonstrate that our framework gives
rise to adaptive exploration strategies that outperform standard ones across
several MuJoCo environments.

</details>


### [10] [Accountability Framework for Healthcare AI Systems: Towards Joint Accountability in Decision Making](https://arxiv.org/abs/2509.03286)
*Prachi Bagave,Marcus Westberg,Marijn Janssen,Aaron Yi Ding*

Main category: cs.AI

TL;DR: 本文探究医疗AI系统问责制，提出一个三层级框架，促进利益相关者合作，弥合监管指南的“是什么”与“怎么做”之间的差距。


<details>
  <summary>Details</summary>
Motivation: 医疗AI日益普及，问责制成为关键问题，现有法规侧重“是什么”而非“怎么做”。

Method: 分析问责制概念，构建问责制框架及三层级结构。

Result: 提出一个医疗AI问责制框架和三层级结构，强调共享依赖和合作的重要性，并指出可解释性在促进沟通和信息共享中的作用。

Conclusion: 医疗AI问责制需共享责任，框架有助于利益相关者合作，可解释性促进沟通。

Abstract: AI is transforming the healthcare domain and is increasingly helping
practitioners to make health-related decisions. Therefore, accountability
becomes a crucial concern for critical AI-driven decisions. Although regulatory
bodies, such as the EU commission, provide guidelines, they are highlevel and
focus on the ''what'' that should be done and less on the ''how'', creating a
knowledge gap for actors. Through an extensive analysis, we found that the term
accountability is perceived and dealt with in many different ways, depending on
the actor's expertise and domain of work. With increasing concerns about AI
accountability issues and the ambiguity around this term, this paper bridges
the gap between the ''what'' and ''how'' of AI accountability, specifically for
AI systems in healthcare. We do this by analysing the concept of
accountability, formulating an accountability framework, and providing a
three-tier structure for handling various accountability mechanisms. Our
accountability framework positions the regulations of healthcare AI systems and
the mechanisms adopted by the actors under a consistent accountability regime.
Moreover, the three-tier structure guides the actors of the healthcare AI
system to categorise the mechanisms based on their conduct. Through our
framework, we advocate that decision-making in healthcare AI holds shared
dependencies, where accountability should be dealt with jointly and should
foster collaborations. We highlight the role of explainability in instigating
communication and information sharing between the actors to further facilitate
the collaborative process.

</details>


### [11] [app.build: A Production Framework for Scaling Agentic Prompt-to-App Generation with Environment Scaffolding](https://arxiv.org/abs/2509.03310)
*Evgenii Kniazev,Arseny Kravchenko,Igor Rekun,James Broadhead,Nikita Shamgunov,Pranav Sah,Pratik Nichite,Ivan Yamshchikov*

Main category: cs.AI

TL;DR: app.build框架通过系统验证和结构化环境改进基于LLM的应用生成


<details>
  <summary>Details</summary>
Motivation: 提升基于LLM的应用生成的可靠性

Method: 多层验证管道、特定于栈的编排和模型无关架构

Result: 30个生成任务中，73.3%可行，30%达到完美质量；开放权重模型在结构化环境下达到闭源模型80.8%的性能；社区已采用，生成超过3000个应用

Conclusion: 扩展可靠的AI代理需要扩展环境，而不仅仅是模型

Abstract: We present app.build (https://github.com/appdotbuild/agent/), an open-source
framework that improves LLM-based application generation through systematic
validation and structured environments. Our approach combines multi-layered
validation pipelines, stack-specific orchestration, and model-agnostic
architecture, implemented across three reference stacks. Through evaluation on
30 generation tasks, we demonstrate that comprehensive validation achieves
73.3% viability rate with 30% reaching perfect quality scores, while
open-weights models achieve 80.8% of closed-model performance when provided
structured environments. The open-source framework has been adopted by the
community, with over 3,000 applications generated to date. This work
demonstrates that scaling reliable AI agents requires scaling environments, not
just models -- providing empirical insights and complete reference
implementations for production-oriented agent systems.

</details>


### [12] [Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive and Abductive Reasoning](https://arxiv.org/abs/2509.03345)
*Yunxin Sun,Abulhair Saparov*

Main category: cs.AI

TL;DR: 大型语言模型(LLM)在演绎推理方面取得显著进展，但对归纳推理和溯因推理的研究较少。本文介绍了一个名为InAbHyD的可编程合成数据集，用于评估LLM在这两方面的能力，并提出了一种新的基于奥卡姆剃刀的度量方法。结果表明，LLM在简单场景下能够进行归纳和溯因推理，但在复杂场景下表现较差。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型(LLM)的归纳推理和溯因推理能力，这是解决现实世界问题的重要能力。

Method: 提出一个名为InAbHyD的可编程合成数据集，用于评估LLM的归纳和溯因推理能力，并提出一个基于奥卡姆剃刀的新的评价指标。对一些最先进的LLM进行了评估和分析。

Result: LLM在简单场景下能够进行归纳和溯因推理，但在复杂的世界模型和生成高质量假设方面表现挣扎，即使使用了流行的推理增强技术，如上下文学习和RLVR。

Conclusion: LLM在归纳和溯因推理方面仍有很大的改进空间，未来的研究应该集中在如何提高LLM在复杂场景下的推理能力。

Abstract: Reasoning is a core capability in artificial intelligence systems, for which
large language models (LLMs) have recently shown remarkable progress. However,
most work focuses exclusively on deductive reasoning, which is problematic
since other types of reasoning are also essential in solving real-world
problems, and they are less explored. This work focuses on evaluating LLMs'
inductive and abductive reasoning capabilities. We introduce a programmable and
synthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example
consists of an incomplete world model and a set of observations. The task for
the intelligent agent is to produce hypotheses to explain observations under
the incomplete world model to solve each reasoning example. We propose a new
metric to evaluate the quality of hypotheses based on Occam's Razor. We
evaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs
can perform inductive and abductive reasoning in simple scenarios, but struggle
with complex world models and producing high-quality hypotheses, even with
popular reasoning-enhancing techniques such as in-context learning and RLVR.

</details>


### [13] [Situating AI Agents in their World: Aspective Agentic AI for Dynamic Partially Observable Information Systems](https://arxiv.org/abs/2509.03380)
*Peter J. Bentley,Soo Ling Lim,Fuyuki Ishikawa*

Main category: cs.AI

TL;DR: 本文介绍了一种自下而上的AI智能体框架，通过引入'方面'的概念，减少信息泄露，提高安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有智能体模型常受控于不可靠的外部指令，信息泄露严重。

Method: 提出了一种基于'方面'的自下而上框架，赋予智能体环境感知能力，不同智能体感知环境不同。

Result: 实验表明，该框架信息泄露为零，显著优于传统架构。

Conclusion: 基于专业化智能体的信息分工，能提升安全性和效率。

Abstract: Agentic LLM AI agents are often little more than autonomous chatbots: actors
following scripts, often controlled by an unreliable director. This work
introduces a bottom-up framework that situates AI agents in their environment,
with all behaviors triggered by changes in their environments. It introduces
the notion of aspects, similar to the idea of umwelt, where sets of agents
perceive their environment differently to each other, enabling clearer control
of information. We provide an illustrative implementation and show that
compared to a typical architecture, which leaks up to 83% of the time,
aspective agentic AI enables zero information leakage. We anticipate that this
concept of specialist agents working efficiently in their own information
niches can provide improvements to both security and efficiency.

</details>


### [14] [ANNIE: Be Careful of Your Robots](https://arxiv.org/abs/2509.03383)
*Yiyang Huang,Zixuan Wang,Zishen Wan,Yapeng Tian,Haobo Xu,Yinhe Han,Yiming Gan*

Main category: cs.AI

TL;DR: 本文首次系统性研究了具身AI系统的对抗安全攻击，发现攻击成功率超过50%，凸显了安全防御的迫切需求。


<details>
  <summary>Details</summary>
Motivation: 具身AI系统中，受损的视觉-语言-动作模型可将对抗性扰动直接转化为不安全的物理行为，带来安全风险。

Method: 提出基于ISO标准的安全违规分类法，构建了ANNIEBench基准和ANNIE-Attack对抗框架，并进行了物理机器人实验。

Result: 攻击成功率超过所有安全类别50%，证实了具身AI系统中存在安全漏洞。

Conclusion: 具身AI系统存在安全隐患，需要开发安全防御机制。

Abstract: The integration of vision-language-action (VLA) models into embodied AI (EAI)
robots is rapidly advancing their ability to perform complex, long-horizon
tasks in humancentric environments. However, EAI systems introduce critical
security risks: a compromised VLA model can directly translate adversarial
perturbations on sensory input into unsafe physical actions. Traditional safety
definitions and methodologies from the machine learning community are no longer
sufficient. EAI systems raise new questions, such as what constitutes safety,
how to measure it, and how to design effective attack and defense mechanisms in
physically grounded, interactive settings. In this work, we present the first
systematic study of adversarial safety attacks on embodied AI systems, grounded
in ISO standards for human-robot interactions. We (1) formalize a principled
taxonomy of safety violations (critical, dangerous, risky) based on physical
constraints such as separation distance, velocity, and collision boundaries;
(2) introduce ANNIEBench, a benchmark of nine safety-critical scenarios with
2,400 video-action sequences for evaluating embodied safety; and (3)
ANNIE-Attack, a task-aware adversarial framework with an attack leader model
that decomposes long-horizon goals into frame-level perturbations. Our
evaluation across representative EAI models shows attack success rates
exceeding 50% across all safety categories. We further demonstrate sparse and
adaptive attack strategies and validate the real-world impact through physical
robot experiments. These results expose a previously underexplored but highly
consequential attack surface in embodied AI systems, highlighting the urgent
need for security-driven defenses in the physical AI era. Code is available at
https://github.com/RLCLab/Annie.

</details>


### [15] [sam-llm: interpretable lane change trajectoryprediction via parametric finetuning](https://arxiv.org/abs/2509.03462)
*Zhuo Cao,Yunxiao Shi,Min Xu*

Main category: cs.AI

TL;DR: SAM-LLM结合LLM的上下文推理能力和运动学车道变换模型的物理精度，实现了可解释的车道变换轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 弥合LLM的上下文推理能力和自动驾驶中运动学车道变换模型的物理精度之间的差距。

Method: 微调LLM以输出轨迹模型的核心物理参数而非原始坐标。对于车道保持场景，预测离散坐标；对于车道变换，生成增强型正弦加速度模型(SAM)的参数，包括横向位移、操纵持续时间、初始横向速度和纵向速度变化。

Result: 实现了98.73%的意图预测精度，比基于坐标的方法减少了80%的输出大小，具有可解释性和计算效率高。

Conclusion: SAM-LLM在保持与传统LLM预测器相当的性能的同时，在可解释性和资源效率方面具有显著优势。

Abstract: This work introduces SAM-LLM, a novel hybrid architecture that bridges the
gap between the contextual reasoning of Large Language Models (LLMs) and the
physical precision of kinematic lane change models for autonomous driving. The
system is designed for interpretable lane change trajectory prediction by
finetuning an LLM to output the core physical parameters of a trajectory model
instead of raw coordinates. For lane-keeping scenarios, the model predicts
discrete coordinates, but for lane change maneuvers, it generates the
parameters for an enhanced Sinusoidal Acceleration Model (SAM), including
lateral displacement, maneuver duration, initial lateral velocity, and
longitudinal velocity change. This parametric approach yields a complete,
continuous, and physically plausible trajectory model that is inherently
interpretable and computationally efficient, achieving an 80% reduction in
output size compared to coordinate-based methods. The SAM-LLM achieves a
state-of-the-art overall intention prediction accuracy of 98.73%, demonstrating
performance equivalent to traditional LLM predictors while offering significant
advantages in explainability and resource efficiency.

</details>
