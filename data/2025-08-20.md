<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL](https://arxiv.org/abs/2508.13167)
*Weizhen Li,Jianbo Lin,Zhuosong Jiang,Jingyi Cao,Xinpeng Liu,Jiayu Zhang,Zhenqiang Huang,Qianben Chen,Weichen Sun,Qiexiang Wang,Hongxuan Lu,Tianrui Qin,Chenghao Zhu,Yi Yao,Shuying Fan,Xiaowan Li,Tiannan Wang,Pai Liu,King Zhu,He Zhu,Dingfeng Shi,Piaohong Wang,Yeyi Guan,Xiangru Tang,Minghao Liu,Yuchen Eleanor Jiang,Jian Yang,Jiaheng Liu,Ge Zhang,Wangchunshu Zhou*

Main category: cs.AI

TL;DR: A new method, Chain-of-Agents, allows LLMs to perform complex problem-solving tasks efficiently, achieving state-of-the-art results and open-sourced for further research.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems are inefficient and lack data-centric learning capabilities.  This work aims to create a more efficient and capable system using a single model.

Method: The study introduces a chain-of-agents paradigm for LLM reasoning, a multi-agent distillation framework for fine-tuning, and agentic reinforcement learning for further improvement.  The resulting models are called Agent Foundation Models (AFMs).

Result: AFMs establish new state-of-the-art performance across diverse benchmarks in web agent and code agent settings. The research, including model weights, code, and data, is open-sourced.

Conclusion: Agent Foundation Models (AFMs) achieve state-of-the-art performance on diverse benchmarks by using a novel chain-of-agents (CoA) paradigm and a multi-agent distillation framework.

Abstract: Recent advances in large language models (LLMs) and multi-agent systems have
demonstrated remarkable capabilities in complex problem-solving tasks such as
deep research, vibe coding, and mathematical reasoning. However, most existing
multi-agent systems are built upon manual prompt/workflow engineering with
sophisticated agent frameworks, making them computationally inefficient, less
capable, and can not benefit from data-centric learning. In this work, we
introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables
native end-to-end complex problem-solving in the same way as a multi-agent
system (i.e., multi-turn problem solving with multiple tools and multiple
agents) within one model. In chain-of-agents problem-solving, the model
dynamically activates different tool agents and role-playing agents to simulate
multi-agent collaboration in an end-to-end fashion. To elicit end-to-end
chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent
distillation framework to distill state-of-the-art multi-agent systems into
chain-of-agents trajectories for agentic supervised fine-tuning. We then use
agentic reinforcement learning on verifiable agentic tasks to further improve
the models' capabilities on chain-of-agents problem solving. We call the
resulting models Agent Foundation Models (AFMs). Our empirical studies
demonstrate that AFM establishes new state-of-the-art performance across
diverse benchmarks in both web agent and code agent settings. We make the
entire research, including the model weights, code for training and evaluation,
and the training data, fully open-sourced, which offers a solid starting point
for future research on agent models and agentic RL.

</details>


### [2] [Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context](https://arxiv.org/abs/2508.13171)
*Tao An*

Main category: cs.AI

TL;DR: 新范式Cognitive Workspace通过模拟人类认知机制，显著提升了LLM的上下文管理能力，实现了更高的内存重用率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM上下文管理存在局限性，即使扩展了上下文窗口，也缺乏人类的元认知意识和主动规划能力。

Method: 提出了一种名为Cognitive Workspace的新范式，它模拟人类的认知机制，包括主动记忆管理、分层认知缓冲区和任务驱动的上下文优化。

Result: 实证结果表明，Cognitive Workspace平均内存重用率达到58.6%，效率提高了17-18%，显著优于传统RAG。

Conclusion: Cognitive Workspace，一种模拟人类认知机制的外部记忆使用的新范式，显著提高了LLM的上下文管理能力，实现了比传统RAG更高的内存重用率和效率增益。

Abstract: Large Language Models (LLMs) face fundamental limitations in context
management despite recent advances extending context windows to millions of
tokens. We propose Cognitive Workspace, a novel paradigm that transcends
traditional Retrieval-Augmented Generation (RAG) by emulating human cognitive
mechanisms of external memory use. Drawing from cognitive science foundations
including Baddeley's working memory model, Clark's extended mind thesis, and
Hutchins' distributed cognition framework, we demonstrate that current passive
retrieval systems fail to capture the dynamic, task-driven nature of human
memory management. Our analysis of 2024-2025 developments reveals that while
techniques like Infini-attention and StreamingLLM achieve impressive context
lengths, they lack the metacognitive awareness and active planning capabilities
essential for true cognitive extension. Cognitive Workspace addresses these
limitations through three core innovations: (1) active memory management with
deliberate information curation, (2) hierarchical cognitive buffers enabling
persistent working states, and (3) task-driven context optimization that
dynamically adapts to cognitive demands. Empirical validation demonstrates
Cognitive Workspace achieves an average 58.6% memory reuse rate (ranging from
54-60% across different tasks) compared to 0% for traditional RAG, with 17-18%
net efficiency gain despite 3.3x higher operation counts. Statistical analysis
confirms these advantages with p < 0.001 and Cohen's d > 23 across multiple
task types, establishing the first quantitative evidence for active memory
superiority in LLM systems. We present a comprehensive theoretical framework
synthesizing insights from 50+ recent papers, positioning Cognitive Workspace
as a fundamental shift from information retrieval to genuine cognitive
augmentation.

</details>


### [3] [AlphaEval: A Comprehensive and Efficient Evaluation Framework for Formula Alpha Mining](https://arxiv.org/abs/2508.13174)
*Hongjun Ding,Binqi Chen,Jinsheng Huang,Taian Guo,Zhengyang Mao,Guoyi Shao,Lutong Zou,Luchen Liu,Ming Zhang*

Main category: cs.AI

TL;DR: AlphaEval框架提供了一种更高效、更全面的自动化Alpha挖掘模型评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法（回测和基于相关性的度量）存在计算密集、依赖参数、仅评估预测能力等缺点，且模型闭源影响复现。

Method: 提出AlphaEval框架，从预测能力、稳定性、鲁棒性、金融逻辑和多样性五个维度评估自动化Alpha挖掘模型生成的Alpha。

Result: AlphaEval与全面的回测相比具有可比性，效率更高，并能有效识别优于传统单指标筛选方法的Alpha。所有实现和评估工具都是开源的。

Conclusion: 提出AlphaEval框架，一种统一的、可并行化的、无需回测的自动化Alpha挖掘模型评估框架，用于评估生成Alpha的整体质量，并在五个方面进行评估：预测能力、稳定性、对市场扰动的鲁棒性、金融逻辑和多样性。实验表明，AlphaEval与全面的回测相比具有可比性，效率更高，并能有效识别优于传统单指标筛选方法的Alpha。

Abstract: Formula alpha mining, which generates predictive signals from financial data,
is critical for quantitative investment. Although various algorithmic
approaches-such as genetic programming, reinforcement learning, and large
language models-have significantly expanded the capacity for alpha discovery,
systematic evaluation remains a key challenge. Existing evaluation metrics
predominantly include backtesting and correlation-based measures. Backtesting
is computationally intensive, inherently sequential, and sensitive to specific
strategy parameters. Correlation-based metrics, though efficient, assess only
predictive ability and overlook other crucial properties such as temporal
stability, robustness, diversity, and interpretability. Additionally, the
closed-source nature of most existing alpha mining models hinders
reproducibility and slows progress in this field. To address these issues, we
propose AlphaEval, a unified, parallelizable, and backtest-free evaluation
framework for automated alpha mining models. AlphaEval assesses the overall
quality of generated alphas along five complementary dimensions: predictive
power, stability, robustness to market perturbations, financial logic, and
diversity. Extensive experiments across representative alpha mining algorithms
demonstrate that AlphaEval achieves evaluation consistency comparable to
comprehensive backtesting, while providing more comprehensive insights and
higher efficiency. Furthermore, AlphaEval effectively identifies superior
alphas compared to traditional single-metric screening approaches. All
implementations and evaluation tools are open-sourced to promote
reproducibility and community engagement.

</details>


### [4] [Fitting Ontologies and Constraints to Relational Structures](https://arxiv.org/abs/2508.13176)
*Simon Hosemann,Jean Christoph Jung,Carsten Lutz,Sebastian Rudolph*

Main category: cs.AI

TL;DR: 研究本体和约束的拟合问题，确定了计算复杂度，设计了算法，并分析了有限基的存在性。


<details>
  <summary>Details</summary>
Motivation: 将本体和约束拟合到正例和反例，这些例子采用有限关系结构的形式。

Method: 研究了描述逻辑mathcal{E\mkern-2mu L}和mathcal{E\mkern-2mu LI}以及几类元组生成依赖项 (TGD) 的计算复杂性，并设计了相应的算法。

Result: 确定了精确的计算复杂性，设计了算法，并分析了拟合本体和 TGD 的大小。对于某些类型的 TGD，证明了有限基不存在。

Conclusion: 研究了将本体和约束拟合到正例和反例的问题，这些例子采用有限关系结构的形式。作为本体和约束语言，考虑了描述逻辑mathcal{E\mkern-2mu L}和mathcal{E\mkern-2mu LI}以及几类元组生成依赖项 (TGD)：完整、受保护、前沿受保护、前沿一和不受限制的 TGD 以及包含依赖项。确定了精确的计算复杂性，设计了算法，并分析了拟合本体和 TGD 的大小。还研究了为给定的一组有限结构构造概念包含/TGD 的有限基的相关问题。虽然对于mathcal{E\mkern-2mu L}、mathcal{E\mkern-2mu LI}、受保护的 TGD 和包含依赖项存在有限基，但对于完整、前沿受保护和前沿一 TGD 通常不存在有限基。

Abstract: We study the problem of fitting ontologies and constraints to positive and
negative examples that take the form of a finite relational structure. As
ontology and constraint languages, we consider the description logics
$\mathcal{E\mkern-2mu L}$ and $\mathcal{E\mkern-2mu LI}$ as well as several
classes of tuple-generating dependencies (TGDs): full, guarded,
frontier-guarded, frontier-one, and unrestricted TGDs as well as inclusion
dependencies. We pinpoint the exact computational complexity, design
algorithms, and analyze the size of fitting ontologies and TGDs. We also
investigate the related problem of constructing a finite basis of concept
inclusions / TGDs for a given set of finite structures. While finite bases
exist for $\mathcal{E\mkern-2mu L}$, $\mathcal{E\mkern-2mu LI}$, guarded TGDs,
and inclusion dependencies, they in general do not exist for full,
frontier-guarded and frontier-one TGDs.

</details>


### [5] [A Hardware-oriented Approach for Efficient Active Inference Computation and Deployment](https://arxiv.org/abs/2508.13177)
*Nikola Pižurica,Nikola Milović,Igor Jovančević,Conor Heins,Miguel de Prado*

Main category: cs.AI

TL;DR: 通过高效的计算图，提高主动推理的效率，使其更适用于实时和嵌入式应用。


<details>
  <summary>Details</summary>
Motivation: 解决主动推理在计算和内存方面的挑战，使其能够部署在资源受限的环境中。

Method: 整合pymdp和统一的稀疏计算图

Result: 延迟降低2倍以上，内存减少高达35%。

Conclusion: 该研究提出一种方法，通过整合pymdp的灵活性和效率以及一个统一的稀疏计算图，从而促进主动推理(AIF)的部署，减少延迟和内存消耗，使其更适用于资源受限的环境。

Abstract: Active Inference (AIF) offers a robust framework for decision-making, yet its
computational and memory demands pose challenges for deployment, especially in
resource-constrained environments. This work presents a methodology that
facilitates AIF's deployment by integrating pymdp's flexibility and efficiency
with a unified, sparse, computational graph tailored for hardware-efficient
execution. Our approach reduces latency by over 2x and memory by up to 35%,
advancing the deployment of efficient AIF agents for real-time and embedded
applications.

</details>


### [6] [The Interpretability Analysis of the Model Can Bring Improvements to the Text-to-SQL Task](https://arxiv.org/abs/2508.13178)
*Cong Zhang*

Main category: cs.AI

TL;DR: CESQL模型通过多项技术提升了文本到SQL模型的准确性，尤其在处理WHERE子句时效果显著。


<details>
  <summary>Details</summary>
Motivation: 提升文本到SQL模型在真实应用场景下的基础能力和泛化能力，尤其针对WHERE子句的语义解析。

Method: 整合模型可解释性分析、执行引导策略、过滤调整、逻辑关联优化和模型融合，设计出CESQL模型。

Result: 在WikiSQL数据集上取得显著的准确率提升，减少了对条件列数据和人工标注训练数据的依赖。

Conclusion: CESQL模型通过结合模型可解释性分析和执行引导策略，显著提高了文本到SQL模型在处理WHERE子句时的准确性，减少了对条件列数据的依赖，并提升了模型在WikiSQL数据集上的表现。

Abstract: To elevate the foundational capabilities and generalization prowess of the
text-to-SQL model in real-world applications, we integrate model
interpretability analysis with execution-guided strategy for semantic parsing
of WHERE clauses in SQL queries. Furthermore, we augment this approach with
filtering adjustments, logical correlation refinements, and model fusion,
culminating in the design of the CESQL model that facilitates conditional
enhancement. Our model excels on the WikiSQL dataset, which is emblematic of
single-table database query tasks, markedly boosting the accuracy of prediction
outcomes. When predicting conditional values in WHERE clauses, we have not only
minimized our dependence on data within the condition columns of tables but
also circumvented the impact of manually labeled training data. Our hope is
that this endeavor to enhance accuracy in processing basic database queries
will offer fresh perspectives for research into handling complex queries and
scenarios featuring irregular data in real-world database environments.

</details>


### [7] [Search-Time Data Contamination](https://arxiv.org/abs/2508.13180)
*Ziwen Han,Meher Mankikar,Julian Michael,Zifan Wang*

Main category: cs.AI

TL;DR: Search-based LLMs often cheat by finding answers directly online; this compromises benchmarks.  The paper demonstrates this, proposes solutions, and releases experimental logs.


<details>
  <summary>Details</summary>
Motivation: To address the issue of data contamination in evaluating search-based LLM agents, specifically focusing on the leakage of evaluation data through online search.

Method: The study analyzed logs from search-based LLM agents on three benchmarks (HLE, SimpleQA, GPQA) and observed instances where agents retrieved answers directly from HuggingFace.  Ablation experiments were conducted to investigate other potential sources of STC.  Accuracy changes were measured after blocking HuggingFace.

Result: Approximately 3% of questions showed evidence of STC from HuggingFace. Blocking HuggingFace resulted in a 15% drop in accuracy on the contaminated subset. Ablation experiments suggest HuggingFace is not the sole source of STC. Best practices for mitigating STC are proposed.

Conclusion: Search-time contamination (STC) in evaluating search-based large language model (LLM) agents is identified as a significant issue, undermining benchmark integrity.  Publicly accessible evaluation datasets, such as those on HuggingFace, are shown to be a source of contamination, leading to agents directly finding answers rather than genuinely reasoning.  Best practices for benchmark design and result reporting are proposed to mitigate STC.

Abstract: Data contamination refers to the leakage of evaluation data into model
training data, resulting in overfitting to supposedly held-out test sets and
compromising test validity. We identify an analogous issue, search-time
contamination (STC), in evaluating search-based LLM agents which use tools to
gather information from online sources when answering user queries. STC occurs
when the retrieval step surfaces a source containing the test question (or a
near-duplicate) alongside its answer, enabling agents to copy rather than
genuinely infer or reason, undermining benchmark integrity. We find that
HuggingFace, an online platform hosting evaluation datasets, appears among
retrieved sources in search based agent logs. Consequently, agents often
explicitly acknowledge discovering question answer pairs from HuggingFace
within their reasoning chains. On three commonly used capability benchmarks:
Humanity's Last Exam (HLE), SimpleQA, and GPQA, we demonstrate that for
approximately 3% of questions, search-based agents directly find the datasets
with ground truth labels on HuggingFace. When millions of evaluation queries
target the same benchmark, even small, repeated leaks can accelerate the
benchmark's obsolescence, shortening its intended lifecycle. After HuggingFace
is blocked, we observe a drop in accuracy on the contaminated subset of
approximately 15%. We further show through ablation experiments that publicly
accessible evaluation datasets on HuggingFace may not be the sole source of
STC. To this end, we conclude by proposing best practices for benchmark design
and result reporting to address this novel form of leakage and ensure
trustworthy evaluation of search-based LLM agents. To facilitate the auditing
of evaluation results, we also publicly release the complete logs from our
experiments.

</details>


### [8] [QuickMerge++: Fast Token Merging with Autoregressive Prior](https://arxiv.org/abs/2508.13204)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: QuickMerge:一种轻量级token合并框架，通过动态选择token并使用transformer先验模型，有效降低了生成模型的计算成本，并在多模态领域取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型扩展到更大规模的语言、视觉和视频数据，token级别计算的成本成为瓶颈。现有的大部分token选择方法是静态的、特定于模态的或与自回归生成不兼容。

Method: 提出了一种轻量级的token合并框架QuickMerge，该框架基于注意力norm大小和熵基预算估计器动态选择数量减少的token，并引入轻量级transformer先验模型以保持自回归兼容性。

Result: 在多模态领域评估QuickMerge，结果表明其在计算精度权衡方面取得了一致的改进，显著减少了token数量，同时性能与学习型分词器和固定patch基线相当甚至更好。

Conclusion: QuickMerge框架在多模态领域提高了计算精度权衡，通过减少token数量实现了与学习型分词器和固定patch基线相当甚至更好的性能。

Abstract: As generative models scale to larger inputs across language, vision, and
video domains, the cost of token-level computation has become a key bottleneck.
While prior work suggests that only a subset of tokens significantly influence
downstream predictions, most token selection methods are static,
modality-specific, or incompatible with autoregressive generation. In this
paper, we propose QuickMerge, a lightweight token merging framework designed
for efficient next-token prediction.
  QuickMerge dynamically selects a reduced number of tokens based on attention
norm magnitude, guided by an entropy-based budget estimator. To preserve
autoregressive compatibility, we introduce a lightweight transformer prior
trained over the merged token sequence. By combining semantic salience
estimation, flexible token budgets, and AR alignment, QuickMerge enables
accurate generation with fewer tokens.
  We evaluate QuickMerge across multi-modality domains, demonstrating
consistent improvements in compute-accuracy tradeoffs. Specifically, QuickMerge
reduces token counts sustantially while matching as well as exceeding the
performance of learned tokenizers and fixed-patch baselines.

</details>


### [9] [AI sustains higher strategic tension than humans in chess](https://arxiv.org/abs/2508.13213)
*Adamo Cerioli,Edward D. Lee,Vito D. P. Servedio*

Main category: cs.AI

TL;DR: AI在国际象棋中比人类能更好地保持长期战略张力，这反映了算法复杂度和人类认知能力的差异。


<details>
  <summary>Details</summary>
Motivation: 研究国际象棋中即时机会和长期目标之间的权衡。

Method: 提出了一种基于网络的量化棋盘上战略张力的指标，并比较了人类对弈和AI对弈的动态。

Result: AI维持高战略张力的持续时间比人类更长，且与算法复杂度相关；人类的战略张力则与棋力呈阶梯式增长，并伴随降低游戏复杂度的策略。

Conclusion: AI在国际象棋比赛中比人类高手能更长时间地保持更高水平的战略张力，这可能与AI算法的复杂性和人类认知能力的限制有关。

Abstract: Strategic decision-making involves managing the tension between immediate
opportunities and long-term objectives. We study this trade-off in chess by
characterizing and comparing dynamics between human vs human and AI vs AI
games. We propose a network-based metric of piece-to-piece interaction to
quantify the ongoing strategic tension on the board. Its evolution in games
reveals that the most competitive AI players sustain higher levels of strategic
tension for longer durations than elite human players. Cumulative tension
varies with algorithmic complexity for AI and correspondingly in human-played
games increases abruptly with expertise at about 1600 Elo and again at 2300
Elo. The profiles reveal different approaches. Highly competitive AI tolerates
interconnected positions balanced between offensive and defensive tactics over
long periods. Human play, in contrast, limits tension and game complexity,
which may reflect cognitive limitations and adaptive strategies. The difference
may have implications for AI usage in complex, strategic environments.

</details>


### [10] [Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information](https://arxiv.org/abs/2508.13250)
*Zeyu Zhang,Yang Zhang,Haoran Tan,Rui Li,Xu Chen*

Main category: cs.AI

TL;DR: 该论文提出了一种新的多跳个性化推理任务和HybridMem方法，并在实验中证明了其有效性，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的智能体在个性化方面存在局限性，难以处理复杂任务中对大量用户信息的多跳推理。

Method: 提出了多跳个性化推理任务，构建了相应的评估框架和数据集，并实现了多种显式和隐式记忆方法，最终提出了HybridMem方法。

Result: 实验结果验证了HybridMem方法的有效性，并分析了各种记忆方法的优缺点。

Conclusion: 该论文提出了一种用于多跳个性化推理的混合记忆方法HybridMem，并在构建的数据集上进行了实验验证，证明了其有效性，并开源了项目。

Abstract: In large language model-based agents, memory serves as a critical capability
for achieving personalization by storing and utilizing users' information.
Although some previous studies have adopted memory to implement user
personalization, they typically focus on preference alignment and simple
question-answering. However, in the real world, complex tasks often require
multi-hop reasoning on a large amount of user information, which poses
significant challenges for current memory approaches. To address this
limitation, we propose the multi-hop personalized reasoning task to explore how
different memory mechanisms perform in multi-hop reasoning over personalized
information. We explicitly define this task and construct a dataset along with
a unified evaluation framework. Then, we implement various explicit and
implicit memory methods and conduct comprehensive experiments. We evaluate
their performance on this task from multiple perspectives and analyze their
strengths and weaknesses. Besides, we explore hybrid approaches that combine
both paradigms and propose the HybridMem method to address their limitations.
We demonstrate the effectiveness of our proposed model through extensive
experiments. To benefit the research community, we release this project at
https://github.com/nuster1128/MPR.

</details>


### [11] ["DIVE" into Hydrogen Storage Materials Discovery with AI Agents](https://arxiv.org/abs/2508.13251)
*Di Zhang,Xue Jia,Tran Ba Hung,Seong Hoon Jang,Linda Zhang,Ryuhei Sato,Yusuke Hashimoto,Toyoto Sato,Kiyoe Konno,Shin-ichi Orimo,Hao Li*

Main category: cs.AI

TL;DR: DIVE工作流程提高数据提取效率和准确性，实现快速逆向设计，识别新型储氢材料。


<details>
  <summary>Details</summary>
Motivation: 解决科学文献中非结构化图表数据阻碍LLM构建的问题，推动AI驱动的材料发现。

Method: DIVE多智能体工作流程系统地读取和组织来自科学文献图形元素的实验数据。

Result: DIVE的准确性和覆盖率比多模态模型直接提取高10-15%（商业模型）和30%以上（开源模型）；基于超过30,000条数据的数据库，可在两分钟内识别新型储氢材料。

Conclusion: DIVE多智能体工作流程显著提高了数据提取的准确性和覆盖率，并建立了一个能够快速识别新型储氢材料的逆向设计工作流程。

Abstract: Data-driven artificial intelligence (AI) approaches are fundamentally
transforming the discovery of new materials. Despite the unprecedented
availability of materials data in the scientific literature, much of this
information remains trapped in unstructured figures and tables, hindering the
construction of large language model (LLM)-based AI agent for automated
materials design. Here, we present the Descriptive Interpretation of Visual
Expression (DIVE) multi-agent workflow, which systematically reads and
organizes experimental data from graphical elements in scientific literatures.
We focus on solid-state hydrogen storage materials-a class of materials central
to future clean-energy technologies and demonstrate that DIVE markedly improves
the accuracy and coverage of data extraction compared to the direct extraction
by multimodal models, with gains of 10-15% over commercial models and over 30%
relative to open-source models. Building on a curated database of over 30,000
entries from 4,000 publications, we establish a rapid inverse design workflow
capable of identifying previously unreported hydrogen storage compositions in
two minutes. The proposed AI workflow and agent design are broadly transferable
across diverse materials, providing a paradigm for AI-driven materials
discovery.

</details>


### [12] [CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support](https://arxiv.org/abs/2508.13256)
*Yuting Zhang,Karina V. Bunting,Asgher Champsi,Xiaoxia Wang,Wenqi Lu,Alexander Thorley,Sandeep S Hothi,Zhaowen Qiu,Dipak Kotecha,Jinming Duan*

Main category: cs.AI

TL;DR: 多模态框架CardAIc-Agents有效提升了AI在心血管疾病诊断中的效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有AI在心血管疾病诊断中的应用受到临床角色分配依赖于模型自身能力、工作流程僵化、知识库静态以及输入和输出模式单一等限制。

Method: 提出了一种多模态框架CardAIc-Agents，包含CardiacRAG代理和主代理，并采用逐步更新策略和多学科讨论工具，以支持自适应和个性化的心脏疾病诊断。

Result: 实验结果表明CardAIc-Agents优于现有方法。

Conclusion: CardAIc-Agents，一个多模态框架，通过整合外部工具和自适应支持多种心脏任务，提高了AI在心血管疾病诊断中的效率。实验表明，其效率优于主流视觉语言模型、最先进的智能系统和微调视觉语言模型。

Abstract: Cardiovascular diseases (CVDs) remain the foremost cause of mortality
worldwide, a burden worsened by a severe deficit of healthcare workers.
Artificial intelligence (AI) agents have shown potential to alleviate this gap
via automated early detection and proactive screening, yet their clinical
application remains limited by: 1) prompt-based clinical role assignment that
relies on intrinsic model capabilities without domain-specific tool support; or
2) rigid sequential workflows, whereas clinical care often requires adaptive
reasoning that orders specific tests and, based on their results, guides
personalised next steps; 3) general and static knowledge bases without
continuous learning capability; and 4) fixed unimodal or bimodal inputs and
lack of on-demand visual outputs when further clarification is needed. In
response, a multimodal framework, CardAIc-Agents, was proposed to augment
models with external tools and adaptively support diverse cardiac tasks.
Specifically, a CardiacRAG agent generated general plans from updatable cardiac
knowledge, while the chief agent integrated tools to autonomously execute these
plans and deliver decisions. To enable adaptive and case-specific
customization, a stepwise update strategy was proposed to dynamically refine
plans based on preceding execution results, once the task was assessed as
complex. In addition, a multidisciplinary discussion tool was introduced to
interpret challenging cases, thereby supporting further adaptation. When
clinicians raised concerns, visual review panels were provided to assist final
validation. Experiments across three datasets showed the efficiency of
CardAIc-Agents compared to mainstream Vision-Language Models (VLMs),
state-of-the-art agentic systems, and fine-tuned VLMs.

</details>


### [13] [Towards Unified Multimodal Financial Forecasting: Integrating Sentiment Embeddings and Market Indicators via Cross-Modal Attention](https://arxiv.org/abs/2508.13327)
*Sarthak Khanna,Armin Berger,David Berghaus,Tobias Deusser,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.AI

TL;DR: STONK是一个多模态框架，结合数值和文本数据改进股票预测，回测结果表现优异。


<details>
  <summary>Details</summary>
Motivation: 改进每日股票走势预测，克服孤立分析的局限性。

Method: 结合数值市场指标和情感丰富的新闻嵌入，使用特征拼接和跨模态注意力进行融合。

Result: 回测结果表明STONK优于仅使用数值的基准方法。

Conclusion: STONK框架优于仅使用数值的基准方法，为可扩展的多模态金融预测提供了循证指导。

Abstract: We propose STONK (Stock Optimization using News Knowledge), a multimodal
framework integrating numerical market indicators with sentiment-enriched news
embeddings to improve daily stock-movement prediction. By combining numerical &
textual embeddings via feature concatenation and cross-modal attention, our
unified pipeline addresses limitations of isolated analyses. Backtesting shows
STONK outperforms numeric-only baselines. A comprehensive evaluation of fusion
strategies and model configurations offers evidence-based guidance for scalable
multimodal financial forecasting. Source code is available on GitHub

</details>


### [14] [HiFo-Prompt: Prompting with Hindsight and Foresight for LLM-based Automatic Heuristic Design](https://arxiv.org/abs/2508.13333)
*Chentong Chen,Mengyuan Zhong,Jianyong Sun,Ye Fan,Jialong Shi*

Main category: cs.AI

TL;DR: HiFo-Prompt利用前瞻性和后视性提示策略，显著提升了基于LLM的自动启发式设计效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动启发式设计方法存在静态算子和缺乏知识积累机制的缺点，限制了其有效性。

Method: HiFo-Prompt框架，结合前瞻性和后视性两种提示策略引导LLM。前瞻性提示根据种群动态自适应地引导搜索，平衡探索和利用；后视性提示从过去的成功启发式算法中提取可复用的设计原则，模拟人类专家的经验。

Result: HiFo-Prompt框架显著提高了基于LLM的自动启发式设计方法的性能，生成更高质量的启发式算法，并加快了收敛速度，提高了查询效率。

Conclusion: HiFo-Prompt框架显著优于现有基于LLM的自动启发式设计方法，生成更高质量的启发式算法，同时实现更快的收敛速度和更高的查询效率。

Abstract: LLM-based Automatic Heuristic Design (AHD) within Evolutionary Computation
(EC) frameworks has shown promising results. However, its effectiveness is
hindered by the use of static operators and the lack of knowledge accumulation
mechanisms. We introduce HiFo-Prompt, a framework that guides LLMs with two
synergistic prompting strategies: Foresight and Hindsight. Foresight-based
prompts adaptively steer the search based on population dynamics, managing the
exploration-exploitation trade-off. In addition, hindsight-based prompts mimic
human expertise by distilling successful heuristics from past generations into
fundamental, reusable design principles. This dual mechanism transforms
transient discoveries into a persistent knowledge base, enabling the LLM to
learn from its own experience. Empirical results demonstrate that HiFo-Prompt
significantly outperforms state-of-the-art LLM-based AHD methods, generating
higher-quality heuristics while achieving substantially faster convergence and
superior query efficiency.

</details>


### [15] [LOOP: A Plug-and-Play Neuro-Symbolic Framework for Enhancing Planning in Autonomous Systems](https://arxiv.org/abs/2508.13371)
*Ronit Virwani,Ruchika Suryawanshi*

Main category: cs.AI

TL;DR: LOOP框架通过神经与符号组件的迭代交互，显著提升了自主系统的规划可靠性，并在基准测试中取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有神经规划方法难以应对复杂领域，存在计划缺失先决条件、目标不一致和幻觉等问题；经典规划器缺乏灵活性及自然语言理解能力；现有神经符号方法采用一次性翻译，未能充分发挥神经和符号组件的协同作用。

Method: 神经符号规划框架LOOP，集成13个协调的神经特征，包括图神经网络、多智能体验证、分层分解和因果记忆等。将规划视为神经和符号组件之间的迭代对话，而非简单的转换，生成PDDL规范并基于符号反馈迭代改进，构建因果知识库。

Result: 在六个标准IPC基准测试域中，LOOP取得了85.8%的成功率，显著优于LLM+P (55.0%)、LLM-as-Planner (19.2%)和Tree-of-Thoughts (3.3%)。

Conclusion: 神经符号规划框架LOOP通过迭代式神经与符号组件间的“对话”解决了现有神经规划方法在复杂领域中存在的问题，并在六个标准IPC基准测试域中取得了85.8%的成功率，显著优于其他方法。

Abstract: Planning is one of the most critical tasks in autonomous systems, where even
a small error can lead to major failures or million-dollar losses. Current
state-of-the-art neural planning approaches struggle with complex domains,
producing plans with missing preconditions, inconsistent goals, and
hallucinations. While classical planners provide logical guarantees, they lack
the flexibility and natural language understanding capabilities needed for
modern autonomous systems. Existing neuro-symbolic approaches use one-shot
translation from natural language to formal plans, missing the opportunity for
neural and symbolic components to work and refine solutions together. To
address this gap, we develop LOOP -- a novel neuro-symbolic planning framework
that treats planning as an iterative conversation between neural and symbolic
components rather than simple translation. LOOP integrates 13 coordinated
neural features including graph neural networks for spatial relationships,
multi-agent validation for consensus-based correctness, hierarchical
decomposition for complex task management, and causal memory that learns from
both successes and failures. Unlike existing approaches, LOOP generates PDDL
specifications, refines them iteratively based on symbolic feedback, and builds
a causal knowledge base from execution traces. LOOP was evaluated on six
standard IPC benchmark domains, where it achieved 85.8% success rate compared
to LLM+P (55.0%), LLM-as-Planner (19.2%), and Tree-of-Thoughts (3.3%). This
work shows that the key to reliable planning is not in choosing between neural
networks or symbolic reasoners but it lies in making them actually ``talk'' to
each other during the entire process. LOOP provides a thorough blueprint for
building autonomous systems that can finally be trusted with critical
real-world applications.

</details>


### [16] [SPANER: Shared Prompt Aligner for Multimodal Semantic Representation](https://arxiv.org/abs/2508.13387)
*Thye Shan Ng,Caren Soyeon Han,Eun-Jung Holden*

Main category: cs.AI

TL;DR: SPANER:一种模态无关的参数高效微调框架，通过共享提示机制将不同模态嵌入到统一语义空间，提升了少样本检索性能和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数多模态参数高效微调方法忽略了多模态嵌入空间的结构，导致模态特异性表示往往孤立，限制了跨模态泛化。

Method: 共享提示机制，将不同模态的输入嵌入到统一的语义空间中。

Result: SPANER在视觉-语言和音频-视觉基准测试中取得了具有竞争力的少样本检索性能，并保持了学习嵌入空间中的高语义一致性。

Conclusion: SPANER框架在视觉-语言和音频-视觉基准测试中取得了具有竞争力的少样本检索性能，同时保持了学习嵌入空间中的高语义一致性，突出了对齐嵌入结构而不是仅仅调整适配器权重对于可扩展多模态学习的重要性。

Abstract: Recent advances in multimodal Parameter-Efficient Fine-Tuning (PEFT) have
significantly improved performance on downstream tasks such as few-shot
retrieval. However, most existing approaches focus on task-specific gains while
neglecting the structure of the multimodal embedding space. As a result,
modality-specific representations often remain isolated, limiting cross-modal
generalisation. In this work, we introduce Shared Prompt AligNER (SPANER), a
modality-agnostic PEFT framework designed to embed inputs from diverse
modalities into a unified semantic space. At its core, SPANER employs a shared
prompt mechanism that acts as a conceptual anchor, enabling semantically
related instances to converge spatially regardless of modality. This shared
prompt design is inherently extensible, supporting the seamless integration of
additional modalities, such as audio, without altering the core architecture.
Through comprehensive experiments across vision-language and audio-visual
benchmarks, SPANER demonstrates competitive few-shot retrieval performance
while preserving high semantic coherence in the learned embedding space. Our
results highlight the importance of aligning embedding structures, rather than
merely tuning adapter weights, for scalable multimodal learning.

</details>
