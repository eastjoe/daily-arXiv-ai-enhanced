{"id": "2507.10562", "categories": ["cs.AI", "cs.CR", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10562", "abs": "https://arxiv.org/abs/2507.10562", "authors": ["Hari Masoor"], "title": "SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents", "comment": "7 pages, 4 figures, 3 implementation examples. Original work\n  submitted as a preprint", "summary": "Current AI agent architectures suffer from ephemeral memory limitations,\npreventing effective collaboration and knowledge sharing across sessions and\nagent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a\nnovel framework that enables persistent, secure, and semantically searchable\nmemory sharing among AI agents. Our protocol addresses three critical\nchallenges: (1) persistent context preservation across agent sessions, (2)\nsecure multi-agent collaboration with fine-grained access control, and (3)\nefficient semantic discovery of relevant historical context. SAMEP implements a\ndistributed memory repository with vector-based semantic search, cryptographic\naccess controls (AES-256-GCM), and standardized APIs compatible with existing\nagent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness\nacross diverse domains including multi-agent software development, healthcare\nAI with HIPAA compliance, and multi-modal processing pipelines. Experimental\nresults show 73% reduction in redundant computations, 89% improvement in\ncontext relevance scores, and complete compliance with regulatory requirements\nincluding audit trail generation. SAMEP enables a new paradigm of persistent,\ncollaborative AI agent ecosystems while maintaining security and privacy\nguarantees.", "AI": {"tldr": "SAMEP \u6846\u67b6\u5b9e\u73b0\u4e86AI \u667a\u80fd\u4f53\u4e4b\u95f4\u6301\u4e45\u3001\u5b89\u5168\u3001\u8bed\u4e49\u53ef\u641c\u7d22\u7684\u5185\u5b58\u5171\u4eab\uff0c\u6709\u6548\u63d0\u5347\u4e86\u534f\u4f5c\u6548\u7387\u548c\u6570\u636e\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524dAI\u667a\u80fd\u4f53\u67b6\u6784\u7684\u77ed\u6682\u8bb0\u5fc6\u9650\u5236\u963b\u788d\u4e86\u6709\u6548\u7684\u8de8\u4f1a\u8bdd\u548c\u667a\u80fd\u4f53\u8fb9\u754c\u7684\u534f\u4f5c\u548c\u77e5\u8bc6\u5171\u4eab\u3002", "method": "SAMEP\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u5206\u5e03\u5f0f\u5185\u5b58\u5e93\u3001\u57fa\u4e8e\u5411\u91cf\u7684\u8bed\u4e49\u641c\u7d22\u3001\u52a0\u5bc6\u8bbf\u95ee\u63a7\u5236\uff08AES-256-GCM\uff09\u4ee5\u53ca\u4e0e\u73b0\u6709\u667a\u80fd\u4f53\u901a\u4fe1\u534f\u8bae\uff08MCP, A2A\uff09\u517c\u5bb9\u7684\u6807\u51c6\u5316API\u6765\u5b9e\u73b0\u6301\u4e45\u3001\u5b89\u5168\u548c\u8bed\u4e49\u53ef\u641c\u7d22\u7684\u5185\u5b58\u5171\u4eab\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSAMEP\u6846\u67b6\u51cf\u5c11\u4e8673%\u7684\u5197\u4f59\u8ba1\u7b97\uff0c\u5c06\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u8bc4\u5206\u63d0\u9ad8\u4e8689%\uff0c\u5e76\u5b8c\u5168\u7b26\u5408\u5305\u62ec\u5ba1\u8ba1\u8ddf\u8e2a\u751f\u6210\u5728\u5185\u7684\u6cd5\u89c4\u8981\u6c42\u3002", "conclusion": "SAMEP\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86AI\u667a\u80fd\u4f53\u5728\u8de8\u4f1a\u8bdd\u548c\u667a\u80fd\u4f53\u8fb9\u754c\u534f\u4f5c\u548c\u77e5\u8bc6\u5171\u4eab\u65b9\u9762\u7684\u77ed\u6682\u8bb0\u5fc6\u9650\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6301\u4e45\u3001\u5b89\u5168\u548c\u8bed\u4e49\u53ef\u641c\u7d22\u7684\u5185\u5b58\u5171\u4eab\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4f8b\u5982\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u63d0\u9ad8\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u8bc4\u5206\uff0c\u5e76\u5b8c\u5168\u7b26\u5408\u6cd5\u89c4\u8981\u6c42\u3002"}}
{"id": "2507.10566", "categories": ["cs.AI", "cs.GT", "cs.LG", "cs.MA", "cs.NE", "68T07, 68T40, 91A20", "I.2.6; I.2.11; I.2.4"], "pdf": "https://arxiv.org/pdf/2507.10566", "abs": "https://arxiv.org/abs/2507.10566", "authors": ["Hung Ming Liu"], "title": "AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems", "comment": "30 pages, 4 figures", "summary": "In Decentralized Multi-Agent Reinforcement Learning (MARL), the development\nof Emergent Communication has long been constrained by the ``Joint Exploration\nDilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' .\nTraditional methods address this by introducing inductive biases to facilitate\ncommunication emergence . This study fundamentally questions whether such\nartificial inductive biases are, in fact, over-engineering. Through experiments\nwith the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized\nVariational Autoencoder (VQ-VAE), we demonstrate that when agents possess an\nendogenous symbol system, their neural representations naturally exhibit\nspontaneous semantic compression and Nash equilibrium-driven semantic\nconvergence, achieving effective symbolic communication without external\ninductive biases. This aligns with recent neuroscience findings suggesting that\nthe human brain does not directly use human language for internal thought , and\nresonates with research on ``soft thinking'' capabilities in Large Language\nModels (LLMs) . Compared to traditional explicit communication methods, AIM\ndemonstrates stronger generality and efficiency. The interpretable analysis\ntoolkit developed in this study confirms that symbol usage exhibits a\nsignificant power-law distribution, leading to three major theoretical\ninsights: the ``Neural Communication Hypothesis'', the ``Tool-First\nPrinciple'', and the ``Semantic Interpretability Paradigm''. Future research\nwill explore the integration of Hierarchical Quantized Variational Autoencoders\n(HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the\npotential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This\ndiscovery offers new avenues for bridging symbolism and connectionism.", "AI": {"tldr": "\u65e0\u9700\u4eba\u5de5\u5f52\u7eb3\u504f\u7f6e\uff0c\u667a\u80fd\u4f53\u5373\u53ef\u901a\u8fc7\u5185\u751f\u7b26\u53f7\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u6548\u7684\u6d8c\u73b0\u5f0f\u6c9f\u901a\u3002", "motivation": "\u89e3\u51b3\u5206\u6563\u5f0f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7531\u4e8e\u8054\u5408\u63a2\u7d22\u56f0\u5883\u5bfc\u81f4\u7684\u6c9f\u901a\u771f\u7a7a\u5e73\u8861\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u77e2\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668(VQ-VAE)\u7684AI\u6bcd\u8bed(AIM)\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAIM\u6846\u67b6\u4e0b\u7684\u667a\u80fd\u4f53\u80fd\u591f\u81ea\u53d1\u5730\u8fdb\u884c\u8bed\u4e49\u538b\u7f29\u548c\u7eb3\u4ec0\u5747\u8861\u9a71\u52a8\u7684\u8bed\u4e49\u6536\u655b\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u7b26\u53f7\u4ea4\u6d41\uff0c\u5e76\u5c55\u73b0\u51fa\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5f3a\u7684\u6cdb\u5316\u6027\u548c\u6548\u7387\u3002\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e09\u4e2a\u4e3b\u8981\u7406\u8bba\u89c1\u89e3\uff1a\u795e\u7ecf\u5143\u4ea4\u6d41\u5047\u8bf4\u3001\u5de5\u5177\u4f18\u5148\u539f\u5219\u548c\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\u8303\u5f0f\u3002", "conclusion": "\u8be5\u7814\u7a76\u6311\u6218\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u4eba\u5de5\u5f15\u5165\u5f52\u7eb3\u504f\u7f6e\u4ee5\u4fc3\u8fdb\u591a\u667a\u80fd\u4f53\u6d8c\u73b0\u5f0f\u6c9f\u901a\u7684\u5fc5\u8981\u6027\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u667a\u80fd\u4f53\u62e5\u6709\u5185\u751f\u7b26\u53f7\u7cfb\u7edf\u65f6\uff0c\u65e0\u9700\u5916\u90e8\u5f52\u7eb3\u504f\u7f6e\u5373\u53ef\u5b9e\u73b0\u6709\u6548\u7684\u7b26\u53f7\u4ea4\u6d41\u3002"}}
{"id": "2507.10571", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10571", "abs": "https://arxiv.org/abs/2507.10571", "authors": ["Konstantinos I. Roumeliotis", "Ranjan Sapkota", "Manoj Karkee", "Nikolaos D. Tselikas"], "title": "Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning", "comment": null, "summary": "Modern Artificial Intelligence (AI) increasingly relies on multi-agent\narchitectures that blend visual and language understanding. Yet, a pressing\nchallenge remains: How can we trust these agents especially in zero-shot\nsettings with no fine-tuning? We introduce a novel modular Agentic AI visual\nclassification framework that integrates generalist multimodal agents with a\nnon-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)\nmodule. Applied to apple leaf disease diagnosis, we benchmark three\nconfigurations: (I) zero-shot with confidence-based orchestration, (II)\nfine-tuned agents with improved performance, and (III) trust-calibrated\norchestration enhanced by CLIP-based image retrieval and re-evaluation loops.\nUsing confidence calibration metrics (ECE, OCR, CCC), the orchestrator\nmodulates trust across agents. Our results demonstrate a 77.94\\% accuracy\nimprovement in the zero-shot setting using trust-aware orchestration and RAG,\nachieving 85.63\\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL\ndisplayed overconfidence. Furthermore, image-RAG grounded predictions with\nvisually similar cases, enabling correction of agent overconfidence via\niterative re-evaluation. The proposed system separates perception (vision\nagents) from meta-reasoning (orchestrator), enabling scalable and interpretable\nmulti-agent AI. This blueprint is extensible to diagnostics, biology, and other\ntrust-critical domains. All models, prompts, results, and system components\nincluding the complete software source code are openly released to support\nreproducibility, transparency, and community benchmarking at Github:\nhttps://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u591a\u667a\u80fd\u4f53AI\u89c6\u89c9\u5206\u7c7b\u6846\u67b6\uff0c\u5728\u96f6\u6837\u672c\u82f9\u679c\u53f6\u7247\u75c5\u5bb3\u8bca\u65ad\u4e2d\u51c6\u786e\u7387\u8fbe85.63%\uff0c\u5e76\u5f00\u6e90\u4e86\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u4fe1\u4efb\u95ee\u9898\uff0c\u7279\u522b\u662f\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u7406\u89e3\u7684\u573a\u666f\u3002", "method": "\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u901a\u7528\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u3001\u975e\u89c6\u89c9\u63a8\u7406\u534f\u8c03\u5668\u548cRAG\u6a21\u5757\u3002\u4f7f\u7528\u4e86\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u534f\u8c03\u3001\u5fae\u8c03\u7684\u667a\u80fd\u4f53\u4ee5\u53ca\u7531CLIP\u9a71\u52a8\u7684\u56fe\u50cf\u68c0\u7d22\u548c\u91cd\u65b0\u8bc4\u4f30\u5faa\u73af\u589e\u5f3a\u7684\u4fe1\u4efb\u6821\u51c6\u534f\u8c03\u4e09\u79cd\u914d\u7f6e\u3002", "result": "\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u4f7f\u7528\u4fe1\u4efb\u611f\u77e5\u7684\u534f\u8c03\u548cRAG\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e8677.94%\uff0c\u8fbe\u523085.63%\u3002GPT-4o\u663e\u793a\u51fa\u66f4\u597d\u7684\u6821\u51c6\uff0c\u800cQwen-2.5-VL\u5219\u8868\u73b0\u51fa\u8fc7\u5ea6\u81ea\u4fe1\u3002\u56fe\u50cfRAG\u4f7f\u9884\u6d4b\u5177\u6709\u89c6\u89c9\u4e0a\u7684\u76f8\u4f3c\u6027\uff0c\u4ece\u800c\u53ef\u4ee5\u901a\u8fc7\u8fed\u4ee3\u91cd\u65b0\u8bc4\u4f30\u6765\u7ea0\u6b63\u667a\u80fd\u4f53\u7684\u8fc7\u5ea6\u81ea\u4fe1\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u5757\u5316\u667a\u80fd\u4f53AI\u89c6\u89c9\u5206\u7c7b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u901a\u7528\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u4e0e\u975e\u89c6\u89c9\u63a8\u7406\u534f\u8c03\u5668\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6a21\u5757\u76f8\u7ed3\u5408\uff0c\u5e76\u5728\u82f9\u679c\u53f6\u7247\u75c5\u5bb3\u8bca\u65ad\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u5c24\u5176\u662f\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u901a\u8fc7\u4fe1\u4efb\u611f\u77e5\u7684\u534f\u8c03\u548cRAG\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e8677.94%\uff0c\u8fbe\u523085.63%\u3002\u8be5\u7cfb\u7edf\u5c06\u611f\u77e5\uff08\u89c6\u89c9\u667a\u80fd\u4f53\uff09\u4e0e\u5143\u63a8\u7406\uff08\u534f\u8c03\u5668\uff09\u5206\u79bb\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u7684\u591a\u667a\u80fd\u4f53AI\u3002"}}
{"id": "2507.10624", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10624", "abs": "https://arxiv.org/abs/2507.10624", "authors": ["Zheng Zhang"], "title": "Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning", "comment": "Substantial change to previous version (experiments, theorem,\n  analysis and related work); currently under review at TMLR", "summary": "Large Language Models (LLMs) display striking surface fluency yet\nsystematically fail at tasks requiring symbolic reasoning, arithmetic accuracy,\nand logical consistency. This paper offers a structural diagnosis of such\nfailures, revealing a persistent gap between \\textit{comprehension} and\n\\textit{competence}. Through controlled experiments and architectural analysis,\nwe demonstrate that LLMs often articulate correct principles without reliably\napplying them--a failure rooted not in knowledge access, but in computational\nexecution. We term this phenomenon the computational \\textit{split-brain\nsyndrome}, where instruction and action pathways are geometrically and\nfunctionally dissociated. This core limitation recurs across domains, from\nmathematical operations to relational inferences, and explains why model\nbehavior remains brittle even under idealized prompting. We argue that LLMs\nfunction as powerful pattern completion engines, but lack the architectural\nscaffolding for principled, compositional reasoning. Our findings delineate the\nboundary of current LLM capabilities and motivate future models with\nmetacognitive control, principle lifting, and structurally grounded execution.\nThis diagnosis also clarifies why mechanistic interpretability findings may\nreflect training-specific pattern coordination rather than universal\ncomputational principles, and why the geometric separation between instruction\nand execution pathways suggests limitations in neural introspection and\nmechanistic analysis.", "AI": {"tldr": "LLM\u64c5\u957f\u6a21\u5f0f\u5339\u914d\uff0c\u4f46\u7f3a\u4e4f\u8fdb\u884c\u6709\u539f\u5219\u63a8\u7406\u7684\u67b6\u6784\u3002", "motivation": "\u8bca\u65adLLM\u5728\u7b26\u53f7\u63a8\u7406\u3001\u7b97\u672f\u7cbe\u5ea6\u548c\u903b\u8f91\u4e00\u81f4\u6027\u7b49\u4efb\u52a1\u4e0a\u7684\u5931\u8d25\u539f\u56e0\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u548c\u67b6\u6784\u5206\u6790\uff0c\u7814\u7a76\u4eba\u5458\u63ed\u793a\u4e86LLM\u4e2d\u7406\u89e3\u548c\u80fd\u529b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "result": "LLM\u5b58\u5728\u8ba1\u7b97\u4e0a\u7684\u201c\u88c2\u8111\u7efc\u5408\u5f81\u201d\uff0c\u6307\u4ee4\u548c\u884c\u4e3a\u8def\u5f84\u5206\u79bb\uff0c\u6a21\u578b\u884c\u4e3a\u8106\u5f31\uff0c\u96be\u4ee5\u8fdb\u884c\u6709\u539f\u5219\u7684\u7ec4\u5408\u5f0f\u63a8\u7406\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u9700\u8981\u7b26\u53f7\u63a8\u7406\u3001\u7b97\u672f\u7cbe\u5ea6\u548c\u903b\u8f91\u4e00\u81f4\u6027\u7684\u4efb\u52a1\u4e2d\u7cfb\u7edf\u6027\u5730\u5931\u8d25\uff0c\u5176\u6839\u672c\u539f\u56e0\u5e76\u975e\u77e5\u8bc6\u83b7\u53d6\uff0c\u800c\u662f\u8ba1\u7b97\u6267\u884c\u4e0a\u7684\u7f3a\u9677\uff0c\u8868\u73b0\u4e3a\u6307\u4ee4\u548c\u884c\u4e3a\u8def\u5f84\u7684\u51e0\u4f55\u548c\u529f\u80fd\u5206\u79bb\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u8fdb\u884c\u6709\u539f\u5219\u7684\u3001\u7ec4\u5408\u5f0f\u63a8\u7406\u3002"}}
{"id": "2507.10630", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10630", "abs": "https://arxiv.org/abs/2507.10630", "authors": ["Ye Yang", "Xue Xiao", "Ping Yin", "Taotao Xie"], "title": "Enhancing the Capabilities of Large Language Models for API calls through Knowledge Graphs", "comment": null, "summary": "API calls by large language models (LLMs) offer a cutting-edge approach for\ndata analysis. However, their ability to effectively utilize tools via API\ncalls remains underexplored in knowledge-intensive domains like meteorology.\nThis paper introduces KG2data, a system that integrates knowledge graphs, LLMs,\nReAct agents, and tool-use technologies to enable intelligent data acquisition\nand query handling in the meteorological field. Using a virtual API, we\nevaluate API call accuracy across three metrics: name recognition failure,\nhallucination failure, and call correctness. KG2data achieves superior\nperformance (1.43%, 0%, 88.57%) compared to RAG2data (16%, 10%, 72.14%) and\nchat2data (7.14%, 8.57%, 71.43%). KG2data differs from typical LLM-based\nsystems by addressing their limited access to domain-specific knowledge, which\nhampers performance on complex or terminology-rich queries. By using a\nknowledge graph as persistent memory, our system enhances content retrieval,\ncomplex query handling, domain-specific reasoning, semantic relationship\nresolution, and heterogeneous data integration. It also mitigates the high cost\nof fine-tuning LLMs, making the system more adaptable to evolving domain\nknowledge and API structures. In summary, KG2data provides a novel solution for\nintelligent, knowledge-based question answering and data analysis in domains\nwith high knowledge demands.", "AI": {"tldr": "KG2data\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3aLLM\u7684\u6570\u636e\u5206\u6790\u80fd\u529b\uff0c\u5728\u6c14\u8c61\u9886\u57df\u53d6\u5f97\u663e\u8457\u6210\u679c\u3002", "motivation": "\u73b0\u6709LLM\u7cfb\u7edf\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\uff08\u5982\u6c14\u8c61\u5b66\uff09\u5229\u7528API\u8c03\u7528\u8fdb\u884c\u6570\u636e\u5206\u6790\u7684\u80fd\u529b\u4e0d\u8db3\uff0cKG2data\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "KG2data\u7cfb\u7edf\u96c6\u6210\u4e86\u77e5\u8bc6\u56fe\u8c31\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001ReAct\u667a\u80fd\u4f53\u548c\u5de5\u5177\u4f7f\u7528\u6280\u672f\u3002\u901a\u8fc7\u865a\u62dfAPI\u8bc4\u4f30\u4e86API\u8c03\u7528\u51c6\u786e\u6027\uff0c\u5728\u4e09\u4e2a\u6307\u6807\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002", "result": "KG2data\u5728API\u8c03\u7528\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u7cfb\u7edf\uff0c\u5206\u522b\u5728\u540d\u79f0\u8bc6\u522b\u5931\u8d25\u7387\u3001\u5e7b\u89c9\u5931\u8d25\u7387\u548c\u8c03\u7528\u6b63\u786e\u7387\u4e0a\u53d6\u5f97\u4e861.43%\u30010%\u548c88.57%\u7684\u4f18\u5f02\u7ed3\u679c\u3002", "conclusion": "KG2data\u7cfb\u7edf\u5728\u6c14\u8c61\u9886\u57df\u667a\u80fd\u6570\u636e\u83b7\u53d6\u548c\u67e5\u8be2\u5904\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8eRAG2data\u548cchat2data\u7cfb\u7edf\u3002"}}
{"id": "2507.10644", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.HC", "cs.MA", "I.2.11; I.2.7; C.2.4; K.6.5; I.2.4"], "pdf": "https://arxiv.org/pdf/2507.10644", "abs": "https://arxiv.org/abs/2507.10644", "authors": ["Tatiana Petrova", "Aleksandr Puzikov", "Boris Bliznukov", "Radu State"], "title": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents", "comment": "33 pages, 9 figures, 8 tables", "summary": "The concept of the Web of Agents (WoA), which transforms the static,\ndocument-centric Web into an environment of autonomous agents acting on users'\nbehalf, has attracted growing interest as large language models (LLMs) become\nmore capable. However, research in this area is still fragmented across\ndifferent communities. Contemporary surveys catalog the latest LLM-powered\nframeworks, while the rich histories of Multi-Agent Systems (MAS) and the\nSemantic Web are often treated as separate, legacy domains. This fragmentation\nobscures the intellectual lineage of modern systems and hinders a holistic\nunderstanding of the field's trajectory. We present the first comprehensive\nevolutionary overview of the WoA. We show that modern protocols like A2A and\nthe MCP, are direct evolutionary responses to the well-documented limitations\nof earlier standards like FIPA standards and OWL-based semantic agents. To\nsystematize this analysis, we introduce a four-axis taxonomy (semantic\nfoundation, communication paradigm, locus of intelligence, discovery\nmechanism). This framework provides a unified analytical lens for comparing\nagent architectures across all generations, revealing a clear line of descent\nwhere others have seen a disconnect. Our analysis identifies a paradigm shift\nin the 'locus of intelligence': from being encoded in external data (Semantic\nWeb) or the platform (MAS) to being embedded within the agent's core model\n(LLM). This shift is foundational to modern Agentic AI, enabling the scalable\nand adaptive systems the WoA has long envisioned. We conclude that while new\nprotocols are essential, they are insufficient for building a robust, open,\ntrustworthy ecosystem. Finally, we argue that the next research frontier lies\nin solving persistent socio-technical challenges, and we map out a new agenda\nfocused on decentralized identity, economic models, security, and governance\nfor the emerging WoA.", "AI": {"tldr": "The Web of Agents is evolving towards LLM-powered agents, but needs better decentralized identity, economic models, security, and governance.", "motivation": "To provide a comprehensive evolutionary overview of the WoA, bridging the gap between contemporary LLM-powered frameworks and the historical context of MAS and the Semantic Web.", "method": "A four-axis taxonomy (semantic foundation, communication paradigm, locus of intelligence, discovery mechanism) is used to analyze the evolution of WoA, comparing agent architectures across generations.", "result": "A unified analytical lens revealing a paradigm shift in the 'locus of intelligence' and highlighting the need to address socio-technical challenges for a robust WoA ecosystem.", "conclusion": "The Web of Agents (WoA) is evolving, shifting from external data or platform-based intelligence to LLM-embedded intelligence.  Future research should focus on socio-technical challenges like decentralized identity, economic models, security, and governance."}}
{"id": "2507.10740", "categories": ["cs.AI", "cs.NE", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.10740", "abs": "https://arxiv.org/abs/2507.10740", "authors": ["Maziar Kanani", "Sean O Leary", "James McDermott"], "title": "Parsing Musical Structure to Enable Meaningful Variations", "comment": null, "summary": "This paper presents a novel rule-based approach for generating music by\nvarying existing tunes. We parse each tune to find the Pathway Assembly (PA) [\n1], that is a structure representing all repetitions in the tune. The Sequitur\nalgorithm [2 ] is used for this. The result is a grammar. We then carry out\nmutation on the grammar, rather than on a tune directly. There are potentially\n19 types of mutations such as adding, removing, swapping or reversing parts of\nthe grammar that can be applied to the grammars. The system employs one of the\nmutations randomly in this step to automatically manipulate the grammar.\nFollowing the mutation, we need to expand the grammar which returns a new tune.\nThe output after 1 or more mutations will be a new tune related to the original\ntune. Our study examines how tunes change gradually over the course of multiple\nmutations. Edit distances, structural complexity and length of the tunes are\nused to show how a tune is changed after multiple mutations. In addition, the\nsize of effect of each mutation type is analyzed. As a final point, we review\nthe musical aspect of the output tunes. It should be noted that the study only\nfocused on generating new pitch sequences. The study is based on an Irish\ntraditional tune dataset and a list of integers has been used to represent each\ntune's pitch values.", "AI": {"tldr": "\u57fa\u4e8e\u89c4\u5219\u7684\u97f3\u4e50\u53d8\u594f\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u66f2\u8c03\u7684\u8bed\u6cd5\u8fdb\u884c\u7a81\u53d8\u751f\u6210\u65b0\u7684\u66f2\u8c03\uff0c\u5e76\u5206\u6790\u4e86\u7a81\u53d8\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u97f3\u4e50\u751f\u6210\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u53d8\u594f\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u89c4\u5219\uff0c\u4f7f\u7528Sequitur\u7b97\u6cd5\u89e3\u6790\u66f2\u8c03\u7684PA\u7ed3\u6784\uff0c\u7136\u540e\u5bf9\u8bed\u6cd5\u8fdb\u884c19\u79cd\u7c7b\u578b\u7684\u7a81\u53d8\u64cd\u4f5c\uff0c\u6700\u540e\u6269\u5c55\u8bed\u6cd5\u751f\u6210\u65b0\u7684\u66f2\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u4e0e\u539f\u59cb\u66f2\u8c03\u76f8\u5173\u7684\u65b0\u7684\u66f2\u8c03\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u7a81\u53d8\u7c7b\u578b\u5bf9\u66f2\u8c03\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u97f3\u4e50\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u73b0\u6709\u66f2\u8c03\u8fdb\u884c\u53d8\u594f\u751f\u6210\u65b0\u7684\u66f2\u8c03\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528Sequitur\u7b97\u6cd5\u89e3\u6790\u66f2\u8c03\uff0c\u627e\u5230\u5176Pathway Assembly (PA)\u7ed3\u6784\uff0c\u7136\u540e\u5bf9\u8be5\u8bed\u6cd5\u8fdb\u884c\u7a81\u53d8\u64cd\u4f5c\uff0c\u800c\u975e\u76f4\u63a5\u5bf9\u66f2\u8c03\u8fdb\u884c\u7a81\u53d8\u3002\u5b9e\u9a8c\u5206\u6790\u4e86\u4e0d\u540c\u7a81\u53d8\u7c7b\u578b\u5bf9\u66f2\u8c03\u7684\u5f71\u54cd\uff0c\u5e76\u4ece\u7f16\u8f91\u8ddd\u79bb\u3001\u7ed3\u6784\u590d\u6742\u5ea6\u548c\u957f\u5ea6\u7b49\u65b9\u9762\u8bc4\u4f30\u4e86\u7a81\u53d8\u6548\u679c\uff0c\u6700\u540e\u5bf9\u751f\u6210\u7684\u66f2\u8c03\u7684\u97f3\u4e50\u6027\u8fdb\u884c\u4e86\u8bc4\u4ef7\u3002"}}
{"id": "2507.10750", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10750", "abs": "https://arxiv.org/abs/2507.10750", "authors": ["Pandu Devarakota", "Nicolas Tsesmetzis", "Faruk O. Alpak", "Apurva Gala", "Detlef Hohl"], "title": "AI and the Net-Zero Journey: Energy Demand, Emissions, and the Potential for Transition", "comment": "Technical article to be submitted to Data Centric Engineering Journal", "summary": "Thanks to the availability of massive amounts of data, computing resources,\nand advanced algorithms, AI has entered nearly every sector. This has sparked\nsignificant investment and interest, particularly in building data centers with\nthe necessary hardware and software to develop and operate AI models and\nAI-based workflows. In this technical review article, we present energy\nconsumption scenarios of data centers and impact on GHG emissions, considering\nboth near-term projections (up to 2030) and long-term outlook (2035 and\nbeyond). We address the quintessential question of whether AI will have a net\npositive, neutral, or negative impact on CO2 emissions by 2035. Additionally,\nwe discuss AI's potential to automate, create efficient and disruptive\nworkflows across various fields related to energy production, supply and\nconsumption. In the near-term scenario, the growing demand for AI will likely\nstrain computing resources, lead to increase in electricity consumption and\ntherefore associated CO2 emissions. This is due to the power-hungry nature of\nbig data centers and the requirements for training and running of large and\ncomplex AI models, as well as the penetration of AI assistant search and\napplications for public use. However, the long-term outlook could be more\npromising. AI has the potential to be a game-changer in CO2 reduction. Its\nability to further automate and optimize processes across industries, from\nenergy production to logistics, could significantly decrease our carbon\nfootprint. This positive impact is anticipated to outweigh the initial\nemissions bump, creating value for businesses and society in areas where\ntraditional solutions have fallen short. In essence, AI might cause some\ninitial growing pains for the environment, but it has the potential to support\nclimate mitigation efforts.", "AI": {"tldr": "AI\u77ed\u671f\u589e\u6392\uff0c\u957f\u671f\u51cf\u6392", "motivation": "\u63a2\u8ba8AI\u5bf9CO2\u6392\u653e\u7684\u51c0\u5f71\u54cd\uff0c\u4ee5\u53caAI\u5728\u80fd\u6e90\u751f\u4ea7\u3001\u4f9b\u5e94\u548c\u6d88\u8d39\u9886\u57df\u63d0\u9ad8\u6548\u7387\u7684\u6f5c\u529b\u3002", "method": "\u6280\u672f\u7efc\u8ff0\u6587\u7ae0\uff0c\u5206\u6790\u6570\u636e\u4e2d\u5fc3\u80fd\u8017\u53ca\u5bf9\u6e29\u5ba4\u6c14\u4f53\u6392\u653e\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u8ba8AI\u5728\u8282\u80fd\u51cf\u6392\u65b9\u9762\u7684\u6f5c\u529b\u3002", "result": "\u77ed\u671f\u5185AI\u4f1a\u589e\u52a0\u80fd\u8017\u548cCO2\u6392\u653e\uff0c\u4f46\u957f\u671f\u6765\u770b\uff0cAI\u4f18\u5316\u5404\u884c\u5404\u4e1a\u6d41\u7a0b\u7684\u80fd\u529b\u5c06\u8d85\u8fc7\u5176\u521d\u59cb\u6392\u653e\u7684\u5f71\u54cd\uff0c\u6700\u7ec8\u51cf\u5c11\u78b3\u8db3\u8ff9\u3002", "conclusion": "AI\u5bf9\u73af\u5883\u7684\u5f71\u54cd\u5728\u77ed\u671f\u5185\u53ef\u80fd\u662f\u8d1f\u9762\u7684\uff0c\u4f46\u957f\u671f\u6765\u770b\u53ef\u80fd\u5bf9\u6c14\u5019\u53d8\u5316\u7f13\u89e3\u5de5\u4f5c\u6709\u79ef\u6781\u4f5c\u7528\u3002"}}
{"id": "2507.10758", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10758", "abs": "https://arxiv.org/abs/2507.10758", "authors": ["Nikesh Prajapati", "Bimal Karki", "Saroj Gopali", "Akbar Siami Namin"], "title": "IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models", "comment": null, "summary": "This paper intends to detect IoT malicious attacks through deep learning\nmodels and demonstrates a comprehensive evaluation of the deep learning and\ngraph-based models regarding malicious network traffic detection. The models\nparticularly are based on GraphSAGE, Bidirectional encoder representations from\ntransformers (BERT), Temporal Convolutional Network (TCN) as well as Multi-Head\nAttention, together with Bidirectional Long Short-Term Memory (BI-LSTM)\nMulti-Head Attention and BI-LSTM and LSTM models. The chosen models\ndemonstrated great performance to model temporal patterns and detect feature\nsignificance. The observed performance are mainly due to the fact that IoT\nsystem traffic patterns are both sequential and diverse, leaving a rich set of\ntemporal patterns for the models to learn. Experimental results showed that\nBERT maintained the best performance. It achieved 99.94% accuracy rate\nalongside high precision and recall, F1-score and AUC-ROC score of 99.99% which\ndemonstrates its capabilities through temporal dependency capture. The\nMulti-Head Attention offered promising results by providing good detection\ncapabilities with interpretable results. On the other side, the Multi-Head\nAttention model required significant processing time like BI-LSTM variants. The\nGraphSAGE model achieved good accuracy while requiring the shortest training\ntime but yielded the lowest accuracy, precision, and F1 score compared to the\nother models", "AI": {"tldr": "BERT\u6a21\u578b\u5728\u7269\u8054\u7f51\u6076\u610f\u653b\u51fb\u68c0\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe99.94%\u3002", "motivation": "\u68c0\u6d4b\u7269\u8054\u7f51\u6076\u610f\u653b\u51fb\u3002", "method": "\u4f7f\u7528GraphSAGE\u3001BERT\u3001TCN\u3001\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u3001BI-LSTM\u548cLSTM\u7b49\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u548c\u56fe\u6a21\u578b\u5bf9\u7269\u8054\u7f51\u6076\u610f\u7f51\u7edc\u6d41\u91cf\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "BERT\u6a21\u578b\u51c6\u786e\u7387\u8fbe99.94%\uff0c\u5176\u4ed6\u6a21\u578b\u5404\u6709\u4f18\u52a3\uff0c\u8bad\u7ec3\u65f6\u95f4\u548c\u51c6\u786e\u7387\u5b58\u5728\u6743\u8861\u3002", "conclusion": "BERT\u6a21\u578b\u5728\u7269\u8054\u7f51\u6076\u610f\u653b\u51fb\u68c0\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe\u523099.94%\uff0c\u5176\u4ed6\u6307\u6807\u4e5f\u6781\u5176\u4f18\u79c0\u3002GraphSAGE\u6a21\u578b\u8bad\u7ec3\u901f\u5ea6\u6700\u5feb\uff0c\u4f46\u51c6\u786e\u7387\u6700\u4f4e\u3002"}}
{"id": "2507.10761", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.10761", "abs": "https://arxiv.org/abs/2507.10761", "authors": ["Tyler King", "Nikolos Gurney", "John H. Miller", "Volkan Ustun"], "title": "Detecting AI Assistance in Abstract Complex Tasks", "comment": "Accepted to HCII 2025", "summary": "Detecting assistance from artificial intelligence is increasingly important\nas they become ubiquitous across complex tasks such as text generation, medical\ndiagnosis, and autonomous driving. Aid detection is challenging for humans,\nespecially when looking at abstract task data. Artificial neural networks excel\nat classification thanks to their ability to quickly learn from and process\nlarge amounts of data -- assuming appropriate preprocessing. We posit detecting\nhelp from AI as a classification task for such models. Much of the research in\nthis space examines the classification of complex but concrete data classes,\nsuch as images. Many AI assistance detection scenarios, however, result in data\nthat is not machine learning-friendly. We demonstrate that common models can\neffectively classify such data when it is appropriately preprocessed. To do so,\nwe construct four distinct neural network-friendly image formulations along\nwith an additional time-series formulation that explicitly encodes the\nexploration/exploitation of users, which allows for generalizability to other\nabstract tasks. We benchmark the quality of each image formulation across three\nclassical deep learning architectures, along with a parallel CNN-RNN\narchitecture that leverages the additional time series to maximize testing\nperformance, showcasing the importance of encoding temporal and spatial\nquantities for detecting AI aid in abstract tasks.", "AI": {"tldr": "\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u5bf9AI\u8f85\u52a9\u8fdb\u884c\u5206\u7c7b\uff0c\u901a\u8fc7\u6570\u636e\u9884\u5904\u7406\u548c\u4e0d\u540c\u6a21\u578b\u7ed3\u6784\u63d0\u5347\u4e86\u62bd\u8c61\u4efb\u52a1\u4e2dAI\u8f85\u52a9\u68c0\u6d4b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u68c0\u6d4b\u4eba\u5de5\u667a\u80fd\u7684\u8f85\u52a9\u4f5c\u7528\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u6587\u672c\u751f\u6210\u3001\u533b\u5b66\u8bca\u65ad\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u590d\u6742\u4efb\u52a1\u4e2d\u3002\u7136\u800c\uff0c\u5bf9\u4eba\u7c7b\u6765\u8bf4\uff0c\u7279\u522b\u662f\u5f53\u67e5\u770b\u62bd\u8c61\u4efb\u52a1\u6570\u636e\u65f6\uff0c\u8f85\u52a9\u68c0\u6d4b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u6784\u5efa\u56db\u79cd\u4e0d\u540c\u7684\u795e\u7ecf\u7f51\u7edc\u53cb\u597d\u578b\u56fe\u50cf\u516c\u5f0f\u548c\u4e00\u79cd\u663e\u5f0f\u7f16\u7801\u7528\u6237\u63a2\u7d22/\u5229\u7528\u7684\u65f6\u95f4\u5e8f\u5217\u516c\u5f0f\uff0c\u5e76\u4f7f\u7528\u4e09\u79cd\u7ecf\u5178\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u548c\u4e00\u79cd\u5e76\u884cCNN-RNN\u67b6\u6784\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u65f6\u95f4\u548c\u7a7a\u95f4\u91cf\u8fdb\u884c\u7f16\u7801\u5bf9\u4e8e\u68c0\u6d4b\u62bd\u8c61\u4efb\u52a1\u4e2d\u7684AI\u8f85\u52a9\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u6784\u5efa\u56db\u79cd\u4e0d\u540c\u7684\u795e\u7ecf\u7f51\u7edc\u53cb\u597d\u578b\u56fe\u50cf\u516c\u5f0f\u548c\u4e00\u79cd\u663e\u5f0f\u7f16\u7801\u7528\u6237\u63a2\u7d22/\u5229\u7528\u7684\u65f6\u95f4\u5e8f\u5217\u516c\u5f0f\uff0c\u8bc1\u660e\u4e86\u5e38\u7528\u6a21\u578b\u5728\u5bf9\u62bd\u8c61\u4efb\u52a1\u6570\u636e\u8fdb\u884c\u9002\u5f53\u9884\u5904\u7406\u540e\u53ef\u4ee5\u6709\u6548\u5730\u8fdb\u884c\u5206\u7c7b\u3002"}}
{"id": "2507.10798", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10798", "abs": "https://arxiv.org/abs/2507.10798", "authors": ["Asim H. Gazi", "Bhanu T. Gullapalli", "Daiqi Gao", "Benjamin M. Marlin", "Vivek Shetty", "Susan A. Murphy"], "title": "Uncertainty-Informed Scheduling of Decision Points for Intelligent Mobile Health Interventions", "comment": "4 pages, 3 figures", "summary": "Timely decision making is critical to the effectiveness of mobile health\n(mHealth) interventions. At predefined timepoints called \"decision points,\"\nintelligent mHealth systems such as just-in-time adaptive interventions\n(JITAIs) estimate an individual's biobehavioral context from sensor or survey\ndata and determine whether and how to intervene. For interventions targeting\nhabitual behavior (e.g., oral hygiene), effectiveness often hinges on\ndelivering support shortly before the target behavior is likely to occur.\nCurrent practice schedules decision points at a fixed interval (e.g., one hour)\nbefore user-provided behavior times, and the fixed interval is kept the same\nfor all individuals. However, this one-size-fits-all approach performs poorly\nfor individuals with irregular routines, often scheduling decision points after\nthe target behavior has already occurred, rendering interventions ineffective.\nIn this paper, we propose SigmaScheduling, a method to dynamically schedule\ndecision points based on uncertainty in predicted behavior times. When behavior\ntiming is more predictable, SigmaScheduling schedules decision points closer to\nthe predicted behavior time; when timing is less certain, SigmaScheduling\nschedules decision points earlier, increasing the likelihood of timely\nintervention. We evaluated SigmaScheduling using real-world data from 68\nparticipants in a 10-week trial of Oralytics, a JITAI designed to improve daily\ntoothbrushing. SigmaScheduling increased the likelihood that decision points\npreceded brushing events in at least 70% of cases, preserving opportunities to\nintervene and impact behavior. Our results indicate that SigmaScheduling can\nadvance precision mHealth, particularly for JITAIs targeting time-sensitive,\nhabitual behaviors such as oral hygiene or dietary habits.", "AI": {"tldr": "Dynamic scheduling of decision points in mHealth improves intervention timeliness, particularly for irregular routines, as shown by a real-world trial of SigmaScheduling.", "motivation": "Current fixed-interval scheduling in mHealth interventions is ineffective for individuals with irregular routines.  This paper aims to improve the timeliness of interventions for habitual behaviors by dynamically adjusting decision point scheduling based on the uncertainty of predicted behavior times.", "method": "The paper proposes SigmaScheduling, a method that dynamically schedules decision points based on the uncertainty of predicted behavior times.  It uses real-world data from a 10-week trial to evaluate its effectiveness.", "result": "SigmaScheduling increased the likelihood of decision points preceding brushing events in at least 70% of cases, demonstrating its effectiveness in improving the timeliness of interventions and its potential to advance precision mHealth.", "conclusion": "SigmaScheduling, a dynamic decision point scheduling method for mHealth interventions, significantly improves the timeliness of interventions, especially for individuals with irregular routines.  It adapts scheduling based on the uncertainty of predicted behavior times, ensuring timely support before target behaviors."}}
{"id": "2507.10803", "categories": ["cs.AI", "cs.CL", "cs.ET", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.10803", "abs": "https://arxiv.org/abs/2507.10803", "authors": ["JaMor Hairston", "Ritvik Ranjan", "Sahithi Lakamana", "Anthony Spadaro", "Selen Bozkurt", "Jeanmarie Perrone", "Abeed Sarker"], "title": "Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case", "comment": "Pages: 19, Abstract word count: 151 words, Manuscript word count:\n  2185 words, References: 14, Figures: 3, Tables: 2", "summary": "Background Large language models (LLMs) face challenges in inductive thematic\nanalysis, a task requiring deep interpretive and domain-specific expertise. We\nevaluated the feasibility of using LLMs to replicate expert-driven thematic\nanalysis of social media data. Methods Using two temporally non-intersecting\nReddit datasets on xylazine (n=286 and n=686, for model optimization and\nvalidation, respectively) with twelve expert-derived themes, we evaluated five\nLLMs against expert coding. We modeled the task as a series of binary\nclassifications, rather than a single, multi-label classification, employing\nzero-, single-, and few-shot prompting strategies and measuring performance via\naccuracy, precision, recall, and F1-score. Results On the validation set,\nGPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:\n0.71). For high-prevalence themes, model-derived thematic distributions closely\nmirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:\n16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based\napproaches can automate thematic analyses, offering a scalable supplement for\nqualitative research. Keywords: thematic analysis, large language models,\nnatural language processing, qualitative analysis, social media, prompt\nengineering, public health", "AI": {"tldr": "LLM\u53ef\u90e8\u5206\u81ea\u52a8\u5316\u4e3b\u9898\u5206\u6790\uff0c\u8f85\u52a9\u5b9a\u6027\u7814\u7a76\uff0c\u4f46\u51c6\u786e\u6027\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "motivation": "\u8bc4\u4f30LLM\u5728\u5f52\u7eb3\u6027\u4e3b\u9898\u5206\u6790\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4ee5\u8f85\u52a9\u5bf9\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u7684\u5206\u6790\u3002", "method": "\u4f7f\u7528\u4e24\u4e2aReddit\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e86\u4e94\u79cdLLM\u5728\u4e3b\u9898\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u91c7\u7528\u4e8c\u5143\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u6d4b\u8bd5\u4e86\u96f6\u6837\u672c\u3001\u5355\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\u7b56\u7565\u3002", "result": "GPT-4o\u5728\u5c11\u6837\u672c\u63d0\u793a\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe90.9%\uff0cF1\u5206\u6570\u4e3a0.71\u3002\u9ad8\u9891\u4e3b\u9898\u7684\u6a21\u578b\u7ed3\u679c\u4e0e\u4e13\u5bb6\u5206\u7c7b\u7ed3\u679c\u8f83\u4e3a\u63a5\u8fd1\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u53ef\u7528\u4e8e\u8f85\u52a9\u4e3b\u9898\u5206\u6790\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u9ad8\u9891\u4e3b\u9898\u7684\u5206\u6790\u4e2d\uff0c\u80fd\u6709\u6548\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u4ecd\u9700\u4eba\u5de5\u5ba1\u6838\u3002"}}
{"id": "2507.10831", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10831", "abs": "https://arxiv.org/abs/2507.10831", "authors": ["Yilin Xia", "Heng Zheng", "Shawn Bowers", "Bertram Lud\u00e4scher"], "title": "AF-XRAY: Visual Explanation and Resolution of Ambiguity in Legal Argumentation Frameworks", "comment": "International Conference on Artificial Intelligence and Law (ICAIL),\n  June 16-20, 2025. Chicago, IL, USA", "summary": "Argumentation frameworks (AFs) provide formal approaches for legal reasoning,\nbut identifying sources of ambiguity and explaining argument acceptance remains\nchallenging for non-experts. We present AF-XRAY, an open-source toolkit for\nexploring, analyzing, and visualizing abstract AFs in legal reasoning. AF-XRAY\nintroduces: (i) layered visualizations based on game-theoretic argument length\nrevealing well-founded derivation structures; (ii) classification of attack\nedges by semantic roles (primary, secondary, blunders); (iii) overlay\nvisualizations of alternative 2-valued solutions on ambiguous 3-valued grounded\nsemantics; and (iv) identification of critical attack sets whose suspension\nresolves undecided arguments. Through systematic generation of critical attack\nsets, AF-XRAY transforms ambiguous scenarios into grounded solutions, enabling\nusers to pinpoint specific causes of ambiguity and explore alternative\nresolutions. We use real-world legal cases (e.g., Wild Animals as modeled by\nBench-Capon) to show that our tool supports teleological legal reasoning by\nrevealing how different assumptions lead to different justified conclusions.", "AI": {"tldr": "AF-XRAY\u662f\u4e00\u4e2a\u7528\u4e8e\u5206\u6790\u6cd5\u5f8b\u63a8\u7406\u4e2d\u8bba\u8bc1\u6846\u67b6\u7684\u5f00\u6e90\u5de5\u5177\uff0c\u5b83\u901a\u8fc7\u53ef\u89c6\u5316\u548c\u5173\u952e\u653b\u51fb\u96c6\u8bc6\u522b\u6765\u89e3\u51b3\u6a21\u7cca\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u8bba\u8bc1\u6846\u67b6\u65b9\u6cd5\u96be\u4ee5\u89e3\u91ca\u8bba\u8bc1\u7684\u63a5\u53d7\u6027\uff0c\u5c24\u5176\u5bf9\u4e8e\u975e\u4e13\u5bb6\u800c\u8a00\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aAF-XRAY\u7684\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u8be5\u5de5\u5177\u5305\u5229\u7528\u5206\u5c42\u53ef\u89c6\u5316\u3001\u653b\u51fb\u8fb9\u5206\u7c7b\u548c\u66ff\u4ee3\u89e3\u7684\u53ef\u89c6\u5316\u53e0\u52a0\u7b49\u65b9\u6cd5\uff0c\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u548c\u5206\u6790AFs\u3002", "result": "AF-XRAY\u5de5\u5177\u6709\u6548\u5730\u5e2e\u52a9\u7528\u6237\u8bc6\u522b\u6a21\u7cca\u6027\u7684\u6765\u6e90\uff0c\u5e76\u63a2\u7d22\u66ff\u4ee3\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u76ee\u7684\u8bba\u6cd5\u5f8b\u63a8\u7406\u3002", "conclusion": "AF-XRAY\u5de5\u5177\u80fd\u591f\u5e2e\u52a9\u975e\u4e13\u5bb6\u63a2\u7d22\u3001\u5206\u6790\u548c\u53ef\u89c6\u5316\u6cd5\u5f8b\u63a8\u7406\u4e2d\u7684\u62bd\u8c61\u8bba\u8bc1\u6846\u67b6 (AFs)\uff0c\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u653b\u51fb\u96c6\u6765\u89e3\u51b3\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u652f\u6301\u76ee\u7684\u8bba\u6cd5\u5f8b\u63a8\u7406\u3002"}}
{"id": "2507.10860", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10860", "abs": "https://arxiv.org/abs/2507.10860", "authors": ["Atila Orhon", "Arda Okan", "Berkin Durmus", "Zach Nagengast", "Eduardo Pacheco"], "title": "WhisperKit: On-device Real-time ASR with Billion-Scale Transformers", "comment": "ICML 2025 - On-Device Learning for Foundational Models Workshop", "summary": "Real-time Automatic Speech Recognition (ASR) is a fundamental building block\nfor many commercial applications of ML, including live captioning, dictation,\nmeeting transcriptions, and medical scribes. Accuracy and latency are the most\nimportant factors when companies select a system to deploy. We present\nWhisperKit, an optimized on-device inference system for real-time ASR that\nsignificantly outperforms leading cloud-based systems. We benchmark against\nserver-side systems that deploy a diverse set of models, including a frontier\nmodel (OpenAI gpt-4o-transcribe), a proprietary model (Deepgram nova-3), and an\nopen-source model (Fireworks large-v3-turbo).Our results show that WhisperKit\nmatches the lowest latency at 0.46s while achieving the highest accuracy 2.2%\nWER. The optimizations behind the WhisperKit system are described in detail in\nthis paper.", "AI": {"tldr": "WhisperKit\u5b9e\u65f6ASR\u7cfb\u7edf\u4f18\u4e8e\u73b0\u6709\u4e91\u7aef\u7cfb\u7edf\uff0c\u5ef6\u8fdf\u4f4e\uff0c\u51c6\u786e\u7387\u9ad8\u3002", "motivation": "\u5b9e\u65f6ASR\u662f\u8bb8\u591a\u5546\u4e1a\u5e94\u7528\uff08\u5982\u5b9e\u65f6\u5b57\u5e55\u3001\u542c\u5199\u3001\u4f1a\u8bae\u8f6c\u5f55\u548c\u533b\u7597\u901f\u8bb0\uff09\u7684\u57fa\u7840\u7ec4\u6210\u90e8\u5206\uff0c\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u662f\u516c\u53f8\u9009\u62e9\u90e8\u7f72\u7cfb\u7edf\u7684\u6700\u91cd\u8981\u56e0\u7d20\u3002", "method": "\u5bf9WhisperKit\u7cfb\u7edf\u7684\u4f18\u5316\u8fdb\u884c\u4e86\u8be6\u7ec6\u63cf\u8ff0\u3002\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u8c61\u5305\u62ec\u591a\u79cd\u6a21\u578b\uff0c\u5305\u62ec\u524d\u6cbf\u6a21\u578b\uff08OpenAI gpt-4o-transcribe\uff09\u3001\u4e13\u6709\u6a21\u578b\uff08Deepgram nova-3\uff09\u548c\u5f00\u6e90\u6a21\u578b\uff08Fireworks large-v3-turbo\uff09\u3002", "result": "WhisperKit\u5339\u914d\u6700\u4f4e\u5ef6\u8fdf0.46\u79d2\uff0c\u540c\u65f6\u5b9e\u73b0\u6700\u9ad8\u51c6\u786e\u73872.2% WER\u3002", "conclusion": "WhisperKit\u7cfb\u7edf\u5728\u5b9e\u65f6ASR\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u9886\u5148\u7684\u4e91\u7aef\u7cfb\u7edf\uff0c\u57280.46\u79d2\u7684\u6700\u4f4e\u5ef6\u8fdf\u4e0b\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u51c6\u786e\u7387\uff082.2% WER\uff09\u3002"}}
{"id": "2507.10894", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10894", "abs": "https://arxiv.org/abs/2507.10894", "authors": ["Zongtao He", "Liuyi Wang", "Lu Chen", "Chengju Liu", "Qijun Chen"], "title": "NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization", "comment": null, "summary": "Language-guided navigation is a cornerstone of embodied AI, enabling agents\nto interpret language instructions and navigate complex environments. However,\nexpert-provided instructions are limited in quantity, while synthesized\nannotations often lack quality, making them insufficient for large-scale\nresearch. To address this, we propose NavComposer, a novel framework for\nautomatically generating high-quality navigation instructions. NavComposer\nexplicitly decomposes semantic entities such as actions, scenes, and objects,\nand recomposes them into natural language instructions. Its modular\narchitecture allows flexible integration of state-of-the-art techniques, while\nthe explicit use of semantic entities enhances both the richness and accuracy\nof instructions. Moreover, it operates in a data-agnostic manner, supporting\nadaptation to diverse navigation trajectories without domain-specific training.\nComplementing NavComposer, we introduce NavInstrCritic, a comprehensive\nannotation-free evaluation system that assesses navigation instructions on\nthree dimensions: contrastive matching, semantic consistency, and linguistic\ndiversity. NavInstrCritic provides a holistic evaluation of instruction\nquality, addressing limitations of traditional metrics that rely heavily on\nexpert annotations. By decoupling instruction generation and evaluation from\nspecific navigation agents, our method enables more scalable and generalizable\nresearch. Extensive experiments provide direct and practical evidence for the\neffectiveness of our method.", "AI": {"tldr": "\u63d0\u51faNavComposer\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u5bfc\u822a\u6307\u4ee4\u548cNavInstrCritic\u65e0\u6807\u6ce8\u8bc4\u4f30\u7cfb\u7edf\uff0c\u63d0\u5347\u8bed\u8a00\u5f15\u5bfc\u5bfc\u822a\u7814\u7a76\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u5f15\u5bfc\u5bfc\u822a\u7814\u7a76\u53d7\u9650\u4e8e\u4e13\u5bb6\u63d0\u4f9b\u7684\u6307\u4ee4\u6570\u91cf\u548c\u5408\u6210\u6807\u6ce8\u7684\u8d28\u91cf\uff0c\u96be\u4ee5\u8fdb\u884c\u5927\u89c4\u6a21\u7814\u7a76\u3002", "method": "NavComposer\u6846\u67b6\u5c06\u8bed\u4e49\u5b9e\u4f53\u5206\u89e3\u91cd\u7ec4\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0cNavInstrCritic\u7cfb\u7edf\u4ece\u5bf9\u6bd4\u5339\u914d\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u8bed\u8a00\u591a\u6837\u6027\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u6307\u4ee4\u8d28\u91cf\u3002", "result": "NavComposer\u751f\u6210\u7684\u6307\u4ee4\u8d28\u91cf\u9ad8\uff0cNavInstrCritic\u8bc4\u4f30\u7cfb\u7edf\u5168\u9762\u6709\u6548\uff0c\u4e24\u8005\u5171\u540c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7814\u7a76\u7684\u8fdb\u5c55\u3002", "conclusion": "NavComposer\u6846\u67b6\u548cNavInstrCritic\u8bc4\u4f30\u7cfb\u7edf\u63d0\u9ad8\u4e86\u8bed\u8a00\u5f15\u5bfc\u5bfc\u822a\u7814\u7a76\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u5bfc\u822a\u6307\u4ee4\u548c\u65e0\u6807\u6ce8\u8bc4\u4f30\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.10911", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10911", "abs": "https://arxiv.org/abs/2507.10911", "authors": ["Yicong Wu", "Ting Chen", "Irit Hochberg", "Zhoujian Sun", "Ruth Edry", "Zhengxing Huang", "Mor Peleg"], "title": "Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation", "comment": null, "summary": "Therapy recommendation for chronic patients with multimorbidity is\nchallenging due to risks of treatment conflicts. Existing decision support\nsystems face scalability limitations. Inspired by the way in which general\npractitioners (GP) manage multimorbidity patients, occasionally convening\nmultidisciplinary team (MDT) collaboration, this study investigated the\nfeasibility and value of using a Large Language Model (LLM)-based multi-agent\nsystem (MAS) for safer therapy recommendations. We designed a single agent and\na MAS framework simulating MDT decision-making by enabling discussion among LLM\nagents to resolve medical conflicts. The systems were evaluated on therapy\nplanning tasks for multimorbidity patients using benchmark cases. We compared\nMAS performance with single-agent approaches and real-world benchmarks. An\nimportant contribution of our study is the definition of evaluation metrics\nthat go beyond the technical precision and recall and allow the inspection of\nclinical goals met and medication burden of the proposed advices to a gold\nstandard benchmark. Our results show that with current LLMs, a single agent GP\nperforms as well as MDTs. The best-scoring models provide correct\nrecommendations that address all clinical goals, yet the advices are\nincomplete. Some models also present unnecessary medications, resulting in\nunnecessary conflicts between medication and conditions or drug-drug\ninteractions.", "AI": {"tldr": "LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7528\u4e8e\u591a\u75c5\u5171\u60a3\u6162\u6027\u75c5\u6cbb\u7597\u65b9\u6848\u63a8\u8350\u53ef\u884c\uff0c\u4f46\u5355\u667a\u80fd\u4f53\u8868\u73b0\u540c\u6837\u51fa\u8272\uff0c\u4e14\u6700\u4f73\u6a21\u578b\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u96be\u4ee5\u5e94\u5bf9\u591a\u75c5\u5171\u60a3\u6162\u6027\u75c5\u60a3\u8005\u7684\u6cbb\u7597\u65b9\u6848\u63a8\u8350\uff0c\u5b58\u5728\u6cbb\u7597\u51b2\u7a81\u7684\u98ce\u9669\u3002", "method": "\u8bbe\u8ba1\u4e86\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\uff0c\u6a21\u62df\u591a\u5b66\u79d1\u56e2\u961f\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u4f7f\u7528\u57fa\u51c6\u6848\u4f8b\u8bc4\u4f30\u4e86\u7cfb\u7edf\u7684\u6cbb\u7597\u89c4\u5212\u4efb\u52a1\u3002", "result": "\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u4e0e\u591a\u5b66\u79d1\u56e2\u961f\u7684\u8868\u73b0\u76f8\u5f53\uff0c\u6700\u4f73\u6a21\u578b\u80fd\u6ee1\u8db3\u6240\u6709\u4e34\u5e8a\u76ee\u6807\uff0c\u4f46\u5efa\u8bae\u53ef\u80fd\u4e0d\u5b8c\u6574\u6216\u5b58\u5728\u4e0d\u5fc5\u8981\u7684\u836f\u7269\uff0c\u5bfc\u81f4\u836f\u7269\u4e0e\u75be\u75c5\u6216\u836f\u7269\u95f4\u7684\u51b2\u7a81\u3002", "conclusion": "\u4f7f\u7528LLM\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e3a\u591a\u75c5\u5171\u60a3\u6162\u6027\u75c5\u60a3\u8005\u5b89\u5168\u5730\u63a8\u8350\u6cbb\u7597\u65b9\u6848\u662f\u53ef\u884c\u7684\uff0c\u4f46\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u8868\u73b0\u4e0e\u591a\u5b66\u79d1\u56e2\u961f\u76f8\u5f53\uff0c\u4e14\u6700\u4f73\u6a21\u578b\u4ecd\u5b58\u5728\u5efa\u8bae\u4e0d\u5b8c\u6574\u6216\u836f\u7269\u51b2\u7a81\u7b49\u95ee\u9898\u3002"}}
