<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation](https://arxiv.org/abs/2507.19489)
*Simone Bendazzoli,Sanna Persson,Mehdi Astaraki,Sebastian Pettersson,Vitali Grozman,Rodrigo Moreno*

Main category: cs.AI

TL;DR: MAIA是一个开源平台，促进医疗AI的跨学科合作和临床转化。


<details>
  <summary>Details</summary>
Motivation: 弥合AI技术创新与实际医疗应用之间的差距，促进临床医生、研究人员和AI开发人员之间的跨学科合作。

Method: 构建了基于Kubernetes的模块化、可扩展的MAIA平台，集成数据管理、模型开发、标注、部署和临床反馈工具。

Result: MAIA已在学术和临床环境中成功部署，支持医学影像AI等真实世界用例。

Conclusion: MAIA平台旨在加速AI研究转化为临床应用，促进可重复性、透明度和以用户为中心的设计。

Abstract: The integration of Artificial Intelligence (AI) into clinical workflows
requires robust collaborative platforms that are able to bridge the gap between
technical innovation and practical healthcare applications. This paper
introduces MAIA (Medical Artificial Intelligence Assistant), an open-source
platform designed to facilitate interdisciplinary collaboration among
clinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a
modular, scalable environment with integrated tools for data management, model
development, annotation, deployment, and clinical feedback. Key features
include project isolation, CI/CD automation, integration with high-computing
infrastructures and in clinical workflows. MAIA supports real-world use cases
in medical imaging AI, with deployments in both academic and clinical
environments. By promoting collaborations and interoperability, MAIA aims to
accelerate the translation of AI research into impactful clinical solutions
while promoting reproducibility, transparency, and user-centered design. We
showcase the use of MAIA with different projects, both at KTH Royal Institute
of Technology and Karolinska University Hospital.

</details>


### [2] [Agent WARPP: Workflow Adherence via Runtime Parallel Personalization](https://arxiv.org/abs/2507.19543)
*Maria Emilia Mazzolenis,Ruirui Zhang*

Main category: cs.AI

TL;DR: WARPP框架通过运行时个性化和多Agent协同，提升了LLM在复杂工作流中的性能，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: LLM在面向任务对话系统中的应用日益增多，但难以处理涉及外部工具调用和依赖用户特定信息的长条件工作流。

Method: WARPP是一个免训练的模块化框架，结合多Agent编排和运行时个性化，动态修剪条件分支，减少推理开销，缩小运行时工具选择范围。采用并行架构，个性化Agent与特定领域Agent协同工作，动态调整执行路径。

Result: 在三个领域（银行、航班、医疗）的五个用户意图上评估，结果表明WARPP优于非个性化方法和ReAct基线，随着意图复杂性的增加，参数保真度和工具准确性提升越大，同时减少了平均token使用量。

Conclusion: WARPP框架通过结合多Agent编排和运行时个性化，提高了基于LLM的系统中工作流的遵循性，在参数保真度和工具准确性方面优于非个性化方法和ReAct基线，并减少了平均token使用量，无需额外训练。

Abstract: Large language models (LLMs) are increasingly applied in task-oriented
dialogue (TOD) systems but often struggle with long, conditional workflows that
involve external tool calls and depend on user-specific information. We present
Workflow Adherence via Runtime Parallel Personalization, or WARPP, a
training-free, modular framework that combines multi-agent orchestration with
runtime personalization to improve workflow adherence in LLM-based systems. By
dynamically pruning conditional branches based on user attributes, the
framework reduces reasoning overhead and narrows tool selection at runtime.
WARPP deploys a parallelized architecture where a dedicated Personalizer agent
operates alongside modular, domain-specific agents to dynamically tailor
execution paths in real time. The framework is evaluated across five
representative user intents of varying complexity within three domains:
banking, flights, and healthcare. Our evaluation leverages synthetic datasets
and LLM-powered simulated users to test scenarios with conditional
dependencies. Our results demonstrate that WARPP outperforms both the
non-personalized method and the ReAct baseline, achieving increasingly larger
gains in parameter fidelity and tool accuracy as intent complexity grows, while
also reducing average token usage, without any additional training.

</details>


### [3] [Hypergames: Modeling Misaligned Perceptions and Nested Beliefs for Multi-agent Systems](https://arxiv.org/abs/2507.19593)
*Vince Trencsenyi,Agnieszka Mensfelt,Kostas Stathis*

Main category: cs.AI

TL;DR: A systematic review shows hypergame theory enhances MAS realism but faces limitations in practical application, suggesting future research directions.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of classical game-theoretic models in real-world MAS characterized by uncertainty and misaligned perceptions, the authors explore the applicability of hypergame theory, which explicitly models agents' subjective perceptions of the strategic scenario.

Method: Systematic review of 44 studies on agent-compatible applications of hypergame theory, using an agent-based classification framework and agent-compatibility criteria.

Result: The review reveals prevailing tendencies and structural gaps in the application of hypergame theory to MAS, providing a roadmap for future research.

Conclusion: This paper systematically reviews agent-compatible applications of hypergame theory in multi-agent systems (MAS), analyzing 44 studies across various domains.  It identifies prevailing trends, such as the use of hierarchical and graph-based models, and highlights structural gaps, including limited adoption of HNF-based models and a lack of formal hypergame languages.

Abstract: Classical game-theoretic models typically assume rational agents, complete
information, and common knowledge of payoffs - assumptions that are often
violated in real-world MAS characterized by uncertainty, misaligned
perceptions, and nested beliefs. To overcome these limitations, researchers
have proposed extensions that incorporate models of cognitive constraints,
subjective beliefs, and heterogeneous reasoning. Among these, hypergame theory
extends the classical paradigm by explicitly modeling agents' subjective
perceptions of the strategic scenario, known as perceptual games, in which
agents may hold divergent beliefs about the structure, payoffs, or available
actions. We present a systematic review of agent-compatible applications of
hypergame theory, examining how its descriptive capabilities have been adapted
to dynamic and interactive MAS contexts. We analyze 44 selected studies from
cybersecurity, robotics, social simulation, communications, and general
game-theoretic modeling. Building on a formal introduction to hypergame theory
and its two major extensions - hierarchical hypergames and HNF - we develop
agent-compatibility criteria and an agent-based classification framework to
assess integration patterns and practical applicability. Our analysis reveals
prevailing tendencies, including the prevalence of hierarchical and graph-based
models in deceptive reasoning and the simplification of extensive theoretical
frameworks in practical applications. We identify structural gaps, including
the limited adoption of HNF-based models, the lack of formal hypergame
languages, and unexplored opportunities for modeling human-agent and
agent-agent misalignment. By synthesizing trends, challenges, and open research
directions, this review provides a new roadmap for applying hypergame theory to
enhance the realism and effectiveness of strategic modeling in dynamic
multi-agent environments.

</details>


### [4] [DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference](https://arxiv.org/abs/2507.19608)
*Jiawen Qi,Chang Gao,Zhaochun Ren,Qinyu Chen*

Main category: cs.AI

TL;DR: DeltaLLM是一个无需训练的框架，通过时间稀疏性提高大型语言模型在边缘设备上的推理效率，在多个任务上取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 现有的动态注意力剪枝方法不适用于资源受限的边缘设备，DeltaLLM旨在解决这一问题。

Method: DeltaLLM框架利用注意力模式的时间稀疏性，通过精度和内存感知的增量矩阵构建策略和上下文感知的混合注意力机制，提高了大型语言模型在边缘设备上的推理效率。

Result: 在BitNet和Llama模型上的实验结果表明，DeltaLLM能够在预填充阶段将注意力稀疏性提高到60%左右，在预填充和解码阶段提高到57%左右，同时保持或略微提高模型精度和F1分数。

Conclusion: DeltaLLM框架在边缘设备上高效部署大型语言模型方面取得了显著成果，无需微调即可与现有推理流程无缝集成。

Abstract: Deploying Large Language Models (LLMs) on edge devices remains challenging
due to their quadratically increasing computations with the sequence length.
Existing studies for dynamic attention pruning are designed for hardware with
massively parallel computation capabilities, such as GPUs or TPUs, and aim at
long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We
present DeltaLLM, a training-free framework that exploits temporal sparsity in
attention patterns to enable efficient LLM inference across both the prefilling
and decoding stages, on resource-constrained edge devices. DeltaLLM introduces
an accuracy- and memory-aware delta matrix construction strategy that
introduces temporal sparsity, and a context-aware hybrid attention mechanism
that combines full attention in a local context window with delta approximation
outside it to increase accuracy. We evaluate our framework on the
edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model
across diverse language tasks. The results show that on BitNet, our framework
increases the attention sparsity from 0% to 60% during the prefilling stage
with slight accuracy improvement on the WG task, and 0% to 57% across both the
prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97
on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity
during the prefilling stage and around 57% across both stages with negligible
accuracy drop. These results demonstrate that DeltaLLM offers a promising
solution for efficient edge deployment, requiring no fine-tuning and seamlessly
integrating with existing inference pipelines.

</details>


### [5] [Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges](https://arxiv.org/abs/2507.19672)
*Haoran Lu,Luyang Fang,Ruidong Zhang,Xinliang Li,Jiazhang Cai,Huimin Cheng,Lin Tang,Ziyu Liu,Zeliang Sun,Tao Wang,Yingchuan Zhang,Arif Hassan Zidan,Jinwen Xu,Jincheng Yu,Meizhi Yu,Hanqi Jiang,Xilin Gong,Weidi Luo,Bolun Sun,Yongkai Chen,Terry Ma,Shushan Wu,Yifan Zhou,Junhao Chen,Haotian Xiang,Jing Zhang,Afrar Jahin,Wei Ruan,Ke Deng,Yi Pan,Peilong Wang,Jiahui Li,Zhengliang Liu,Lu Zhang,Lin Zhao,Wei Liu,Dajiang Zhu,Xin Xing,Fei Dou,Wei Zhang,Chao Huang,Rongjie Liu,Mengrui Zhang,Yiwen Liu,Xiaoxiao Sun,Qin Lu,Zhen Xiang,Wenxuan Zhong,Tianming Liu,Ping Ma*

Main category: cs.AI

TL;DR: 该综述全面概述了大型语言模型对齐技术，并指出了该领域未来的挑战。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型与人类价值观和意图保持一致是一项关键挑战。

Method: 分析了不同范例中对齐方法的发展，对核心对齐目标之间的基本权衡进行了表征，讨论了包括直接偏好优化 (DPO)、宪法 AI、脑启发方法和对齐不确定性量化 (AUQ) 等现有技术。

Result: 综述表明，虽然监督微调能够实现基本的指令遵循，但基于偏好的方法为与细微的人类意图保持一致提供了更大的灵活性。

Conclusion: 该综述总结了大型语言模型（LLM）对齐的最新技术、训练协议和经验发现，指出了现有评估框架和基准数据集的局限性，并概述了对齐领域中未解决的问题。

Abstract: Due to the remarkable capabilities and growing impact of large language
models (LLMs), they have been deeply integrated into many aspects of society.
Thus, ensuring their alignment with human values and intentions has emerged as
a critical challenge. This survey provides a comprehensive overview of
practical alignment techniques, training protocols, and empirical findings in
LLM alignment. We analyze the development of alignment methods across diverse
paradigms, characterizing the fundamental trade-offs between core alignment
objectives. Our analysis shows that while supervised fine-tuning enables basic
instruction-following, preference-based methods offer more flexibility for
aligning with nuanced human intent. We discuss state-of-the-art techniques,
including Direct Preference Optimization (DPO), Constitutional AI,
brain-inspired methods, and alignment uncertainty quantification (AUQ),
highlighting their approaches to balancing quality and efficiency. We review
existing evaluation frameworks and benchmarking datasets, emphasizing
limitations such as reward misspecification, distributional robustness, and
scalable oversight. We summarize strategies adopted by leading AI labs to
illustrate the current state of practice. We conclude by outlining open
problems in oversight, value pluralism, robustness, and continuous alignment.
This survey aims to inform both researchers and practitioners navigating the
evolving landscape of LLM alignment.

</details>


### [6] [The wall confronting large language models](https://arxiv.org/abs/2507.19703)
*Peter V. Coveney,Sauro Succi*

Main category: cs.AI

TL;DR: 大型语言模型难以提高预测可靠性，其学习能力与准确性之间存在矛盾，这可能是其固有缺陷。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型的可靠性问题，以及其学习能力和准确性之间的矛盾。

Method: 分析大型语言模型的扩展定律及其对预测不确定性的影响。

Result: LLM的扩展定律限制了其提高预测不确定性的能力，导致错误累积、信息灾难和退化性AI行为。这种学习能力和准确性之间的矛盾可能是导致扩展组件值低的原因。

Conclusion: 大型语言模型（LLM）的扩展定律严重限制了其提高预测不确定性的能力，使其达到科学研究标准的可靠性是难以实现的。

Abstract: We show that the scaling laws which determine the performance of large
language models (LLMs) severely limit their ability to improve the uncertainty
of their predictions. As a result, raising their reliability to meet the
standards of scientific inquiry is intractable by any reasonable measure. We
argue that the very mechanism which fuels much of the learning power of LLMs,
namely the ability to generate non-Gaussian output distributions from Gaussian
input ones, might well be at the roots of their propensity to produce error
pileup, ensuing information catastrophes and degenerative AI behaviour. This
tension between learning and accuracy is a likely candidate mechanism
underlying the observed low values of the scaling components. It is
substantially compounded by the deluge of spurious correlations pointed out by
Calude and Longo which rapidly increase in any data set merely as a function of
its size, regardless of its nature. The fact that a degenerative AI pathway is
a very probable feature of the LLM landscape does not mean that it must
inevitably arise in all future AI research. Its avoidance, which we also
discuss in this paper, necessitates putting a much higher premium on insight
and understanding of the structural characteristics of the problems being
investigated.

</details>


### [7] [Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors](https://arxiv.org/abs/2507.19725)
*Leonardo Villalobos-Arias,Grant Forbes,Jianxun Wang,David L Roberts,Arnav Jhala*

Main category: cs.AI

TL;DR: 内在动机方法会改变强化学习智能体的行为，但广义奖励匹配方法有助于减轻这个问题。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习 (RL) 智能体在奖励稀疏游戏中难以学习的问题，并研究内在动机方法是否以及如何在何种程度上改变智能体行为。

Method: 实证评估三种内在动机技术对 MiniGrid 游戏环境的影响，并与广义奖励匹配 (GRM) 方法进行比较。

Result: IM 会增加初始奖励并改变智能体行为，GRM 在某些情况下可以减轻奖励作弊问题。

Conclusion: 这项研究实证评估了三种内在动机 (IM) 技术对 MiniGrid 游戏环境中智能体行为的影响，并使用广义奖励匹配 (GRM) 方法进行比较，结果表明 IM 会增加初始奖励并改变智能体行为，而 GRM 在某些情况下可以减轻奖励作弊问题。

Abstract: Games are challenging for Reinforcement Learning~(RL) agents due to their
reward-sparsity, as rewards are only obtainable after long sequences of
deliberate actions. Intrinsic Motivation~(IM) methods -- which introduce
exploration rewards -- are an effective solution to reward-sparsity. However,
IM also causes an issue known as `reward hacking' where the agent optimizes for
the new reward at the expense of properly playing the game. The larger problem
is that reward hacking itself is largely unknown; there is no answer to
whether, and to what extent, IM rewards change the behavior of RL agents. This
study takes a first step by empirically evaluating the impact on behavior of
three IM techniques on the MiniGrid game-like environment. We compare these IM
models with Generalized Reward Matching~(GRM), a method that can be used with
any intrinsic reward function to guarantee optimality. Our results suggest that
IM causes noticeable change by increasing the initial rewards, but also
altering the way the agent plays; and that GRM mitigated reward hacking in some
scenarios.

</details>


### [8] [HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision Healthcare](https://arxiv.org/abs/2507.19726)
*Yuzhang Xie,Xu Han,Ran Xu,Xiao Hu,Jiaying Lu,Carl Yang*

Main category: cs.AI

TL;DR: HypKG框架整合电子病历信息到知识图谱，生成情境化知识表示，提升医疗预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱缺乏对患者具体状态等重要情境信息的考虑，而电子病历提供了丰富的个人数据，可以作为补充。

Method: 该框架使用先进的实体链接技术连接通用知识图谱中的相关知识和电子病历中的患者信息，并利用超图模型对知识进行情境化处理，最终使用超图转换器进行联合学习。

Result: 实验表明，HypKG在医疗预测任务中取得了显著改进。

Conclusion: HypKG框架通过整合电子病历中的患者信息到知识图谱中，生成情境化的知识表示，从而提高医疗预测的准确性，并在多个评估指标上取得显著改进。

Abstract: Knowledge graphs (KGs) are important products of the semantic web, which are
widely used in various application domains. Healthcare is one of such domains
where KGs are intensively used, due to the high requirement for knowledge
accuracy and interconnected nature of healthcare data. However, KGs storing
general factual information often lack the ability to account for important
contexts of the knowledge such as the status of specific patients, which are
crucial in precision healthcare. Meanwhile, electronic health records (EHRs)
provide rich personal data, including various diagnoses and medications, which
provide natural contexts for general KGs. In this paper, we propose HypKG, a
framework that integrates patient information from EHRs into KGs to generate
contextualized knowledge representations for accurate healthcare predictions.
Using advanced entity-linking techniques, we connect relevant knowledge from
general KGs with patient information from EHRs, and then utilize a hypergraph
model to "contextualize" the knowledge with the patient information. Finally,
we employ hypergraph transformers guided by downstream prediction tasks to
jointly learn proper contextualized representations for both KGs and patients,
fully leveraging existing knowledge in KGs and patient contexts in EHRs. In
experiments using a large biomedical KG and two real-world EHR datasets, HypKG
demonstrates significant improvements in healthcare prediction tasks across
multiple evaluation metrics. Additionally, by integrating external contexts,
HypKG can learn to adjust the representations of entities and relations in KG,
potentially improving the quality and real-world utility of knowledge.

</details>


### [9] [Integrating Activity Predictions in Knowledge Graphs](https://arxiv.org/abs/2507.19733)
*Alec Scully,Cameron Stockton,Forrest Hare*

Main category: cs.AI

TL;DR: 利用本体结构知识图谱和马尔科夫链预测未来事件，并改进概率论模型。


<details>
  <summary>Details</summary>
Motivation: 现有预测模型存在局限性，作者旨在利用本体结构知识图谱提高预测精度和效率。

Method: 利用BFO和CCO本体框架构建知识图谱，组织和检索数据（例如渔船的移动轨迹），构建马尔科夫链模型进行预测，并提出了一种新的概率观，将概率视为关于过程概要的陈述。

Result: 提出了一种基于本体结构知识图谱和马尔科夫链的预测模型，并对概率论的本体模型进行了改进。

Conclusion: 本文论证了本体结构知识图谱在生成未来事件预测中的关键作用，并提出了一种基于马尔科夫链和改进概率论的预测模型，最终将计算结果集成回知识图谱。

Abstract: We argue that ontology-structured knowledge graphs can play a crucial role in
generating predictions about future events. By leveraging the semantic
framework provided by Basic Formal Ontology (BFO) and Common Core Ontologies
(CCO), we demonstrate how data such as the movements of a fishing vessel can be
organized in and retrieved from a knowledge graph. These query results are then
used to create Markov chain models, allowing us to predict future states based
on the vessel's history. To fully support this process, we introduce the term
`spatiotemporal instant' to complete the necessary structural semantics.
Additionally, we critique the prevailing ontological model of probability,
which conflates probability with likelihood and relies on the problematic
concept of modal measurements: measurements of future entities. We propose an
alternative view, where probabilities are treated as being about process
profiles, which better captures the dynamics of real world phenomena. Finally,
we demonstrate how our Markov chain based probability calculations can be
seamlessly integrated back into the knowledge graph, enabling further analysis
and decision-making. Keywords: predictive analytics, ontology, Markov chains,
probability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL.

</details>


### [10] [Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)](https://arxiv.org/abs/2507.19749)
*Lin Ren,Guohui Xiao,Guilin Qi,Yishuai Geng,Haohan Xue*

Main category: cs.AI

TL;DR: LLM在ASP求解方面能力有限，尤其在回答集计算方面，需要改进。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLM在ASP中的能力存在局限性，缺乏专门为ASP求解设计的基准测试。

Method: 构建了一个全面的ASP基准测试ASPBench，包括三个特定任务：ASP蕴涵、回答集验证和回答集计算，并对14个最先进的LLM进行了评估。

Result: 评估结果显示，LLM在较简单的ASP蕴涵和回答集验证任务上表现较好，但在回答集计算方面表现较差。

Conclusion: 大型语言模型(LLM)在回答集编程(ASP)求解方面能力有限，尤其在核心任务——回答集计算方面表现较差，这表明需要新的方法更有效地整合符号推理能力。

Abstract: Answer Set Programming (ASP) is a powerful paradigm for non-monotonic
reasoning. Recently, large language models (LLMs) have demonstrated promising
capabilities in logical reasoning. Despite this potential, current evaluations
of LLM capabilities in ASP are often limited. Existing works normally employ
overly simplified ASP programs, do not support negation, disjunction, or
multiple answer sets. Furthermore, there is a lack of benchmarks that introduce
tasks specifically designed for ASP solving. To bridge this gap, we introduce
ASPBench, a comprehensive ASP benchmark, including three ASP specific tasks:
ASP entailment, answer set verification, and answer set computation. Our
extensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs,
including \emph{deepseek-r1}, \emph{o4-mini}, and
\emph{gemini-2.5-flash-thinking}, perform relatively well on the first two
simpler tasks, they struggle with answer set computation, which is the core of
ASP solving. These findings offer insights into the current limitations of LLMs
in ASP solving. This highlights the need for new approaches that integrate
symbolic reasoning capabilities more effectively. The code and dataset are
available at https://github.com/HomuraT/ASPBench.

</details>


### [11] [Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation](https://arxiv.org/abs/2507.19788)
*Rifny Rachman,Josh Tingey,Richard Allmendinger,Pradyumn Shukla,Wei Pan*

Main category: cs.AI

TL;DR: 该研究提出一种基于马尔可夫决策过程的多目标多级供应链优化模型，并通过多目标强化学习方法进行评估，结果表明该方法在复杂环境下具有优越的性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在非平稳市场中，考虑经济、环境和社会因素，优化多目标、多级供应链的模型。

Method: 该研究使用多目标强化学习 (RL) 方法评估模型，并将其与改进的单目标 RL 算法和基于多目标进化算法 (MOEA) 的方法进行基准测试。

Result: 该模型在复杂环境中实现了比基于MOEA的方法高75%的超体积，并产生比改进的单目标RL方法密度高约11倍的解决方案，确保了稳定的生产和库存水平，同时最大限度地减少了需求损失。

Conclusion: 该研究开发了一个基于马尔可夫决策过程的通用多目标、多级供应链优化模型，该模型考虑了经济、环境和社会因素，并在非平稳市场中进行评估。结果表明，该模型在优化性、多样性和密度之间取得了最均衡的权衡。

Abstract: This study develops a generalised multi-objective, multi-echelon supply chain
optimisation model with non-stationary markets based on a Markov decision
process, incorporating economic, environmental, and social considerations. The
model is evaluated using a multi-objective reinforcement learning (RL) method,
benchmarked against an originally single-objective RL algorithm modified with
weighted sum using predefined weights, and a multi-objective evolutionary
algorithm (MOEA)-based approach. We conduct experiments on varying network
complexities, mimicking typical real-world challenges using a customisable
simulator. The model determines production and delivery quantities across
supply chain routes to achieve near-optimal trade-offs between competing
objectives, approximating Pareto front sets. The results demonstrate that the
primary approach provides the most balanced trade-off between optimality,
diversity, and density, further enhanced with a shared experience buffer that
allows knowledge transfer among policies. In complex settings, it achieves up
to 75\% higher hypervolume than the MOEA-based method and generates solutions
that are approximately eleven times denser, signifying better robustness, than
those produced by the modified single-objective RL method. Moreover, it ensures
stable production and inventory levels while minimising demand loss.

</details>


### [12] [Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation](https://arxiv.org/abs/2507.19882)
*Xinshu Li,Ruoyu Wang,Erdun Gao,Mingming Gong,Lina Yao*

Main category: cs.AI

TL;DR: DiCap模型利用扩散过程生成因果不变提示，提升了提示学习的泛化能力，在多个任务上取得了优异结果。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法缺乏理论基础，难以获得因果不变的提示，限制了其泛化能力。

Method: 基于扩散过程的逆反事实提示学习框架，迭代采样因果模型的边际分布和条件分布的梯度，生成满足最小充分性准则的逆反事实。

Result: DiCap模型通过生成满足最小充分性准则的逆反事实，并利用对比学习框架，提取与数据因果特征精确对齐的提示，从而提高了模型的泛化能力。

Conclusion: DiCap模型在图像分类、图文检索和视觉问答等任务中表现出色，尤其是在未见类别上具有显著优势。

Abstract: Prompt learning has garnered attention for its efficiency over traditional
model training and fine-tuning. However, existing methods, constrained by
inadequate theoretical foundations, encounter difficulties in achieving
causally invariant prompts, ultimately falling short of capturing robust
features that generalize effectively across categories. To address these
challenges, we introduce the $\textit{\textbf{DiCap}}$ model, a theoretically
grounded $\textbf{Di}$ffusion-based $\textbf{C}$ounterf$\textbf{a}$ctual
$\textbf{p}$rompt learning framework, which leverages a diffusion process to
iteratively sample gradients from the marginal and conditional distributions of
the causal model, guiding the generation of counterfactuals that satisfy the
minimal sufficiency criterion. Grounded in rigorous theoretical derivations,
this approach guarantees the identifiability of counterfactual outcomes while
imposing strict bounds on estimation errors. We further employ a contrastive
learning framework that leverages the generated counterfactuals, thereby
enabling the refined extraction of prompts that are precisely aligned with the
causal features of the data. Extensive experimental results demonstrate that
our method performs excellently across tasks such as image classification,
image-text retrieval, and visual question answering, with particularly strong
advantages in unseen categories.

</details>


### [13] [What Does 'Human-Centred AI' Mean?](https://arxiv.org/abs/2507.19960)
*Olivia Guest*

Main category: cs.AI

TL;DR: AI与人类认知息息相关，忽视认知会阻碍以人为中心的AI发展。


<details>
  <summary>Details</summary>
Motivation: 澄清以人为中心的AI的真正含义，避免对AI的迷信。

Method: 通过比较技术与认知的例子（算盘与心算、闹钟与叫醒服务、相机与视觉、血汗工厂与裁缝）分析社会技术关系，将其分为三种类型：置换（有害）、增强（有益）和替代（中性）。

Result: 将AI定义为技术与人类之间的关系，其中人工制品可在不同程度上执行人类认知劳动。

Conclusion: 所有AI都与人类认知有关，忽视认知会导致扭曲和对人类中心AI工程的限制。

Abstract: While it seems sensible that human-centred artificial intelligence (AI) means
centring "human behaviour and experience," it cannot be any other way. AI, I
argue, is usefully seen as a relationship between technology and humans where
it appears that artifacts can perform, to a greater or lesser extent, human
cognitive labour. This is evinced using examples that juxtapose technology with
cognition, inter alia: abacus versus mental arithmetic; alarm clock versus
knocker-upper; camera versus vision; and sweatshop versus tailor. Using novel
definitions and analyses, sociotechnical relationships can be analysed into
varying types of: displacement (harmful), enhancement (beneficial), and/or
replacement (neutral) of human cognitive labour. Ultimately, all AI implicates
human cognition; no matter what. Obfuscation of cognition in the AI context --
from clocks to artificial neural networks -- results in distortion, in slowing
critical engagement, perverting cognitive science, and indeed in limiting our
ability to truly centre humans and humanity in the engineering of AI systems.
To even begin to de-fetishise AI, we must look the human-in-the-loop in the
eyes.

</details>


### [14] [Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization](https://arxiv.org/abs/2507.19973)
*Ebrahim Rasromani,Stella K. Kang,Yanqi Xu,Beisong Liu,Garvit Luhadia,Wan Fung Chui,Felicia L. Pasadyn,Yu Chih Hung,Julie Y. An,Edwin Mathieu,Zehui Gu,Carlos Fernandez-Granda,Ammar A. Javed,Greg D. Sacks,Tamas Gonda,Chenchan Huang,Yiqiu Shen*

Main category: cs.AI

TL;DR: 开源LLM模型在辅助胰腺囊性病变(PCL)特征提取和风险分级方面表现出色，能达到与GPT-4o相当的水平，极大提高了科研效率。


<details>
  <summary>Details</summary>
Motivation: 人工提取放射学报告中的胰腺囊性病变(PCL)特征费时费力，限制了推进PCL研究所需的大规模研究。

Method: 使用QLoRA微调两个开源LLM，并使用GPT-4o生成的思维链数据进行训练。将提取的特征根据2017年ACR白皮书中的机构指南映射到风险类别。

Result: 微调后的LLaMA和DeepSeek模型在特征提取准确率和风险分类F1分数上均取得了显著提升，与GPT-4o的结果非常接近。放射科医师与模型的一致性也达到了很高的水平。

Conclusion: 微调后的开源LLM结合思维链监督能够准确、可解释且高效地进行表型分析，从而支持大规模PCL研究，其性能可与GPT-4o媲美。

Abstract: Background: Manual extraction of pancreatic cystic lesion (PCL) features from
radiology reports is labor-intensive, limiting large-scale studies needed to
advance PCL research. Purpose: To develop and evaluate large language models
(LLMs) that automatically extract PCL features from MRI/CT reports and assign
risk categories based on guidelines. Materials and Methods: We curated a
training dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134
patients that described PCLs. Labels were generated by GPT-4o using
chain-of-thought (CoT) prompting to extract PCL and main pancreatic duct
features. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated
CoT data. Features were mapped to risk categories per institutional guideline
based on the 2017 ACR White Paper. Evaluation was performed on 285 held-out
human-annotated reports. Model outputs for 100 cases were independently
reviewed by three radiologists. Feature extraction was evaluated using exact
match accuracy, risk categorization with macro-averaged F1 score, and
radiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning
improved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79%
to 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved
(LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no
statistically significant differences. Radiologist inter-reader agreement was
high (Fleiss' Kappa = 0.888) and showed no statistically significant difference
with the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT
(Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels
on par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT
supervision enable accurate, interpretable, and efficient phenotyping for
large-scale PCL research, achieving performance comparable to GPT-4o.

</details>


### [15] [Digital Twin Channel-Enabled Online Resource Allocation for 6G: Principle, Architecture and Application](https://arxiv.org/abs/2507.19974)
*Tongjie Li,Jianhua Zhang,Li Yu,Yuxiang Zhang,Yunlong Cai,Fan Xu,Guangyi Liu*

Main category: cs.AI

TL;DR: 利用数字孪生信道预测 CSI，并用博弈论算法进行资源分配，提高了 6G 网络的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于统计建模的方法在动态环境下难以达到最优性能，并且获取实时 CSI 需要大量的导频开销。

Method: 提出了一种基于数字孪生信道 (DTC) 的在线优化框架，利用 DTC 预测 CSI，并使用轻量级博弈论算法进行在线资源分配。

Result: 仿真结果表明，该方法比基于导频的理想 CSI 方案吞吐量提高了 11.5%。

Conclusion: 提出了一种基于数字孪生信道 (DTC) 的在线优化框架，用于解决 6G 网络中资源分配的挑战，该框架利用 DTC 预测 CSI，并使用轻量级博弈论算法进行在线资源分配，仿真结果表明该方法比基于导频的理想 CSI 方案吞吐量提高了 11.5%。

Abstract: Emerging applications such as holographic communication, autonomous driving,
and the industrial Internet of Things impose stringent requirements on
flexible, low-latency, and reliable resource allocation in 6G networks.
Conventional methods, which rely on statistical modeling, have proven effective
in general contexts but may fail to achieve optimal performance in specific and
dynamic environments. Furthermore, acquiring real-time channel state
information (CSI) typically requires excessive pilot overhead. To address these
challenges, a digital twin channel (DTC)-enabled online optimization framework
is proposed, in which DTC is employed to predict CSI based on environmental
sensing. The predicted CSI is then utilized by lightweight game-theoretic
algorithms to perform online resource allocation in a timely and efficient
manner. Simulation results based on a digital replica of a realistic industrial
workshop demonstrate that the proposed method achieves throughput improvements
of up to 11.5\% compared with pilot-based ideal CSI schemes, validating its
effectiveness for scalable, low-overhead, and environment-aware communication
in future 6G networks.

</details>


### [16] [Matching Game Preferences Through Dialogical Large Language Models: A Perspective](https://arxiv.org/abs/2507.20000)
*Renaud Fabre,Daniel Egret,Patrice Bellot*

Main category: cs.AI

TL;DR: 该论文探讨了如何通过结合大型语言模型和网络系统，创建更透明和可信赖的人工智能系统，以更好地理解和满足人类需求。


<details>
  <summary>Details</summary>
Motivation: 为了使人工智能推理过程透明和可追溯，让人们能够看到并理解人工智能如何得出结论，从而提高人工智能系统的透明度和可信度。

Method: 提出了一种概念框架“通过对话式大型语言模型匹配游戏偏好 (D-LLM)”，该框架包含三个主要组件：分析不同搜索体验并指导性能的推理过程；识别用户偏好模式的分类系统；以及帮助人类解决冲突信息的对话方法。

Result: 提出了一个概念框架，该框架可以使人工智能推理过程透明和可追溯，并允许个性化LLM。

Conclusion: 该论文探讨了将大型语言模型 (LLM) 与 GRAPHYP 网络系统相结合以更好地理解人类对话和偏好的潜力，提出了一种名为“通过对话式大型语言模型匹配游戏偏好 (D-LLM)”的概念框架，旨在创建可解释的 AI 系统，使用户能够检查、理解和结合影响 AI 响应的不同人类偏好。

Abstract: This perspective paper explores the future potential of "conversational
intelligence" by examining how Large Language Models (LLMs) could be combined
with GRAPHYP's network system to better understand human conversations and
preferences. Using recent research and case studies, we propose a conceptual
framework that could make AI rea-soning transparent and traceable, allowing
humans to see and understand how AI reaches its conclusions. We present the
conceptual perspective of "Matching Game Preferences through Dialogical Large
Language Models (D-LLMs)," a proposed system that would allow multiple users to
share their different preferences through structured conversations. This
approach envisions personalizing LLMs by embedding individual user preferences
directly into how the model makes decisions. The proposed D-LLM framework would
require three main components: (1) reasoning processes that could analyze
different search experiences and guide performance, (2) classification systems
that would identify user preference patterns, and (3) dialogue approaches that
could help humans resolve conflicting information. This perspective framework
aims to create an interpretable AI system where users could examine,
understand, and combine the different human preferences that influence AI
responses, detected through GRAPHYP's search experience networks. The goal of
this perspective is to envision AI systems that would not only provide answers
but also show users how those answers were reached, making artificial
intelligence more transparent and trustworthy for human decision-making.

</details>


### [17] [Finding Personalized Good-Enough Solutions to Unsatisfiable Stable Roommates Problems](https://arxiv.org/abs/2507.20010)
*Müge Fidan,Esra Erdem*

Main category: cs.AI

TL;DR: 本文提出一种考虑社交网络的个性化稳定室友匹配方法，解决了稳定室友问题求解困难的问题。


<details>
  <summary>Details</summary>
Motivation: 由于稳定室友问题并不总是有解，因此研究如何计算“足够好”的匹配。

Method: 提出了一种考虑代理偏好和社交网络的个性化匹配方法。

Result: 提出了一种生成稳定室友问题个性化解决方案的方法，并通过实例和实证评估验证了其有效性。

Conclusion: 本文研究了稳定室友问题的个性化解决方案，考虑了代理的偏好和社交网络，并通过实例和实证评估说明了该方法的有效性。

Abstract: The Stable Roommates problems are characterized by the preferences of agents
over other agents as roommates. A solution is a partition of the agents into
pairs that are acceptable to each other (i.e., they are in the preference lists
of each other), and the matching is stable (i.e., there do not exist any two
agents who prefer each other to their roommates, and thus block the matching).
Motivated by real-world applications, and considering that stable roommates
problems do not always have solutions, we continue our studies to compute
"good-enough" matchings. In addition to the agents' habits and habitual
preferences, we consider their networks of preferred friends, and introduce a
method to generate personalized solutions to stable roommates problems. We
illustrate the usefulness of our method with examples and empirical
evaluations.

</details>
