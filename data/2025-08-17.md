<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions](https://arxiv.org/abs/2508.10047)
*Ziyang Xiao,Jingrong Xie,Lilin Xu,Shisi Guan,Jingyan Zhu,Xiongwei Han,Xiaojin Fu,WingYin Yu,Han Wu,Wei Shi,Qingcan Kang,Jiahui Duan,Tao Zhong,Mingxuan Yuan,Jia Zeng,Yuan Wang,Gang Chen,Dongxiang Zhang*

Main category: cs.AI

TL;DR: 使用LLM自动化数学建模的综述，发现并修正了现有基准数据集的错误，创建了新的排行榜和资源门户。


<details>
  <summary>Details</summary>
Motivation: 优化建模在解决实际问题中具有重要作用，但需要专业知识。LLM 的出现为自动化建模过程提供了新的机会。

Method: 对现有文献进行综述，分析基准数据集的质量，构建新的排行榜和在线资源门户。

Result: 对 LLM 自动化数学建模技术进行了全面综述，发现了基准数据集的高错误率，并构建了新的、更准确的排行榜和在线资源门户。

Conclusion: 这篇综述文章回顾了使用大型语言模型 (LLM) 自动化数学建模的最新进展，指出了现有基准数据集存在的高错误率，并构建了一个新的排行榜和在线资源门户。

Abstract: By virtue of its great utility in solving real-world problems, optimization
modeling has been widely employed for optimal decision-making across various
sectors, but it requires substantial expertise from operations research
professionals. With the advent of large language models (LLMs), new
opportunities have emerged to automate the procedure of mathematical modeling.
This survey presents a comprehensive and timely review of recent advancements
that cover the entire technical stack, including data synthesis and fine-tuning
for the base model, inference frameworks, benchmark datasets, and performance
evaluation. In addition, we conducted an in-depth analysis on the quality of
benchmark datasets, which was found to have a surprisingly high error rate. We
cleaned the datasets and constructed a new leaderboard with fair performance
evaluation in terms of base LLM model and datasets. We also build an online
portal that integrates resources of cleaned datasets, code and paper repository
to benefit the community. Finally, we identify limitations in current
methodologies and outline future research opportunities.

</details>


### [2] [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
*Sattvik Sahai,Prasoon Goyal,Michael Johnston,Anna Gottardi,Yao Lu,Lucy Hu,Luke Dai,Shaohua Liu,Samyuth Sagi,Hangjie Shi,Desheng Zhang,Lavina Vaz,Leslie Ball,Maureen Murray,Rahul Gupta,Shankar Ananthakrishna*

Main category: cs.AI

TL;DR: Nova AI挑战赛推动AI软件开发安全技术进步


<details>
  <summary>Details</summary>
Motivation: 解决AI软件开发中安全挑战。

Method: 通过对抗赛的形式，红队与AI编码助手进行多轮对话，测试其安全性。

Result: 参赛队伍开发出先进技术，提高了AI安全水平。

Conclusion: 亚马逊Nova AI挑战赛促进了AI软件开发安全性的研究，参赛队伍开发了先进技术，包括基于推理的安全对齐、鲁棒模型防护、多轮越狱和高效的大型语言模型探测等。

Abstract: AI systems for software development are rapidly gaining prominence, yet
significant challenges remain in ensuring their safety. To address this, Amazon
launched the Trusted AI track of the Amazon Nova AI Challenge, a global
competition among 10 university teams to drive advances in secure AI. In the
challenge, five teams focus on developing automated red teaming bots, while the
other five create safe AI assistants. This challenge provides teams with a
unique platform to evaluate automated red-teaming and safety alignment methods
through head-to-head adversarial tournaments where red teams have multi-turn
conversations with the competing AI coding assistants to test their safety
alignment. Along with this, the challenge provides teams with a feed of high
quality annotated data to fuel iterative improvement. Throughout the challenge,
teams developed state-of-the-art techniques, introducing novel approaches in
reasoning-based safety alignment, robust model guardrails, multi-turn
jail-breaking, and efficient probing of large language models (LLMs). To
support these efforts, the Amazon Nova AI Challenge team made substantial
scientific and engineering investments, including building a custom baseline
coding specialist model for the challenge from scratch, developing a tournament
orchestration service, and creating an evaluation harness. This paper outlines
the advancements made by university teams and the Amazon Nova AI Challenge team
in addressing the safety challenges of AI for software development,
highlighting this collaborative effort to raise the bar for AI safety.

</details>


### [3] [MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection](https://arxiv.org/abs/2508.10143)
*Alexandru-Andrei Avram,Adrian Groza,Alexandru Lecu*

Main category: cs.AI

TL;DR: 多智能体系统利用关系抽取，准确高效地检测新闻中的虚假信息。


<details>
  <summary>Details</summary>
Motivation: 解决数字平台上虚假信息大量传播对信息完整性造成的重大挑战。

Method: 该系统结合了机器学习、维基百科知识检查、连贯性检测和网络数据分析四个智能体，并通过模型上下文协议 (MCP) 进行协调。

Result: 多智能体集成系统准确率达95.3%，F1分数为0.964，加权聚合方法优于算法阈值优化。

Conclusion: 该多智能体系统在新闻标题和简短文本片段中检测虚假信息的准确率达到95.3%，F1分数为0.964，显著优于单个智能体和传统方法。

Abstract: The large spread of disinformation across digital platforms creates
significant challenges to information integrity. This paper presents a
multi-agent system that uses relation extraction to detect disinformation in
news articles, focusing on titles and short text snippets. The proposed Agentic
AI system combines four agents: (i) a machine learning agent (logistic
regression), (ii) a Wikipedia knowledge check agent (which relies on named
entity recognition), (iii) a coherence detection agent (using LLM prompt
engineering), and (iv) a web-scraped data analyzer that extracts relational
triplets for fact checking. The system is orchestrated via the Model Context
Protocol (MCP), offering shared context and live learning across components.
Results demonstrate that the multi-agent ensemble achieves 95.3% accuracy with
an F1 score of 0.964, significantly outperforming individual agents and
traditional approaches. The weighted aggregation method, mathematically derived
from individual agent misclassification rates, proves superior to algorithmic
threshold optimization. The modular architecture makes the system easily
scalable, while also maintaining details of the decision processes.

</details>


### [4] [Agentic AI Frameworks: Architectures, Protocols, and Design Challenges](https://arxiv.org/abs/2508.10146)
*Hana Derouiche,Zaki Brahmi,Haithem Mazeni*

Main category: cs.AI

TL;DR: 本文综述了自主智能代理框架，分析了其架构、通信机制等，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的出现推动了自主智能代理领域的发展，该领域中的智能代理表现出目标导向的自主性、上下文推理和动态多代理协调。

Method: 本文对自主智能代理框架进行了系统的回顾和比较分析，并对代理通信协议（如CNP、A2A、ANP和Agora）进行了深入分析。

Result: 本文建立了自主智能代理系统的基础分类法，并提出了增强可扩展性、鲁棒性和互操作性的未来研究方向。

Conclusion: 本文对领先的自主智能代理框架（包括CrewAI、LangGraph、AutoGen、Semantic Kernel、Agno、Google ADK和MetaGPT）进行了系统的回顾和比较分析，评估了它们的架构原则、通信机制、内存管理、安全防护和与面向服务的计算范例的一致性，并确定了该领域的关键限制、新兴趋势和开放性挑战。

Abstract: The emergence of Large Language Models (LLMs) has ushered in a transformative
paradigm in artificial intelligence, Agentic AI, where intelligent agents
exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent
coordination. This paper provides a systematic review and comparative analysis
of leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen,
Semantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural
principles, communication mechanisms, memory management, safety guardrails, and
alignment with service-oriented computing paradigms. Furthermore, we identify
key limitations, emerging trends, and open challenges in the field. To address
the issue of agent communication, we conduct an in-depth analysis of protocols
such as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network
Protocol (ANP), and Agora. Our findings not only establish a foundational
taxonomy for Agentic AI systems but also propose future research directions to
enhance scalability, robustness, and interoperability. This work serves as a
comprehensive reference for researchers and practitioners working to advance
the next generation of autonomous AI systems.

</details>


### [5] [Improving and Evaluating Open Deep Research Agents](https://arxiv.org/abs/2508.10152)
*Doaa Allabadi,Kyle Bradbury,Jordan M. Malof*

Main category: cs.AI

TL;DR: 开源深度研究代理ODR改进后取得显著进展，在基准测试中表现优于其他闭源系统。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究代理大多为闭源系统，缺乏可供学术界研究和改进的开源系统。

Method: 对现有开源深度研究代理ODR进行改进，并在BrowseComp-Small基准测试集上进行评估。

Result: 改进后的ODR+模型在BrowseComp-Small基准测试中取得了10%的成功率，超过了其他闭源系统。

Conclusion: 本文通过改进开源深度研究代理ODR，使其在BrowseComp-Small基准测试中取得了10%的成功率，优于其他闭源系统。

Abstract: We focus here on Deep Research Agents (DRAs), which are systems that can take
a natural language prompt from a user, and then autonomously search for, and
utilize, internet-based content to address the prompt. Recent DRAs have
demonstrated impressive capabilities on public benchmarks however, recent
research largely involves proprietary closed-source systems. At the time of
this work, we only found one open-source DRA, termed Open Deep Research (ODR).
In this work we adapt the challenging recent BrowseComp benchmark to compare
ODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small),
comprising a subset of BrowseComp, as a more computationally-tractable DRA
benchmark for academic labs. We benchmark ODR and two other proprietary systems
on BC-Small: one system from Anthropic and one system from Google. We find that
all three systems achieve 0% accuracy on the test set of 60 questions. We
introduce three strategic improvements to ODR, resulting in the ODR+ model,
which achieves a state-of-the-art 10% success rate on BC-Small among both
closed-source and open-source systems. We report ablation studies indicating
that all three of our improvements contributed to the success of ODR+.

</details>


### [6] [Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization](https://arxiv.org/abs/2508.10164)
*Bin Hong,Jiayu Liu,Zhenya Huang,Kai Zhang,Mengdi Zhang*

Main category: cs.AI

TL;DR: 提出长度控制偏好优化 (LCPO) 方法，有效缩短大型推理模型输出长度，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型 (LRM) 的冗长输出增加了计算成本并可能导致过度思考，本文旨在研究减少 LRM 生成长度的有效方法，以平衡推理有效性和效率。

Method: 分析生成路径分布和难度估计，过滤生成的轨迹，并基于 Bradley-Terry 损失框架分析不同偏好优化方法的目标收敛行为，最终提出长度控制偏好优化 (LCPO) 方法。

Result: LCPO 方法显著减少了平均输出长度 (超过 50%)，同时保持了推理性能。

Conclusion: 提出了一种名为长度控制偏好优化 (LCPO) 的方法，该方法通过分析生成路径分布和难度估计来过滤生成的轨迹，有效平衡了 NLL 损失相关的隐式奖励，在多个基准测试中将平均输出长度减少了 50% 以上，同时保持了推理性能。

Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated strong
performance on complex tasks through long Chain-of-Thought (CoT) reasoning.
However, their lengthy outputs increase computational costs and may lead to
overthinking, raising challenges in balancing reasoning effectiveness and
efficiency. Current methods for efficient reasoning often compromise reasoning
quality or require extensive resources. This paper investigates efficient
methods to reduce the generation length of LRMs. We analyze generation path
distributions and filter generated trajectories through difficulty estimation.
Subsequently, we analyze the convergence behaviors of the objectives of various
preference optimization methods under a Bradley-Terry loss based framework.
Based on the analysis, we propose Length Controlled Preference Optimization
(LCPO) that directly balances the implicit reward related to NLL loss. LCPO can
effectively learn length preference with limited data and training. Extensive
experiments demonstrate that our approach significantly reduces the average
output length by over 50\% across multiple benchmarks while maintaining the
reasoning performance. Our work highlights the potential for computationally
efficient approaches in guiding LRMs toward efficient reasoning.

</details>


### [7] [KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems](https://arxiv.org/abs/2508.10177)
*Stepan Kulibaba,Artem Dzhalilov,Roman Pakhomov,Oleg Svidchenko,Alexander Gasnikov,Aleksei Shpilman*

Main category: cs.AI

TL;DR: KompeteAI通过动态探索、解决方案合并和RAG，解决了LLM-based AutoML系统的瓶颈问题，并在基准测试中取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM-based AutoML系统存在探索策略受限（单次方法缺乏多样性，MCTS方法无法重组强部分解决方案）和执行瓶颈（代码验证周期长）的问题。

Method: KompeteAI引入了一个合并阶段来组合顶级候选方案，并通过整合检索增强生成（RAG）来扩展假设空间，利用Kaggle笔记本和arXiv论文中的真实策略。此外，它采用预测评分模型和加速调试方法来加快评估速度。

Result: KompeteAI在主要的AutoML基准MLE-Bench上平均超越领先方法3%，在提出的Kompete-bench上也取得了最先进的结果，pipeline评估速度提升了6.9倍。

Conclusion: KompeteAI，一个新型AutoML框架，通过动态解决方案空间探索克服了基于LLM的AutoML系统中探索策略受限和执行瓶颈的问题，在MLE-Bench和提出的Kompete-bench基准测试中均取得了最先进的结果。

Abstract: Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive
capabilities but face significant limitations such as constrained exploration
strategies and a severe execution bottleneck. Exploration is hindered by
one-shot methods lacking diversity and Monte Carlo Tree Search (MCTS)
approaches that fail to recombine strong partial solutions. The execution
bottleneck arises from lengthy code validation cycles that stifle iterative
refinement. To overcome these challenges, we introduce KompeteAI, a novel
AutoML framework with dynamic solution space exploration. Unlike previous MCTS
methods that treat ideas in isolation, KompeteAI introduces a merging stage
that composes top candidates. We further expand the hypothesis space by
integrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle
notebooks and arXiv papers to incorporate real-world strategies. KompeteAI also
addresses the execution bottleneck via a predictive scoring model and an
accelerated debugging method, assessing solution potential using early stage
metrics to avoid costly full-code execution. This approach accelerates pipeline
evaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent,
AIDE, and Ml-Master) by an average of 3\% on the primary AutoML benchmark,
MLE-Bench. Additionally, we propose Kompete-bench to address limitations in
MLE-Bench, where KompeteAI also achieves state-of-the-art results

</details>


### [8] [Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence](https://arxiv.org/abs/2508.10241)
*Mark Zilberman*

Main category: cs.AI

TL;DR: 事件熵势概念可增强AI中的不确定性量化、决策和可解释性。


<details>
  <summary>Details</summary>
Motivation: 提升AI系统中不确定性建模，增强可解释性。

Method: 将物理学中事件熵势的概念应用于AI，引入了一种事件中心的度量方法，用于捕获离散事件对未来不确定性的影响。

Result: 形式化了事件熵势的定义，并在强化学习、贝叶斯推理和异常检测中进行了应用，讨论了其在复杂AI模型中的计算。

Conclusion: 该工作展示了事件熵势概念如何增强AI中的不确定性量化、决策和可解释性，并将其应用于策略评估、内在奖励设计、可解释AI和异常检测。

Abstract: This work demonstrates how the concept of the entropic potential of events --
a parameter quantifying the influence of discrete events on the expected future
entropy of a system -- can enhance uncertainty quantification, decision-making,
and interpretability in artificial intelligence (AI). Building on its original
formulation in physics, the framework is adapted for AI by introducing an
event-centric measure that captures how actions, observations, or other
discrete occurrences impact uncertainty at future time horizons. Both the
original and AI-adjusted definitions of entropic potential are formalized, with
the latter emphasizing conditional expectations to account for counterfactual
scenarios. Applications are explored in policy evaluation, intrinsic reward
design, explainable AI, and anomaly detection, highlighting the metric's
potential to unify and strengthen uncertainty modeling in intelligent systems.
Conceptual examples illustrate its use in reinforcement learning, Bayesian
inference, and anomaly detection, while practical considerations for
computation in complex AI models are discussed. The entropic potential
framework offers a theoretically grounded, interpretable, and versatile
approach to managing uncertainty in AI, bridging principles from
thermodynamics, information theory, and machine learning.

</details>


### [9] [Why Cannot Large Language Models Ever Make True Correct Reasoning?](https://arxiv.org/abs/2508.10265)
*Jingde Cheng*

Main category: cs.AI

TL;DR: LLM无法真正理解和推理。


<details>
  <summary>Details</summary>
Motivation: 纠正对LLM能力的误解，澄清其本质局限性。

Method: 分析LLM的工作原理，论证其无法进行真正正确的推理。

Result: 证明LLM由于其工作原理的本质限制，不可能拥有真正的理解和推理能力。

Conclusion: 大型语言模型（LLM）不具备真正的理解和推理能力，其所谓的“理解”和“推理”能力只是人们的错觉。

Abstract: Recently, with the application progress of AIGC tools based on large language
models (LLMs), led by ChatGPT, many AI experts and more non-professionals are
trumpeting the "understanding ability" and "reasoning ability" of the LLMs. The
present author considers that the so-called "understanding ability" and
"reasoning ability" of LLMs are just illusions of those people who with vague
concepts. In fact, the LLMs can never have the true understanding ability and
true reasoning ability. This paper intents to explain that, because the
essential limitations of their working principle, the LLMs can never have the
ability of true correct reasoning.

</details>


### [10] [Promoting Efficient Reasoning with Verifiable Stepwise Reward](https://arxiv.org/abs/2508.10293)
*Chuhuai Yue,Chengqi Dong,Yinan Gao,Hang He,Jiajun Chai,Guojun Yin,Wei Lin*

Main category: cs.AI

TL;DR: 通过新的奖励机制，有效解决了大型推理模型的过度思考问题，提高了推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的LRM模型在复杂推理任务中取得了显著进展，但容易过度思考，导致效率低下。现有高效推理方法依赖于准确的任务评估，限制了其灵活性和可靠性。

Method: 提出了一种基于规则的可验证逐步奖励机制（VSRM），并将其与PPO和Reinforce++算法结合使用。

Result: 实验证明，该方法在AIME24和AIME25基准测试中显著减少了输出长度，同时保持了原始推理性能，有效地抑制了无效步骤，促进了有效推理。

Conclusion: 该论文提出了一种基于规则的可验证逐步奖励机制（VSRM），通过奖励有效的推理步骤并惩罚无效步骤来解决大型推理模型（LRM）的过度思考问题，从而在保持推理性能的同时显著减少输出长度。

Abstract: Large reasoning models (LRMs) have recently achieved significant progress in
complex reasoning tasks, aided by reinforcement learning with verifiable
rewards. However, LRMs often suffer from overthinking, expending excessive
computation on simple problems and reducing efficiency. Existing efficient
reasoning methods typically require accurate task assessment to preset token
budgets or select reasoning modes, which limits their flexibility and
reliability. In this work, we revisit the essence of overthinking and identify
that encouraging effective steps while penalizing ineffective ones is key to
its solution. To this end, we propose a novel rule-based verifiable stepwise
reward mechanism (VSRM), which assigns rewards based on the performance of
intermediate states in the reasoning trajectory. This approach is intuitive and
naturally fits the step-by-step nature of reasoning tasks. We conduct extensive
experiments on standard mathematical reasoning benchmarks, including AIME24 and
AIME25, by integrating VSRM with PPO and Reinforce++. Results show that our
method achieves substantial output length reduction while maintaining original
reasoning performance, striking an optimal balance between efficiency and
accuracy. Further analysis of overthinking frequency and pass@k score before
and after training demonstrates that our approach in deed effectively
suppresses ineffective steps and encourages effective reasoning, fundamentally
alleviating the overthinking problem. All code will be released upon
acceptance.

</details>


### [11] [A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering](https://arxiv.org/abs/2508.10337)
*Chenliang Zhang,Lin Wang,Yuanyuan Lu,Yusheng Qi,Kexin Wang,Peixu Hou,Wenshi Chen*

Main category: cs.AI

TL;DR: Dianping团队的方案在META CRAG-MM挑战赛中表现优异，通过结合多种技术，取得了领先成绩。


<details>
  <summary>Details</summary>
Motivation: 为了应对META CRAG-MM挑战赛中多模态多轮问答的挑战。

Method: 该方案结合视觉大型语言模型、知识蒸馏、强化学习和课程学习策略，并利用网络搜索API整合外部知识。

Result: 在第一阶段取得了52.38%的领先优势，在第三阶段获得第三名。

Conclusion: 该论文描述了Dianping-Trust-Safety团队在META CRAG-MM挑战赛中的解决方案，并在第一阶段取得了显著领先优势，在第三阶段取得了第三名。

Abstract: This paper describes the solutions of the Dianping-Trust-Safety team for the
META CRAG-MM challenge. The challenge requires building a comprehensive
retrieval-augmented generation system capable for multi-modal multi-turn
question answering. The competition consists of three tasks: (1) answering
questions using structured data retrieved from an image-based mock knowledge
graph, (2) synthesizing information from both knowledge graphs and web search
results, and (3) handling multi-turn conversations that require context
understanding and information aggregation from multiple sources. For Task 1,
our solution is based on the vision large language model, enhanced by
supervised fine-tuning with knowledge distilled from GPT-4.1. We further
applied curriculum learning strategies to guide reinforcement learning,
resulting in improved answer accuracy and reduced hallucination. For Task 2 and
Task 3, we additionally leveraged web search APIs to incorporate external
knowledge, enabling the system to better handle complex queries and multi-turn
conversations. Our approach achieved 1st place in Task 1 with a significant
lead of 52.38\%, and 3rd place in Task 3, demonstrating the effectiveness of
the integration of curriculum learning with reinforcement learning in our
training pipeline.

</details>


### [12] [Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach](https://arxiv.org/abs/2508.10340)
*Chak Lam Shek,Guangyao Shi,Pratap Tokekar*

Main category: cs.AI

TL;DR: 改进HATRPO算法，通过优化KL阈值分配，提升多智能体强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有HATRPO算法中为所有智能体分配相同KL阈值导致训练缓慢和局部最优的问题，尤其是在异构环境中。

Method: 提出两种KL散度阈值分配方法：基于KKT的HATRPO-W和贪婪算法HATRPO-G，结合顺序策略优化和约束阈值调度。

Result: HATRPO-W和HATRPO-G都实现了超过22.5%的性能提升，HATRPO-W的学习动态更稳定。

Conclusion: 两种改进的HATRPO算法(HATRPO-W和HATRPO-G)显著提高了多智能体强化学习的性能，实现了更快的收敛速度和更高的最终奖励。HATRPO-W在稳定性方面表现更好。

Abstract: Multi-agent reinforcement learning (MARL) requires coordinated and stable
policy updates among interacting agents. Heterogeneous-Agent Trust Region
Policy Optimization (HATRPO) enforces per-agent trust region constraints using
Kullback-Leibler (KL) divergence to stabilize training. However, assigning each
agent the same KL threshold can lead to slow and locally optimal updates,
especially in heterogeneous settings. To address this limitation, we propose
two approaches for allocating the KL divergence threshold across agents:
HATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes
threshold assignment under global KL constraints, and HATRPO-G, a greedy
algorithm that prioritizes agents based on improvement-to-divergence ratio. By
connecting sequential policy optimization with constrained threshold
scheduling, our approach enables more flexible and effective learning in
heterogeneous-agent settings. Experimental results demonstrate that our methods
significantly boost the performance of HATRPO, achieving faster convergence and
higher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and
HATRPO-G achieve comparable improvements in final performance, each exceeding
22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as
reflected by its lower variance.

</details>


### [13] [What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles](https://arxiv.org/abs/2508.10358)
*Mengtao Zhou,Sifan Wu,Huan Zhang,Qi Sima,Bang Liu*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在富有想象力的推理方面能力有限，并提出一个新的基准和评估框架用于未来研究。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法捕捉富有想象力的推理过程的动态和探索性本质。

Method: 提出了一种基于经典“龟汤”游戏的综合研究框架，包括一个大型双语交互式基准测试TurtleSoup-Bench，一个名为Mosaic-Agent的新型代理，以及一个多维评估协议。

Result: 实验结果揭示了LLM在富有想象力的推理方面的能力限制、常见错误模式以及与人类相比的显著性能差距。

Conclusion: 大型语言模型(LLM)在信息稀疏环境下进行富有想象力的推理能力有限，该研究提出了一个基于“龟汤”游戏的综合研究框架，包括基准、代理和评估协议，以评估LLM的性能。

Abstract: We investigate the capacity of Large Language Models (LLMs) for imaginative
reasoning--the proactive construction, testing, and revision of hypotheses in
information-sparse environments. Existing benchmarks, often static or focused
on social deduction, fail to capture the dynamic, exploratory nature of this
reasoning process. To address this gap, we introduce a comprehensive research
framework based on the classic "Turtle Soup" game, integrating a benchmark, an
agent, and an evaluation protocol. We present TurtleSoup-Bench, the first
large-scale, bilingual, interactive benchmark for imaginative reasoning,
comprising 800 turtle soup puzzles sourced from both the Internet and expert
authors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'
performance in this setting. To evaluate reasoning quality, we develop a
multi-dimensional protocol measuring logical consistency, detail completion,
and conclusion alignment. Experiments with leading LLMs reveal clear capability
limits, common failure patterns, and a significant performance gap compared to
humans. Our work offers new insights into LLMs' imaginative reasoning and
establishes a foundation for future research on exploratory agent behavior.

</details>


### [14] [LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval](https://arxiv.org/abs/2508.10391)
*Yaoze Zhang,Rong Wu,Pinlong Cai,Xiaoman Wang,Guohang Yan,Song Mao,Ding Wang,Botian Shi*

Main category: cs.AI

TL;DR: LeanRAG框架通过语义聚合和结构引导检索，有效解决了基于知识图谱的RAG方法的语义孤岛和低效检索问题，显著提升了问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于知识图谱的RAG方法存在两个问题：高层次概念性摘要之间缺乏显式关系，检索过程效率低下。

Method: LeanRAG框架首先使用一种新的语义聚合算法形成实体集群并构建聚合级摘要之间的显式关系，创建一个完全可导航的语义网络。然后，采用自下而上的结构引导检索策略，将查询锚定到最相关的细粒度实体，并系统地遍历图的语义路径，以收集简洁且上下文完整的证据集。

Result: 在四个具有挑战性的问答基准测试中，LeanRAG在响应质量方面显著优于现有方法，同时减少了46%的检索冗余。

Conclusion: LeanRAG框架通过结合知识聚合和检索策略，解决了现有基于知识图谱的RAG方法中语义孤岛和低效检索的问题，显著提高了问答任务的响应质量并减少了检索冗余。

Abstract: Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large
Language Models by leveraging external knowledge, whereas the effectiveness is
often compromised by the retrieval of contextually flawed or incomplete
information. To address this, knowledge graph-based RAG methods have evolved
towards hierarchical structures, organizing knowledge into multi-level
summaries. However, these approaches still suffer from two critical,
unaddressed challenges: high-level conceptual summaries exist as disconnected
``semantic islands'', lacking the explicit relations needed for cross-community
reasoning; and the retrieval process itself remains structurally unaware, often
degenerating into an inefficient flat search that fails to exploit the graph's
rich topology. To overcome these limitations, we introduce LeanRAG, a framework
that features a deeply collaborative design combining knowledge aggregation and
retrieval strategies. LeanRAG first employs a novel semantic aggregation
algorithm that forms entity clusters and constructs new explicit relations
among aggregation-level summaries, creating a fully navigable semantic network.
Then, a bottom-up, structure-guided retrieval strategy anchors queries to the
most relevant fine-grained entities and then systematically traverses the
graph's semantic pathways to gather concise yet contextually comprehensive
evidence sets. The LeanRAG can mitigate the substantial overhead associated
with path retrieval on graphs and minimizes redundant information retrieval.
Extensive experiments on four challenging QA benchmarks with different domains
demonstrate that LeanRAG significantly outperforming existing methods in
response quality while reducing 46\% retrieval redundancy. Code is available
at: https://github.com/RaZzzyz/LeanRAG

</details>


### [15] [HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation](https://arxiv.org/abs/2508.10425)
*Yan Ting Chok,Soyon Park,Seungheun Baek,Hajung Kim,Junhyun Lee,Jaewoo Kang*

Main category: cs.AI

TL;DR: HiRef模型通过结合医学本体的层次语义和细化的EHR共现模式，有效提高了药物推荐的鲁棒性和泛化能力，在处理真实世界EHR数据方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的数据驱动模型难以处理真实世界EHR数据中存在的稀有医疗实体和不完整记录等问题，泛化能力不足。

Method: HiRef框架结合了两种互补结构：（i）来自医学本体的层次语义；（ii）来自真实世界EHR的细化共现模式。利用双曲空间嵌入本体实体，并引入先验引导的稀疏正则化方案来细化EHR共现图。

Result: HiRef模型在EHR基准测试中取得了优异性能，并展现出对未见医疗代码的强大鲁棒性。消融实验验证了模型的有效性。

Conclusion: HiRef模型在EHR基准测试（MIMIC-III和MIMIC-IV）上取得了显著成果，并在模拟的未见代码设置下保持了高精度，且对未见医疗代码具有很强的鲁棒性。

Abstract: Medication recommendation is a crucial task for assisting physicians in
making timely decisions from longitudinal patient medical records. However,
real-world EHR data present significant challenges due to the presence of
rarely observed medical entities and incomplete records that may not fully
capture the clinical ground truth. While data-driven models trained on
longitudinal Electronic Health Records often achieve strong empirical
performance, they struggle to generalize under missing or novel conditions,
largely due to their reliance on observed co-occurrence patterns. To address
these issues, we propose Hierarchical Ontology and Network Refinement for
Robust Medication Recommendation (HiRef), a unified framework that combines two
complementary structures: (i) the hierarchical semantics encoded in curated
medical ontologies, and (ii) refined co-occurrence patterns derived from
real-world EHRs. We embed ontology entities in hyperbolic space, which
naturally captures tree-like relationships and enables knowledge transfer
through shared ancestors, thereby improving generalizability to unseen codes.
To further improve robustness, we introduce a prior-guided sparse
regularization scheme that refines the EHR co-occurrence graph by suppressing
spurious edges while preserving clinically meaningful associations. Our model
achieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and
maintains high accuracy under simulated unseen-code settings. Extensive
experiments with comprehensive ablation studies demonstrate HiRef's resilience
to unseen medical codes, supported by in-depth analyses of the learned
sparsified graph structure and medical code embeddings.

</details>


### [16] [MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance](https://arxiv.org/abs/2508.10429)
*Yi Dong,Yusuke Muraoka,Scott Shi,Yi Zhang*

Main category: cs.AI

TL;DR: 发布了一个大型多模态食品数据集MM-Food-100K，并验证了其在营养预测方面的实用性。


<details>
  <summary>Details</summary>
Motivation: 缺乏高质量、可验证来源的多模态食品数据集限制了食品智能领域的发展。

Method: 收集了来自87000多名贡献者的120万张高质量食品图片，并使用Codatta贡献模型结合社区资源和AI辅助质量检查进行整理，最终发布了10万张图片的公开数据集，剩余部分用于商业用途。

Result: 构建了MM-Food-100K数据集，并验证了其在图像营养预测任务中的有效性。

Conclusion: 构建了一个包含10万张图片的MM-Food-100K多模态食品智能数据集，并通过微调大型视觉语言模型验证了其在图像营养预测方面的效用，取得了优于基准模型的结果。

Abstract: We present MM-Food-100K, a public 100,000-sample multimodal food intelligence
dataset with verifiable provenance. It is a curated approximately 10% open
subset of an original 1.2 million, quality-accepted corpus of food images
annotated for a wide range of information (such as dish name, region of
creation). The corpus was collected over six weeks from over 87,000
contributors using the Codatta contribution model, which combines community
sourcing with configurable AI-assisted quality checks; each submission is
linked to a wallet address in a secure off-chain ledger for traceability, with
a full on-chain protocol on the roadmap. We describe the schema, pipeline, and
QA, and validate utility by fine-tuning large vision-language models (ChatGPT
5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning
yields consistent gains over out-of-box baselines across standard metrics; we
report results primarily on the MM-Food-100K subset. We release MM-Food-100K
for publicly free access and retain approximately 90% for potential commercial
access with revenue sharing to contributors.

</details>


### [17] [We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning](https://arxiv.org/abs/2508.10433)
*Runqi Qiao,Qiuna Tan,Peiqing Yang,Yanzi Wang,Xiaowan Wang,Enhui Wan,Sitong Zhou,Guanting Dong,Yuchen Zeng,Yida Xu,Jie Wang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.AI

TL;DR: We-Math 2.0 improves MLLM mathematical reasoning through a unified system incorporating structured knowledge, model-centric data modeling, and reinforcement learning, showing strong performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with complex mathematical reasoning due to lack of comprehensive knowledge-driven design and model-centric data space modeling.

Method: Developed We-Math 2.0, including a five-level hierarchical knowledge system (MathBook Knowledge System), two datasets (MathBook-Standard & Pro), and a two-stage RL framework (MathBook-RL).  Used a comprehensive benchmark (MathBookEval) for evaluation.

Result: We-Math 2.0 shows competitive performance on existing benchmarks and strong results on MathBookEval, demonstrating promising generalization in mathematical reasoning.

Conclusion: We-Math 2.0, a unified system integrating structured mathematical knowledge, model-centric data space modeling, and reinforcement learning, significantly enhances MLLMs' mathematical reasoning abilities.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across various tasks, but still struggle with complex mathematical
reasoning. Existing research primarily focuses on dataset construction and
method optimization, often overlooking two critical aspects: comprehensive
knowledge-driven design and model-centric data space modeling. In this paper,
we introduce We-Math 2.0, a unified system that integrates a structured
mathematical knowledge system, model-centric data space modeling, and a
reinforcement learning (RL)-based training paradigm to comprehensively enhance
the mathematical reasoning abilities of MLLMs. The key contributions of We-Math
2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level
hierarchical system encompassing 491 knowledge points and 1,819 fundamental
principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a
dataset that ensures broad conceptual coverage and flexibility through dual
expansion. Additionally, we define a three-dimensional difficulty space and
generate 7 progressive variants per problem to build MathBook-Pro, a
challenging dataset for robust training. (3) MathBook-RL: We propose a
two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the
model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive
Alignment RL, leveraging average-reward learning and dynamic data scheduling to
achieve progressive alignment across difficulty levels. (4) MathBookEval: We
introduce a comprehensive benchmark covering all 491 knowledge points with
diverse reasoning step distributions. Experimental results show that
MathBook-RL performs competitively with existing baselines on four widely-used
benchmarks and achieves strong results on MathBookEval, suggesting promising
generalization in mathematical reasoning.

</details>
