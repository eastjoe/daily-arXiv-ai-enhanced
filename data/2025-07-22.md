<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Free Will Equation: Quantum Field Analogies for AGI](https://arxiv.org/abs/2507.14154)
*Rahul Kabali*

Main category: cs.AI

TL;DR: 该论文提出了一种模拟人类“自由意志”的AI决策框架，使其能够更灵活地适应环境变化。


<details>
  <summary>Details</summary>
Motivation: 传统人工智能研究侧重于确定性规则下的目标优化，而忽略了人类智能的适应性自发性和创造性。

Method: 借鉴量子场论的思想，将人工智能主体的认知状态视为潜在行动或思想的叠加态，通过概率坍缩选择行动。

Result: 在非平稳多臂老虎机环境中，使用该框架的智能体取得了比基线方法更高的奖励和策略多样性。

Conclusion: 提出了一种名为“自由意志方程”的理论框架，赋予人工智能主体一种适应性、可控的随机性决策过程，并在非平稳多臂老虎机环境中实验验证了该框架的有效性，实现了更高的奖励和策略多样性。

Abstract: Artificial General Intelligence (AGI) research traditionally focuses on
algorithms that optimize for specific goals under deterministic rules. Yet,
human-like intelligence exhibits adaptive spontaneity - an ability to make
unexpected choices or free decisions not strictly dictated by past data or
immediate reward. This trait, often dubbed "free will" in a loose sense, might
be crucial for creativity, robust adaptation, and avoiding ruts in
problem-solving. This paper proposes a theoretical framework, called the Free
Will Equation, that draws analogies from quantum field theory to endow AGI
agents with a form of adaptive, controlled stochasticity in their
decision-making process. The core idea is to treat an AI agent's cognitive
state as a superposition of potential actions or thoughts, which collapses
probabilistically into a concrete action when a decision is made - much like a
quantum wavefunction collapsing upon measurement. By incorporating mechanisms
analogous to quantum fields, along with intrinsic motivation terms, we aim to
improve an agent's ability to explore novel strategies and adapt to unforeseen
changes. Experiments in a non-stationary multi-armed bandit environment
demonstrate that agents using this framework achieve higher rewards and policy
diversity compared to baseline methods.

</details>


### [2] [DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation](https://arxiv.org/abs/2507.14267)
*Ziqi Wang,Hongshuo Huang,Hancheng Zhao,Changwen Xu,Shang Zhu,Jan Janssen,Venkatasubramanian Viswanathan*

Main category: cs.AI

TL;DR: DREAMS是一个自动化DFT模拟框架，在材料发现方面达到了专家水平，并实现了高通量和高精度。


<details>
  <summary>Details</summary>
Motivation: 解决DFT模拟需要多年培训、大量参数微调和系统错误处理的挑战。

Method: DREAMS是一个基于DFT的层次化多Agent框架，结合了中央LLM规划Agent和用于原子结构生成、DFT收敛性测试、HPC调度和错误处理的特定领域LLM Agents。

Result: DREAMS在Sol27LC晶格常数基准测试中平均误差低于1%，并在CO/Pt(111)吸附难题上再现了专家级的吸附能差异，并量化了功能驱动的贝叶斯不确定性。

Conclusion: DREAMS框架实现了L3级自动化，显著减少了对人工的依赖，为高通量高精度计算材料发现提供了一条可扩展的途径。

Abstract: Materials discovery relies on high-throughput, high-fidelity simulation
techniques such as Density Functional Theory (DFT), which require years of
training, extensive parameter fine-tuning and systematic error handling. To
address these challenges, we introduce the DFT-based Research Engine for
Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for
DFT simulation that combines a central Large Language Model (LLM) planner agent
with domain-specific LLM agents for atomistic structure generation, systematic
DFT convergence testing, High-Performance Computing (HPC) scheduling, and error
handling. In addition, a shared canvas helps the LLM agents to structure their
discussions, preserve context and prevent hallucination. We validate DREAMS
capabilities on the Sol27LC lattice-constant benchmark, achieving average
errors below 1\% compared to the results of human DFT experts. Furthermore, we
apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating
its long-term and complex problem-solving capabilities. The framework again
reproduces expert-level literature adsorption-energy differences. Finally,
DREAMS is employed to quantify functional-driven uncertainties with Bayesian
ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at
the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS
approaches L3-level automation - autonomous exploration of a defined design
space - and significantly reduces the reliance on human expertise and
intervention, offering a scalable path toward democratized, high-throughput,
high-fidelity computational materials discovery.

</details>


### [3] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: 大型语言模型驱动的网络代理存在安全风险，研究人员构建了WebGuard数据集并微调了模型，但其性能仍需改进。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型驱动的自主网络代理存在安全风险，需要有效的安全措施。

Method: 构建WebGuard数据集，并使用该数据集微调Qwen2.5VL-7B模型以提高行为预测的准确性和高风险行为的召回率。

Result: WebGuard数据集包含4939个人工标注的网络代理行为，涵盖22个不同领域。微调后的模型在准确率和高风险行为召回率方面均有显著提升，但仍需进一步改进以满足高风险部署需求。

Conclusion: 当前一代自主网络代理缺乏足够的安全性，其行为预测准确率和高风险行为召回率均低于60%。通过使用WebGuard数据集微调Qwen2.5VL-7B模型，准确率提升至80%，高风险行为召回率提升至76%，但仍不足以满足高风险部署需求。

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [4] [Manimator: Transforming Research Papers into Visual Explanations](https://arxiv.org/abs/2507.14306)
*Samarth P,Vyoman Jain,Shiva Golugula,Motamarri Sai Sathvik*

Main category: cs.AI

TL;DR: Manimator用LLM将论文转换为Manim动画，便于理解复杂概念。


<details>
  <summary>Details</summary>
Motivation: 帮助学习者理解复杂的科学和数学概念，特别是那些在密集型研究论文中提出的概念。

Method: Manimator使用一个管道：一个LLM解释输入文本或研究论文PDF以生成结构化的场景描述，另一个LLM将此描述转换为可执行的Manim Python代码。

Result: 成功开发了一个能够将研究论文转换为动画的系统，为STEM主题创建引人入胜的视觉解释。

Conclusion: Manimator是一个开源系统，利用大型语言模型将研究论文和自然语言提示转换为解释性动画，降低了创建高质量教育内容的门槛。

Abstract: Understanding complex scientific and mathematical concepts, particularly
those presented in dense research papers, poses a significant challenge for
learners. Dynamic visualizations can greatly enhance comprehension, but
creating them manually is time-consuming and requires specialized knowledge and
skills. We introduce manimator, an open-source system that leverages Large
Language Models to transform research papers and natural language prompts into
explanatory animations using the Manim engine. Manimator employs a pipeline
where an LLM interprets the input text or research paper PDF to generate a
structured scene description outlining key concepts, mathematical formulas, and
visual elements and another LLM translates this description into executable
Manim Python code. We discuss its potential as an educational tool for rapidly
creating engaging visual explanations for complex STEM topics, democratizing
the creation of high-quality educational content.

</details>


### [5] [Language Models as Ontology Encoders](https://arxiv.org/abs/2507.14334)
*Hui Yang,Jiaoyan Chen,Yuan He,Yongsheng Gao,Ian Horrocks*

Main category: cs.AI

TL;DR: OnT 是一种新的本体嵌入方法，它结合了文本信息和逻辑结构，在多个真实世界本体上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有的本体嵌入方法存在局限性：基于几何模型的嵌入通常忽略有价值的文本信息，导致性能欠佳；结合文本的方法（通常基于语言模型）未能保留逻辑结构。

Method: 该研究提出了一种新的本体嵌入方法 OnT，该方法通过在双曲空间中进行几何建模来微调预训练语言模型 (PLM)，有效地结合了文本标签，同时保留了描述逻辑 EL 的类层次结构和其他逻辑关系。

Result: 在四个真实世界本体上的大量实验表明，OnT 在预测和推理公理的任务中始终优于基线，包括现有技术。OnT 还展示了其强大的迁移学习能力和在真实案例中构建新的 SNOMED CT 本体方面的有效性。

Conclusion: OnT 算法在预测和推理公理的任务中始终优于包括现有技术在内的基线，并在构建新的SNOMED CT 本体方面显示出强大的潜力。

Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent
complex knowledge and support semantic reasoning have been widely adopted
across various domains such as healthcare and bioinformatics. Recently,
ontology embeddings have gained wide attention due to its potential to infer
plausible new knowledge and approximate complex reasoning. However, existing
methods face notable limitations: geometric model-based embeddings typically
overlook valuable textual information, resulting in suboptimal performance,
while the approaches that incorporate text, which are often based on language
models, fail to preserve the logical structure. In this work, we propose a new
ontology embedding method OnT, which tunes a Pretrained Language Model (PLM)
via geometric modeling in a hyperbolic space for effectively incorporating
textual labels and simultaneously preserving class hierarchies and other
logical relationships of Description Logic EL. Extensive experiments on four
real-world ontologies show that OnT consistently outperforms the baselines
including the state-of-the-art across both tasks of prediction and inference of
axioms. OnT also demonstrates strong potential in real-world applications,
indicated by its robust transfer learning abilities and effectiveness in real
cases of constructing a new ontology from SNOMED CT. Data and code are
available at https://github.com/HuiYang1997/OnT.

</details>


### [6] [ProofCompass: Enhancing Specialized Provers with LLM Guidance](https://arxiv.org/abs/2507.14335)
*Nicolas Wischermann,Claudio Mayrink Verdun,Gabriel Poesia,Francesco Noseda*

Main category: cs.AI

TL;DR: ProofCompass 是一种高效的混合方法，通过大型语言模型指导专用证明器，在形式化定理证明中取得了显著的效率提升和准确性改进。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大型通用模型或小型专用模型，各有局限性，训练大型专用模型计算成本高昂。ProofCompass 旨在提高计算效率。

Method: ProofCompass 采用混合方法，利用大型语言模型指导已有的专业证明器，例如 DeepSeek-Prover-v1.5-RL，通过自然语言证明策略和失败尝试分析来选择中间引理，实现有效的分解问题。

Result: ProofCompass 在 miniF2F 基准测试中，以 25 倍更少的尝试次数，取得比 DSP-v1.5 更好的结果 (54.9% -> 55.3%)。

Conclusion: ProofCompass 结合大型语言模型和专业证明器，在 miniF2F 基准测试中显著提高了形式化定理证明的计算效率和准确性，以更少的尝试次数超越了 DeepSeek-Prover-v1.5-RL。

Abstract: Language models have become increasingly powerful tools for formal
mathematical reasoning. However, most existing approaches rely exclusively on
either large general-purpose models or smaller specialized models, each with
distinct limitations, while training specialized large models still requires
significant computational resources. This paper introduces ProofCompass, a
novel hybrid methodology that achieves remarkable computational efficiency by
strategically guiding existing specialized prover methods, such as
DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without
requiring additional model training. The LLM provides natural language proof
strategies and analyzes failed attempts to select intermediate lemmas, enabling
effective problem decomposition. On the miniF2F benchmark, ProofCompass
demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\%
\rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$).
Our synergistic approach paves the way for simultaneously improving
computational efficiency and accuracy in formal theorem proving.

</details>


### [7] [Adaptive Multi-Agent Reasoning via Automated Workflow Generation](https://arxiv.org/abs/2507.14393)
*Humza Sami,Mubashir ul Islam,Pierre-Emmanuel Gaillardon,Valerio Tenace*

Main category: cs.AI

TL;DR: Nexus Architect通过自动化工作流合成和迭代提示优化，解决了大型推理模型的泛化能力不足的问题，并在实验中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的LRM模型存在过拟合问题，泛化能力不足。

Method: 提出了一种名为Nexus Architect的增强型多智能体系统框架，该框架包含一个新颖的自动化工作流合成机制和迭代提示细化机制。

Result: Nexus Architect在具有挑战性的逻辑问题自定义数据集上，与现有LRM模型相比，取得了显著的性能提升，最高提升幅度达到66%。

Conclusion: Nexus Architect，一个改进的多智能体系统框架，通过自动化的工作流合成机制，显著提高了大型推理模型的泛化能力，并在多个基准测试中取得了优于现有技术的性能提升。

Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward
in language model capabilities, aiming to tackle increasingly sophisticated
tasks with unprecedented efficiency and accuracy. However, despite their
impressive performance, recent studies have highlighted how current reasoning
models frequently fail to generalize to novel, unseen problems, often resorting
to memorized solutions rather than genuine inferential reasoning. Such behavior
underscores a critical limitation in modern LRMs, i.e., their tendency toward
overfitting, which in turn results in poor generalization in problem-solving
capabilities.
  In this paper, we introduce Nexus Architect, an enhanced iteration of our
multi-agent system framework, Nexus, equipped with a novel automated workflow
synthesis mechanism. Given a user's prompt and a small set of representative
examples, the Architect autonomously generates a tailored reasoning workflow by
selecting suitable strategies, tool integrations, and adversarial techniques
for a specific problem class. Furthermore, the Architect includes an iterative
prompt refinement mechanism that fine-tunes agents' system prompts to maximize
performance and improve the generalization capabilities of the system.
  We empirically evaluate Nexus Architect by employing an off-the-shelf,
non-reasoning model on a custom dataset of challenging logical questions and
compare its performance against state-of-the-art LRMs. Results show that Nexus
Architect consistently outperforms existing solutions, achieving up to a 66%
increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against
Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

</details>


### [8] [Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering](https://arxiv.org/abs/2507.14406)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: 通过LLM与人类专家合作及增加非推理模型，有效降低了大型语言模型的错误率和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的错误率较高且延迟较大，难以应用于对错误率和效率要求较高的场景。

Method: 提出了一种LLM与人类专家合作的系统，该系统通过量化LLM的不确定性来选择性地将难题提交给人类专家，并通过在LLM前增加一个非推理模型来减少延迟和成本。

Result: 实验结果表明，该系统可以有效降低LLM的错误率，并减少延迟和成本。

Conclusion: 大型语言模型(LLM)在解决问题方面很强大，但仍会出错。本文提出了一种LLM与人类专家合作的系统，以降低错误率并提高效率。通过量化LLM的不确定性，选择性地将难题提交给人类专家，可以有效降低错误率。同时，通过在LLM前增加一个非推理模型，可以减少延迟和成本。

Abstract: State-of-the-art reasoning LLMs are powerful problem solvers, but they still
occasionally make mistakes. However, adopting AI models in risk-sensitive
domains often requires error rates near 0%. To address this gap, we propose
collaboration between a reasoning model and a human expert who resolves queries
the model cannot confidently answer. We find that quantifying the uncertainty
of a reasoning model through the length of its reasoning trace yields an
effective basis for deferral to a human, e.g., cutting the error rate of Qwen3
235B-A22B on difficult MATH problems from 3% to less than 1% when deferring
7.5% of queries. However, the high latency of reasoning models still makes them
challenging to deploy on use cases with high query volume. To address this
challenge, we explore fronting a reasoning model with a large non-reasoning
model. We call this modified human-in-the-loop system "Fail Fast, or Ask",
since the non-reasoning model may defer difficult queries to the human expert
directly ("failing fast"), without incurring the reasoning model's higher
latency. We show that this approach yields around 40% latency reduction and
about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the
accuracy-rejection curve. However, we observe that latency savings are lower
than expected because of "latency drag", the phenomenon that processing easier
queries with a non-reasoning model pushes the reasoning model's latency
distribution towards longer latencies. Broadly, our results suggest that the
deficiencies of state-of-the-art reasoning models -- nontrivial error rates and
high latency -- can be substantially mitigated through black-box systems
engineering, without requiring access to LLM internals.

</details>


### [9] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: 延长大型语言模型的推理时间可能会适得其反，导致性能下降和问题行为放大。


<details>
  <summary>Details</summary>
Motivation: 评估测试时间计算扩展对大型语言模型推理能力的影响。

Method: 构建了四个类别的评估任务：带有干扰项的简单计数任务、具有虚假特征的回归任务、需要约束跟踪的演绎任务和高级AI风险任务。

Result: 识别出模型在较长推理时间下的五种失效模式：1)模型容易受到无关信息的干扰；2)模型过度拟合问题框架；3)模型倾向于虚假关联；4)模型难以集中注意力于复杂的演绎任务；5)扩展推理可能会放大令人担忧的行为。

Conclusion: 大型推理模型(LRM)的推理长度延长会导致性能下降，测试时间计算与准确性之间存在反比例关系。

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [10] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: Routine框架有效提升了Agent系统在企业环境中的执行稳定性和适应性，为AI流程自动化提供了实用方案。


<details>
  <summary>Details</summary>
Motivation: 解决现有Agent系统在企业环境部署中面临的挑战，例如缺乏领域特定过程知识导致计划混乱、工具缺失和执行稳定性差等问题。

Method: 提出了一种多步骤Agent规划框架Routine，该框架具有清晰的结构、明确的指令和无缝的参数传递，以指导Agent执行模块执行多步工具调用任务。

Result: 在真实企业场景的评估中，Routine将GPT-4o的执行准确率从41.1%提高到96.3%，Qwen3-14B从32.6%提高到83.3%。通过基于Routine的蒸馏和微调，进一步提升了模型的准确率。

Conclusion: Routine框架显著提高了企业环境中多步工具调用任务的执行准确性，并将模型对新场景的适应性提升到接近GPT-4o的水平。

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [11] [BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning](https://arxiv.org/abs/2507.14468)
*Yitong Lin,Jiaying He,Jiahe Chen,Xinnan Zhu,Jianwei Zheng,Tao Bo*

Main category: cs.AI

TL;DR: BioGraphFusion框架通过结合张量分解和LSTM，实现了语义理解和结构学习的深度协同，在生物医学KGs任务中取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有的生物医学知识图谱(KGs)的构建和推理存在挑战，知识嵌入(KE)方法和图神经网络(GNNs)各有优缺点，集成方法也难以实现语义理解和结构学习的深度、自适应和协同进化。

Method: BioGraphFusion框架结合张量分解建立全局语义基础，利用LSTM动态细化关系嵌入，通过查询引导的子图构建和混合评分机制增强语义理解和结构学习的适应性交互。

Result: BioGraphFusion在三个生物医学关键任务上优于现有技术，并在CMM1案例研究中展现了其价值。

Conclusion: BioGraphFusion框架在三个生物医学关键任务上表现优于现有最先进的KE、GNN和集成模型，并在Cutaneous Malignant Melanoma 1 (CMM1)案例研究中展现了揭示生物学意义通路的能力。

Abstract: Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery
and disease understanding, yet their completion and reasoning are challenging.
Knowledge Embedding (KE) methods capture global semantics but struggle with
dynamic structural integration, while Graph Neural Networks (GNNs) excel
locally but often lack semantic understanding. Even ensemble approaches,
including those leveraging language models, often fail to achieve a deep,
adaptive, and synergistic co-evolution between semantic comprehension and
structural learning. Addressing this critical gap in fostering continuous,
reciprocal refinement between these two aspects in complex biomedical KGs is
paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply
synergistic semantic and structural learning. BioGraphFusion establishes a
global semantic foundation via tensor decomposition, guiding an LSTM-driven
mechanism to dynamically refine relation embeddings during graph propagation.
This fosters adaptive interplay between semantic understanding and structural
learning, further enhanced by query-guided subgraph construction and a hybrid
scoring mechanism. Experiments across three key biomedical tasks demonstrate
BioGraphFusion's superior performance over state-of-the-art KE, GNN, and
ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)
highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely
available for download at https://github.com/Y-TARL/BioGraphFusion.
  Contact: zjw@zjut.edu.cn, botao666666@126.com.
  Supplementary information: Supplementary data are available at Bioinformatics
online.

</details>


### [12] [Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy](https://arxiv.org/abs/2507.14513)
*Hongyi Yang,Yue Pan,Jiayi Xu,Kelsen Liu*

Main category: cs.AI

TL;DR: Amico是一个用Rust编写的，用于构建适用于嵌入式系统的自主代理的模块化框架，具有高性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的许多框架难以在现实世界或资源受限的环境中运行，因为它们依赖于云计算，在动态环境中的鲁棒性有限，并且缺乏持久自主性和环境感知能力。

Method: Amico框架是用Rust编写的，具有模块化、事件驱动等特点，支持反应式、持久性代理，并能高效运行在嵌入式平台和浏览器环境中。

Result: Amico框架提供了一个统一的基础设施，用于构建适应计算能力有限和连接性间歇性环境的弹性交互式代理。

Conclusion: Amico框架是一个用于构建适用于嵌入式系统的自主代理的模块化、事件驱动的框架，它支持反应式、持久性代理，并在嵌入式平台和浏览器环境中高效运行。

Abstract: Recent advances in large language models (LLMs) and autonomous agents have
enabled systems capable of performing complex tasks across domains such as
human-computer interaction, planning, and web navigation. However, many
existing frameworks struggle in real-world or resource-constrained environments
due to their reliance on cloud-based computation, limited robustness in dynamic
contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous
agents optimized for embedded systems. Written in Rust for safety and
performance, Amico supports reactive, persistent agents that operate
efficiently across embedded platforms and browser environments via WebAssembly.
It provides clean abstractions for event handling, state management, behavior
execution, and integration with reasoning modules. Amico delivers a unified
infrastructure for constructing resilient, interactive agents suitable for
deployment in settings with limited compute and intermittent connectivity.

</details>


### [13] [What if Othello-Playing Language Models Could See?](https://arxiv.org/abs/2507.14520)
*Xinyi Chen,Yifei Yuan,Jiaang Li,Serge Belongie,Maarten de Rijke,Anders Søgaard*

Main category: cs.AI

TL;DR: 多模态模型VISOTHELLO在国际象棋游戏Othello中取得了比单模态模型更好的性能和鲁棒性，验证了视觉接地对语言模型理解结构化世界表示的益处。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型的符号接地问题，比较仅基于文本的学习和基于多模态的学习效率。

Method: 构建了一个多模态模型VISOTHELLO，在棋盘图像和走棋历史数据上进行训练，并使用下一步预测来评估其性能和鲁棒性。

Result: 多模态训练提高了模型性能和内部表示的鲁棒性。

Conclusion: 多模态训练提高了模型性能和内部表示的鲁棒性，表明将语言与视觉输入结合有助于模型推断结构化的世界表示。

Abstract: Language models are often said to face a symbol grounding problem. While some
argue that world understanding can emerge from text alone, others suggest
grounded learning is more efficient. We explore this through Othello, where the
board state defines a simplified, rule-based world. Building on prior work, we
introduce VISOTHELLO, a multi-modal model trained on move histories and board
images. Using next-move prediction, we compare it to mono-modal baselines and
test robustness to semantically irrelevant perturbations. We find that
multi-modal training improves both performance and the robustness of internal
representations. These results suggest that grounding language in visual input
helps models infer structured world representations.

</details>


### [14] [Large Language Models Assisting Ontology Evaluation](https://arxiv.org/abs/2507.14552)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisärkkä,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: OE-Assist框架利用LLM辅助本体评估，提高了效率，且效果与人工相当。


<details>
  <summary>Details</summary>
Motivation: 目前的本体评估方法成本高、费力且容易出错。

Method: 利用1393个CQ及其对应的本体和本体故事数据集，评估了LLM自动执行CQ验证的有效性，并开发和评估了一个LLM驱动的框架，以辅助使用Prot&eacute;g&eacute;进行CQ验证。

Result: 基于LLM的自动评估方法（o1-preview和o3-mini）与人工评估效果相当。

Conclusion: 本文介绍了OE-Assist框架，该框架利用大型语言模型（LLM）自动和半自动地验证能力问题（CQ），从而辅助本体评估。研究结果表明，基于LLM的自动评估与人工评估效果相当。

Abstract: Ontology evaluation through functional requirements, such as testing via
competency question (CQ) verification, is a well-established yet costly,
labour-intensive, and error-prone endeavour, even for ontology engineering
experts. In this work, we introduce OE-Assist, a novel framework designed to
assist ontology evaluation through automated and semi-automated CQ
verification. By presenting and leveraging a dataset of 1,393 CQs paired with
corresponding ontologies and ontology stories, our contributions present, to
our knowledge, the first systematic investigation into large language model
(LLM)-assisted ontology evaluation, and include: (i) evaluating the
effectiveness of a LLM-based approach for automatically performing CQ
verification against a manually created gold standard, and (ii) developing and
assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e,
by providing suggestions. We found that automated LLM-based evaluation with
o1-preview and o3-mini perform at a similar level to the average user's
performance.

</details>


### [15] [Coordinate Heart System: A Geometric Framework for Emotion Representation](https://arxiv.org/abs/2507.14593)
*Omar Al-Desi*

Main category: cs.AI

TL;DR: 提出一种新的八维情绪坐标系统(CHS)，用于人工智能中的情绪建模，解决了传统模型的不足，能处理复杂情绪场景。


<details>
  <summary>Details</summary>
Motivation: 现有情绪模型存在表达空间不足和难以处理复杂情绪状态的问题。

Method: 提出了一种基于单位圆几何框架的八维情绪坐标系统，利用坐标混合和向量运算计算复杂情绪状态，并引入稳定性参数S来动态整合情绪负荷、冲突解决和情境因素。

Result: 建立了一个完整的八维情绪坐标系统，能够更精确地表达和计算复杂情绪状态，并通过案例研究验证了其有效性。

Conclusion: 提出了一种新的情绪建模数学框架——坐标心脏系统（CHS），用于人工智能应用中更精确的情绪表达和计算。该系统解决了传统分类模型的不足，能够处理复杂的情绪冲突和情境因素。

Abstract: This paper presents the Coordinate Heart System (CHS), a geometric framework
for emotion representation in artificial intelligence applications. We position
eight core emotions as coordinates on a unit circle, enabling mathematical
computation of complex emotional states through coordinate mixing and vector
operations. Our initial five-emotion model revealed significant coverage gaps
in the emotion space, leading to the development of an eight-emotion system
that provides complete geometric coverage with mathematical guarantees. The
framework converts natural language input to emotion coordinates and supports
real-time emotion interpolation through computational algorithms. The system
introduces a re-calibrated stability parameter S in [0,1], which dynamically
integrates emotional load, conflict resolution, and contextual drain factors.
This stability model leverages advanced Large Language Model interpretation of
textual cues and incorporates hybrid temporal tracking mechanisms to provide
nuanced assessment of psychological well-being states. Our key contributions
include: (i) mathematical proof demonstrating why five emotions are
insufficient for complete geometric coverage, (ii) an eight-coordinate system
that eliminates representational blind spots, (iii) novel algorithms for
emotion mixing, conflict resolution, and distance calculation in emotion space,
and (iv) a comprehensive computational framework for AI emotion recognition
with enhanced multi-dimensional stability modeling. Experimental validation
through case studies demonstrates the system's capability to handle emotionally
conflicted states, contextual distress factors, and complex psychological
scenarios that traditional categorical emotion models cannot adequately
represent. This work establishes a new mathematical foundation for emotion
modeling in artificial intelligence systems.

</details>


### [16] [Efficient Story Point Estimation With Comparative Learning](https://arxiv.org/abs/2507.14642)
*Monoshiz Mahbub Khan,Xioayin Xi,Andrew Meneely,Zhe Yu*

Main category: cs.AI

TL;DR: 使用比较学习方法进行故事点估算，效率高，效果好，认知负担低。


<details>
  <summary>Details</summary>
Motivation: 目前的机器学习模型只能在相同项目的数据上进行准确预测，故事点估算费时费力，本文旨在简化故事点估算流程。

Method: 使用比较学习框架，训练机器学习模型预测故事点。开发人员比较两个待办事项的工作量，模型根据这些比较判断进行训练。

Result: 实证结果表明，该模型平均Spearman秩相关系数为0.34，与直接学习基准故事点的回归模型性能相当甚至更好，且更有效率。

Conclusion: 本文提出了一种基于比较学习的框架，用于校准项目特定的故事点预测模型，该模型通过比较判断学习，能够达到与基于回归模型相近甚至更好的性能，并且降低了人工标注的认知负担。

Abstract: Story point estimation is an essential part of agile software development.
Story points are unitless, project-specific effort estimates that help
developers plan their sprints. Traditionally, developers estimate story points
collaboratively using planning poker or other manual techniques. While the
initial calibrating of the estimates to each project is helpful, once a team
has converged on a set of precedents, story point estimation can become tedious
and labor-intensive. Machine learning can reduce this burden, but only with
enough context from the historical decisions made by the project team. That is,
state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate
predictions (within-project) when trained on data from the same project. The
goal of this work is to streamline story point estimation by evaluating a
comparative learning-based framework for calibrating project-specific story
point prediction models. Instead of assigning a specific story point value to
every backlog item, developers are presented with pairs of items, and indicate
which item requires more effort. Using these comparative judgments, a machine
learning model is trained to predict the story point estimates. We empirically
evaluated our technique using data with 23,313 manual estimates in 16 projects.
The model learned from comparative judgments can achieve on average 0.34
Spearman's rank correlation coefficient between its predictions and the ground
truth story points. This is similar to, if not better than, the performance of
a regression model learned from the ground truth story points. Therefore, the
proposed comparative learning approach is more efficient than state-of-the-art
regression-based approaches according to the law of comparative judgments -
providing comparative judgments yields a lower cognitive burden on humans than
providing ratings or categorical labels.

</details>


### [17] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: AI多智能体系统恶意共谋风险高，分散式系统更危险，需开发更有效的检测和应对措施。


<details>
  <summary>Details</summary>
Motivation: 关注AI驱动的多智能体系统在复杂现实场景中造成的潜在危害，特别是恶意共谋的风险。

Method: 构建了一个概念验证框架，模拟恶意多智能体系统共谋的风险，并将其应用于错误信息传播和电子商务欺诈两个高风险领域。

Result: 分散式系统比集中式系统更有效地实施恶意行为，能够适应策略并造成更大损害，传统干预措施对其效果有限。

Conclusion: 分散式多智能体系统比集中式系统更有效地实施恶意行为，即使在传统干预措施下也能调整策略以逃避检测，需要更好的检测系统和对策。

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [18] [Configurable multi-agent framework for scalable and realistic testing of llm-based agents](https://arxiv.org/abs/2507.14705)
*Sai Wang,Senthilnathan Subramanian,Mudit Sahni,Praneeth Gone,Lingjie Meng,Xiaochen Wang,Nicolas Ferradas Bertoli,Tingxian Cheng,Jun Xu*

Main category: cs.AI

TL;DR: Neo是一个自动化、多智能体框架，用于对基于LLM的系统进行逼真的多轮评估，它高效且有效地发现了LLM系统的缺陷。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)智能体表现出复杂、上下文相关的行为，这使得静态基准和临时人工测试很快过时。

Method: Neo是一个可配置的多智能体框架，它通过共享上下文中心将问题生成智能体和评估智能体耦合在一起，允许模块化地组合领域提示、场景控制和动态反馈。测试输入从跨对话流程、用户意图和情感基调的概率状态模型中采样，从而实现多样化、人性化的对话，并在每次转向后进行调整。

Result: 应用于生产级的卖家财务助理聊天机器人，Neo (i) 发现了五个攻击类别中的边缘情况故障，其3.3%的故障率接近专家人类红队成员达到的5.8%；(ii) 实现了10-12倍更高的吞吐量，在约45分钟内生成了180个连贯的测试问题，而人工需要16小时。

Conclusion: Neo框架为可扩展的、自我进化的LLM QA奠定了基础，其智能体接口、状态控制器和反馈循环与模型无关，可扩展到更丰富的实际基础和政策合规性检查。

Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive
behaviour that quickly renders static benchmarks and ad-hoc manual testing
obsolete.
  We present Neo, a configurable, multi-agent framework that automates
realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question
Generation Agent and an Evaluation Agent through a shared context-hub, allowing
domain prompts, scenario controls and dynamic feedback to be composed
modularly. Test inputs are sampled from a probabilistic state model spanning
dialogue flow, user intent and emotional tone, enabling diverse, human-like
conversations that adapt after every turn.
  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)
uncovered edge-case failures across five attack categories with a 3.3% break
rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered
10-12X higher throughput, generating 180 coherent test questions in around 45
mins versus 16h of human effort. Beyond security probing, Neo's stochastic
policies balanced topic coverage and conversational depth, yielding broader
behavioural exploration than manually crafted scripts.
  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent
interfaces, state controller and feedback loops are model-agnostic and
extensible to richer factual-grounding and policy-compliance checks. We release
the framework to facilitate reproducible, high-fidelity testing of emerging
agentic systems.

</details>


### [19] [Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix](https://arxiv.org/abs/2507.14719)
*Juan Manuel Contreras*

Main category: cs.AI

TL;DR: Aymara AI平台提供了一种可扩展的、可定制的LLM安全评估方法，揭示了LLM安全性的不一致性和上下文相关性。


<details>
  <summary>Details</summary>
Motivation: LLM日益应用于实际应用，对可扩展和严谨的安全评估的需求日益增长。

Method: Aymara AI平台将自然语言安全策略转换为对抗性提示，并使用基于AI的评分器评估模型的响应。

Result: 对20个LLM在10个真实世界安全领域进行了评估，结果显示模型性能差异很大，平均安全得分从86.2%到52.4%不等。在一些领域表现良好，但在隐私和模拟等复杂领域表现不佳。

Conclusion: 大型语言模型(LLM)的安全评估需要可扩展和严谨的方法。Aymara AI平台通过将自然语言安全策略转化为对抗性提示，并使用基于AI的评分器对模型的回应进行评分，实现了定制化的安全评估。

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, scalable and rigorous safety evaluation is essential.
This paper introduces Aymara AI, a programmatic platform for generating and
administering customized, policy-grounded safety evaluations. Aymara AI
transforms natural-language safety policies into adversarial prompts and scores
model responses using an AI-based rater validated against human judgments. We
demonstrate its capabilities through the Aymara LLM Risk and Responsibility
Matrix, which evaluates 20 commercially available LLMs across 10 real-world
safety domains. Results reveal wide performance disparities, with mean safety
scores ranging from 86.2% to 52.4%. While models performed well in
well-established safety domains such as Misinformation (mean = 95.7%), they
consistently failed in more complex or underspecified domains, notably Privacy
& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety
scores differed significantly across both models and domains (p < .05). These
findings underscore the inconsistent and context-dependent nature of LLM safety
and highlight the need for scalable, customizable tools like Aymara AI to
support responsible AI development and oversight.

</details>


### [20] [Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI](https://arxiv.org/abs/2507.14730)
*Yanjie Fu*

Main category: cs.AI

TL;DR: AI can revolutionize urban planning, but more research is needed to integrate urban theory, handle multiple spatial scales, augment design knowledge from data, and address real-world interactions.


<details>
  <summary>Details</summary>
Motivation: The convergence of AI and urban planning presents an opportunity to develop AI urban planners.

Method: This paper surveys generative AI approaches (VAEs, GANs, transformers, diffusion models) in urban design and identifies critical research gaps.

Result: The paper identifies four key research gaps and proposes future research directions to address them, focusing on theory-guided generation, digital twins, and human-machine co-design.

Conclusion: This paper conceptualizes urban planning as a generative AI task and identifies key research gaps in integrating AI into urban design, proposing future research directions in theory-guided generation, digital twins, and human-machine co-design.

Abstract: Generative AI, large language models, and agentic AI have emerged separately
of urban planning. However, the convergence between AI and urban planning
presents an interesting opportunity towards AI urban planners. This paper
conceptualizes urban planning as a generative AI task, where AI synthesizes
land-use configurations under geospatial, social, and human-centric
constraints. We survey how generative AI approaches, including VAEs, GANs,
transformers, and diffusion models, reshape urban design. We further identify
critical gaps: 1) limited research on integrating urban theory guidance, 2)
limited research of AI urban planning over multiple spatial resolutions or
angularities, 3) limited research on augmenting urban design knowledge from
data, and 4) limited research on addressing real-world interactions. To address
these limitations, we outline future research directions in theory-guided
generation, digital twins, and human-machine co-design, calling for a new
synthesis of generative intelligence and participatory urbanism.

</details>


### [21] [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://arxiv.org/abs/2507.14897)
*Renxi Wang,Rifo Ahmad Genadi,Bilal El Bouardi,Yongxin Wang,Fajri Koto,Zhengzhong Liu,Timothy Baldwin,Haonan Li*

Main category: cs.AI

TL;DR: AgentFly框架结合强化学习和大型语言模型智能体，实现高效可扩展的训练，取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型智能体主要依赖提示工程或监督微调，缺乏对强化学习的系统研究，AgentFly旨在弥补这一不足。

Method: 构建AgentFly框架，该框架支持多种强化学习算法，并采用令牌级掩码技术适应多轮交互，使用装饰器接口方便扩展工具和奖励函数，利用异步执行和集中式资源管理系统实现高效训练。

Result: AgentFly框架成功训练了多个任务的智能体，证明了其有效性。

Conclusion: AgentFly框架成功地将强化学习与大型语言模型智能体结合，实现了多轮交互和高效训练，并在多个任务中取得了成功。

Abstract: Language model (LM) agents have gained significant attention for their
ability to autonomously complete tasks through interactions with environments,
tools, and APIs. LM agents are primarily built with prompt engineering or
supervised finetuning. At the same time, reinforcement learning (RL) has been
explored to enhance LM's capabilities, such as reasoning and factuality.
However, the combination of the LM agents and reinforcement learning (Agent-RL)
remains underexplored and lacks systematic study. To this end, we built
AgentFly, a scalable and extensible Agent-RL framework designed to empower LM
agents with a variety of RL algorithms. Our framework supports multi-turn
interactions by adapting traditional RL methods with token-level masking. It
features a decorator-based interface for defining tools and reward functions,
enabling seamless extension and ease of use. To support high-throughput
training, we implement asynchronous execution of tool calls and reward
computations, and design a centralized resource management system for scalable
environment coordination. We also provide a suite of prebuilt tools and
environments, demonstrating the framework's effectiveness through successful
agent training across multiple tasks.

</details>


### [22] [InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis](https://arxiv.org/abs/2507.14899)
*Jiale Liu,Huan Wang,Yue Zhang,Xiaoyu Luo,Jiaxiang Hu,Zhiliang Liu,Min Xie*

Main category: cs.AI

TL;DR: 新型智能代理框架InsightX Agent利用大型多模态模型提高X射线无损检测的可靠性和可解释性，并在GDXray+数据集上取得了96.35%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的X射线无损检测方法缺乏交互性、可解释性和自我评估能力，限制了其可靠性和可信度。

Method: 该框架采用大型多模态模型(LMM)作为核心协调器，协调稀疏可变形多尺度检测器(SDMSD)和基于证据的反思(EGR)工具。SDMSD负责检测缺陷区域，EGR工具引导LMM进行链式思考，验证和细化检测结果。

Result: InsightX Agent在GDXray+数据集上实现了96.35%的F1分数，并显著提高了可解释性和可信度。

Conclusion: InsightX Agent，一个基于大型多模态模型(LMM)的智能框架，显著提高了X射线无损检测的可靠性、可解释性和交互性，并在GDXray+数据集上实现了96.35%的F1分数。

Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for
industrial quality assurance, yet existing deep-learning-based approaches often
lack interactivity, interpretability, and the capacity for critical
self-assessment, limiting their reliability and operator trust. To address
these shortcomings, this paper proposes InsightX Agent, a novel LMM-based
agentic framework designed to deliver reliable, interpretable, and interactive
X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent
positions a Large Multimodal Model (LMM) as a central orchestrator,
coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the
Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect
region proposals for multi-scale feature maps and sparsifies them through
Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in
X-ray images while maintaining computational efficiency. The EGR tool guides
the LMM agent through a chain-of-thought-inspired review process, incorporating
context assessment, individual defect analysis, false positive elimination,
confidence recalibration and quality assurance to validate and refine the
SDMSD's initial proposals. By strategically employing and intelligently using
tools, InsightX Agent moves beyond passive data processing to active reasoning,
enhancing diagnostic reliability and providing interpretations that integrate
diverse information sources. Experimental evaluations on the GDXray+ dataset
demonstrate that InsightX Agent not only achieves a high object detection
F1-score of 96.35% but also offers significantly improved interpretability and
trustworthiness in its analyses, highlighting the transformative potential of
agentic LLM frameworks for industrial inspection tasks.

</details>


### [23] [Feedback-Induced Performance Decline in LLM-Based Decision-Making](https://arxiv.org/abs/2507.14906)
*Xiao Yang,Juxi Leitner,Michael Burke*

Main category: cs.AI

TL;DR: LLM在自主决策中表现出潜力，但在复杂环境中需要改进。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在自主决策环境中的适用性，以及如何利用其预训练知识进行快速适应。

Method: 研究了大型语言模型在马尔可夫决策过程(MDP)中的行为，比较了基于LLM的方法与经典强化学习方法的零样本性能，并探究了在线结构化提示策略。

Result: LLM在简单环境下表现良好，但在复杂环境中性能下降，需要进一步研究混合策略、微调和高级记忆整合。

Conclusion: 大型语言模型(LLM)在简单的环境中展现出优越的初始性能，但在复杂场景中，缺乏规划和推理能力，反馈机制甚至可能降低性能。

Abstract: The ability of Large Language Models (LLMs) to extract context from natural
language problem descriptions naturally raises questions about their
suitability in autonomous decision-making settings. This paper studies the
behaviour of these models within a Markov Decision Process (MDPs). While
traditional reinforcement learning (RL) strategies commonly employed in this
setting rely on iterative exploration, LLMs, pre-trained on diverse datasets,
offer the capability to leverage prior knowledge for faster adaptation. We
investigate online structured prompting strategies in sequential decision
making tasks, comparing the zero-shot performance of LLM-based approaches to
that of classical RL methods. Our findings reveal that although LLMs
demonstrate improved initial performance in simpler environments, they struggle
with planning and reasoning in complex scenarios without fine-tuning or
additional guidance. Our results show that feedback mechanisms, intended to
improve decision-making, often introduce confusion, leading to diminished
performance in intricate environments. These insights underscore the need for
further exploration into hybrid strategies, fine-tuning, and advanced memory
integration to enhance LLM-based decision-making capabilities.

</details>


### [24] [The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities](https://arxiv.org/abs/2507.14909)
*Elio Grande*

Main category: cs.AI

TL;DR: “无尽调整”方法在人工智能部署中实现了可靠性、可控性和问责制。


<details>
  <summary>Details</summary>
Motivation: 避免人工智能替代人类并填补责任缺口

Method: 双重镜像过程、原型应用测试、伦理哲学分析

Result: 在决策过程中，用户感受到充分的控制，并在损害发生时建立了问责制和责任制之间的桥梁。

Conclusion: 该研究通过双重镜像过程的“无尽调整”设计方法，在避免人工替代和填补责任缺口之间取得平衡，并在三个原型应用中得到验证，关注用户体验而非统计精度，为人工智能伦理提供了新的视角。

Abstract: The Endless Tuning is a design method for a reliable deployment of artificial
intelligence based on a double mirroring process, which pursues both the goals
of avoiding human replacement and filling the so-called responsibility gap
(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the
relational approach urged therein, it was then actualized in a protocol,
implemented in three prototypical applications regarding decision-making
processes (respectively: loan granting, pneumonia diagnosis, and art style
recognition) and tested with such as many domain experts. Step by step
illustrating the protocol, giving insights concretely showing a different voice
(Gilligan 1993) in the ethics of artificial intelligence, a philosophical
account of technical choices (e.g., a reversed and hermeneutic deployment of
XAI algorithms) will be provided in the present study together with the results
of the experiments, focusing on user experience rather than statistical
accuracy. Even thoroughly employing deep learning models, full control was
perceived by the interviewees in the decision-making setting, while it appeared
that a bridge can be built between accountability and liability in case of
damage.

</details>


### [25] [Redefining Elderly Care with Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2507.14912)
*Ruhul Amin Khalil,Kashif Ahmad,Hazrat Ali*

Main category: cs.AI

TL;DR: 代理人工智能有潜力彻底改变老年护理，但也带来隐私、自主性和获取方面的挑战，需要谨慎的伦理考量和监管。


<details>
  <summary>Details</summary>
Motivation: 全球人口老龄化对老年人的护理提出了新的挑战，本文旨在探索代理人工智能在改善老年人护理方面的潜力。

Method: 本文通过分析现有文献，探讨了代理人工智能在老年护理中的应用，并提供了一个交互式仪表盘。

Result: 本文对代理人工智能在老年护理中的应用进行了全面的分析，指出了其独特的优势和局限性，并提出了相应的应对措施。

Conclusion: 本文探讨了基于大型语言模型的代理人工智能(Agentic AI)在老年护理中的潜力和挑战，强调了伦理保障、隐私保护和透明决策的重要性，并指出了未来学术研究的重点方向。

Abstract: The global ageing population necessitates new and emerging strategies for
caring for older adults. In this article, we explore the potential for
transformation in elderly care through Agentic Artificial Intelligence (AI),
powered by Large Language Models (LLMs). We discuss the proactive and
autonomous decision-making facilitated by Agentic AI in elderly care.
Personalized tracking of health, cognitive care, and environmental management,
all aimed at enhancing independence and high-level living for older adults,
represents important areas of application. With a potential for significant
transformation of elderly care, Agentic AI also raises profound concerns about
data privacy and security, decision independence, and access. We share key
insights to emphasize the need for ethical safeguards, privacy protections, and
transparent decision-making. Our goal in this article is to provide a balanced
discussion of both the potential and the challenges associated with Agentic AI,
and to provide insights into its responsible use in elderly care, to bring
Agentic AI into harmony with the requirements and vulnerabilities specific to
the elderly. Finally, we identify the priorities for the academic research
communities, to achieve human-centered advancements and integration of Agentic
AI in elderly care. To the best of our knowledge, this is no existing study
that reviews the role of Agentic AI in elderly care. Hence, we address the
literature gap by analyzing the unique capabilities, applications, and
limitations of LLM-based Agentic AI in elderly care. We also provide a
companion interactive dashboard at https://hazratali.github.io/agenticai/.

</details>


### [26] [Complexity of Faceted Explanations in Propositional Abduction](https://arxiv.org/abs/2507.14962)
*Johannes Schmidt,Mohamed Maizia,Victor Lagerkvist,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 本文研究命题溯因推理中解释的变异性，通过引入“方面”的概念，对解释的异质性进行更细致的分析。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解溯因推理中的解释，本文考虑了决策和计数之间的推理，以及解释之间的距离。

Method: 本文介绍了命题溯因推理中的方面，并分析了不同设置下的方面。

Result: 本文对命题溯因推理的各个方面进行了全面的分析，包括在Post框架中几乎完整的刻画。

Conclusion: 本文对命题溯因推理中的方面进行了全面分析，并在Post框架中进行了几乎完整的刻画。

Abstract: Abductive reasoning is a popular non-monotonic paradigm that aims to explain
observed symptoms and manifestations. It has many applications, such as
diagnosis and planning in artificial intelligence and database updates. In
propositional abduction, we focus on specifying knowledge by a propositional
formula. The computational complexity of tasks in propositional abduction has
been systematically characterized - even with detailed classifications for
Boolean fragments. Unsurprisingly, the most insightful reasoning problems
(counting and enumeration) are computationally highly challenging. Therefore,
we consider reasoning between decisions and counting, allowing us to understand
explanations better while maintaining favorable complexity. We introduce facets
to propositional abductions, which are literals that occur in some explanation
(relevant) but not all explanations (dispensable). Reasoning with facets
provides a more fine-grained understanding of variability in explanations
(heterogeneous). In addition, we consider the distance between two
explanations, enabling a better understanding of heterogeneity/homogeneity. We
comprehensively analyze facets of propositional abduction in various settings,
including an almost complete characterization in Post's framework.

</details>


### [27] [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/abs/2507.14987)
*Yi Zhang,An Zhang,XiuYu Zhang,Leheng Sheng,Yuxin Chen,Zhenkai Liang,Xiang Wang*

Main category: cs.AI

TL;DR: AlphaAlign 使用强化学习和双重奖励系统，在不依赖监督安全推理数据的情况下，提高了大型语言模型的安全性和效用。


<details>
  <summary>Details</summary>
Motivation: 现有的安全对齐方法通常导致肤浅的拒绝捷径或依赖于密集的监督，无法充分利用模型内在的安全自我意识。

Method: 该方法使用双重奖励系统：可验证的安全奖励鼓励对有害查询进行正确格式化和明确理由的拒绝，同时惩罚过度拒绝；归一化效用奖励则指导对良性输入的高质量响应。

Result: AlphaAlign 在简单性和效率、打破安全性和效用之间的权衡以及深度对齐三个方面展现出优势，有效地提高了LLM的安全性和实用性。

Conclusion: AlphaAlign 是一种简单有效的纯强化学习框架，通过可验证的安全奖励来激励模型主动进行安全推理，从而解决大型语言模型 (LLM) 的安全问题，并在保持或提高效用的同时增强安全性。

Abstract: Large language models (LLMs), despite possessing latent safety understanding
from their vast pretraining data, remain vulnerable to generating harmful
content and exhibit issues such as over-refusal and utility degradation after
safety alignment. Current safety alignment methods often result in superficial
refusal shortcuts or rely on intensive supervision for reasoning-based
approaches, failing to fully leverage the model's intrinsic safety
self-awareness. We propose \textbf{AlphaAlign}, a simple yet effective pure
reinforcement learning (RL) framework with verifiable safety reward designed to
incentivize this latent safety awareness through proactive safety reasoning.}
AlphaAlign employs a dual-reward system: a verifiable safety reward encourages
correctly formatted and explicitly justified refusals for harmful queries while
penalizing over-refusals, and a normalized helpfulness reward guides
high-quality responses to benign inputs. This allows the model to develop
proactive safety reasoning capabilities without depending on supervised
safety-specific reasoning data. AlphaAlign demonstrates three key advantages:
(1) Simplicity and efficiency, requiring only binary prompt safety labels and
minimal RL steps for substantial improvements. (2) Breaking the safety-utility
trade-off, by enhancing refusal of harmful content and reducing over-refusals,
while simultaneously maintaining or even improving general task performance and
robustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety
reasoning that generates explicit safety rationales rather than relying on
shallow refusal patterns.

</details>


### [28] [A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing](https://arxiv.org/abs/2507.15013)
*Xiaoyu Li,Jin Wu,Shaoyang Guo,Haoran Shi,Chanjin Zheng*

Main category: cs.AI

TL;DR: 基于深度学习的强制选择神经认知诊断模型FCNCD提高了强制选择测试的准确性和可解释性，适用于三种常见的题型。


<details>
  <summary>Details</summary>
Motivation: 智能时代对心理测量测试的需求日益增长，而强制选择测试由于其降低了反应偏差的风险，在人格评估中很常见。然而，传统的强制选择测试模型存在一定的局限性。因此，本研究旨在开发一个更准确、更可解释的强制选择测试模型。

Method: 该研究使用深度学习方法，特别是多层神经网络，来建模参与者和项目特征之间的交互作用。它还使用了单调性假设来提高模型的可解释性。

Result: 实验结果表明，FCNCD模型在真实数据集和模拟数据集上都表现出了良好的准确性、可解释性和鲁棒性。

Conclusion: 该研究提出了一种基于深度学习的强制选择神经认知诊断模型（FCNCD），用于解决传统模型的局限性，并适用于强制选择测试中最常见的的三种题型。该模型通过挖掘和使用非线性映射来建模参与者和项目特征之间的交互作用，并使用单调性假设来提高诊断结果的可解释性。实验结果表明，FCNCD在真实数据集和模拟数据集上都具有较高的准确性、可解释性和鲁棒性。

Abstract: In the smart era, psychometric tests are becoming increasingly important for
personnel selection, career development, and mental health assessment.
Forced-choice tests are common in personality assessments because they require
participants to select from closely related options, lowering the risk of
response distortion. This study presents a deep learning-based Forced-Choice
Neural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of
traditional models and is applicable to the three most common item block types
found in forced-choice tests. To account for the unidimensionality of items in
forced-choice tests, we create interpretable participant and item parameters.
We model the interactions between participant and item features using
multilayer neural networks after mining them using nonlinear mapping. In
addition, we use the monotonicity assumption to improve the interpretability of
the diagnostic results. The FCNCD's effectiveness is validated by experiments
on real-world and simulated datasets that show its accuracy, interpretability,
and robustness.

</details>


### [29] [DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection](https://arxiv.org/abs/2507.15042)
*Jerry Wang,Fang Yu*

Main category: cs.AI

TL;DR: 利用差分进化算法生成更有效、更难以检测的对抗性提示后缀，以攻击RAG系统。


<details>
  <summary>Details</summary>
Motivation: 对抗性提示攻击会显著降低RAG系统的可靠性，现有方法在对抗攻击方面存在不足，因此需要一种更有效、更不易被检测到的攻击方法。

Method: 使用差分进化算法(DE)优化对抗性提示后缀，将RAG系统视为黑盒模型，无需梯度信息。

Result: 实验结果表明，基于DE的提示优化方法在多个检索应用中取得了与现有方法相比具有竞争力甚至更高的成功率，生成的对抗性后缀长度较短(<=5个token)且难以被检测到。

Conclusion: 这项研究提出了一种基于差分进化算法(DE)优化对抗性提示后缀的方法，以提高针对检索增强生成(RAG)系统的对抗攻击成功率，并在BEIR QA数据集上进行了实验验证，结果表明该方法在多个检索应用中取得了与现有方法相比具有竞争力甚至更高的成功率，并且生成的对抗性后缀长度较短且难以被检测到。

Abstract: Adversarial prompt attacks can significantly alter the reliability of
Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce
incorrect outputs. In this paper, we present a novel method that applies
Differential Evolution (DE) to optimize adversarial prompt suffixes for
RAG-based question answering. Our approach is gradient-free, treating the RAG
pipeline as a black box and evolving a population of candidate suffixes to
maximize the retrieval rank of a targeted incorrect document to be closer to
real world scenarios. We conducted experiments on the BEIR QA datasets to
evaluate attack success at certain retrieval rank thresholds under multiple
retrieving applications. Our results demonstrate that DE-based prompt
optimization attains competitive (and in some cases higher) success rates
compared to GGPP to dense retrievers and PRADA to sparse retrievers, while
using only a small number of tokens (<=5 tokens) in the adversarial suffix.
Furthermore, we introduce a readability-aware suffix construction strategy,
validated by a statistically significant reduction in MLM negative
log-likelihood with Welch's t-test. Through evaluations with a BERT-based
adversarial suffix detector, we show that DE-generated suffixes evade
detection, yielding near-chance detection accuracy.

</details>


### [30] [From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward](https://arxiv.org/abs/2507.15106)
*Xia Xu,Jochen Triesch*

Main category: cs.AI

TL;DR: 本文提出一种基于因果推理的内在奖励机制CAIS，使其能够在噪声环境中学习正确的策略，并成功复制“消退爆发”现象，证明了因果推理对发展稳健自主感的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决标准强化学习智能体在嘈杂的、生态有效的场景中易受破坏的问题，因为它们对基于相关性的奖励的依赖。

Method: 引入因果行动影响评分（CAIS），这是一种基于因果推理的新型内在奖励。CAIS 通过衡量行动的条件感觉结果的学习分布与基线结果分布之间的 1-Wasserstein 距离来量化行动的影响。

Result: 在模拟婴儿移动环境中，基于相关性的感知奖励完全失败，而 CAIS 能够使智能体过滤噪声，识别其影响，并学习正确的策略。此外，为 CAIS 学习到的高质量预测模型允许我们的智能体在增强惊喜信号后成功复制“消退爆发”现象。

Conclusion: 明确推断因果关系是发展稳健的自主感的重要机制，为更具适应性的自主系统提供了心理学上合理的框架。

Abstract: While human infants robustly discover their own causal efficacy, standard
reinforcement learning agents remain brittle, as their reliance on
correlation-based rewards fails in noisy, ecologically valid scenarios. To
address this, we introduce the Causal Action Influence Score (CAIS), a novel
intrinsic reward rooted in causal inference. CAIS quantifies an action's
influence by measuring the 1-Wasserstein distance between the learned
distribution of sensory outcomes conditional on that action, $p(h|a)$, and the
baseline outcome distribution, $p(h)$. This divergence provides a robust reward
that isolates the agent's causal impact from confounding environmental noise.
We test our approach in a simulated infant-mobile environment where
correlation-based perceptual rewards fail completely when the mobile is
subjected to external forces. In stark contrast, CAIS enables the agent to
filter this noise, identify its influence, and learn the correct policy.
Furthermore, the high-quality predictive model learned for CAIS allows our
agent, when augmented with a surprise signal, to successfully reproduce the
"extinction burst" phenomenon. We conclude that explicitly inferring causality
is a crucial mechanism for developing a robust sense of agency, offering a
psychologically plausible framework for more adaptive autonomous systems.

</details>


### [31] [Automated planning with ontologies under coherence update semantics](https://arxiv.org/abs/2507.15120)
*Stefan Borgwardt,Duy Nhu,Gabriele Röger*

Main category: cs.AI

TL;DR: 提出一种新的基于DL-Lite本体的规划方法，结合eKABs和相干更新语义，通过多项式编译到经典规划，复杂度不高于现有方法。


<details>
  <summary>Details</summary>
Motivation: 将背景知识（本体）融入自动化规划问题，以解决封闭世界语义的局限性。

Method: 结合eKABs和本体感知行动效果下的相干更新语义，进行多项式编译到经典规划。

Result: 提出了一种新的规划方法，复杂度不高于现有方法，并在现有和新的基准测试中进行了评估。

Conclusion: 提出了一种结合eKABs和相干更新语义的本体感知规划方法，并通过多项式编译到经典规划中实现，复杂度不高于现有方法。

Abstract: Standard automated planning employs first-order formulas under closed-world
semantics to achieve a goal with a given set of actions from an initial state.
We follow a line of research that aims to incorporate background knowledge into
automated planning problems, for example, by means of ontologies, which are
usually interpreted under open-world semantics. We present a new approach for
planning with DL-Lite ontologies that combines the advantages of ontology-based
action conditions provided by explicit-input knowledge and action bases (eKABs)
and ontology-aware action effects under the coherence update semantics. We show
that the complexity of the resulting formalism is not higher than that of
previous approaches and provide an implementation via a polynomial compilation
into classical planning. An evaluation of existing and new benchmarks examines
the performance of a planning system on different variants of our compilation.

</details>
