<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [From Reasoning to Super-Intelligence: A Search-Theoretic Perspective](https://arxiv.org/abs/2507.15865)
*Shai Shalev-Shwartz,Amnon Shashua*

Main category: cs.AI

TL;DR: 提出Diligent Learner，一种新的CoT学习范式，解决了现有方法在复杂推理任务上的不足，并证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的CoT学习方法（如SFT、RL、ToT和MCTS）在复杂的推理任务上常常失败，主要障碍包括分布漂移、缺乏嵌入式搜索和指数推理成本。

Method: 提出了一种新的学习范式Diligent Learner，它将推理建模为由验证器引导的深度优先搜索，并在失败时支持回溯。

Result: 证明Diligent Learner可以在两个温和且现实的假设下有效地从CoT数据中学习，而现有方法却无法做到。

Conclusion: Diligent Learner，一种新的学习范式，通过验证器引导的深度优先搜索对推理建模，并在失败时支持回溯，可以有效地从CoT数据中学习，而现有方法却无法做到。

Abstract: Chain-of-Thought (CoT) reasoning has emerged as a powerful tool for enhancing
the problem-solving capabilities of large language models (LLMs). However, the
theoretical foundations of learning from CoT data remain underdeveloped, and
existing approaches -- such as Supervised Fine-Tuning (SFT), Reinforcement
Learning (RL), Tree-of-Thoughts (ToT), and Monte Carlo Tree Search (MCTS) --
often fail on complex reasoning tasks. In this work, we identify core obstacles
that hinder effective CoT learning, including distribution drift, lack of
embedded search, and exponential inference costs. We introduce the Diligent
Learner, a new learning paradigm that explicitly models reasoning as a
depth-first search guided by a validator and supports backtracking upon
failure. Under two mild and realistic assumptions, we prove that the Diligent
Learner can efficiently learn from CoT data while existing methods fail to do
so. This framework offers a path toward building scalable and reliable
reasoning systems trained on naturally occurring, incomplete data -- paving the
way for the development of Large Reasoning Models (LRMs) with robust,
interpretable problem-solving abilities.

</details>


### [2] [Purchase and Production Optimization in a Meat Processing Plant](https://arxiv.org/abs/2507.15866)
*Marek Vlk,Premysl Sucha,Jaroslaw Rudy,Radoslaw Idzikowski*

Main category: cs.AI

TL;DR: 针对肉类加工的物料采购和加工优化问题，提出一种高效的基于整数线性规划的迭代方法，可在几秒内求解实际案例。


<details>
  <summary>Details</summary>
Motivation: 提高肉类加工行业的生产效率和盈利能力，解决物料采购和加工过程中的优化问题。

Method: 基于整数线性规划的迭代方法

Result: 该方法能够在几秒钟内为所有考虑的用例找到最优解，并有效缓解数值问题。

Conclusion: 本文针对肉类加工公司中物料采购和加工的优化问题提出了一种基于整数线性规划的迭代方法，该方法能够有效解决实际案例，并在几秒钟内找到最优解。

Abstract: The food production industry, especially the meat production sector, faces
many challenges that have even escalated due to the recent outbreak of the
energy crisis in the European Union. Therefore, efficient use of input
materials is an essential aspect affecting the profit of such companies. This
paper addresses an optimization problem concerning the purchase and subsequent
material processing we solved for a meat processing company. Unlike the
majority of existing papers, we do not concentrate on how this problem concerns
supply chain management, but we focus purely on the production stage. The
problem involves the concept of alternative ways of material processing, stock
of material with different expiration dates, and extra constraints widely
neglected in the current literature, namely, the minimum order quantity and the
minimum percentage in alternatives. We prove that each of these two constraints
makes the problem \mbox{$\mathcal{NP}$-hard}, and hence we design a simple
iterative approach based on integer linear programming that allows us to solve
real-life instances even using an open-source integer linear programming
solver. Another advantage of this approach is that it mitigates numerical
issues, caused by the extensive range of data values, we experienced with a
commercial solver. The results obtained using real data from the meat
processing company showed that our algorithm can find the optimum solution in a
few seconds for all considered use cases.

</details>


### [3] [Why Braking? Scenario Extraction and Reasoning Utilizing LLM](https://arxiv.org/abs/2507.15874)
*Yin Wu,Daniel Slieter,Vivek Subramanian,Ahmed Abouelazm,Robin Bohn,J. Marius Zöllner*

Main category: cs.AI

TL;DR: 利用LLM分析ADAS数据中的刹车事件，以识别安全关键的极端情况，优于现有基于规则的方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖基于规则的启发式方法，在复杂的城市环境中缺乏泛化能力，因此本文提出利用LLM解决此问题。

Method: 该方法桥接了低级数值信号和自然语言描述之间的差距，使LLM能够解释和分类驾驶场景，并提出了一种双路径场景检索方法，支持基于类别的已知场景搜索和基于嵌入的未知场景检索。

Result: 实验结果表明，该方法优于基于规则的基线方法，并且能够很好地泛化到OOD场景。

Conclusion: 该论文提出了一种利用大型语言模型 (LLM) 进行场景理解和推理的新型框架，用于识别和理解自动驾驶辅助系统 (ADAS) 数据中安全关键的极端情况，该方法优于基于规则的基线方法，并且能够很好地泛化到未知场景。

Abstract: The growing number of ADAS-equipped vehicles has led to a dramatic increase
in driving data, yet most of them capture routine driving behavior. Identifying
and understanding safety-critical corner cases within this vast dataset remains
a significant challenge. Braking events are particularly indicative of
potentially hazardous situations, motivating the central question of our
research: Why does a vehicle brake? Existing approaches primarily rely on
rule-based heuristics to retrieve target scenarios using predefined condition
filters. While effective in simple environments such as highways, these methods
lack generalization in complex urban settings. In this paper, we propose a
novel framework that leverages Large Language Model (LLM) for scenario
understanding and reasoning. Our method bridges the gap between low-level
numerical signals and natural language descriptions, enabling LLM to interpret
and classify driving scenarios. We propose a dual-path scenario retrieval that
supports both category-based search for known scenarios and embedding-based
retrieval for unknown Out-of-Distribution (OOD) scenarios. To facilitate
evaluation, we curate scenario annotations on the Argoverse 2 Sensor Dataset.
Experimental results show that our method outperforms rule-based baselines and
generalizes well to OOD scenarios.

</details>


### [4] [Differential Multimodal Transformers](https://arxiv.org/abs/2507.15875)
*Jerry Li,Timothy Oh,Joseph Hoang,Vardhit Veeramachaneni*

Main category: cs.AI

TL;DR: 将Differential Attention应用于文本视觉模型PaliGemma，有效减少噪声和幻觉，提升了信息检索和问答性能。


<details>
  <summary>Details</summary>
Motivation: 小语言模型的上下文窗口有限，结合视觉等多模态信息会加剧这个问题，Transformer注意力机制容易关注无关上下文。

Method: 微调PaliGemma 3B模型，使用LoRA并结合Differential Attention机制，实验不同的参数设置和配置。

Result: 实验证明Differential Attention可以增强噪声信息检索和问答能力。

Conclusion: Differential Attention机制可以有效应用于文本视觉模型PaliGemma，以减轻噪声信息检索和减少幻觉。

Abstract: Small language models have gained significant popularity due to their
efficiency and growing capabilities. However, incorporating additional
modalities, such as vision, can exacerbate the challenge of limited context
windows by introducing noise. Recent studies have highlighted that Transformer
attention mechanisms often disproportionately focus on irrelevant contexts. In
this work, we extend the Differential Attention mechanism, originally designed
for text-only models, to the text-vision model PaliGemma. Our aim is to
evaluate its ability to mitigate noisy information retrieval and reduce
hallucinations. To this end, we fine-tuned the PaliGemma 3B model using LoRA,
incorporating Differential Attention, and experimented with various parameter
settings and configurations. We demonstrate that Differential Attention can be
adapted and integrated into the fine-tuning of existing models to enhance noisy
information retrieval and question-answering capabilities.

</details>


### [5] [Re-evaluating Short- and Long-Term Trend Factors in CTA Replication: A Bayesian Graphical Approach](https://arxiv.org/abs/2507.15876)
*Eric Benhamou,Jean-Jacques Ohana,Alban Etienne,Béatrice Guez,Ethan Setrouk,Thomas Jacquot*

Main category: cs.AI

TL;DR: 本文利用贝叶斯图模型分析CTA策略中短期和长期趋势的风险调整后绩效，发现两者融合方式影响最终结果


<details>
  <summary>Details</summary>
Motivation: 探讨短期和长期趋势系统在CTA策略中的相对优劣势及相互作用，尚存在争议。

Method: 使用贝叶斯图模型动态分解CTA收益为短期趋势、长期趋势和市场Beta因素。

Result: 不同时间范围趋势的融合会影响策略的风险调整后绩效。

Conclusion: 本文通过贝叶斯图模型动态分解CTA收益，探讨了短期和长期趋势系统在风险调整后的绩效中的相对优劣势及相互作用，结果表明不同时间范围趋势的融合会影响策略的风险调整后绩效。

Abstract: Commodity Trading Advisors (CTAs) have historically relied on trend-following
rules that operate on vastly different horizons from long-term breakouts that
capture major directional moves to short-term momentum signals that thrive in
fast-moving markets. Despite a large body of work on trend following, the
relative merits and interactions of short-versus long-term trend systems remain
controversial. This paper adds to the debate by (i) dynamically decomposing CTA
returns into short-term trend, long-term trend and market beta factors using a
Bayesian graphical model, and (ii) showing how the blend of horizons shapes the
strategy's risk-adjusted performance.

</details>


### [6] [Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning](https://arxiv.org/abs/2507.15877)
*Simon Ouellette*

Main category: cs.AI

TL;DR: 执行引导的神经程序合成在ARC-AGI组合泛化方面表现最佳，测试时微调主要依赖分布内知识。


<details>
  <summary>Details</summary>
Motivation: 研究在ARC-AGI开放式问题领域中，模型的分布外泛化能力。

Method: 受控组合泛化实验，比较神经程序合成和测试时微调方法。

Result: 执行引导的神经程序合成优于其他算法；测试时微调主要利用分布内知识。

Conclusion: 执行引导的神经程序合成在组合新颖解决方案的能力方面优于所有参考算法。测试时微调在ARC-AGI上的成功主要在于引出LLM原本无法直接依赖的分布内知识。

Abstract: We run a controlled compositional generalization experiment in the ARC-AGI
domain: an open-world problem domain in which the ability to generalize
out-of-distribution is, by design, an essential characteristic for success. We
compare neural program synthesis and test-time fine-tuning approaches on this
experiment. We find that execution-guided neural program synthesis outperforms
all reference algorithms in its ability to compose novel solutions. Our
empirical findings also suggest that the success of TTFT on ARC-AGI lies mainly
in eliciting in-distribution knowledge that the LLM otherwise fails to rely on
directly.

</details>


### [7] [The Recursive Coherence Principle: A Formal Constraint on Scalable Intelligence, Alignment, and Reasoning Architecture](https://arxiv.org/abs/2507.15880)
*Andy E. Williams*

Main category: cs.AI

TL;DR: 要构建可扩展、一致的人工智能，需要满足递归一致性原理 (RCP)，而功能智能模型 (FMI) 是满足该原理的唯一已知方法。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能系统在扩展性、一致性和对齐性方面的问题。

Method: 形式化定义了递归一致性原理 (RCP) 和功能智能模型 (FMI)，并证明了任何缺乏 FMI 的系统都会随着规模的扩大而出现递归一致性崩溃。

Result: 提出了递归一致性原理 (RCP) 和功能智能模型 (FMI)，为构建安全、可泛化、鲁棒一致的人工智能提供了途径。

Conclusion: 递归一致性原理 (RCP) 是构建可扩展、一致人工智能系统的基础约束，缺乏RCP会导致错位、幻觉和不稳定等问题。功能智能模型 (FMI) 是唯一已知的能够满足RCP的算子。

Abstract: Intelligence-biological, artificial, or collective-requires structural
coherence across recursive reasoning processes to scale effectively. As complex
systems grow, coherence becomes fragile unless a higher-order structure ensures
semantic consistency. This paper introduces the Recursive Coherence Principle
(RCP): a foundational constraint stating that for any reasoning system of order
N, composed of systems operating over conceptual spaces of order N-1, semantic
coherence is preserved only by a recursively evaluable generalization operator
that spans and aligns those lower-order conceptual spaces. Crucially, this
coherence enables structural alignment. Without recursive coherence, no system
can reliably preserve goals, meanings, or reasoning consistency at scale. We
formally define the Functional Model of Intelligence (FMI) as the only known
operator capable of satisfying the RCP at any scale. The FMI is a minimal,
composable architecture with internal functions (evaluation, modeling,
adaptation, stability, decomposition, bridging) and external functions
(storage, recall, System 1 and System 2 reasoning) vital for preserving
semantic structure across inference and coordination layers. We prove that any
system lacking the FMI will experience recursive coherence breakdown as it
scales, arguing that common AI issues like misalignment, hallucination, and
instability are symptoms of this structural coherence loss. Unlike other
foundational principles, RCP uniquely captures the internal, recursive dynamics
needed for coherent, alignable intelligence, modeling semantic coherence under
recursion. This work significantly impacts AI alignment, advocating a shift
from behavioral constraints to structural coherence, and offers a pathway for
safely generalizable, robustly coherent AI at scale.

</details>


### [8] [ADEPTS: A Capability Framework for Human-Centered Agent Design](https://arxiv.org/abs/2507.15885)
*Pierluca D'Oro,Caley Drooff,Joy Chen,Joseph Tighe*

Main category: cs.AI

TL;DR: 提出ADEPTS框架，用于指导AI代理开发，使其更易于理解、控制和信任。


<details>
  <summary>Details</summary>
Motivation: 当前关于以人为中心的人工智能代理开发的指导分散且缺乏统一的用户视角。

Method: 提出了一种名为ADEPTS的能力框架。

Result: 提出了一个名为ADEPTS的能力框架，以帮助开发人员构建更易理解、可控和值得信赖的AI代理。

Conclusion: 介绍了ADEPTS，一个定义核心用户能力的框架，为人工智能代理的开发提供统一指导，弥合了技术和体验开发之间的差距。

Abstract: Large language models have paved the way to powerful and flexible AI agents,
assisting humans by increasingly integrating into their daily life. This
flexibility, potential, and growing adoption demands a holistic and
cross-disciplinary approach to developing, monitoring and discussing the
capabilities required for agent-driven user experiences. However, current
guidance on human-centered AI agent development is scattered: UX heuristics
focus on interface behaviors, engineering taxonomies describe internal
pipelines, and ethics checklists address high-level governance. There is no
concise, user-facing vocabulary that tells teams what an agent should
fundamentally be able to do. We introduce ADEPTS, a capability framework
defining a set of core user-facing capabilities to provide unified guidance
around the development of AI agents. ADEPTS is based on six principles for
human-centered agent design, that express the minimal, user-facing capabilities
an AI agent should demonstrate to be understandable, controllable and
trustworthy in everyday use. ADEPTS complements existing frameworks and
taxonomies; differently from them, it sits at the interface between technical
and experience development. By presenting ADEPTS, we aim to condense complex
AI-UX requirements into a compact framework that is actionable guidance for AI
researchers, designers, engineers, and policy reviewers alike. We believe
ADEPTS has the potential of accelerating the improvement of user-relevant agent
capabilities, of easing the design of experiences that take advantage of those
capabilities, and of providing a shared language to track and discuss progress
around the development of AI agents.

</details>


### [9] [Integrating Reason-Based Moral Decision-Making in the Reinforcement Learning Architecture](https://arxiv.org/abs/2507.15895)
*Lisa Dargasz*

Main category: cs.AI

TL;DR: This paper introduces RBAMAs, which use reinforcement learning and normative reasoning to create ethically behaving AI agents, demonstrating a first implementation and its potential.


<details>
  <summary>Details</summary>
Motivation: The increasing capability of artificial autonomous agents necessitates the development of ethically behaving agents, addressing challenges at the intersection of computer science and philosophy.

Method: The study extends the reinforcement learning architecture to enable moral decision-making based on normative reasoning, using case-based feedback to learn a reason-theory for processing morally relevant propositions and deriving moral obligations.

Result: The developed RBAMA adapts its behavior to conform to moral obligations while pursuing tasks, contributing to moral justifiability, robustness, and trustworthiness.

Conclusion: This study presents a first implementation of a Reason-Based Artificial Moral Agent (RBAMA) and demonstrates its potential in initial experiments.

Abstract: Reinforcement Learning is a machine learning methodology that has
demonstrated strong performance across a variety of tasks. In particular, it
plays a central role in the development of artificial autonomous agents. As
these agents become increasingly capable, market readiness is rapidly
approaching, which means those agents, for example taking the form of humanoid
robots or autonomous cars, are poised to transition from laboratory prototypes
to autonomous operation in real-world environments. This transition raises
concerns leading to specific requirements for these systems - among them, the
requirement that they are designed to behave ethically. Crucially, research
directed toward building agents that fulfill the requirement to behave
ethically - referred to as artificial moral agents(AMAs) - has to address a
range of challenges at the intersection of computer science and philosophy.
This study explores the development of reason-based artificial moral agents
(RBAMAs). RBAMAs are build on an extension of the reinforcement learning
architecture to enable moral decision-making based on sound normative
reasoning, which is achieved by equipping the agent with the capacity to learn
a reason-theory - a theory which enables it to process morally relevant
propositions to derive moral obligations - through case-based feedback. They
are designed such that they adapt their behavior to ensure conformance to these
obligations while they pursue their designated tasks. These features contribute
to the moral justifiability of the their actions, their moral robustness, and
their moral trustworthiness, which proposes the extended architecture as a
concrete and deployable framework for the development of AMAs that fulfills key
ethical desiderata. This study presents a first implementation of an RBAMA and
demonstrates the potential of RBAMAs in initial experiments.

</details>


### [10] [Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation](https://arxiv.org/abs/2507.15901)
*Joydeep Chandra,Satyam Kumar Navneet*

Main category: cs.AI

TL;DR: Ethical smart home systems need tailored explainability, granular consent, and robust override controls to protect vulnerable users from surveillance and bias.


<details>
  <summary>Details</summary>
Motivation: To provide a conceptual foundation and suggestions for developing transparent, inclusive, and trustworthy agentic AI in household automation, addressing ethical concerns for vulnerable user groups.

Method: Review of responsible innovation frameworks, human-centered design principles, governance practices, and data-driven insights (including social media analysis via NLP).

Result: Design imperatives such as tailored explainability, granular consent mechanisms, and robust override controls are highlighted, supported by participatory and inclusive methodologies.

Conclusion: This article analyzes the ethical challenges of proactive autonomous AI agents in household environments, focusing on privacy, fairness, and user control, and offers practical guidance for developing ethical smart home systems.

Abstract: The implementation of Artificial Intelligence (AI) in household environments,
especially in the form of proactive autonomous agents, brings about
possibilities of comfort and attention as well as it comes with intra or
extramural ethical challenges. This article analyzes agentic AI and its
applications, focusing on its move from reactive to proactive autonomy,
privacy, fairness and user control. We review responsible innovation
frameworks, human-centered design principles, and governance practices to
distill practical guidance for ethical smart home systems. Vulnerable user
groups such as elderly individuals, children, and neurodivergent who face
higher risks of surveillance, bias, and privacy risks were studied in detail in
context of Agentic AI. Design imperatives are highlighted such as tailored
explainability, granular consent mechanisms, and robust override controls,
supported by participatory and inclusive methodologies. It was also explored
how data-driven insights, including social media analysis via Natural Language
Processing(NLP), can inform specific user needs and ethical concerns. This
survey aims to provide both a conceptual foundation and suggestions for
developing transparent, inclusive, and trustworthy agentic AI in household
automation.

</details>


### [11] [Does More Inference-Time Compute Really Help Robustness?](https://arxiv.org/abs/2507.15974)
*Tong Wu,Chong Xiang,Jiachen T. Wang,Weichen Yu,Chawin Sitawarin,Vikash Sehwag,Prateek Mittal*

Main category: cs.AI

TL;DR: 推理时间扩展提高模型鲁棒性，但前提是中间推理步骤隐藏；否则，反而降低鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 先前工作表明增加推理时间计算可以提高大型专有推理LLM的鲁棒性，本文研究小型开源模型是否也适用，并探讨了中间推理步骤可见性对模型鲁棒性的影响。

Method: 首先验证了小型开源模型也能从推理时间扩展中获益；然后，通过放宽先前工作中关于中间推理步骤隐藏的假设，揭示并验证了一个逆向缩放规律：如果中间推理步骤可访问，则增加推理时间计算会降低模型鲁棒性；最后，讨论了具有隐藏推理链的模型仍然容易受到攻击的情况。

Result: 小型开源模型同样受益于推理时间扩展；中间推理步骤可见性会降低推理时间扩展带来的鲁棒性提升；具有工具集成推理和高级推理提取攻击的模型容易受到攻击。

Conclusion: 推理时间扩展对模型鲁棒性的影响取决于对抗环境和部署环境，在安全敏感的实际应用中需要谨慎权衡。

Abstract: Recently, Zaremba et al. demonstrated that increasing inference-time
computation improves robustness in large proprietary reasoning LLMs. In this
paper, we first show that smaller-scale, open-source models (e.g., DeepSeek R1,
Qwen3, Phi-reasoning) can also benefit from inference-time scaling using a
simple budget forcing strategy. More importantly, we reveal and critically
examine an implicit assumption in prior work: intermediate reasoning steps are
hidden from adversaries. By relaxing this assumption, we identify an important
security risk, intuitively motivated and empirically verified as an inverse
scaling law: if intermediate reasoning steps become explicitly accessible,
increased inference-time computation consistently reduces model robustness.
Finally, we discuss practical scenarios where models with hidden reasoning
chains are still vulnerable to attacks, such as models with tool-integrated
reasoning and advanced reasoning extraction attacks. Our findings collectively
demonstrate that the robustness benefits of inference-time scaling depend
heavily on the adversarial setting and deployment context. We urge
practitioners to carefully weigh these subtle trade-offs before applying
inference-time scaling in security-sensitive, real-world applications.

</details>


### [12] [Micromobility Flow Prediction: A Bike Sharing Station-level Study via Multi-level Spatial-Temporal Attention Neural Network](https://arxiv.org/abs/2507.16020)
*Xi Yang,Jiachen Wang,Song Han,Suining He*

Main category: cs.AI

TL;DR: BikeMAN神经网络有效预测了整个自行车共享系统的站点级别自行车流量。


<details>
  <summary>Details</summary>
Motivation: 解决现有自行车共享系统中站点级别供需不平衡导致的维护难题，以及现有方法难以对整个系统进行准确预测的问题。

Method: 提出了一种多层时空注意力神经网络BikeMAN，该网络包含编码器和解码器，并使用注意力机制来表示自行车站点特征之间的空间相关性和自行车站点流量的时间特征。

Result: 在纽约市超过700个站点，1000万次行程的数据集上，BikeMAN网络在预测自行车站点流量方面表现出高精度。

Conclusion: BikeMAN，一个多层时空注意力神经网络，能够准确预测整个自行车共享系统中各个站点级别的自行车流量，并在纽约市超过700个站点，1000万次行程的数据集上验证了其有效性。

Abstract: Efficient use of urban micromobility resources such as bike sharing is
challenging due to the unbalanced station-level demand and supply, which causes
the maintenance of the bike sharing systems painstaking. Prior efforts have
been made on accurate prediction of bike traffics, i.e., demand/pick-up and
return/drop-off, to achieve system efficiency. However, bike station-level
traffic prediction is difficult because of the spatial-temporal complexity of
bike sharing systems. Moreover, such level of prediction over entire bike
sharing systems is also challenging due to the large number of bike stations.
To fill this gap, we propose BikeMAN, a multi-level spatio-temporal attention
neural network to predict station-level bike traffic for entire bike sharing
systems. The proposed network consists of an encoder and a decoder with an
attention mechanism representing the spatial correlation between features of
bike stations in the system and another attention mechanism describing the
temporal characteristic of bike station traffic. Through experimental study on
over 10 millions trips of bike sharing systems (> 700 stations) of New York
City, our network showed high accuracy in predicting the bike station traffic
of all stations in the city.

</details>


### [13] [From Logic to Language: A Trust Index for Problem Solving with LLMs](https://arxiv.org/abs/2507.16028)
*Tehseen Rug,Felix Böhmer,Tessa Pfattheicher*

Main category: cs.AI

TL;DR: 本文构建了一个框架，对比了经典计算和基于LLM的计算在解决问题上的差异，并提出了评估LLM解决方案质量的新方法。


<details>
  <summary>Details</summary>
Motivation: 经典计算难以处理模糊、动态和主观性强的问题，而LLM能够处理自然语言，进而解决这类问题。

Method: 定义了形式语言和自然语言可解决的问题空间，并引入了衡量LLM答案稳健性和概念多样性的指标，以及将主观评价量化的方法。

Result: 提出了一个统一框架，用于理解和对比两种范式，并定义了衡量自然语言解决方案质量的向量值信任指数 Q 和相应的统计质量维度。

Conclusion: 本文介绍了一个统一的框架来理解和对比经典计算和基于大型语言模型 (LLM) 的计算范式在解决问题上的差异，并提出了一个向量值信任指数 Q 来衡量基于自然语言的解决方案的质量。

Abstract: Classical computation, grounded in formal, logical systems, has been the
engine of technological progress for decades, excelling at problems that can be
described with unambiguous rules. This paradigm, however, leaves a vast ocean
of human problems -- those characterized by ambiguity, dynamic environments,
and subjective context -- largely untouched. The advent of Large Language
Models (LLMs) represents a fundamental shift, enabling computational systems to
engage with this previously inaccessible domain using natural language. This
paper introduces a unified framework to understand and contrast these
problem-solving paradigms. We define and delineate the problem spaces
addressable by formal languages versus natural language. While solutions to the
former problem class can be evaluated using binary quality measures, the latter
requires a much more nuanced definition of approximate solution space taking
into account the vagueness, subjectivity and ambiguity inherent to natural
language. We therefore introduce a vector-valued trust index Q, which reflects
solution quality and distinguishes the binary correctness of formal solutions
from the continuous adequacy spectrum characteristic of natural language
solutions. Within this framework, we propose two statistical quality
dimensions. Normalized bi-semantic entropy measures robustness and conceptual
diversity of LLM answers given semantic variation in problem formulations.
Emotional valence maps subjective valuation of a solution to a quantifiable
metric that can be maximized by invoking statistical measures. The concepts
introduced in this work will provide a more rigorous understanding of the
capabilities, limitations, and inherent nature of problem-solving in the age of
LLMs.

</details>


### [14] [A Unifying Framework for Semiring-Based Constraint Logic Programming With Negation (full version)](https://arxiv.org/abs/2507.16067)
*Jeroen Spaans,Jesse Heyninck*

Main category: cs.AI

TL;DR: 本文扩展了CLP以允许在体中使用否定，并使用近似不动点理论框架提供了其语义。


<details>
  <summary>Details</summary>
Motivation: 现有的CLP扩展没有研究允许在体中使用否定的子句，本文旨在扩展CLP以统一这些扩展并允许在体中使用否定。

Method: 使用近似不动点理论框架

Result: 提供了一种统一的框架，该框架捕获了现有方法并允许使用更具表达力的语言扩展它们。

Conclusion: 本文研究了允许在体中使用否定的CLP的扩展，并使用近似不动点理论框架提供了此类程序的语义，详细概述了半环属性对结果语义的影响。

Abstract: Constraint Logic Programming (CLP) is a logic programming formalism used to
solve problems requiring the consideration of constraints, like resource
allocation and automated planning and scheduling. It has previously been
extended in various directions, for example to support fuzzy constraint
satisfaction, uncertainty, or negation, with different notions of semiring
being used as a unifying abstraction for these generalizations. None of these
extensions have studied clauses with negation allowed in the body. We
investigate an extension of CLP which unifies many of these extensions and
allows negation in the body. We provide semantics for such programs, using the
framework of approximation fixpoint theory, and give a detailed overview of the
impacts of properties of the semirings on the resulting semantics. As such, we
provide a unifying framework that captures existing approaches and allows
extending them with a more expressive language.

</details>


### [15] [Expert-Guided LLM Reasoning for Battery Discovery: From AI-Driven Hypothesis to Synthesis and Characterization](https://arxiv.org/abs/2507.16110)
*Shengchao Liu,Hannan Xu,Yan Ai,Huanxin Li,Yoshua Bengio,Harry Guo*

Main category: cs.AI

TL;DR: AI驱动的ChatBattery框架实现了从设计到合成到表征的完整闭环，成功发现了三种新型锂离子电池正极材料，显著提高了电池容量。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在特定领域应用（如电池发现）中的潜力。

Method: ChatBattery框架，一种结合领域知识来引导大型语言模型进行更有效推理的代理框架。

Result: 成功识别、合成和表征三种新型锂离子电池正极材料，容量分别提高28.8%、25.2%和18.5%。

Conclusion: ChatBattery框架成功地识别、合成和表征了三种新型锂离子电池正极材料，其容量分别比常用的NMC811正极材料提高了28.8%、25.2%和18.5%，并展示了AI驱动的推理在材料发现中的变革潜力。

Abstract: Large language models (LLMs) leverage chain-of-thought (CoT) techniques to
tackle complex problems, representing a transformative breakthrough in
artificial intelligence (AI). However, their reasoning capabilities have
primarily been demonstrated in solving math and coding problems, leaving their
potential for domain-specific applications-such as battery discovery-largely
unexplored. Inspired by the idea that reasoning mirrors a form of guided
search, we introduce ChatBattery, a novel agentic framework that integrates
domain knowledge to steer LLMs toward more effective reasoning in materials
design. Using ChatBattery, we successfully identify, synthesize, and
characterize three novel lithium-ion battery cathode materials, which achieve
practical capacity improvements of 28.8%, 25.2%, and 18.5%, respectively, over
the widely used cathode material, LiNi0.8Mn0.1Co0.1O2 (NMC811). Beyond this
discovery, ChatBattery paves a new path by showing a successful LLM-driven and
reasoning-based platform for battery materials invention. This complete
AI-driven cycle-from design to synthesis to characterization-demonstrates the
transformative potential of AI-driven reasoning in revolutionizing materials
discovery.

</details>


### [16] [TaxCalcBench: Evaluating Frontier Models on the Tax Calculation Task](https://arxiv.org/abs/2507.16126)
*Michael R. Bock,Kara Molisee,Zachary Ozer,Sumit Shah*

Main category: cs.AI

TL;DR: AI尚无法准确计算美国个人所得税。


<details>
  <summary>Details</summary>
Motivation: 探索AI计算个人所得税的可行性。

Method: 构建TaxCalcBench基准测试，评估模型计算个人所得税的能力。

Result: 结果表明，即使在简化的样本集上，最先进的模型也难以胜任这项任务。

Conclusion: 现有模型在计算个人所得税方面准确率不足三分之一，错误主要体现在误用税表、计算错误和资格认定错误。

Abstract: Can AI file your taxes? Not yet. Calculating US personal income taxes is a
task that requires building an understanding of vast amounts of English text
and using that knowledge to carefully compute results. We propose TaxCalcBench,
a benchmark for determining models' abilities to calculate personal income tax
returns given all of the necessary information. Our experiment shows that
state-of-the-art models succeed in calculating less than a third of federal
income tax returns even on this simplified sample set. Our analysis concludes
that models consistently misuse tax tables, make errors in tax calculation, and
incorrectly determine eligibility. Our findings point to the need for
additional infrastructure to apply LLMs to the personal income tax calculation
task.

</details>
