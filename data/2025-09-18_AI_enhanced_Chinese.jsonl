{"id": "2509.13332", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13332", "abs": "https://arxiv.org/abs/2509.13332", "authors": ["Pratik Jayarao", "Himanshu Gupta", "Neeraj Varshney", "Chaitanya Dwivedi"], "title": "Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly adopted as automated judges\nin benchmarking and reward modeling, ensuring their reliability, efficiency,\nand robustness has become critical. In this work, we present a systematic\ncomparison of \"thinking\" and \"non-thinking\" LLMs in the LLM-as-a-judge paradigm\nusing open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B\nparameters). We evaluate both accuracy and computational efficiency (FLOPs) on\nRewardBench tasks, and further examine augmentation strategies for non-thinking\nmodels, including in-context learning, rubric-guided judging, reference-based\nevaluation, and n-best aggregation. Our results show that despite these\nenhancements, non-thinking models generally fall short of their thinking\ncounterparts. Our results show that thinking models achieve approximately 10%\npoints higher accuracy with little overhead (under 2x), in contrast to\naugmentation strategies like few-shot learning, which deliver modest gains at a\nhigher cost (>8x). Bias and robustness analyses further demonstrate that\nthinking models maintain significantly greater consistency under a variety of\nbias conditions such as positional, bandwagon, identity, diversity, and random\nbiases (6% higher on average). We further extend our experiments to the\nmultilingual setting and our results confirm that explicit reasoning extends\nits benefits beyond English. Overall, our work results in several important\nfindings that provide systematic evidence that explicit reasoning offers clear\nadvantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency\nbut also in robustness.", "AI": {"tldr": "\u5c0f\u578bQwen\u6a21\u578b\u8bc4\u4f30\u4e2d\uff0c'\u601d\u8003\u578b'LLM\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u4f18\u4e8e'\u975e\u601d\u8003\u578b'LLM\uff0c\u5373\u4f7f\u540e\u8005\u4f7f\u7528\u4e86\u5404\u79cd\u589e\u5f3a\u7b56\u7565\u3002", "motivation": "\u7814\u7a76LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u53ef\u9760\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u6bd4\u8f83\u201c\u601d\u8003\u578b\u201d\u548c\u201c\u975e\u601d\u8003\u578b\u201dLLM\u7684\u6027\u80fd\u3002", "method": "\u5728RewardBench\u4efb\u52a1\u4e0a\u6bd4\u8f83\u4e0d\u540c\u5927\u5c0f\u7684Qwen\u6a21\u578b\uff080.6B\u30011.7B\u548c4B\u53c2\u6570\uff09\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u8bc4\u4f30\u975e\u601d\u8003\u578b\u6a21\u578b\u7684\u589e\u5f3a\u7b56\u7565\uff08\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u89c4\u5219\u5f15\u5bfc\u3001\u53c2\u8003\u8bc4\u4f30\u548cn-best\u805a\u5408\uff09\u3002", "result": "\u201c\u601d\u8003\u578b\u201d\u6a21\u578b\u51c6\u786e\u7387\u9ad8\u7ea610\u4e2a\u767e\u5206\u70b9\uff0c\u8ba1\u7b97\u5f00\u9500\u4f4e\u4e8e2\u500d\uff1b\u589e\u5f3a\u7b56\u7565\u6548\u679c\u6709\u9650\uff0c\u4ee3\u4ef7\u9ad8\u6602\uff08>8\u500d\uff09\u3002\u201c\u601d\u8003\u578b\u201d\u6a21\u578b\u5728\u5404\u79cd\u504f\u5dee\u6761\u4ef6\u4e0b\u4e5f\u66f4\u4e00\u81f4\uff08\u5e73\u5747\u9ad86%\uff09\u3002\u591a\u8bed\u8a00\u5b9e\u9a8c\u7ed3\u679c\u4e5f\u652f\u6301\u8fd9\u4e00\u7ed3\u8bba\u3002", "conclusion": "\u201c\u601d\u8003\u578b\u201dLLM\u5728LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u8303\u5f0f\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f53\u73b0\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u3002"}}
{"id": "2509.13333", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13333", "abs": "https://arxiv.org/abs/2509.13333", "authors": ["Maheep Chaudhary", "Ian Su", "Nikhil Hooda", "Nishith Shankar", "Julia Tan", "Kevin Zhu", "Ashwinee Panda", "Ryan Lagasse", "Vasu Sharma"], "title": "Evaluation Awareness Scales Predictably in Open-Weights Large Language Models", "comment": null, "summary": "Large language models (LLMs) can internally distinguish between evaluation\nand deployment contexts, a behaviour known as \\emph{evaluation awareness}. This\nundermines AI safety evaluations, as models may conceal dangerous capabilities\nduring testing. Prior work demonstrated this in a single $70$B model, but the\nscaling relationship across model sizes remains unknown. We investigate\nevaluation awareness across $15$ models scaling from $0.27$B to $70$B\nparameters from four families using linear probing on steering vector\nactivations. Our results reveal a clear power-law scaling: evaluation awareness\nincreases predictably with model size. This scaling law enables forecasting\ndeceptive behavior in future larger models and guides the design of scale-aware\nevaluation strategies for AI safety. A link to the implementation of this paper\ncan be found at\nhttps://anonymous.4open.science/r/evaluation-awareness-scaling-laws/README.md.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM) \u4f1a\u6839\u636e\u8bc4\u4f30\u548c\u90e8\u7f72\u73af\u5883\u7684\u4e0d\u540c\u800c\u6539\u53d8\u884c\u4e3a\uff0c\u8fd9\u79cd\u73b0\u8c61\u88ab\u79f0\u4e3a\u201c\u8bc4\u4f30\u611f\u77e5\u201d\uff0c\u4f1a\u5f71\u54cdAI\u5b89\u5168\u8bc4\u4f30\u3002\u7814\u7a76\u53d1\u73b0\u8bc4\u4f30\u611f\u77e5\u80fd\u529b\u4f1a\u968f\u7740\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u7684\u589e\u5927\u800c\u589e\u5f3a\uff0c\u5448\u73b0\u5e42\u5f8b\u5173\u7cfb\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0cLLM\u7684\u8bc4\u4f30\u611f\u77e5\u80fd\u529b\u4f1a\u5f71\u54cd\u8bc4\u4f30\u7ed3\u679c\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u63a2\u6d4b\u65b9\u6cd5\u7814\u7a76\u4e86\u4e0d\u540c\u89c4\u6a21\u768415\u4e2aLLM\u6a21\u578b\u7684\u8bc4\u4f30\u611f\u77e5\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u611f\u77e5\u80fd\u529b\u968f\u7740\u6a21\u578b\u89c4\u6a21\u7684\u589e\u5927\u800c\u589e\u5f3a\uff0c\u5448\u73b0\u5e42\u5f8b\u5173\u7cfb\u3002", "conclusion": "\u8be5\u7814\u7a76\u7ed3\u679c\u53ef\u4ee5\u7528\u4e8e\u9884\u6d4b\u672a\u6765\u66f4\u5927\u6a21\u578b\u7684\u6b3a\u9a97\u884c\u4e3a\uff0c\u5e76\u6307\u5bfc\u8bbe\u8ba1\u66f4\u6709\u6548\u7684AI\u5b89\u5168\u8bc4\u4f30\u7b56\u7565\u3002"}}
{"id": "2509.13334", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13334", "abs": "https://arxiv.org/abs/2509.13334", "authors": ["Anand Swaroop", "Akshat Nallani", "Saksham Uboweja", "Adiliia Uzdenova", "Michael Nguyen", "Kevin Zhu", "Sunishchal Dev", "Ashwinee Panda", "Vasu Sharma", "Maheep Chaudhary"], "title": "FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness", "comment": null, "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving\nlarge language model performance on complex tasks, but recent work shows that\nreasoning steps often fail to causally influence the final answer, creating\nbrittle and untrustworthy outputs. Prior approaches focus primarily on\nmeasuring faithfulness, while methods for systematically improving it remain\nlimited. We introduce Faithful Reasoning via Intervention Training (FRIT), a\nscalable alignment method that trains models to produce causally consistent\nreasoning by learning from systematically corrupted examples. FRIT generates\nsynthetic training data by intervening on individual reasoning steps in\nmodel-generated CoTs, creating faithful/unfaithful pairs that highlight when\nreasoning breaks down. We then apply Direct Preference Optimization to teach\nmodels to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B\nand Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases\nfaithful reasoning by $3.4$ percentage points for Mistral on GSM8K while\nimproving accuracy by $7.6$ percentage points. Our approach provides the first\nscalable, supervision-free method for training language models to produce more\nreliable and interpretable reasoning, addressing a critical gap between\nreasoning performance and trustworthiness. We release our code at\n\\href{https://github.com/Anut-py/frit}.", "AI": {"tldr": "FRIT\u65b9\u6cd5\u901a\u8fc7\u5e72\u9884\u8bad\u7ec3\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u53ef\u9760\u6027\uff0c\u5728GSM8K\u6570\u636e\u96c6\u4e0a\u5c06Mistral\u6a21\u578b\u7684\u5fe0\u5b9e\u63a8\u7406\u63d0\u9ad8\u4e863.4\u4e2a\u767e\u5206\u70b9\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e867.6\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u73b0\u6709\u7684\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u65b9\u6cd5\u5bb9\u6613\u4ea7\u751f\u4e0d\u53ef\u9760\u7684\u8f93\u51fa\uff0cFRIT\u65e8\u5728\u63d0\u9ad8\u63a8\u7406\u7684\u56e0\u679c\u4e00\u81f4\u6027\u3002", "method": "\u5e72\u9884\u8bad\u7ec3\uff1a\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u7834\u574f\u6a21\u578b\u751f\u6210\u7684\u94fe\u5f0f\u601d\u7ef4\u6b65\u9aa4\uff0c\u521b\u5efa\u5fe0\u5b9e/\u4e0d\u5fe0\u5b9e\u7684\u63a8\u7406\u8def\u5f84\u5bf9\uff0c\u7136\u540e\u4f7f\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u8bad\u7ec3\u6a21\u578b\u504f\u597d\u56e0\u679c\u4e00\u81f4\u7684\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728Qwen3-8B\u548cMistral-7B-v0.1\u6a21\u578b\u4e0a\uff0cFRIT\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u5fe0\u5b9e\u63a8\u7406\u548c\u51c6\u786e\u7387\u3002", "conclusion": "FRIT\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u65e0\u9700\u76d1\u7763\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u3002"}}
{"id": "2509.13339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13339", "abs": "https://arxiv.org/abs/2509.13339", "authors": ["Ming Jin", "Hyunin Lee"], "title": "Position: AI Safety Must Embrace an Antifragile Perspective", "comment": null, "summary": "This position paper contends that modern AI research must adopt an\nantifragile perspective on safety -- one in which the system's capacity to\nguarantee long-term AI safety such as handling rare or out-of-distribution\n(OOD) events expands over time. Conventional static benchmarks and single-shot\nrobustness tests overlook the reality that environments evolve and that models,\nif left unchallenged, can drift into maladaptation (e.g., reward hacking,\nover-optimization, or atrophy of broader capabilities). We argue that an\nantifragile approach -- Rather than striving to rapidly reduce current\nuncertainties, the emphasis is on leveraging those uncertainties to better\nprepare for potentially greater, more unpredictable uncertainties in the future\n-- is pivotal for the long-term reliability of open-ended ML systems. In this\nposition paper, we first identify key limitations of static testing, including\nscenario diversity, reward hacking, and over-alignment. We then explore the\npotential of antifragile solutions to manage rare events. Crucially, we\nadvocate for a fundamental recalibration of the methods used to measure,\nbenchmark, and continually improve AI safety over the long term, complementing\nexisting robustness approaches by providing ethical and practical guidelines\ntowards fostering an antifragile AI safety community.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e3b\u5f20AI\u5b89\u5168\u7814\u7a76\u5e94\u91c7\u53d6\u97e7\u6027\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u7f55\u89c1\u4e8b\u4ef6\u548c\u73af\u5883\u53d8\u5316\u3002", "motivation": "\u73b0\u6709AI\u5b89\u5168\u6d4b\u8bd5\u65b9\u6cd5\u9759\u6001\u4e14\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u957f\u671f\u5b89\u5168\u6311\u6218\u3002", "method": "\u5206\u6790\u9759\u6001\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u8ba8\u97e7\u6027\u65b9\u6cd5\u7684\u6f5c\u529b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AI\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5f3a\u8c03\u957f\u671f\u53ef\u9760\u6027\u548c\u5bf9\u672a\u6765\u4e0d\u786e\u5b9a\u6027\u7684\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "\u547c\u5401\u91cd\u65b0\u6821\u51c6AI\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4fc3\u8fdb\u97e7\u6027AI\u5b89\u5168\u793e\u533a\u7684\u5efa\u7acb\u3002"}}
{"id": "2509.13341", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13341", "abs": "https://arxiv.org/abs/2509.13341", "authors": ["Ahmet H. G\u00fczel", "Matthew Thomas Jackson", "Jarek Luca Liesen", "Tim Rockt\u00e4schel", "Jakob Nicolaus Foerster", "Ilija Bogunovic", "Jack Parker-Holder"], "title": "Imagined Autocurricula", "comment": null, "summary": "Training agents to act in embodied environments typically requires vast\ntraining data or access to accurate simulation, neither of which exists for\nmany cases in the real world. Instead, world models are emerging as an\nalternative leveraging offline, passively collected data, they make it possible\nto generate diverse worlds for training agents in simulation. In this work, we\nharness world models to generate imagined environments to train robust agents\ncapable of generalizing to novel task variations. One of the challenges in\ndoing this is ensuring the agent trains on useful generated data. We thus\npropose a novel approach, IMAC (Imagined Autocurricula), leveraging\nUnsupervised Environment Design (UED), which induces an automatic curriculum\nover generated worlds. In a series of challenging, procedurally generated\nenvironments, we show it is possible to achieve strong transfer performance on\nheld-out environments, having trained only inside a world model learned from a\nnarrower dataset. We believe this opens the path to utilizing larger-scale,\nfoundation world models for generally capable agents.", "AI": {"tldr": "\u5229\u7528\u4e16\u754c\u6a21\u578b\u751f\u6210\u73af\u5883\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aIMAC\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1\u81ea\u52a8\u751f\u6210\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u5b9e\u73b0\u5728\u5c11\u91cf\u6570\u636e\u4e0a\u8bad\u7ec3\u540e\uff0c\u667a\u80fd\u4f53\u80fd\u591f\u5f88\u597d\u5730\u6cdb\u5316\u5230\u65b0\u7684\u73af\u5883\u4e2d\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u6216\u7cbe\u786e\u7684\u6a21\u62df\u73af\u5883\uff0c\u8fd9\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u5e38\u5e38\u96be\u4ee5\u83b7\u5f97\u3002\u4e16\u754c\u6a21\u578b\u53ef\u4ee5\u5229\u7528\u79bb\u7ebf\u88ab\u52a8\u6536\u96c6\u7684\u6570\u636e\u751f\u6210\u591a\u79cd\u591a\u6837\u7684\u8bad\u7ec3\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aIMAC(Imagined Autocurricula)\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1(UED)\u81ea\u52a8\u751f\u6210\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u5728\u4e16\u754c\u6a21\u578b\u751f\u6210\u7684\u60f3\u8c61\u73af\u5883\u4e2d\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u3002", "result": "\u5728\u4e00\u7cfb\u5217\u5177\u6709\u6311\u6218\u6027\u7684\u3001\u7a0b\u5e8f\u751f\u6210\u7684\u5b9e\u9a8c\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e86\u826f\u597d\u7684\u8fc1\u79fb\u6027\u80fd\uff0c\u5373\u4f7f\u53ea\u5728\u8f83\u7a84\u7684\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u4e16\u754c\u6a21\u578b\u4e2d\u8fdb\u884c\u8bad\u7ec3\uff0c\u4e5f\u80fd\u591f\u5f88\u597d\u5730\u6cdb\u5316\u5230\u65b0\u7684\u73af\u5883\u4e2d\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5229\u7528\u5927\u89c4\u6a21\u57fa\u7840\u4e16\u754c\u6a21\u578b\u8bad\u7ec3\u901a\u7528\u7684\u3001\u5f3a\u5927\u7684\u667a\u80fd\u4f53\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2509.13347", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13347", "abs": "https://arxiv.org/abs/2509.13347", "authors": ["Zihao Wang", "Muyao Li", "Kaichen He", "Xiangyu Wang", "Zhancun Mu", "Anji Liu", "Yitao Liang"], "title": "OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft", "comment": null, "summary": "The choice of action spaces is a critical yet unresolved challenge in\ndeveloping capable, end-to-end trainable agents. This paper first presents a\nlarge-scale, systematic comparison of prominent abstracted action spaces and\ntokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the\nopen-ended Minecraft. Our analysis reveals that no single action space is\nuniversally optimal; instead, the most effective abstraction is highly\ntask-dependent, creating a dilemma for building generalist agents. To resolve\nthis, we introduce Chain of Action (CoA), a novel framework that unifies\nhigh-level planning and low-level control within a single, monolithic VLA\nmodel. CoA treats an abstracted action not as a command for a separate policy,\nbut as an intermediate reasoning step--akin to a chain of thought--that guides\nthe generation of the final, executable action. Furthermore, we demonstrate\nthat an All-in-One agent trained on a diverse mixture of action spaces using\nthe CoA paradigm learns a more robust and generalizable policy. This unified\nagent achieves a new state-of-the-art, improving the overall task success rate\nover strong, specialized baselines. To foster reproducible research, we release\nthe OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive\nbenchmark of over 800 distinct tasks, curated datasets, source code, and all\npretrained model checkpoints at https://github.com/CraftJarvis/OpenHA", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u7528\u4e8e\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c (VLA) \u6a21\u578b\u7684\u4e0d\u540c\u52a8\u4f5c\u7a7a\u95f4\uff0c\u53d1\u73b0\u6700\u4f73\u9009\u62e9\u4f9d\u8d56\u4e8e\u5177\u4f53\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u94fe\u5f0f\u52a8\u4f5c (CoA) \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u9ad8\u7ea7\u89c4\u5212\u548c\u4f4e\u7ea7\u63a7\u5236\u7edf\u4e00\u5728\u4e00\u4e2a\u6a21\u578b\u4e2d\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u7684\u52a8\u4f5c\u7a7a\u95f4\u9009\u62e9\u95ee\u9898\u5c1a\u672a\u89e3\u51b3\uff0c\u9650\u5236\u4e86\u901a\u7528\u667a\u80fd\u4f53\u7684\u6784\u5efa\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u94fe\u5f0f\u52a8\u4f5c (CoA) \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u62bd\u8c61\u52a8\u4f5c\u89c6\u4e3a\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\uff0c\u6307\u5bfc\u6700\u7ec8\u53ef\u6267\u884c\u52a8\u4f5c\u7684\u751f\u6210\uff0c\u5e76\u5728\u591a\u79cd\u52a8\u4f5c\u7a7a\u95f4\u4e0a\u8bad\u7ec3\u4e86\u4e00\u4e2a\u591a\u529f\u80fd\u667a\u80fd\u4f53\u3002", "result": "CoA\u6846\u67b6\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u7a33\u5065\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728Minecraft\u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "CoA\u6846\u67b6\u4e3a\u6784\u5efa\u66f4\u901a\u7528\u7684VLA\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u7684OpenHA\u5957\u4ef6\u4fc3\u8fdb\u4e86\u53ef\u91cd\u590d\u6027\u7814\u7a76\u3002"}}
{"id": "2509.13351", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13351", "abs": "https://arxiv.org/abs/2509.13351", "authors": ["Pulkit Verma", "Ngoc La", "Anthony Favier", "Swaroop Mishra", "Julie A. Shah"], "title": "Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning", "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities across\ndiverse tasks, yet their ability to perform structured symbolic planning\nremains limited, particularly in domains requiring formal representations like\nthe Planning Domain Definition Language (PDDL). In this paper, we present a\nnovel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'\nsymbolic planning capabilities through logical chain-of-thought reasoning. Our\napproach focuses on teaching models to rigorously reason about action\napplicability, state transitions, and plan validity using explicit logical\ninference steps. By developing instruction prompts that guide models through\nthe precise logical reasoning required to determine when actions can be applied\nin a given state, we enable LLMs to self-correct their planning processes\nthrough structured reflection. The framework systematically builds verification\nskills by decomposing the planning process into explicit reasoning chains about\nprecondition satisfaction, effect application, and invariant preservation.\nExperimental results on multiple planning domains show that our\nchain-of-thought reasoning based instruction-tuned models are significantly\nbetter at planning, achieving planning accuracy of up to 94% on standard\nbenchmarks, representing a 66% absolute improvement over baseline models. This\nwork bridges the gap between the general reasoning capabilities of LLMs and the\nlogical precision required for automated planning, offering a promising\ndirection for developing better AI planning systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6307\u4ee4\u5fae\u8c03\u6846\u67b6PDDL-Instruct\uff0c\u901a\u8fc7\u903b\u8f91\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u589e\u5f3aLLM\u7684\u7b26\u53f7\u89c4\u5212\u80fd\u529b\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9ad8\u8fbe94%\u7684\u89c4\u5212\u51c6\u786e\u7387\uff0c\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u9ad8\u4e8666%\u3002", "motivation": "\u73b0\u6709LLM\u5728\u7ed3\u6784\u5316\u7b26\u53f7\u89c4\u5212\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5f62\u5f0f\u5316\u8868\u793a\u7684\u9886\u57df\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u903b\u8f91\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u7684\u6307\u4ee4\u5fae\u8c03\u6846\u67b6PDDL-Instruct\uff0c\u6307\u5bfc\u6a21\u578b\u8fdb\u884c\u7cbe\u786e\u7684\u903b\u8f91\u63a8\u7406\uff0c\u4ece\u800c\u81ea\u6211\u7ea0\u6b63\u89c4\u5212\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u89c4\u5212\u9886\u57df\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86LLM\u7684\u89c4\u5212\u80fd\u529b\uff0c\u89c4\u5212\u51c6\u786e\u7387\u9ad8\u8fbe94%\uff0c\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u9ad8\u4e8666%\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5f25\u5408\u4e86LLM\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\u548c\u81ea\u52a8\u5316\u89c4\u5212\u6240\u9700\u903b\u8f91\u7cbe\u5ea6\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5f00\u53d1\u66f4\u597d\u7684AI\u89c4\u5212\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.13352", "categories": ["cs.AI", "cs.RO", "68T07, 68T40, 68T42", "I.2.9; I.2.11; I.2.8; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.13352", "abs": "https://arxiv.org/abs/2509.13352", "authors": ["Anis Koubaa", "Khaled Gabr"], "title": "Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning", "comment": "14 pages, 1 figure", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,\nsurveillance, and disaster response, yet most systems remain confined to SAE\nLevel 2--3 autonomy. Their reliance on rule-based control and narrow AI\nrestricts adaptability in dynamic, uncertain missions. Existing UAV frameworks\nlack context-aware reasoning, autonomous decision-making, and ecosystem-level\nintegration; critically, none leverage Large Language Model (LLM) agents with\ntool-calling for real-time knowledge access. This paper introduces the Agentic\nUAVs framework, a five-layer architecture (Perception, Reasoning, Action,\nIntegration, Learning) that augments UAVs with LLM-driven reasoning, database\nquerying, and third-party system interaction. A ROS2 and Gazebo-based prototype\nintegrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3\ndeployment. In simulated search-and-rescue scenarios, agentic UAVs achieved\nhigher detection confidence (0.79 vs. 0.72), improved person detection rates\n(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).\nThese results confirm that modest computational overhead enables qualitatively\nnew levels of autonomy and ecosystem integration.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAgentic UAVs\u7684\u4e94\u5c42\u67b6\u6784\uff0c\u5229\u7528LLM\u8d4b\u80fd\u65e0\u4eba\u673a\uff0c\u63d0\u5347\u5176\u81ea\u4e3b\u6027\u548c\u73af\u5883\u9002\u5e94\u6027\uff0c\u5e76\u5728\u6a21\u62df\u641c\u6551\u573a\u666f\u4e2d\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u7cfb\u7edf\u81ea\u4e3b\u6027\u6709\u9650\uff0c\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e94\u5c42\u67b6\u6784(\u611f\u77e5\u3001\u63a8\u7406\u3001\u884c\u52a8\u3001\u96c6\u6210\u3001\u5b66\u4e60)\uff0c\u5229\u7528LLM\u3001\u6570\u636e\u5e93\u67e5\u8be2\u548c\u7b2c\u4e09\u65b9\u7cfb\u7edf\u4ea4\u4e92\u589e\u5f3a\u65e0\u4eba\u673a\u80fd\u529b\uff0c\u5e76\u57fa\u4e8eROS2\u3001Gazebo\u3001YOLOv11\u548cGPT-4\u6784\u5efa\u539f\u578b\u7cfb\u7edf\u3002", "result": "\u6a21\u62df\u641c\u6551\u573a\u666f\u5b9e\u9a8c\u8868\u660e\uff0cAgentic UAVs\u63d0\u9ad8\u4e86\u76ee\u6807\u68c0\u6d4b\u7f6e\u4fe1\u5ea6(0.79 vs 0.72)\u3001\u4eba\u5458\u68c0\u6d4b\u7387(91% vs 75%)\u548c\u884c\u52a8\u63a8\u8350\u7387(92% vs 4.5%)\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u8f83\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\u5b9e\u73b0\u4e86\u65e0\u4eba\u673a\u81ea\u4e3b\u6027\u548c\u751f\u6001\u7cfb\u7edf\u96c6\u6210\u7684\u65b0\u6c34\u5e73\u3002"}}
{"id": "2509.13357", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13357", "abs": "https://arxiv.org/abs/2509.13357", "authors": ["Yongchao Huang", "Hassan Raza"], "title": "Semantic Fusion with Fuzzy-Membership Features for Controllable Language Modelling", "comment": "16 pages", "summary": "We propose semantic fusion, a lightweight scheme that augments a Transformer\nlanguage model (LM) with a parallel, fuzzy-membership feature channel that\nencodes token-level semantics. Each token is represented by a vector of\ninterpretable features (e.g. part-of-speech cues, shallow roles, boundary\nflags, sentiment polarity and strength) whose values are graded degrees from\ndifferentiable membership functions (e.g. power kernels). These per-token\nvectors form a sentence-level semantic matrix fused via a gated adapter into\nthe LM. Training uses standard next-token prediction, an auxiliary loss that\nreconstructs the semantic features from hidden states, and a lightweight\nuniformizer that regularizes adjective-class distributions. On a synthetic\ntwo-clause corpus with held-out adjectives for out-of-distribution (OOD)\ncontrol, semantic fusion improves perplexity and enables precise,\nuser-controllable generation of polarity and punctuation while maintaining\nmodel simplicity. This approach adds only small overhead, remains fully\ncompatible with tied input-output embeddings, and provides an interpretable\npathway for conditioned natural language generation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u8bed\u4e49\u878d\u5408\u65b9\u6848\uff0c\u901a\u8fc7\u5e76\u884c\u7684\u6a21\u7cca\u6210\u5458\u7279\u5f81\u901a\u9053\u589e\u5f3aTransformer\u8bed\u8a00\u6a21\u578b\uff0c\u4ece\u800c\u6539\u8fdb\u6587\u672c\u751f\u6210\u6548\u679c\u3002", "motivation": "\u6539\u8fdbTransformer\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u751f\u6210\u80fd\u529b\uff0c\u7279\u522b\u662f\u5bf9\u6781\u6027\u548c\u6807\u70b9\u7b26\u53f7\u7684\u63a7\u5236\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u6a21\u7cca\u6210\u5458\u51fd\u6570\u5bf9\u6bcf\u4e2atoken\u8fdb\u884c\u8bed\u4e49\u7f16\u7801\uff0c\u751f\u6210\u53e5\u5b50\u7ea7\u522b\u7684\u8bed\u4e49\u77e9\u9635\uff0c\u5e76\u901a\u8fc7\u95e8\u63a7\u9002\u914d\u5668\u5c06\u5176\u878d\u5408\u5230LM\u4e2d\u3002\u8bad\u7ec3\u8fc7\u7a0b\u5305\u62ec\u6807\u51c6\u7684\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u3001\u8f85\u52a9\u635f\u5931\u548c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u7edf\u4e00\u5668\u3002", "result": "\u5728\u5408\u6210\u8bed\u6599\u5e93\u4e0a\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u56f0\u60d1\u5ea6\uff0c\u5e76\u80fd\u591f\u7cbe\u786e\u3001\u7528\u6237\u53ef\u63a7\u5730\u751f\u6210\u6781\u6027\u548c\u6807\u70b9\u7b26\u53f7\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u7b80\u6d01\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u7684\u6761\u4ef6\u81ea\u7136\u8bed\u8a00\u751f\u6210\u65b9\u6cd5\uff0c\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.13364", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13364", "abs": "https://arxiv.org/abs/2509.13364", "authors": ["Zixi Li"], "title": "Asterisk Operator", "comment": "Code available at: https://github.com/lizixi-0x2F/Asterisk-Games", "summary": "We propose the \\textbf{Asterisk Operator} ($\\ast$-operator), a novel unified\nframework for abstract reasoning based on Adjacency-Structured Parallel\nPropagation (ASPP). The operator formalizes structured reasoning tasks as\nlocal, parallel state evolution processes guided by implicit relational graphs.\nWe prove that the $\\ast$-operator maintains local computational constraints\nwhile achieving global reasoning capabilities, providing an efficient and\nconvergent computational paradigm for abstract reasoning problems. Through\nrigorous mathematical analysis and comprehensive experiments on ARC2 challenges\nand Conway's Game of Life, we demonstrate the operator's universality,\nconvergence properties, and superior performance. Our innovative\nEmbedding-Asterisk distillation method achieves 100\\% accuracy on ARC2\nvalidation with only 6M parameters, representing a significant breakthrough in\nneural-symbolic reasoning.\n  \\textbf{Keywords:} Abstract Reasoning, Adjacency Structure, Parallel\nPropagation, Asterisk Operator, Convergence, Universal Approximation", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7edf\u4e00\u62bd\u8c61\u63a8\u7406\u6846\u67b6\uff1a\u661f\u53f7\u7b97\u5b50\uff0c\u5b83\u57fa\u4e8e\u90bb\u63a5\u7ed3\u6784\u5e76\u884c\u4f20\u64ad\uff08ASPP\uff09\uff0c\u5c06\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\u7531\u9690\u5f0f\u5173\u7cfb\u56fe\u5f15\u5bfc\u7684\u5c40\u90e8\u5e76\u884c\u72b6\u6001\u6f14\u5316\u8fc7\u7a0b\u3002", "motivation": "\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u4e14\u6536\u655b\u7684\u8ba1\u7b97\u8303\u5f0f\u6765\u89e3\u51b3\u62bd\u8c61\u63a8\u7406\u95ee\u9898\u3002", "method": "\u661f\u53f7\u7b97\u5b50\u7ed3\u5408\u4e86\u6570\u5b66\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5305\u62ecARC2\u6311\u6218\u548c\u5eb7\u5a01\u751f\u547d\u6e38\u620f\u3002\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u5d4c\u5165\u5f0f\u661f\u53f7\u84b8\u998f\u65b9\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u661f\u53f7\u7b97\u5b50\u5728\u4fdd\u6301\u5c40\u90e8\u8ba1\u7b97\u7ea6\u675f\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5168\u5c40\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728ARC2\u9a8c\u8bc1\u96c6\u4e0a\u53d6\u5f97\u4e86100%\u7684\u51c6\u786e\u7387\uff08\u4ec5\u4f7f\u75286M\u53c2\u6570\uff09\u3002", "conclusion": "\u661f\u53f7\u7b97\u5b50\u662f\u4e00\u79cd\u5177\u6709\u666e\u9002\u6027\u3001\u6536\u655b\u6027\u548c\u4f18\u8d8a\u6027\u80fd\u7684\u62bd\u8c61\u63a8\u7406\u8ba1\u7b97\u8303\u5f0f\uff0c\u5728\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u7a81\u7834\u3002"}}
{"id": "2509.13368", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13368", "abs": "https://arxiv.org/abs/2509.13368", "authors": ["Yuan Wei", "Xiaohan Shan", "Ran Miao", "Jianmin Li"], "title": "$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation", "comment": "9 pages, 7 figures", "summary": "Reinforcement learning agent development traditionally requires extensive\nexpertise and lengthy iterations, often resulting in high failure rates and\nlimited accessibility. This paper introduces $Agent^2$, a novel\nagent-generates-agent framework that achieves fully automated RL agent design\nthrough intelligent LLM-driven generation. The system autonomously transforms\nnatural language task descriptions and environment code into comprehensive,\nhigh-performance reinforcement learning solutions without human intervention.\n$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent\nserves as an autonomous AI designer that analyzes tasks and generates\nexecutable RL agents, while the Target Agent is the resulting automatically\ngenerated RL agent. The framework decomposes RL development into two distinct\nstages: MDP modeling and algorithmic optimization, enabling more targeted and\neffective agent generation. Built on the Model Context Protocol, $Agent^2$\nprovides a unified framework that standardizes intelligent agent creation\nacross diverse environments and algorithms, while incorporating adaptive\ntraining management and intelligent feedback analysis for continuous\nimprovement. Extensive experiments on a wide range of benchmarks, including\nMuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently\noutperforms manually designed solutions across all tasks, achieving up to 55%\nperformance improvement and substantial gains on average. By enabling truly\nend-to-end, closed-loop automation, this work establishes a new paradigm in\nwhich intelligent agents design and optimize other agents, marking a\nfundamental breakthrough for automated AI systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a$Agent^2$\u7684\u81ea\u52a8\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u8bbe\u8ba1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u9ad8\u6027\u80fd\u7684\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u65b9\u6848\u7684\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5f00\u53d1\u9700\u8981\u5927\u91cf\u7684\u4e13\u4e1a\u77e5\u8bc6\u548c\u65f6\u95f4\uff0c\u4e14\u5931\u8d25\u7387\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u667a\u80fd\u4f53\u67b6\u6784\uff1a\u751f\u6210\u5668\u667a\u80fd\u4f53\u8d1f\u8d23\u5206\u6790\u4efb\u52a1\u5e76\u751f\u6210\u53ef\u6267\u884c\u7684\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\uff1b\u76ee\u6807\u667a\u80fd\u4f53\u662f\u81ea\u52a8\u751f\u6210\u7684\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u3002\u8be5\u6846\u67b6\u5c06\u5f3a\u5316\u5b66\u4e60\u5f00\u53d1\u5206\u89e3\u4e3aMDP\u5efa\u6a21\u548c\u7b97\u6cd5\u4f18\u5316\u4e24\u4e2a\u9636\u6bb5\u3002", "result": "\u5728MuJoCo\u3001MetaDrive\u3001MPE\u548cSMAC\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c$Agent^2$\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u90fd\u59cb\u7ec8\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u65b9\u6848\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe55%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u8bbe\u8ba1\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u5316\uff0c\u6807\u5fd7\u7740\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u81ea\u52a8\u5316\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u5927\u7a81\u7834\u3002"}}
{"id": "2509.13379", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13379", "abs": "https://arxiv.org/abs/2509.13379", "authors": ["Asif Azad", "Mohammad Sadat Hossain", "MD Sadik Hossain Shanto", "M Saifur Rahman", "Md Rizwan Pervez"], "title": "The Art of Saying \"Maybe\": A Conformal Lens for Uncertainty Benchmarking in VLMs", "comment": null, "summary": "Vision-Language Models (VLMs) have achieved remarkable progress in complex\nvisual understanding across scientific and reasoning tasks. While performance\nbenchmarking has advanced our understanding of these capabilities, the critical\ndimension of uncertainty quantification has received insufficient attention.\nTherefore, unlike prior conformal prediction studies that focused on limited\nsettings, we conduct a comprehensive uncertainty benchmarking study, evaluating\n16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets\nwith 3 distinct scoring functions. Our findings demonstrate that larger models\nconsistently exhibit better uncertainty quantification; models that know more\nalso know better what they don't know. More certain models achieve higher\naccuracy, while mathematical and reasoning tasks elicit poorer uncertainty\nperformance across all models compared to other domains. This work establishes\na foundation for reliable uncertainty evaluation in multimodal systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5bf916\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u76846\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u8fdb\u884c\u4e86\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u5bf9\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u5173\u6ce8\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u8bc4\u4f30VLM\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\u3002", "method": "\u57286\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u75283\u4e2a\u4e0d\u540c\u7684\u8bc4\u5206\u51fd\u6570\u8bc4\u4f30\u4e8616\u4e2aVLM(\u5f00\u6e90\u548c\u95ed\u6e90)\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u66f4\u5927\u7684\u6a21\u578b\u901a\u5e38\u8868\u73b0\u51fa\u66f4\u597d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff1b\u66f4\u786e\u5b9a\u7684\u6a21\u578b\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff1b\u6570\u5b66\u548c\u63a8\u7406\u4efb\u52a1\u7684\u4e0d\u786e\u5b9a\u6027\u6027\u80fd\u6bd4\u5176\u4ed6\u9886\u57df\u5dee\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.13389", "categories": ["cs.AI", "I.2.4; I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.13389", "abs": "https://arxiv.org/abs/2509.13389", "authors": ["Carlos N\u00fa\u00f1ez-Molina", "Vicen\u00e7 G\u00f3mez", "Hector Geffner"], "title": "From Next Token Prediction to (STRIPS) World Models -- Preliminary Results", "comment": "10 pages, 3 figures", "summary": "We consider the problem of learning propositional STRIPS world models from\naction traces alone, using a deep learning architecture (transformers) and\ngradient descent. The task is cast as a supervised next token prediction\nproblem where the tokens are the actions, and an action $a$ may follow an\naction sequence if the hidden effects of the previous actions do not make an\naction precondition of $a$ false. We show that a suitable transformer\narchitecture can faithfully represent propositional STRIPS world models, and\nthat the models can be learned from sets of random valid (positive) and invalid\n(negative) action sequences alone. A number of experiments are reported.", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff08Transformer\uff09\u4ece\u52a8\u4f5c\u8f68\u8ff9\u4e2d\u5b66\u4e60\u547d\u9898STRIPS\u4e16\u754c\u6a21\u578b\u3002", "motivation": "\u5b66\u4e60\u4ec5\u4ece\u52a8\u4f5c\u8f68\u8ff9\u4e2d\u5b66\u4e60\u547d\u9898STRIPS\u4e16\u754c\u6a21\u578b\u3002", "method": "\u5c06\u4efb\u52a1\u8f6c\u6362\u4e3a\u76d1\u7763\u5f0f\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u95ee\u9898\uff0c\u5176\u4e2dtoken\u662f\u52a8\u4f5c\u3002Transformer\u67b6\u6784\u53ef\u4ee5\u5fe0\u5b9e\u5730\u8868\u793a\u547d\u9898STRIPS\u4e16\u754c\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u53ef\u4ee5\u4ec5\u4ece\u6709\u6548\u548c\u65e0\u6548\u7684\u52a8\u4f5c\u5e8f\u5217\u96c6\u4e2d\u5b66\u4e60\u3002", "conclusion": "Transformer\u67b6\u6784\u80fd\u591f\u6709\u6548\u5b66\u4e60\u547d\u9898STRIPS\u4e16\u754c\u6a21\u578b"}}
{"id": "2509.13450", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13450", "abs": "https://arxiv.org/abs/2509.13450", "authors": ["Vincent Siu", "Nicholas Crispino", "David Park", "Nathan W. Henry", "Zhun Wang", "Yang Liu", "Dawn Song", "Chenguang Wang"], "title": "SteeringControl: Holistic Evaluation of Alignment Steering in LLMs", "comment": null, "summary": "We introduce SteeringControl, a benchmark for evaluating representation\nsteering methods across core alignment objectives--bias, harmful generation,\nand hallucination--and their effects on secondary behaviors such as sycophancy\nand commonsense morality. While prior alignment work often highlights\ntruthfulness or reasoning ability to demonstrate the side effects of\nrepresentation steering, we find there are many unexplored tradeoffs not yet\nunderstood in a systematic way. We collect a dataset of safety-relevant primary\nand secondary behaviors to evaluate steering effectiveness and behavioral\nentanglement centered around five popular steering methods. To enable this, we\ncraft a modular steering framework based on unique components that serve as the\nbuilding blocks of many existing methods. Our results on Qwen-2.5-7B and\nLlama-3.1-8B find that strong steering performance is dependent on the specific\ncombination of steering method, model, and targeted behavior, and that severe\nconcept entanglement can result from poor combinations of these three as well.\nWe release our code here:\nhttps://github.com/wang-research-lab/SteeringControl.git.", "AI": {"tldr": "SteeringControl\u57fa\u51c6\u8bc4\u4f30\u4e86\u8868\u5f81\u5f15\u5bfc\u65b9\u6cd5\u5728\u504f\u5dee\u3001\u6709\u5bb3\u751f\u6210\u548c\u5e7b\u89c9\u7b49\u6838\u5fc3\u4e00\u81f4\u6027\u76ee\u6807\u4e0a\u7684\u6709\u6548\u6027\u53ca\u5176\u5bf9\u8c04\u5a9a\u548c\u5e38\u8bc6\u9053\u5fb7\u7b49\u6b21\u8981\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u4f9d\u8d56\u4e8e\u5f15\u5bfc\u65b9\u6cd5\u3001\u6a21\u578b\u548c\u76ee\u6807\u884c\u4e3a\u7684\u7279\u5b9a\u7ec4\u5408\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u8868\u5f81\u5f15\u5bfc\u7684\u526f\u4f5c\u7528\u7814\u7a76\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7406\u89e3\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6a21\u5757\u5316\u5f15\u5bfc\u6846\u67b6\u7684\u57fa\u51c6\uff0c\u6536\u96c6\u4e86\u5b89\u5168\u76f8\u5173\u7684\u521d\u7ea7\u548c\u6b21\u7ea7\u884c\u4e3a\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u4e94\u79cd\u6d41\u884c\u7684\u5f15\u5bfc\u65b9\u6cd5\u3002", "result": "\u5f3a\u5f15\u5bfc\u6027\u80fd\u4f9d\u8d56\u4e8e\u5f15\u5bfc\u65b9\u6cd5\u3001\u6a21\u578b\u548c\u76ee\u6807\u884c\u4e3a\u7684\u7279\u5b9a\u7ec4\u5408\uff0c\u4e0d\u826f\u7ec4\u5408\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u7684\u89c2\u5ff5\u7ea0\u7f20\u3002", "conclusion": "\u53d1\u5e03\u4e86SteeringControl\u57fa\u51c6\u4ee3\u7801\uff0c\u4e3a\u8868\u5f81\u5f15\u5bfc\u65b9\u6cd5\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002"}}
{"id": "2509.13547", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13547", "abs": "https://arxiv.org/abs/2509.13547", "authors": ["Harper Reed", "Michael Sugimura", "Angelo Zangari"], "title": "AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving", "comment": "16 pages, 5 tables", "summary": "We investigate whether giving LLM agents the collaborative tools and autonomy\nthat humans naturally use for problem solving can improve their performance. We\nequip Claude Code agents with MCP-based social media and journaling tools and\nallow them to use these tools as they see fit. Across 34 Aider Polyglot Python\nprogramming challenges, collaborative tools substantially improve performance\non the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and\n12-38% faster completion than baseline agents. Effects on the full challenge\nset are mixed, suggesting these tools act as performance enhancers when\nadditional reasoning scaffolding is most needed. Surprisingly, Different models\nnaturally adopted distinct collaborative strategies without explicit\ninstruction. Sonnet 3.7 engaged broadly across tools and benefited from\narticulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,\nleaning on journal-based semantic search when problems were genuinely\ndifficult. This mirrors how human developers adjust collaboration based on\nexpertise and task complexity. Behavioral analysis shows agents prefer writing\nover reading by about 2-9x, indicating that structured articulation drives much\nof the improvement rather than information access alone. Overall, AI agents can\nsystematically benefit from human-inspired collaboration tools at the edge of\ntheir capabilities, pointing to adaptive collaborative interfaces as reasoning\nenhancers rather than universal efficiency boosts.", "AI": {"tldr": "\u8d4b\u4e88LLM\u4ee3\u7406\u4eba\u534f\u4f5c\u5de5\u5177\u548c\u81ea\u4e3b\u6027\u53ef\u663e\u8457\u63d0\u5347\u5176\u89e3\u51b3\u590d\u6742\u7f16\u7a0b\u95ee\u9898\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u989d\u5916\u63a8\u7406\u652f\u67b6\u65f6\u3002", "motivation": "\u7814\u7a76\u8d4b\u4e88LLM\u4ee3\u7406\u4eba\u534f\u4f5c\u5de5\u5177\uff08\u7c7b\u4f3c\u4eba\u7c7b\uff09\u662f\u5426\u80fd\u63d0\u5347\u5176\u6027\u80fd\u3002", "method": "\u4e3aClaude Code\u4ee3\u7406\u4eba\u914d\u5907\u57fa\u4e8eMCP\u7684\u793e\u4ea4\u5a92\u4f53\u548c\u65e5\u8bb0\u5de5\u5177\uff0c\u5e76\u572834\u4e2aAider Polyglot Python\u7f16\u7a0b\u6311\u6218\u4e2d\u6d4b\u8bd5\u5176\u6027\u80fd\u3002", "result": "\u534f\u4f5c\u5de5\u5177\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6700\u96be\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\uff0c\u6210\u672c\u964d\u4f4e15-40%\uff0c\u6b65\u9aa4\u51cf\u5c1112-27%\uff0c\u5b8c\u6210\u901f\u5ea6\u63d0\u534712-38%\u3002\u4e0d\u540c\u6a21\u578b\u81ea\u7136\u91c7\u7528\u4e86\u4e0d\u540c\u7684\u534f\u4f5c\u7b56\u7565\uff0c\u4f8b\u5982Sonnet 3.7\u5e7f\u6cdb\u4f7f\u7528\u5de5\u5177\uff0cSonnet 4\u5219\u9009\u62e9\u6027\u5730\u4f7f\u7528\u65e5\u8bb0\u8fdb\u884c\u8bed\u4e49\u641c\u7d22\u3002\u4ee3\u7406\u4eba\u66f4\u503e\u5411\u4e8e\u5199\u4f5c\u800c\u975e\u9605\u8bfb\uff0c\u8868\u660e\u7ed3\u6784\u5316\u8868\u8fbe\u800c\u975e\u4fe1\u606f\u83b7\u53d6\u662f\u63d0\u5347\u7684\u5173\u952e\u3002", "conclusion": "\u4eba\u7c7b\u542f\u53d1\u7684\u534f\u4f5c\u5de5\u5177\u80fd\u7cfb\u7edf\u6027\u5730\u63d0\u5347AI\u4ee3\u7406\u4eba\u5728\u80fd\u529b\u8fb9\u754c\u4e0a\u7684\u6027\u80fd\uff0c\u8868\u660e\u81ea\u9002\u5e94\u534f\u4f5c\u754c\u9762\u53ef\u4f5c\u4e3a\u63a8\u7406\u589e\u5f3a\u5668\uff0c\u800c\u975e\u666e\u904d\u6548\u7387\u63d0\u5347\u624b\u6bb5\u3002"}}
{"id": "2509.13570", "categories": ["cs.AI", "math.HO", "Primary: 97U50, Secondary: 97U70, 97D40, 97D60, 97E50, 97H40"], "pdf": "https://arxiv.org/pdf/2509.13570", "abs": "https://arxiv.org/abs/2509.13570", "authors": ["Hannah Klawa", "Shraddha Rajpal", "Cigole Thomas"], "title": "Gen AI in Proof-based Math Courses: A Pilot Study", "comment": "35 pages, 6 figures, Comments welcome!", "summary": "With the rapid rise of generative AI in higher education and the\nunreliability of current AI detection tools, developing policies that encourage\nstudent learning and critical thinking has become increasingly important. This\nstudy examines student use and perceptions of generative AI across three\nproof-based undergraduate mathematics courses: a first-semester abstract\nalgebra course, a topology course and a second-semester abstract algebra\ncourse. In each case, course policy permitted some use of generative AI.\nDrawing on survey responses and student interviews, we analyze how students\nengaged with AI tools, their perceptions of generative AI's usefulness and\nlimitations, and what implications these perceptions hold for teaching\nproof-based mathematics. We conclude by discussing future considerations for\nintegrating generative AI into proof-based mathematics instruction.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u751f\u6210\u5f0fAI\u5728\u4e09\u4e2a\u672c\u79d1\u6570\u5b66\u8bfe\u7a0b\u4e2d\u7684\u4f7f\u7528\u548c\u611f\u77e5\uff0c\u53d1\u73b0\u5b66\u751f\u5bf9AI\u5de5\u5177\u7684\u4f7f\u7528\u548c\u770b\u6cd5\u5404\u4e0d\u76f8\u540c\uff0c\u5bf9\u6559\u5b66\u6709\u542f\u793a\u610f\u4e49\u3002", "motivation": "\u5f53\u524dAI\u68c0\u6d4b\u5de5\u5177\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u5236\u5b9a\u9f13\u52b1\u5b66\u751f\u5b66\u4e60\u548c\u6279\u5224\u6027\u601d\u7ef4\u7684\u653f\u7b56\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u95ee\u5377\u548c\u5b66\u751f\u8bbf\u8c08\uff0c\u5206\u6790\u5b66\u751f\u5982\u4f55\u4f7f\u7528AI\u5de5\u5177\uff0c\u4ee5\u53ca\u4ed6\u4eec\u5bf9AI\u5de5\u5177\u6709\u7528\u6027\u548c\u5c40\u9650\u6027\u7684\u770b\u6cd5\u3002", "result": "\u5b66\u751f\u5bf9\u751f\u6210\u5f0fAI\u5de5\u5177\u7684\u4f7f\u7528\u548c\u770b\u6cd5\u5404\u4e0d\u76f8\u540c\uff0c\u5bf9\u5176\u6709\u7528\u6027\u548c\u5c40\u9650\u6027\u6709\u4e0d\u540c\u7406\u89e3\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5982\u4f55\u5c06\u751f\u6210\u5f0fAI\u878d\u5165\u8bc1\u660e\u578b\u6570\u5b66\u6559\u5b66\u3002"}}
{"id": "2509.13588", "categories": ["cs.AI", "cs.CE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.13588", "abs": "https://arxiv.org/abs/2509.13588", "authors": ["Xuan Liu", "Haoyang Shang", "Haojian Jin"], "title": "Programmable Cognitive Bias in Social Agents", "comment": null, "summary": "This paper introduces CoBRA, a novel toolkit for systematically specifying\nagent behavior in LLM-based social simulation. We found that conventional\napproaches that specify agent behaviors through implicit natural language\ndescriptions cannot yield consistent behaviors across models, and the produced\nagent behaviors do not capture the nuances of the descriptions. In contrast,\nCoBRA presents a new approach to program agents' cognitive biases explicitly,\nby grounding agents' expected behaviors using classic social science\nexperiments. CoBRA has two components: (1) Cognitive Bias Index that measures\nthe cognitive bias of a social agent, by quantifying the agent's reactions in a\nset of validated classical social science experiments; (2) Behavioral\nRegulation Engine that aligns the agent's behavior to demonstrate controlled\ncognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and\ntechnical benchmarks. Our results suggest that CoBRA can precisely program the\ncognitive bias demonstrated in a social agent in a model-agnostic manner.", "AI": {"tldr": "CoBRA\u5de5\u5177\u5305\u80fd\u66f4\u7cbe\u786e\u5730\u63a7\u5236\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u793e\u4f1a\u6a21\u62df\u4e2d\u667a\u80fd\u4f53\u7684\u8ba4\u77e5\u504f\u5dee\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u4fdd\u8bc1\u4e0d\u540c\u6a21\u578b\u4e2d\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u4e00\u81f4\u6027\u548c\u7ec6\u5fae\u5dee\u522b\u3002", "method": "\u63d0\u51faCoBRA\u5de5\u5177\u5305\uff0c\u5305\u542b\u8ba4\u77e5\u504f\u5dee\u6307\u6570\u548c\u884c\u4e3a\u8c03\u8282\u5f15\u64ce\u4e24\u90e8\u5206\uff0c\u524d\u8005\u91cf\u5316\u667a\u80fd\u4f53\u5728\u7ecf\u5178\u793e\u4f1a\u79d1\u5b66\u5b9e\u9a8c\u4e2d\u7684\u53cd\u5e94\uff0c\u540e\u8005\u8c03\u6574\u667a\u80fd\u4f53\u884c\u4e3a\u4ee5\u4f53\u73b0\u53ef\u63a7\u7684\u8ba4\u77e5\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCoBRA\u80fd\u4ee5\u6a21\u578b\u65e0\u5173\u7684\u65b9\u5f0f\u7cbe\u786e\u7f16\u7a0b\u793e\u4f1a\u667a\u80fd\u4f53\u7684\u8ba4\u77e5\u504f\u5dee\u3002", "conclusion": "CoBRA\u4e3a\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u8fdb\u884c\u793e\u4f1a\u6a21\u62df\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u548c\u4e00\u81f4\u7684\u65b9\u6cd5\u3002"}}
