<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 15]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Initial Steps in Integrating Large Reasoning and Action Models for Service Composition](https://arxiv.org/abs/2507.18775)
*Ilche Georgievski,Marco Aiello*

Main category: cs.AI

TL;DR: 集成大型推理模型和大型动作模型，实现自动化、用户友好的服务组合。


<details>
  <summary>Details</summary>
Motivation: 现有的服务组合方法受限于推理能力和执行机制，本文旨在通过集成LRM和LAM来克服这些限制。

Method: 探索将LRM和LAM集成到服务组合中，LRM负责语义推理和复杂性处理，LAM负责动态执行和系统互操作性。

Result: 提出了一种集成LRM-LAM的架构框架，能够基于自然语言意图进行全自动服务组合。

Conclusion: 提出了一种集成大型推理模型（LRM）和大型动作模型（LAM）的架构框架，用于改进自动化服务组合，弥合意图与执行之间的差距。

Abstract: Service composition remains a central challenge in building adaptive and
intelligent software systems, often constrained by limited reasoning
capabilities or brittle execution mechanisms. This paper explores the
integration of two emerging paradigms enabled by large language models: Large
Reasoning Models (LRMs) and Large Action Models (LAMs). We argue that LRMs
address the challenges of semantic reasoning and ecosystem complexity while
LAMs excel in dynamic action execution and system interoperability. However,
each paradigm has complementary limitations - LRMs lack grounded action
capabilities, and LAMs often struggle with deep reasoning. We propose an
integrated LRM-LAM architectural framework as a promising direction for
advancing automated service composition. Such a system can reason about service
requirements and constraints while dynamically executing workflows, thus
bridging the gap between intention and execution. This integration has the
potential to transform service composition into a fully automated,
user-friendly process driven by high-level natural language intent.

</details>


### [2] [Simulation-Driven Reinforcement Learning in Queuing Network Routing Optimization](https://arxiv.org/abs/2507.18795)
*Fatima Al-Ani,Molly Wang,Jevon Charles,Aaron Ong,Joshua Forday,Vinayak Modi*

Main category: cs.AI

TL;DR: 模拟驱动的强化学习框架有效优化复杂排队网络系统中的路由决策，在动态环境中表现稳健。


<details>
  <summary>Details</summary>
Motivation: 传统的排队方法难以应对动态、不确定的环境，因此提出了一种鲁棒的强化学习方法。

Method: 采用深度确定性策略梯度 (DDPG) 结合 Dyna 风格规划 (Dyna-DDPG) 的强化学习方法，并包含一个灵活且可配置的模拟环境。

Result: 实验结果表明，该框架能够快速学习有效的路由策略，在中断下保持稳健的性能，并有效地扩展到更大的网络规模。

Conclusion: 该研究开发了一个基于模拟驱动的强化学习框架，用于优化复杂排队网络系统中的路由决策，特别关注制造和通信应用。该框架能够快速学习有效的路由策略，在中断下保持稳健的性能，并有效地扩展到更大的网络规模。

Abstract: This study focuses on the development of a simulation-driven reinforcement
learning (RL) framework for optimizing routing decisions in complex queueing
network systems, with a particular emphasis on manufacturing and communication
applications. Recognizing the limitations of traditional queueing methods,
which often struggle with dynamic, uncertain environments, we propose a robust
RL approach leveraging Deep Deterministic Policy Gradient (DDPG) combined with
Dyna-style planning (Dyna-DDPG). The framework includes a flexible and
configurable simulation environment capable of modeling diverse queueing
scenarios, disruptions, and unpredictable conditions. Our enhanced Dyna-DDPG
implementation incorporates separate predictive models for next-state
transitions and rewards, significantly improving stability and sample
efficiency. Comprehensive experiments and rigorous evaluations demonstrate the
framework's capability to rapidly learn effective routing policies that
maintain robust performance under disruptions and scale effectively to larger
network sizes. Additionally, we highlight strong software engineering practices
employed to ensure reproducibility and maintainability of the framework,
enabling practical deployment in real-world scenarios.

</details>


### [3] [A Neuroscience-Inspired Dual-Process Model of Compositional Generalization](https://arxiv.org/abs/2507.18868)
*Alex Noviello,Claas Beger,Jacob Groner,Kevin Ellis,Weinan Sun*

Main category: cs.AI

TL;DR: MIRAGE框架通过模拟大脑HPC-PFC环路，实现了组合任务上的系统性泛化，在SCAN基准测试中取得了99%以上的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统在组合任务中系统性组合泛化的挑战。

Method: MIRAGE框架由两个模块组成：元训练Transformer神经分解器（类似于大脑的“系统1”计算）和模式引擎（类似于大脑的HPC-PFC“系统2”循环）。

Result: 在SCAN基准测试上取得99%以上的准确率，参数量仅为119万。消融实验证明了模式质量和迭代细化过程的重要性。

Conclusion: MIRAGE框架在SCAN基准测试中实现了99%以上的准确率，证明了其系统性组合泛化能力。其成功关键在于提取高质量的模式和迭代细化过程。

Abstract: Systematic compositional generalization - constructing and understanding
novel combinations of known building blocks - remains a core challenge for AI
systems. Human cognition achieves this flexibility via the interplay of the
hippocampus (HPC) and prefrontal cortex (PFC): the hippocampus rapidly encodes
episodes, and the prefrontal cortex consolidates them into reusable schemas for
reasoning. Drawing on these insights, we present MIRAGE (Meta-Inference with
Rules and Abstractions from Generalized Experience), a framework that achieves
systematic generalization on compositional tasks. MIRAGE has two interacting
modules mirroring the brain's deliberative HPC-PFC loop and intuitive
neocortical pattern recognition. (1) The meta-trained Transformer Neural
Decomposer, paralleling neocortical "System 1" computation, is trained on a
task-agnostic stream of randomly sampled compositional grammars and applies one
decomposition step per pass, with successive passes iteratively refining the
sequence representation. (2) The Schema Engine, analogous to the HPC-PFC
"System 2" loop, dynamically extracts, ranks, and applies reusable schemas,
storing variable bindings in episodic memory and expanding them when needed. By
explicitly equipping the Transformer component of MIRAGE with actively managed
schematic structures, our model performs systematic compositional operations
through explicit schema application and transformation, relying solely on
frozen weights when solving entirely novel tasks. This approach demonstrates
systematic compositional generalization on the SCAN benchmark, achieving > 99%
accuracy on all task splits with only 1.19M parameters in the transformer
module. Ablation studies confirm that MIRAGE's systematicity critically depends
on the quality of extracted schemas and the model's iterative refinement
process.

</details>


### [4] [Success in Humanoid Reinforcement Learning under Partial Observation](https://arxiv.org/abs/2507.18883)
*Wuhao Wang,Zhiyong Chen*

Main category: cs.AI

TL;DR: 首次在部分可观测条件下成功训练类人机器人行走策略，性能媲美完整观测基线，关键在于新颖的历史编码器。


<details>
  <summary>Details</summary>
Motivation: 解决在部分可观测性下，特别是高维任务（例如类人机器人行走）中有效学习策略的难题。

Method: 该研究的关键在于一个新颖的历史编码器，它能并行处理固定长度的过去观测序列，并将其集成到标准的无模型算法中。

Result: 学习到的策略在性能上与使用完整状态信息的先进技术相当，即使只使用了三分之一到三分之二的原始状态。策略还表现出对机器人属性（例如身体部位质量的变化）的适应性。

Conclusion: 这项研究首次成功地在Gymnasium Humanoid-v4环境中，仅使用部分状态信息就学习到稳定的类人机器人行走策略，其性能与使用完整状态信息的先进技术相当，并展现出对机器人属性变化的适应性。

Abstract: Reinforcement learning has been widely applied to robotic control, but
effective policy learning under partial observability remains a major
challenge, especially in high-dimensional tasks like humanoid locomotion. To
date, no prior work has demonstrated stable training of humanoid policies with
incomplete state information in the benchmark Gymnasium Humanoid-v4
environment. The objective in this environment is to walk forward as fast as
possible without falling, with rewards provided for staying upright and moving
forward, and penalties incurred for excessive actions and external contact
forces. This research presents the first successful instance of learning under
partial observability in this environment. The learned policy achieves
performance comparable to state-of-the-art results with full state access,
despite using only one-third to two-thirds of the original states. Moreover,
the policy exhibits adaptability to robot properties, such as variations in
body part masses. The key to this success is a novel history encoder that
processes a fixed-length sequence of past observations in parallel. Integrated
into a standard model-free algorithm, the encoder enables performance on par
with fully observed baselines. We hypothesize that it reconstructs essential
contextual information from recent observations, thereby enabling robust
decision-making.

</details>


### [5] [Towards Improving Long-Tail Entity Predictions in Temporal Knowledge Graphs through Global Similarity and Weighted Sampling](https://arxiv.org/abs/2507.18977)
*Mehrnoosh Mirtaheri,Ryan A. Rossi,Sungchul Kim,Kanak Mahadik,Tong Yu,Xiang Chen,Mohammad Rostami*

Main category: cs.AI

TL;DR: 针对时间知识图谱完成中新实体和稀疏连接问题，提出一种增量训练框架，结合模型无关增强层和加权采样策略，显著提高了预测准确性和对长尾实体的处理能力。


<details>
  <summary>Details</summary>
Motivation: 传统的TKG完成模型假设在训练期间可以访问整个图，这忽略了TKG不断发展带来的挑战，例如模型需要泛化和吸收新知识，以及管理连接稀疏的新实体。

Method: 该方法结合了模型无关的增强层和加权采样策略。增强层利用更广泛的全局实体相似性定义，加权采样策略则强调与不常出现的实体相关的边。

Result: 在两个基准数据集上，该方法在总链接预测、归纳链接预测和处理长尾实体方面均优于现有方法，平均倒数秩(MRR)提高了10%-15%。

Conclusion: 该论文提出了一种增量训练框架，用于解决时间知识图谱(TKG)完成中新实体或稀疏连接实体的问题，该框架结合了模型无关的增强层和加权采样策略，提高了现有TKG完成方法的性能，尤其是在处理长尾实体方面表现出色。

Abstract: Temporal Knowledge Graph (TKG) completion models traditionally assume access
to the entire graph during training. This overlooks challenges stemming from
the evolving nature of TKGs, such as: (i) the model's requirement to generalize
and assimilate new knowledge, and (ii) the task of managing new or unseen
entities that often have sparse connections. In this paper, we present an
incremental training framework specifically designed for TKGs, aiming to
address entities that are either not observed during training or have sparse
connections. Our approach combines a model-agnostic enhancement layer with a
weighted sampling strategy, that can be augmented to and improve any existing
TKG completion method. The enhancement layer leverages a broader, global
definition of entity similarity, which moves beyond mere local neighborhood
proximity of GNN-based methods. The weighted sampling strategy employed in
training accentuates edges linked to infrequently occurring entities. We
evaluate our method on two benchmark datasets, and demonstrate that our
framework outperforms existing methods in total link prediction, inductive link
prediction, and in addressing long-tail entities. Notably, our method achieves
a 10\% improvement and a 15\% boost in MRR for these datasets. The results
underscore the potential of our approach in mitigating catastrophic forgetting
and enhancing the robustness of TKG completion methods, especially in an
incremental training context

</details>


### [6] [Fine-Grained Traffic Inference from Road to Lane via Spatio-Temporal Graph Node Generation](https://arxiv.org/abs/2507.19089)
*Shuhao Li,Weidong Yang,Yue Cui,Xiaoxing Liu,Lingkai Meng,Lipeng Ma,Fan Zhang*

Main category: cs.AI

TL;DR: 提出一种新的细粒度道路交通推理任务(FRTI)和一个名为RoadDiff的框架，利用有限数据实现精确的车道级交通状态推断，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法获取车道级交通数据存在瓶颈，本文提出细粒度道路交通推理(FRTI)任务，旨在利用有限的道路数据生成更详细的车道级交通信息，提供更节能、经济高效的精确交通管理解决方案。

Method: 设计了一个两阶段框架RoadDiff，该框架包含道路-车道相关自动编码器-解码器和车道扩散模块。

Result: 在六个代表不同道路条件的数据集上进行了大量实验，验证了RoadDiff模型的有效性。相关数据集和代码已公开。

Conclusion: 提出了一种名为RoadDiff的两阶段框架来解决细粒度道路交通推理(FRTI)任务，该框架利用道路-车道相关自动编码器-解码器和车道扩散模块来充分利用有限的时空依赖性和道路数据的分布关系，从而准确推断细粒度的车道交通状态。

Abstract: Fine-grained traffic management and prediction are fundamental to key
applications such as autonomous driving, lane change guidance, and traffic
signal control. However, obtaining lane-level traffic data has become a
critical bottleneck for data-driven models due to limitations in the types and
number of sensors and issues with the accuracy of tracking algorithms. To
address this, we propose the Fine-grained Road Traffic Inference (FRTI) task,
which aims to generate more detailed lane-level traffic information using
limited road data, providing a more energy-efficient and cost-effective
solution for precise traffic management. This task is abstracted as the first
scene of the spatio-temporal graph node generation problem. We designed a
two-stage framework--RoadDiff--to solve the FRTI task. solve the FRTI task.
This framework leverages the Road-Lane Correlation Autoencoder-Decoder and the
Lane Diffusion Module to fully utilize the limited spatio-temporal dependencies
and distribution relationships of road data to accurately infer fine-grained
lane traffic states. Based on existing research, we designed several baseline
models with the potential to solve the FRTI task and conducted extensive
experiments on six datasets representing different road conditions to validate
the effectiveness of the RoadDiff model in addressing the FRTI task. The
relevant datasets and code are available at
https://github.com/ShuhaoLii/RoadDiff.

</details>


### [7] [Pareto-NRPA: A Novel Monte-Carlo Search Algorithm for Multi-Objective Optimization](https://arxiv.org/abs/2507.19109)
*Noé Lallouet,Tristan Cazenave,Cyrille Enderli*

Main category: cs.AI

TL;DR: 提出一种新的多目标优化蒙特卡洛算法Pareto-NRPA，在基准测试中表现优异，尤其在受约束搜索空间中。


<details>
  <summary>Details</summary>
Motivation: 针对离散搜索空间上的多目标优化问题，提出了一种新的蒙特卡洛算法Pareto-NRPA。

Method: 扩展了最初为单目标问题设计的嵌套 rollout 策略自适应 (NRPA) 算法，将其推广到多目标优化。使用一组策略同时探索解空间的不同区域，并在每个搜索级别维护非支配前沿。

Result: Pareto-NRPA在双目标旅行商问题和神经架构搜索任务上取得了具有竞争力的结果，优于现有算法。

Conclusion: Pareto-NRPA算法在收敛性和解决方案多样性方面均达到了与最先进的多目标算法竞争的性能，尤其是在受约束的搜索空间中，显著优于最先进的进化多目标算法。

Abstract: We introduce Pareto-NRPA, a new Monte-Carlo algorithm designed for
multi-objective optimization problems over discrete search spaces. Extending
the Nested Rollout Policy Adaptation (NRPA) algorithm originally formulated for
single-objective problems, Pareto-NRPA generalizes the nested search and policy
update mechanism to multi-objective optimization. The algorithm uses a set of
policies to concurrently explore different regions of the solution space and
maintains non-dominated fronts at each level of search. Policy adaptation is
performed with respect to the diversity and isolation of sequences within the
Pareto front. We benchmark Pareto-NRPA on two classes of problems: a novel
bi-objective variant of the Traveling Salesman Problem with Time Windows
problem (MO-TSPTW), and a neural architecture search task on well-known
benchmarks. Results demonstrate that Pareto-NRPA achieves competitive
performance against state-of-the-art multi-objective algorithms, both in terms
of convergence and diversity of solutions. Particularly, Pareto-NRPA strongly
outperforms state-of-the-art evolutionary multi-objective algorithms on
constrained search spaces. To our knowledge, this work constitutes the first
adaptation of NRPA to the multi-objective setting.

</details>


### [8] [OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?](https://arxiv.org/abs/2507.19132)
*Xuetian Chen,Yinghao Chen,Xinfeng Yuan,Zhuo Peng,Lu Chen,Yuekeng Li,Zhoujia Zhang,Yingqian Huang,Leyan Huang,Jiaqing Liang,Tianbao Xie,Zhiyong Wu,Qiushi Sun,Biqing Qi,Bowen Zhou*

Main category: cs.AI

TL;DR: OS-MAP基准测试评估计算机使用自动化代理，发现即使是最新技术在处理高级任务时也存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试无法解释计算机使用代理内部任务的异质性及其与实际用户需求的一致性，这阻碍了目标能力发展和研究成果向实际部署的可靠过渡。

Method: 提出OS-MAP基准测试，该基准测试沿两个关键维度组织其416个现实任务：自动化分类和来自现实世界用户需求层次结构的泛化范围。

Result: 实验表明，即使是具有VLM骨干的最新代理也难以处理涉及感知、推理和协调的高级任务，突出了对更深入理解当前优势和局限性的需求，以推动未来计算机使用代理的研究和部署。

Conclusion: 现有基准测试未能充分考虑内部任务异质性和代理能力及其与实际用户需求的一致性，这阻碍了目标能力发展和研究成果向实际部署的可靠过渡。OS-MAP基准测试通过组织416个现实任务，解决了这一差距，对现有计算机使用自动化代理的能力和局限性进行了全面的评估，突出了对更深入理解当前优势和局限性的需求，以推动未来计算机使用代理的研究和部署。

Abstract: Computer-using agents have shown strong potential to boost human productivity
and enable new application forms across platforms. While recent advances have
led to usable applications, existing benchmarks fail to account for the
internal task heterogeneity and the corresponding agent capabilities, as well
as their alignment with actual user demands-hindering both targeted capability
development and the reliable transition of research progress into practical
deployment. To bridge the gap, we present OS-MAP, a benchmark for daily
computer-using automation that organizes its 416 realistic tasks across 15
applications along two key dimensions: a five-level taxonomy of automation and
a generalization scope derived from a real-world user demand hierarchy. To
enable fine-grained analysis of required capabilities and alignment with
real-world scenarios, OS-MAP evaluates agents along two dimensions: automation
level across a five-level taxonomy, and generalization scope across a demand
hierarchy. This design captures varying levels of required agent autonomy and
generalization, forming a performance-generalization evaluation matrix for
structured and comprehensive assessment. Experiments show that even
State-of-the-Art agents with VLM backbones struggle with higher-level tasks
involving perception, reasoning, and coordination-highlighting the need for a
deeper understanding of current strengths and limitations to drive the future
progress in computer-using agents research and deployment. All code,
environments, baselines, and data are publicly available at
https://github.com/OS-Copilot/OS-Map.

</details>


### [9] [PhysDrive: A Multimodal Remote Physiological Measurement Dataset for In-vehicle Driver Monitoring](https://arxiv.org/abs/2507.19172)
*Jiyao Wang,Xiao Yang,Qingyong Hu,Jiankai Tang,Can Liu,Dengbo He,Yuntao Wang,Yingcong Chen,Kaishun Wu*

Main category: cs.AI

TL;DR: 发布了第一个大型多模态驾驶员生理信号数据集PhysDrive，用于无接触式车内生理传感。


<details>
  <summary>Details</summary>
Motivation: 现有数据集规模有限，模态多样性不足，生物特征注释范围有限，难以满足真实世界驾驶场景的需求。

Method: 收集了48名驾驶员的多模态数据，包括RGB、近红外相机和毫米波雷达数据，以及ECG、BVP、呼吸、心率、呼吸率和SpO2等六种同步地面实况数据。

Result: 建立了涵盖各种驾驶条件的基准，并发布了完整的开源代码。

Conclusion: PhysDrive数据集的发布将推动多模态驾驶员监控和智能座舱系统研究

Abstract: Robust and unobtrusive in-vehicle physiological monitoring is crucial for
ensuring driving safety and user experience. While remote physiological
measurement (RPM) offers a promising non-invasive solution, its translation to
real-world driving scenarios is critically constrained by the scarcity of
comprehensive datasets. Existing resources are often limited in scale, modality
diversity, the breadth of biometric annotations, and the range of captured
conditions, thereby omitting inherent real-world challenges in driving. Here,
we present PhysDrive, the first large-scale multimodal dataset for contactless
in-vehicle physiological sensing with dedicated consideration on various
modality settings and driving factors. PhysDrive collects data from 48 drivers,
including synchronized RGB, near-infrared camera, and raw mmWave radar data,
accompanied with six synchronized ground truths (ECG, BVP, Respiration, HR, RR,
and SpO2). It covers a wide spectrum of naturalistic driving conditions,
including driver motions, dynamic natural light, vehicle types, and road
conditions. We extensively evaluate both signal-processing and deep-learning
methods on PhysDrive, establishing a comprehensive benchmark across all
modalities, and release full open-source code with compatibility for mainstream
public toolboxes. We envision PhysDrive will serve as a foundational resource
and accelerate research on multimodal driver monitoring and smart-cockpit
systems.

</details>


### [10] [Faster Lifting for Ordered Domains with Predecessor Relations](https://arxiv.org/abs/2507.19182)
*Kuncheng Zou,Jiahao Mai,Yonggang Zhang,Yuyi Wang,Ondřej Kuželka,Yuanhong Wang,Yi Chang*

Main category: cs.AI

TL;DR: 新算法高效解决了有序域中提升推理问题，尤其针对前驱关系，实现了数量级加速。


<details>
  <summary>Details</summary>
Motivation: 现有WFOMC算法在处理有序域中包含前驱关系的提升推理问题时效率低下。

Method: 提出一种将前驱关系作为公理一部分的新算法，而不是像以往方法那样使用线性序公理来编码。

Result: 该算法对直接和次直接前驱关系实现了指数级加速，并能处理一般的k阶前驱关系，在提升推理任务和组合数学问题上取得了数量级的加速效果。

Conclusion: 提出一种新的算法，有效解决了有序域中提升推理的问题，尤其是在涉及前驱关系时，实现了数量级的加速。

Abstract: We investigate lifted inference on ordered domains with predecessor
relations, where the elements of the domain respect a total (cyclic) order, and
every element has a distinct (clockwise) predecessor. Previous work has
explored this problem through weighted first-order model counting (WFOMC),
which computes the weighted sum of models for a given first-order logic
sentence over a finite domain. In WFOMC, the order constraint is typically
encoded by the linear order axiom introducing a binary predicate in the
sentence to impose a linear ordering on the domain elements. The immediate and
second predecessor relations are then encoded by the linear order predicate.
Although WFOMC with the linear order axiom is theoretically tractable, existing
algorithms struggle with practical applications, particularly when the
predecessor relations are involved. In this paper, we treat predecessor
relations as a native part of the axiom and devise a novel algorithm that
inherently supports these relations. The proposed algorithm not only provides
an exponential speedup for the immediate and second predecessor relations,
which are known to be tractable, but also handles the general k-th predecessor
relations. The extensive experiments on lifted inference tasks and
combinatorics math problems demonstrate the efficiency of our algorithm,
achieving speedups of a full order of magnitude.

</details>


### [11] [Knowledge Grafting: A Mechanism for Optimizing AI Model Deployment in Resource-Constrained Environments](https://arxiv.org/abs/2507.19261)
*Osama Almurshed,Ashish Kaushal,Asmail Muftah,Nitin Auluck,Omer Rana*

Main category: cs.AI

TL;DR: 通过知识嫁接，在减小AI模型尺寸的同时提升性能，使其能够在资源受限设备上有效部署。


<details>
  <summary>Details</summary>
Motivation: 解决大型AI模型所需的计算资源在许多现实应用场景中往往不可用这一问题。

Method: 提出了一种名为“知识嫁接”的新机制，该机制将大型模型中的选定特征转移到较小的模型中。

Result: 模型尺寸减小88.54%（从64.39 MB到7.38 MB），验证准确率提高到89.97%（对比模型的87.47%），验证损失降低（0.2976 vs. 0.5068），测试数据准确率为90.45%。

Conclusion: 知识嫁接方法能够在减小模型尺寸的同时提高模型性能，并在资源受限环境中实现AI框架的有效部署。

Abstract: The increasing adoption of Artificial Intelligence (AI) has led to larger,
more complex models with numerous parameters that require substantial computing
power -- resources often unavailable in many real-world application scenarios.
Our paper addresses this challenge by introducing knowledge grafting, a novel
mechanism that optimizes AI models for resource-constrained environments by
transferring selected features (the scion) from a large donor model to a
smaller rootstock model. The approach achieves an 88.54% reduction in model
size (from 64.39 MB to 7.38 MB), while improving generalization capability of
the model. Our new rootstock model achieves 89.97% validation accuracy (vs.
donor's 87.47%), maintains lower validation loss (0.2976 vs. 0.5068), and
performs exceptionally well on unseen test data with 90.45% accuracy. It
addresses the typical size vs performance trade-off, and enables deployment of
AI frameworks on resource-constrained devices with enhanced performance. We
have tested our approach on an agricultural weed detection scenario, however,
it can be extended across various edge computing scenarios, potentially
accelerating AI adoption in areas with limited hardware/software support -- by
mirroring in a similar manner the horticultural grafting enables productive
cultivation in challenging agri-based environments.

</details>


### [12] [Modeling Uncertainty: Constraint-Based Belief States in Imperfect-Information Games](https://arxiv.org/abs/2507.19263)
*Achille Morenville,Éric Piette*

Main category: cs.AI

TL;DR: 约束性信念在不完美信息博弈中效果良好


<details>
  <summary>Details</summary>
Motivation: 研究不完美信息博弈中基于信念的策略

Method: 使用约束满足问题和信念传播的约束性模型和概率模型

Result: 约束性信念结果与概率推理结果相当

Conclusion: 约束性信念状态足以有效决策

Abstract: In imperfect-information games, agents must make decisions based on partial
knowledge of the game state. The Belief Stochastic Game model addresses this
challenge by delegating state estimation to the game model itself. This allows
agents to operate on externally provided belief states, thereby reducing the
need for game-specific inference logic. This paper investigates two approaches
to represent beliefs in games with hidden piece identities: a constraint-based
model using Constraint Satisfaction Problems and a probabilistic extension
using Belief Propagation to estimate marginal probabilities. We evaluated the
impact of both representations using general-purpose agents across two
different games. Our findings indicate that constraint-based beliefs yield
results comparable to those of probabilistic inference, with minimal
differences in agent performance. This suggests that constraint-based belief
states alone may suffice for effective decision-making in many settings.

</details>


### [13] [Integrating LLM in Agent-Based Social Simulation: Opportunities and Challenges](https://arxiv.org/abs/2507.19364)
*Patrick Taillandier,Jean Daniel Zucker,Arnaud Grignard,Benoit Gaudou,Nghi Quang Huynh,Alexis Drogoul*

Main category: cs.AI

TL;DR: 大型语言模型在社会模拟中潜力巨大，但需谨慎使用，建议结合传统方法。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在社会模拟中的潜力和局限性。

Method: 首先回顾大型语言模型在复制人类认知方面的最新发现及其局限性；其次，调研大型语言模型在多智能体模拟框架中的应用，重点关注系统架构、规模和验证策略；最后，区分大型语言模型直接有价值的语境和使用存在问题的语境。

Result: 分析了大型语言模型在复制人类认知和多智能体模拟中的应用，并指出了其在行为保真度、校准和可重复性方面的挑战。

Conclusion: 该论文主张将大型语言模型与传统基于规则的建模平台相结合，以提高社会模拟的表达灵活性和分析严谨性。

Abstract: This position paper examines the use of Large Language Models (LLMs) in
social simulation, analyzing both their potential and their limitations from a
computational social science perspective. The first part reviews recent
findings on the ability of LLMs to replicate key aspects of human cognition,
including Theory of Mind reasoning and social inference, while also
highlighting significant limitations such as cognitive biases, lack of true
understanding, and inconsistencies in behavior. The second part surveys
emerging applications of LLMs in multi-agent simulation frameworks, focusing on
system architectures, scale, and validation strategies. Notable projects such
as Generative Agents (Smallville) and AgentSociety are discussed in terms of
their design choices, empirical grounding, and methodological innovations.
Particular attention is given to the challenges of behavioral fidelity,
calibration, and reproducibility in large-scale LLM-driven simulations. The
final section distinguishes between contexts where LLMs, like other black-box
systems, offer direct value-such as interactive simulations and serious
games-and those where their use is more problematic, notably in explanatory or
predictive modeling. The paper concludes by advocating for hybrid approaches
that integrate LLMs into traditional agent-based modeling platforms (GAMA,
Netlogo, etc), enabling modelers to combine the expressive flexibility of
language-based reasoning with the transparency and analytical rigor of
classical rule-based systems.

</details>


### [14] [Learning neuro-symbolic convergent term rewriting systems](https://arxiv.org/abs/2507.19372)
*Flavio Petruzzellis,Alberto Testolin,Alessandro Sperduti*

Main category: cs.AI

TL;DR: 神经符号架构实现的项重写系统在算法问题求解上取得了显著进展，泛化能力强，效率高。


<details>
  <summary>Details</summary>
Motivation: 构建能够学习执行符号算法的神经系统是人工智能领域一个具有挑战性的开放性问题，尤其是在追求强大的泛化能力和分布外性能时。

Method: 提出了一种基于神经符号架构的学习收敛项重写系统的方法，并实现了两个模块化模型：NRS和FastNRS。

Result: FastNRS在内存效率、训练速度和推理时间方面均有显著提高，并在四个数学公式简化任务以及多领域学习场景中显著优于现有神经网络基线，包括Neural Data Router和GPT-4o，甚至与OpenAI最新的o1-preview模型性能相当或更好。

Conclusion: 本文介绍了一种基于神经符号架构的学习收敛项重写系统的方法，该方法的两个模块化实现（NRS和FastNRS）在泛化能力和效率上均优于现有神经网络基线，并在多领域学习场景中展现了其多功能性。

Abstract: Building neural systems that can learn to execute symbolic algorithms is a
challenging open problem in artificial intelligence, especially when aiming for
strong generalization and out-of-distribution performance. In this work, we
introduce a general framework for learning convergent term rewriting systems
using a neuro-symbolic architecture inspired by the rewriting algorithm itself.
We present two modular implementations of such architecture: the Neural
Rewriting System (NRS) and the Fast Neural Rewriting System (FastNRS). As a
result of algorithmic-inspired design and key architectural elements, both
models can generalize to out-of-distribution instances, with FastNRS offering
significant improvements in terms of memory efficiency, training speed, and
inference time. We evaluate both architectures on four tasks involving the
simplification of mathematical formulas and further demonstrate their
versatility in a multi-domain learning scenario, where a single model is
trained to solve multiple types of problems simultaneously. The proposed system
significantly outperforms two strong neural baselines: the Neural Data Router,
a recent transformer variant specifically designed to solve algorithmic
problems, and GPT-4o, one of the most powerful general-purpose large-language
models. Moreover, our system matches or outperforms the latest o1-preview model
from OpenAI that excels in reasoning benchmarks.

</details>


### [15] [Hierarchical Deep Reinforcement Learning Framework for Multi-Year Asset Management Under Budget Constraints](https://arxiv.org/abs/2507.19458)
*Amir Fard,Arnold X. -X. Yuan*

Main category: cs.AI

TL;DR: 一种新的分层深度强化学习方法有效解决了基础设施资产管理中的预算规划和维护优化问题，具有良好的可扩展性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的方法由于组合动作空间、多种资产劣化、严格的预算限制和环境不确定性等复杂性，其可扩展性受到显著限制。

Method: 采用分层深度强化学习方法，将问题分解为高层预算规划器和低层维护规划器两个层次。

Result: 与传统的深度Q学习和增强型遗传算法相比，该方法收敛速度更快，可扩展性更好，并且即使网络规模扩大也能始终提供接近最优的解决方案。案例研究验证了该方法在不同规模下水道网络中的有效性。

Conclusion: 提出了一种用于多年度基础设施规划的分层深度强化学习方法，该方法有效地解决了动作空间的指数增长问题，并确保了严格的预算遵守。

Abstract: Budget planning and maintenance optimization are crucial for infrastructure
asset management, ensuring cost-effectiveness and sustainability. However, the
complexity arising from combinatorial action spaces, diverse asset
deterioration, stringent budget constraints, and environmental uncertainty
significantly limits existing methods' scalability. This paper proposes a
Hierarchical Deep Reinforcement Learning methodology specifically tailored to
multi-year infrastructure planning. Our approach decomposes the problem into
two hierarchical levels: a high-level Budget Planner allocating annual budgets
within explicit feasibility bounds, and a low-level Maintenance Planner
prioritizing assets within the allocated budget. By structurally separating
macro-budget decisions from asset-level prioritization and integrating linear
programming projection within a hierarchical Soft Actor-Critic framework, the
method efficiently addresses exponential growth in the action space and ensures
rigorous budget compliance. A case study evaluating sewer networks of varying
sizes (10, 15, and 20 sewersheds) illustrates the effectiveness of the proposed
approach. Compared to conventional Deep Q-Learning and enhanced genetic
algorithms, our methodology converges more rapidly, scales effectively, and
consistently delivers near-optimal solutions even as network size grows.

</details>
