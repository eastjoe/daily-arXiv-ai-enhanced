<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Strongly Solving $7 \times 6$ Connect-Four on Consumer Grade Hardware](https://arxiv.org/abs/2507.05267)
*Markus Böck*

Main category: cs.AI

TL;DR: 使用符号搜索方法创建了大型Connect-Four查找表，用于快速评估游戏状态和寻找最佳走法。


<details>
  <summary>Details</summary>
Motivation: 挑战已解决的Connect-Four游戏强力解决方案（查找表）的可行性。

Method: 基于二元决策图的符号搜索方法

Result: 生成了一个89.6 GB的Connect-Four游戏查找表，该表包含赢/平/输评估和alpha-beta搜索。

Conclusion: 本文通过高效实现基于二元决策图的符号搜索方法，生成了一个89.6GB的Connect-Four游戏查找表，该表可在单核CPU上运行，并包含赢/平/输评估和alpha-beta搜索以找到最快获胜或最慢失败的走法。

Abstract: While the game Connect-Four has been solved mathematically and the best move
can be effectively computed with search based methods, a strong solution in the
form of a look-up table was believed to be infeasible. In this paper, we
revisit a symbolic search method based on binary decision diagrams to produce
strong solutions. With our efficient implementation we were able to produce a
89.6 GB large look-up table in 47 hours on a single CPU core with 128 GB main
memory for the standard $7 \times 6$ board size. In addition to this
win-draw-loss evaluation, we include an alpha-beta search in our open source
artifact to find the move which achieves the fastest win or slowest loss.

</details>


### [2] [Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management](https://arxiv.org/abs/2507.05283)
*Yue Wang,Miao Zhou,Guijing Huang,Rui Zhuo,Chao Yi,Zhenliang Ma*

Main category: cs.AI

TL;DR: Chat2SPaT 使用LLM将用户对交通信号控制计划的描述转换为精确的SPaT结果，准确率超过94%，提高了交通信号控制计划管理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的预定时间交通信号控制需要繁琐的手动工作来创建和更新信号计划，一个交叉路口通常与多个计划相关联，导致进一步重复手动输入计划参数。

Method: 利用大型语言模型 (LLM) 理解用户计划描述，并将计划重新制定为 JSON 格式的相位序列和相位属性组合。Python脚本用于定位循环中的相位，解决交通信号控制的细微之处，并最终组装完整的交通信号控制计划。

Result: 提出Chat2SPaT方法，准确率超过94%，为交通从业人员和研究人员提供易于使用的计划管理流程。

Conclusion: Chat2SPaT，一种将用户对信号控制计划的半结构化和模糊描述转换为精确信号相位和时间 (SPaT) 结果的方法，可以提高交通信号控制计划管理效率。其准确率超过94%。

Abstract: Pre-timed traffic signal control, commonly used for operating signalized
intersections and coordinated arterials, requires tedious manual work for
signaling plan creating and updating. When the time-of-day or day-of-week plans
are utilized, one intersection is often associated with multiple plans, leading
to further repetitive manual plan parameter inputting. To enable a
user-friendly traffic signal control plan management process, this study
proposes Chat2SPaT, a method to convert users' semi-structured and ambiguous
descriptions on the signal control plan to exact signal phase and timing (SPaT)
results, which could further be transformed into structured stage-based or
ring-based plans to interact with intelligent transportation system (ITS)
software and traffic signal controllers. With curated prompts, Chat2SPaT first
leverages large language models' (LLMs) capability of understanding users' plan
descriptions and reformulate the plan as a combination of phase sequence and
phase attribute results in the json format. Based on LLM outputs, python
scripts are designed to locate phases in a cycle, address nuances of traffic
signal control, and finally assemble the complete traffic signal control plan.
Within a chat, the pipeline can be utilized iteratively to conduct further plan
editing. Experiments show that Chat2SPaT can generate plans with an accuracy of
over 94% for both English and Chinese cases, using a test dataset with over 300
plan descriptions. As the first benchmark for evaluating LLMs' capability of
understanding traffic signal control plan descriptions, Chat2SPaT provides an
easy-to-use plan management pipeline for traffic practitioners and researchers,
serving as a potential new building block for a more accurate and versatile
application of LLMs in the field of ITS. The source codes, prompts and test
dataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.

</details>


### [3] [Fuzzy Classification Aggregation for a Continuum of Agents](https://arxiv.org/abs/2507.05297)
*Zijun Meng*

Main category: cs.AI

TL;DR: 模糊分类聚合函数是加权算术平均值


<details>
  <summary>Details</summary>
Motivation: 研究模糊分类聚合函数的性质

Method: 证明

Result: 证明了最佳、独立且零一致的模糊分类聚合函数是加权算术平均值

Conclusion: 证明了在对m≥3个对象进行2≤p≤m类类型的连续个体分类中，任何最佳、独立且零一致的模糊分类聚合函数必须是加权算术平均值。

Abstract: We prove that any optimal, independent, and zero unanimous fuzzy
classification aggregation function of a continuum of individual
classifications of $m\ge 3$ objects into $2\le p\le m$ types must be a weighted
arithmetic mean.

</details>


### [4] [OLG++: A Semantic Extension of Obligation Logic Graph](https://arxiv.org/abs/2507.05488)
*Subhasis Dasgupta,Jon Stephens,Amarnath Gupta*

Main category: cs.AI

TL;DR: OLG++ 是一个用于建模规章制度的语义扩展，比现有模型更具表达力，支持复杂的规则推理和法律问题解答。


<details>
  <summary>Details</summary>
Motivation: 模拟市政和跨辖区环境中的规章制度，支持对具有上下文条件、优先级和复杂触发器的规则进行结构化推理。

Method: 扩展义务逻辑图 (OLG)，引入更丰富的节点和边类型，包括空间、时间、当事方群体、可反驳性和逻辑分组结构。

Result: OLG++ 比 LegalRuleML 更好，支持 subClassOf、空间约束和重言例外结构，在食物企业法规示例中展示了其表达能力，并支持使用属性图查询进行法律问题解答。

Conclusion: OLG++ 扩展了义务逻辑图 (OLG)，用于模拟市政和跨辖区环境中的规章制度，并通过食物企业法规示例展示了其表达能力，支持使用属性图查询进行法律问题解答，并且比之前的基于图的法律知识表示模型更具表达力。

Abstract: We present OLG++, a semantic extension of the Obligation Logic Graph (OLG)
for modeling regulatory and legal rules in municipal and interjurisdictional
contexts. OLG++ introduces richer node and edge types, including spatial,
temporal, party group, defeasibility, and logical grouping constructs, enabling
nuanced representations of legal obligations, exceptions, and hierarchies. The
model supports structured reasoning over rules with contextual conditions,
precedence, and complex triggers. We demonstrate its expressiveness through
examples from food business regulations, showing how OLG++ supports legal
question answering using property graph queries. OLG++ also improves over
LegalRuleML by providing native support for subClassOf, spatial constraints,
and reified exception structures. Our examples show that OLG++ is more
expressive than prior graph-based models for legal knowledge representation.

</details>


### [5] [Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents](https://arxiv.org/abs/2507.05495)
*Prahaladh Chandrahasan,Jiahe Jin,Zhihan Zhang,Tevin Wang,Andy Tang,Lucy Mo,Morteza Ziyadi,Leonardo F. R. Ribeiro,Zimeng Qiu,Markus Dreyer,Akari Asai,Chenyan Xiong*

Main category: cs.AI

TL;DR: Deep Research Comparator平台提供了一个全面的框架来评估深度研究代理，并提供了一个名为Simple Deepresearch的端到端代理框架。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效评估自主搜索网络、分析信息和生成报告的深度研究代理，尤其是在评估长报告和提供中间步骤的详细反馈方面。

Method: 开发了一个用于评估深度研究代理的平台，该平台允许并排比较不同代理的最终报告及其中间步骤，并收集细粒度的用户反馈。还开发了一个名为Simple Deepresearch的端到端代理框架，方便将各种大型语言模型转换为深度研究代理。

Result: 收集了17位标注者对三个深度研究代理的真实用户偏好数据，证明了该平台对深度研究代理开发的实用性。

Conclusion: 介绍了一个评估深度研究代理的平台Deep Research Comparator，并提供了一个端到端代理框架Simple Deepresearch。

Abstract: Effectively evaluating deep research agents that autonomously search the web,
analyze information, and generate reports remains a major challenge,
particularly when it comes to assessing long reports and giving detailed
feedback on their intermediate steps. To address these gaps, we introduce Deep
Research Comparator, a platform that offers a holistic framework for deep
research agent hosting, side-by-side comparison, fine-grained human feedback
collection, and ranking calculation. Given a user query, our platform displays
the final reports from two different agents along with their intermediate steps
during generation. Annotators can evaluate the overall quality of final reports
based on side-by-side comparison, and also provide detailed feedback separately
by assessing intermediate steps or specific text spans within the final report.
Furthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This
scaffold serves as a baseline that facilitates the easy integration of various
large language models to transform them into deep research agents for
evaluation. To demonstrate the platform's utility for deep research agent
development, we have collected real user preference data from 17 annotators on
three deep research agents. A demo video of our platform can be found at
https://www.youtube.com/watch?v=g4d2dnbdseg.

</details>


### [6] [Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality](https://arxiv.org/abs/2507.05515)
*Haochen Huang,Jiahuan Pei,Mohammad Aliannejadi,Xin Sun,Moonisa Ahsan,Pablo Cesar,Chuang Yu,Zhaochun Ren,Junxiao Wang*

Main category: cs.AI

TL;DR: 现有视觉语言模型在增强现实训练，特别是细粒度任务上表现不佳，需要进一步研究。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在增强现实训练中的应用，特别是针对细粒度装配任务。

Method: 构建了一个用于AR训练的综合数据集，并评估了九个最先进的VLMs。

Result: 即使是先进的VLMs模型，在细粒度装配任务上的F1分数也只有40.54%，这表明需要改进数据集、基准和模型。

Conclusion: 现有视觉语言模型(VLMs)在增强现实(AR)训练中的应用仍有待探索，即使是GPT-4o等先进模型在细粒度装配任务上的表现也不理想，这凸显了改进细粒度视觉语言对齐的需求。

Abstract: Vision-language models (VLMs) are essential for enabling AI-powered smart
assistants to interpret and reason in multimodal environments. However, their
application in augmented reality (AR) training remains largely unexplored. In
this work, we introduce a comprehensive dataset tailored for AR training,
featuring systematized vision-language tasks, and evaluate nine
state-of-the-art VLMs on it. Our results reveal that even advanced models,
including GPT-4o, struggle with fine-grained assembly tasks, achieving a
maximum F1 score of just 40.54% on state detection. These findings highlight
the demand for enhanced datasets, benchmarks, and further research to improve
fine-grained vision-language alignment. Beyond technical contributions, our
work has broader social implications, particularly in empowering blind and
visually impaired users with equitable access to AI-driven learning
opportunities. We provide all related resources, including the dataset, source
code, and evaluation results, to support the research community.

</details>


### [7] [Modeling (Deontic) Modal Operators With the s(CASP) Goal-directed Predicated Answer Set Programming System](https://arxiv.org/abs/2507.05519)
*Gopal Gupta,Abhiramon Rajasekharan,Alexis R. Tudor,Elmer Salazar,Joaquín Arias*

Main category: cs.AI

TL;DR: 用ASP优雅地实现了德昂提克模态逻辑，并解决了其悖论。


<details>
  <summary>Details</summary>
Motivation: 实现德昂提克模态逻辑。

Method: 使用Answer Set Programming (ASP)中的默认否定和强否定以及全局约束来表达德昂提克模态逻辑中的义务和禁止。

Result: 优雅地解决了德昂提克模态逻辑的各种悖论。

Conclusion: 本文展示了如何使用Answer Set Programming (ASP)中的默认否定和强否定优雅地表达(德昂提克)模态算子，并提出使用ASP的全局约束来表示德昂提克模态逻辑中的义务和禁止，从而优雅地解决了德昂提克模态逻辑的各种悖论。

Abstract: We consider the problem of implementing deontic modal logic. We show how
(deontic) modal operators can be expressed elegantly using default negation
(negation-as-failure) and strong negation present in answer set programming
(ASP). We propose using global constraints of ASP to represent obligations and
impermissibilities of deontic modal logic. We show that our proposed
representation results in the various paradoxes of deontic modal logic being
elegantly resolved.

</details>


### [8] [Cultivating Multimodal Intelligence: Interpretive Reasoning and Agentic RAG Approaches to Dermatological Diagnosis](https://arxiv.org/abs/2507.05520)
*Karishma Thakrar,Shreyas Basavatia,Akshay Daftardar*

Main category: cs.AI

TL;DR: 利用多模态模型、结构化推理和知识增强，构建皮肤病问答系统，在挑战赛中取得佳绩，并为远程医疗中的异步诊断提供了解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决远程医疗中异步诊断场景下，基于有限输入做出高准确性和可解释性诊断的挑战。

Method: 该方法结合了微调开源多模态模型（Qwen, Gemma, LLaMA）、结构化推理层和增强型检索生成（agentic RAG）三个核心组件。

Result: 在MEDIQA-MAGIC挑战赛中获得第二名（提交结果排名第六），展现出有竞争力的性能和高准确性。

Conclusion: 该研究在MEDIQA-MAGIC挑战赛中获得第二名，提出了一种结合多模态模型、结构化推理层和增强型检索生成方法的皮肤病问答系统，在异步诊断场景下展现出高准确性和可解释性，为可靠的自动化诊断支持系统提供了途径。

Abstract: The second edition of the 2025 ImageCLEF MEDIQA-MAGIC challenge, co-organized
by researchers from Microsoft, Stanford University, and the Hospital Clinic of
Barcelona, focuses on multimodal dermatology question answering and
segmentation, using real-world patient queries and images. This work addresses
the Closed Visual Question Answering (CVQA) task, where the goal is to select
the correct answer to multiple-choice clinical questions based on both
user-submitted images and accompanying symptom descriptions. The proposed
approach combines three core components: (1) fine-tuning open-source multimodal
models from the Qwen, Gemma, and LLaMA families on the competition dataset, (2)
introducing a structured reasoning layer that reconciles and adjudicates
between candidate model outputs, and (3) incorporating agentic
retrieval-augmented generation (agentic RAG), which adds relevant information
from the American Academy of Dermatology's symptom and condition database to
fill in gaps in patient context. The team achieved second place with a
submission that scored sixth, demonstrating competitive performance and high
accuracy. Beyond competitive benchmarks, this research addresses a practical
challenge in telemedicine: diagnostic decisions must often be made
asynchronously, with limited input and with high accuracy and interpretability.
By emulating the systematic reasoning patterns employed by dermatologists when
evaluating skin conditions, this architecture provided a pathway toward more
reliable automated diagnostic support systems.

</details>


### [9] [Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment](https://arxiv.org/abs/2507.05528)
*Jiahuan Pei,Fanghua Ye,Xin Sun,Wentao Deng,Koen Hindriks,Junxiao Wang*

Main category: cs.AI

TL;DR: WikiHowAgent:一个基于大型语言模型的，可扩展的互动教学系统，在多样化环境下有效。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟教育者和学习者缺乏可扩展性，未能利用多样化的大规模课程内容，且缺乏评估教学质量的框架。

Method: 构建了一个多智能体系统WikiHowAgent，包含教师代理、学习者代理、交互管理器和评估器，并使用基于14,287篇教程的大型数据集进行评估。

Result: WikiHowAgent在不同环境下均有效，结合计算和基于规则的指标以及人工判断，证明了其有效性。数据集和实现均已开源。

Conclusion: WikiHowAgent，一个利用大型语言模型模拟互动教学学习对话的多智能体工作流程，有效且具有可扩展性，并提供对LLM跨领域能力的见解。

Abstract: Large language models (LLMs) have advanced virtual educators and learners,
bridging NLP with AI4Education. Existing work often lacks scalability and fails
to leverage diverse, large-scale course content, with limited frameworks for
assessing pedagogic quality. To this end, we propose WikiHowAgent, a
multi-agent workflow leveraging LLMs to simulate interactive teaching-learning
conversations. It integrates teacher and learner agents, an interaction
manager, and an evaluator to facilitate procedural learning and assess
pedagogic quality. We introduce a dataset of 114,296 teacher-learner
conversations grounded in 14,287 tutorials across 17 domains and 727 topics.
Our evaluation protocol combines computational and rubric-based metrics with
human judgment alignment. Results demonstrate the workflow's effectiveness in
diverse setups, offering insights into LLM capabilities across domains. Our
datasets and implementations are fully open-sourced.

</details>


### [10] [Red Teaming AI Red Teaming](https://arxiv.org/abs/2507.05538)
*Subhabrata Majumdar,Brian Pendleton,Abhishek Gupta*

Main category: cs.AI

TL;DR: AI红队应关注系统级风险，而非仅关注单个模型漏洞


<details>
  <summary>Details</summary>
Motivation: 弥合AI红队最初的批判性思维练习意图与其在生成式AI环境下对模型级缺陷的狭隘关注之间的差距。

Method: 提出一个在宏观和微观两个层面实施AI系统红队的综合框架，并提出了一系列建议。

Result: 提出了一个包含宏观系统红队和微观模型红队的综合框架，并提出了一系列建议，强调有效AI红队需要多功能团队来检查涌现风险、系统漏洞以及技术和社会因素之间的相互作用。

Conclusion: 当前AI红队工作主要关注单个模型漏洞，忽视了模型、用户和环境之间复杂交互产生的更广泛的社会技术系统和涌现行为。

Abstract: Red teaming has evolved from its origins in military applications to become a
widely adopted methodology in cybersecurity and AI. In this paper, we take a
critical look at the practice of AI red teaming. We argue that despite its
current popularity in AI governance, there exists a significant gap between red
teaming's original intent as a critical thinking exercise and its narrow focus
on discovering model-level flaws in the context of generative AI. Current AI
red teaming efforts focus predominantly on individual model vulnerabilities
while overlooking the broader sociotechnical systems and emergent behaviors
that arise from complex interactions between models, users, and environments.
To address this deficiency, we propose a comprehensive framework
operationalizing red teaming in AI systems at two levels: macro-level system
red teaming spanning the entire AI development lifecycle, and micro-level model
red teaming. Drawing on cybersecurity experience and systems theory, we further
propose a set of recommendations. In these, we emphasize that effective AI red
teaming requires multifunctional teams that examine emergent risks, systemic
vulnerabilities, and the interplay between technical and social factors.

</details>


### [11] [SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation](https://arxiv.org/abs/2507.05541)
*Shovito Barua Soumma,Asiful Arefeen,Stephanie M. Carpenter,Melanie Hingle,Hassan Ghasemzadeh*

Main category: cs.AI

TL;DR: 大型语言模型能有效生成反事实解释，提升医疗预测模型的解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型生成反事实解释用于异常预防和增强模型鲁棒性。

Method: 利用GPT-4o-mini模型进行零样本和少样本学习，生成反事实解释，并在压力预测和心脏病检测数据集上进行评估。

Result: 与传统方法相比，该方法在合理性、有效性和稀疏性方面具有竞争力，并能提高下游分类器的准确率（平均提高5%）。

Conclusion: 使用大型语言模型生成反事实解释在医疗预测任务中取得了高准确性和有效性，并能提升下游分类器的性能。

Abstract: Counterfactual explanations (CFs) offer human-centric insights into machine
learning predictions by highlighting minimal changes required to alter an
outcome. Therefore, CFs can be used as (i) interventions for abnormality
prevention and (ii) augmented data for training robust models. In this work, we
explore large language models (LLMs), specifically GPT-4o-mini, for generating
CFs in a zero-shot and three-shot setting. We evaluate our approach on two
datasets: the AI-Readi flagship dataset for stress prediction and a public
dataset for heart disease detection. Compared to traditional methods such as
DiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high
plausibility (up to 99%), strong validity (up to 0.99), and competitive
sparsity. Moreover, using LLM-generated CFs as augmented samples improves
downstream classifier performance (an average accuracy gain of 5%), especially
in low-data regimes. This demonstrates the potential of prompt-based generative
techniques to enhance explainability and robustness in clinical and
physiological prediction tasks. Code base: github.com/anonymous/SenseCF.

</details>


### [12] [SingLoRA: Low Rank Adaptation Using a Single Matrix](https://arxiv.org/abs/2507.05566)
*David Bensaïd,Noam Rotstein,Roy Velich,Daniel Bensaïd,Ron Kimmel*

Main category: cs.AI

TL;DR: SingLoRA通过改进LoRA的权重更新方法，解决了训练不稳定问题，参数更少，性能更好。


<details>
  <summary>Details</summary>
Motivation: 解决LoRA中矩阵间尺度差异导致的不稳定训练动态问题，提升模型性能。

Method: 将权重更新分解为单个低秩矩阵与其转置的乘积。

Result: 在MNLI任务上，SingLoRA微调Llama 7B模型达到91.3%的准确率，优于LoRA(89.1%)和LoRA+(90.2%)；在DreamBooth任务上，SingLoRA微调Stable Diffusion模型的DINO相似度得分达到0.151，优于LoRA(0.143)和DoRA(0.148)。

Conclusion: SingLoRA改进了LoRA，通过学习单个低秩矩阵与其转置的乘积来更新权重，解决了LoRA中矩阵间尺度差异导致的不稳定训练问题，参数量减少一半，并在多个任务上取得了更好的性能。

Abstract: Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient
fine-tuning of large pretrained models. LoRA augments the pre-trained weights
of a model by adding the product of two smaller matrices that together form a
low-rank matrix update. Recent research has shown that scale disparities
between these two matrices often cause unstable training dynamics, leading to
suboptimal performance. In this paper, we propose SingLoRA, which reformulates
low-rank adaptation by learning the weights update as a decomposition of a
single low-rank matrix multiplied by its transpose. This simple design
inherently removes inter-matrix scale conflicts, ensuring stable optimization,
and roughly halves the parameter count. We analyze SingLoRA within the
infinite-width neural network framework, showing that it guarantees stable
feature learning by construction. Extensive experiments on multiple tasks
validate these benefits. In common sense reasoning, fine-tuning LLama 7B on
MNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+
(90.2%) - while using only 60% of their parameter budget. In image generation,
fine-tuning Stable Diffusion with SingLoRA significantly improves image
fidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to
scores of 0.148 and 0.143 for DoRA and LoRA, respectively.

</details>


### [13] [Towards Measurement Theory for Artificial Intelligence](https://arxiv.org/abs/2507.05587)
*Elija Perrier*

Main category: cs.AI

TL;DR: 提出人工智能测量形式理论，构建分层测量体系，以促进AI系统比较和风险评估。


<details>
  <summary>Details</summary>
Motivation: 形式化AI测量能使研究者、实践者和监管者更好地比较系统和评估方法，将前沿AI评估与既有定量风险分析技术联系起来，并阐明AI能力如何取决于我们选择的测量方法。

Method: 构建分层测量体系，区分直接和间接可观察量。

Result: 勾勒了一个分层的测量体系，区分了直接和间接可观察量，并指明了这些要素如何为构建统一的、可校准的AI现象分类法提供途径。

Conclusion: 提出了一套人工智能测量形式理论的纲要，旨在促进AI系统间的比较、与既有风险分析技术的关联，以及对AI能力的重新定义。

Abstract: We motivate and outline a programme for a formal theory of measurement of
artificial intelligence. We argue that formalising measurement for AI will
allow researchers, practitioners, and regulators to: (i) make comparisons
between systems and the evaluation methods applied to them; (ii) connect
frontier AI evaluations with established quantitative risk analysis techniques
drawn from engineering and safety science; and (iii) foreground how what counts
as AI capability is contingent upon the measurement operations and scales we
elect to use. We sketch a layered measurement stack, distinguish direct from
indirect observables, and signpost how these ingredients provide a pathway
toward a unified, calibratable taxonomy of AI phenomena.

</details>


### [14] [MLlm-DR: Towards Explainable Depression Recognition with MultiModal Large Language Models](https://arxiv.org/abs/2507.05591)
*Wei Zhang,Juan Chen,En Zhu,Wenhong Cheng,YunPeng Li,Yanbo J. Wang*

Main category: cs.AI

TL;DR: 新型多模态大型语言模型MLlm-DR可进行可解释的抑郁症诊断，并取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 以往的研究缺乏清晰的抑郁症评分确定方法解释，且现有LLMs缺乏访谈数据训练，导致诊断性能差。

Method: 整合小型LLMs和轻量级查询模块LQ-former，利用一个强大的训练数据集进行微调。

Result: 在CMDC和E-DAIC-WOZ数据集上取得了最先进的结果，证明了其有效性和优越性。

Conclusion: 提出了一种新的多模态大型语言模型MLlm-DR，用于可解释的抑郁症诊断，并在两个基准数据集上取得了最先进的结果。

Abstract: Automated depression diagnosis aims to analyze multimodal information from
interview videos to predict participants' depression scores. Previous studies
often lack clear explanations of how these scores were determined, limiting
their adoption in clinical practice. While the advent of LLMs provides a
possible pathway for explainable depression diagnosis, current LLMs capable of
processing multimodal data lack training on interview data, resulting in poor
diagnostic performance when used directly. In this paper, we propose a novel
multimodal large language model (MLlm-DR) that can understand multimodal
information inputs and supports explainable depression diagnosis. MLlm-DR
integrates a smaller LLMs and a lightweight query module (LQ-former).
Specifically, the smaller LLMs is designed to generate depression scores and
corresponding evaluation rationales. To enhance its logical reasoning for
domain-specific tasks while maintaining practicality, we constructed a robust
training dataset to fine-tune it. Meanwhile, the LQ-former captures
depression-related features from speech and visual data, aiding the model's
ability to process multimodal information, to achieve comprehensive depression
diagnosis. Our approach achieves state-of-the-art results on two
interview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its
effectiveness and superiority.

</details>


### [15] [Domain adaptation of large language models for geotechnical applications](https://arxiv.org/abs/2507.05613)
*Lei Fan,Fangxue Liu,Cheng Chen*

Main category: cs.AI

TL;DR: 大型语言模型正逐渐应用于岩土工程，提高效率并拓展应用范围。


<details>
  <summary>Details</summary>
Motivation: 近年来，大型语言模型在各个领域展现出巨大的潜力，岩土工程也不例外。本文旨在对LLM在岩土工程中的应用进行全面调研，为相关研究和实践提供参考。

Method: 本文采用文献调研的方法，对现有文献进行综述和分析。

Result: 本文概述了LLM在岩土工程中的几种适应方法，包括提示工程、检索增强生成、领域自适应预训练和微调等，并总结了其在岩土工程各个方面的应用，例如地质解释、地下表征、场地规划、设计计算、数值模拟、安全风险评估和教育辅导等。

Conclusion: 本文综述了大型语言模型 (LLM) 在岩土工程中的应用和发展，分析了其优势和局限性，并指出了未来研究方向。

Abstract: Recent developments in large language models (LLMs) are opening up new
opportunities in geotechnical engineering and engineering geology. While
general-purpose LLMs possess broad capabilities, effective application in
geotechnics often requires domain-specific adaptation. Such tailored LLMs are
increasingly employed to streamline geotechnical workflows. This paper presents
the first survey of the adaptation and application of LLMs in geotechnical
engineering. It outlines key methodologies for adaptation to geotechnical
domain, including prompt engineering, retrieval-augmented generation,
domain-adaptive pretraining, and fine-tuning. The survey examines the
state-of-the-art applications of geotechnical-adapted LLMs, including
geological interpretation, subsurface characterization, site planning, design
calculations, numerical modeling, safety and risk assessment, and educational
tutoring. It also analyzes benefits and limitations of geotechnical-adapted
LLMs, and identifies promising directions for future research in this
interdisciplinary discipline. The findings serve as a valuable resource for
practitioners seeking to integrate LLMs into geotechnical practice, while also
providing a foundation to stimulate further investigation within the academic
community.

</details>


### [16] [ADMC: Attention-based Diffusion Model for Missing Modalities Feature Completion](https://arxiv.org/abs/2507.05624)
*Wei Zhang,Juan Chen,Yanbo J. Wang,En Zhu,Xuan Yang,Yiduo Wang*

Main category: cs.AI

TL;DR: 基于注意力的扩散模型ADMC有效解决了多模态情感和意图识别中缺失模态问题，并取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在处理多模态情感和意图识别中缺失模态问题时存在的过度耦合和生成精度低的问题。

Method: 提出了一种基于注意力的扩散模型ADMC，该模型独立训练每个模态的特征提取网络，避免过度耦合，并使用注意力机制生成与真实多模态分布高度一致的缺失模态特征。

Result: 在IEMOCAP和MIntRec基准测试中取得了最先进的结果，有效提升了缺失模态和完整模态场景下的识别性能。

Conclusion: 提出了一种基于注意力的扩散模型ADMC用于处理多模态情感和意图识别中缺失模态的问题，并在IEMOCAP和MIntRec基准测试中取得了最先进的结果。

Abstract: Multimodal emotion and intent recognition is essential for automated
human-computer interaction, It aims to analyze users' speech, text, and visual
information to predict their emotions or intent. One of the significant
challenges is that missing modalities due to sensor malfunctions or incomplete
data. Traditional methods that attempt to reconstruct missing information often
suffer from over-coupling and imprecise generation processes, leading to
suboptimal outcomes. To address these issues, we introduce an Attention-based
Diffusion model for Missing Modalities feature Completion (ADMC). Our framework
independently trains feature extraction networks for each modality, preserving
their unique characteristics and avoiding over-coupling. The Attention-based
Diffusion Network (ADN) generates missing modality features that closely align
with authentic multimodal distribution, enhancing performance across all
missing-modality scenarios. Moreover, ADN's cross-modal generation offers
improved recognition even in full-modality contexts. Our approach achieves
state-of-the-art results on the IEMOCAP and MIntRec benchmarks, demonstrating
its effectiveness in both missing and complete modality scenarios.

</details>
