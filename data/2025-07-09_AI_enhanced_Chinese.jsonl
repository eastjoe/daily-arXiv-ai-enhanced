{"id": "2507.05267", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05267", "abs": "https://arxiv.org/abs/2507.05267", "authors": ["Markus B\u00f6ck"], "title": "Strongly Solving $7 \\times 6$ Connect-Four on Consumer Grade Hardware", "comment": null, "summary": "While the game Connect-Four has been solved mathematically and the best move\ncan be effectively computed with search based methods, a strong solution in the\nform of a look-up table was believed to be infeasible. In this paper, we\nrevisit a symbolic search method based on binary decision diagrams to produce\nstrong solutions. With our efficient implementation we were able to produce a\n89.6 GB large look-up table in 47 hours on a single CPU core with 128 GB main\nmemory for the standard $7 \\times 6$ board size. In addition to this\nwin-draw-loss evaluation, we include an alpha-beta search in our open source\nartifact to find the move which achieves the fastest win or slowest loss.", "AI": {"tldr": "\u4f7f\u7528\u7b26\u53f7\u641c\u7d22\u65b9\u6cd5\u521b\u5efa\u4e86\u5927\u578bConnect-Four\u67e5\u627e\u8868\uff0c\u7528\u4e8e\u5feb\u901f\u8bc4\u4f30\u6e38\u620f\u72b6\u6001\u548c\u5bfb\u627e\u6700\u4f73\u8d70\u6cd5\u3002", "motivation": "\u6311\u6218\u5df2\u89e3\u51b3\u7684Connect-Four\u6e38\u620f\u5f3a\u529b\u89e3\u51b3\u65b9\u6848\uff08\u67e5\u627e\u8868\uff09\u7684\u53ef\u884c\u6027\u3002", "method": "\u57fa\u4e8e\u4e8c\u5143\u51b3\u7b56\u56fe\u7684\u7b26\u53f7\u641c\u7d22\u65b9\u6cd5", "result": "\u751f\u6210\u4e86\u4e00\u4e2a89.6 GB\u7684Connect-Four\u6e38\u620f\u67e5\u627e\u8868\uff0c\u8be5\u8868\u5305\u542b\u8d62/\u5e73/\u8f93\u8bc4\u4f30\u548calpha-beta\u641c\u7d22\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u9ad8\u6548\u5b9e\u73b0\u57fa\u4e8e\u4e8c\u5143\u51b3\u7b56\u56fe\u7684\u7b26\u53f7\u641c\u7d22\u65b9\u6cd5\uff0c\u751f\u6210\u4e86\u4e00\u4e2a89.6GB\u7684Connect-Four\u6e38\u620f\u67e5\u627e\u8868\uff0c\u8be5\u8868\u53ef\u5728\u5355\u6838CPU\u4e0a\u8fd0\u884c\uff0c\u5e76\u5305\u542b\u8d62/\u5e73/\u8f93\u8bc4\u4f30\u548calpha-beta\u641c\u7d22\u4ee5\u627e\u5230\u6700\u5feb\u83b7\u80dc\u6216\u6700\u6162\u5931\u8d25\u7684\u8d70\u6cd5\u3002"}}
{"id": "2507.05283", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05283", "abs": "https://arxiv.org/abs/2507.05283", "authors": ["Yue Wang", "Miao Zhou", "Guijing Huang", "Rui Zhuo", "Chao Yi", "Zhenliang Ma"], "title": "Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management", "comment": null, "summary": "Pre-timed traffic signal control, commonly used for operating signalized\nintersections and coordinated arterials, requires tedious manual work for\nsignaling plan creating and updating. When the time-of-day or day-of-week plans\nare utilized, one intersection is often associated with multiple plans, leading\nto further repetitive manual plan parameter inputting. To enable a\nuser-friendly traffic signal control plan management process, this study\nproposes Chat2SPaT, a method to convert users' semi-structured and ambiguous\ndescriptions on the signal control plan to exact signal phase and timing (SPaT)\nresults, which could further be transformed into structured stage-based or\nring-based plans to interact with intelligent transportation system (ITS)\nsoftware and traffic signal controllers. With curated prompts, Chat2SPaT first\nleverages large language models' (LLMs) capability of understanding users' plan\ndescriptions and reformulate the plan as a combination of phase sequence and\nphase attribute results in the json format. Based on LLM outputs, python\nscripts are designed to locate phases in a cycle, address nuances of traffic\nsignal control, and finally assemble the complete traffic signal control plan.\nWithin a chat, the pipeline can be utilized iteratively to conduct further plan\nediting. Experiments show that Chat2SPaT can generate plans with an accuracy of\nover 94% for both English and Chinese cases, using a test dataset with over 300\nplan descriptions. As the first benchmark for evaluating LLMs' capability of\nunderstanding traffic signal control plan descriptions, Chat2SPaT provides an\neasy-to-use plan management pipeline for traffic practitioners and researchers,\nserving as a potential new building block for a more accurate and versatile\napplication of LLMs in the field of ITS. The source codes, prompts and test\ndataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.", "AI": {"tldr": "Chat2SPaT \u4f7f\u7528LLM\u5c06\u7528\u6237\u5bf9\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u8ba1\u5212\u7684\u63cf\u8ff0\u8f6c\u6362\u4e3a\u7cbe\u786e\u7684SPaT\u7ed3\u679c\uff0c\u51c6\u786e\u7387\u8d85\u8fc794%\uff0c\u63d0\u9ad8\u4e86\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u8ba1\u5212\u7ba1\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u9884\u5b9a\u65f6\u95f4\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u9700\u8981\u7e41\u7410\u7684\u624b\u52a8\u5de5\u4f5c\u6765\u521b\u5efa\u548c\u66f4\u65b0\u4fe1\u53f7\u8ba1\u5212\uff0c\u4e00\u4e2a\u4ea4\u53c9\u8def\u53e3\u901a\u5e38\u4e0e\u591a\u4e2a\u8ba1\u5212\u76f8\u5173\u8054\uff0c\u5bfc\u81f4\u8fdb\u4e00\u6b65\u91cd\u590d\u624b\u52a8\u8f93\u5165\u8ba1\u5212\u53c2\u6570\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7406\u89e3\u7528\u6237\u8ba1\u5212\u63cf\u8ff0\uff0c\u5e76\u5c06\u8ba1\u5212\u91cd\u65b0\u5236\u5b9a\u4e3a JSON \u683c\u5f0f\u7684\u76f8\u4f4d\u5e8f\u5217\u548c\u76f8\u4f4d\u5c5e\u6027\u7ec4\u5408\u3002Python\u811a\u672c\u7528\u4e8e\u5b9a\u4f4d\u5faa\u73af\u4e2d\u7684\u76f8\u4f4d\uff0c\u89e3\u51b3\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u7684\u7ec6\u5fae\u4e4b\u5904\uff0c\u5e76\u6700\u7ec8\u7ec4\u88c5\u5b8c\u6574\u7684\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u8ba1\u5212\u3002", "result": "\u63d0\u51faChat2SPaT\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u8d85\u8fc794%\uff0c\u4e3a\u4ea4\u901a\u4ece\u4e1a\u4eba\u5458\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u6613\u4e8e\u4f7f\u7528\u7684\u8ba1\u5212\u7ba1\u7406\u6d41\u7a0b\u3002", "conclusion": "Chat2SPaT\uff0c\u4e00\u79cd\u5c06\u7528\u6237\u5bf9\u4fe1\u53f7\u63a7\u5236\u8ba1\u5212\u7684\u534a\u7ed3\u6784\u5316\u548c\u6a21\u7cca\u63cf\u8ff0\u8f6c\u6362\u4e3a\u7cbe\u786e\u4fe1\u53f7\u76f8\u4f4d\u548c\u65f6\u95f4 (SPaT) \u7ed3\u679c\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u8ba1\u5212\u7ba1\u7406\u6548\u7387\u3002\u5176\u51c6\u786e\u7387\u8d85\u8fc794%\u3002"}}
{"id": "2507.05297", "categories": ["cs.AI", "econ.TH"], "pdf": "https://arxiv.org/pdf/2507.05297", "abs": "https://arxiv.org/abs/2507.05297", "authors": ["Zijun Meng"], "title": "Fuzzy Classification Aggregation for a Continuum of Agents", "comment": null, "summary": "We prove that any optimal, independent, and zero unanimous fuzzy\nclassification aggregation function of a continuum of individual\nclassifications of $m\\ge 3$ objects into $2\\le p\\le m$ types must be a weighted\narithmetic mean.", "AI": {"tldr": "\u6a21\u7cca\u5206\u7c7b\u805a\u5408\u51fd\u6570\u662f\u52a0\u6743\u7b97\u672f\u5e73\u5747\u503c", "motivation": "\u7814\u7a76\u6a21\u7cca\u5206\u7c7b\u805a\u5408\u51fd\u6570\u7684\u6027\u8d28", "method": "\u8bc1\u660e", "result": "\u8bc1\u660e\u4e86\u6700\u4f73\u3001\u72ec\u7acb\u4e14\u96f6\u4e00\u81f4\u7684\u6a21\u7cca\u5206\u7c7b\u805a\u5408\u51fd\u6570\u662f\u52a0\u6743\u7b97\u672f\u5e73\u5747\u503c", "conclusion": "\u8bc1\u660e\u4e86\u5728\u5bf9m\u22653\u4e2a\u5bf9\u8c61\u8fdb\u884c2\u2264p\u2264m\u7c7b\u7c7b\u578b\u7684\u8fde\u7eed\u4e2a\u4f53\u5206\u7c7b\u4e2d\uff0c\u4efb\u4f55\u6700\u4f73\u3001\u72ec\u7acb\u4e14\u96f6\u4e00\u81f4\u7684\u6a21\u7cca\u5206\u7c7b\u805a\u5408\u51fd\u6570\u5fc5\u987b\u662f\u52a0\u6743\u7b97\u672f\u5e73\u5747\u503c\u3002"}}
{"id": "2507.05488", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.05488", "abs": "https://arxiv.org/abs/2507.05488", "authors": ["Subhasis Dasgupta", "Jon Stephens", "Amarnath Gupta"], "title": "OLG++: A Semantic Extension of Obligation Logic Graph", "comment": null, "summary": "We present OLG++, a semantic extension of the Obligation Logic Graph (OLG)\nfor modeling regulatory and legal rules in municipal and interjurisdictional\ncontexts. OLG++ introduces richer node and edge types, including spatial,\ntemporal, party group, defeasibility, and logical grouping constructs, enabling\nnuanced representations of legal obligations, exceptions, and hierarchies. The\nmodel supports structured reasoning over rules with contextual conditions,\nprecedence, and complex triggers. We demonstrate its expressiveness through\nexamples from food business regulations, showing how OLG++ supports legal\nquestion answering using property graph queries. OLG++ also improves over\nLegalRuleML by providing native support for subClassOf, spatial constraints,\nand reified exception structures. Our examples show that OLG++ is more\nexpressive than prior graph-based models for legal knowledge representation.", "AI": {"tldr": "OLG++ \u662f\u4e00\u4e2a\u7528\u4e8e\u5efa\u6a21\u89c4\u7ae0\u5236\u5ea6\u7684\u8bed\u4e49\u6269\u5c55\uff0c\u6bd4\u73b0\u6709\u6a21\u578b\u66f4\u5177\u8868\u8fbe\u529b\uff0c\u652f\u6301\u590d\u6742\u7684\u89c4\u5219\u63a8\u7406\u548c\u6cd5\u5f8b\u95ee\u9898\u89e3\u7b54\u3002", "motivation": "\u6a21\u62df\u5e02\u653f\u548c\u8de8\u8f96\u533a\u73af\u5883\u4e2d\u7684\u89c4\u7ae0\u5236\u5ea6\uff0c\u652f\u6301\u5bf9\u5177\u6709\u4e0a\u4e0b\u6587\u6761\u4ef6\u3001\u4f18\u5148\u7ea7\u548c\u590d\u6742\u89e6\u53d1\u5668\u7684\u89c4\u5219\u8fdb\u884c\u7ed3\u6784\u5316\u63a8\u7406\u3002", "method": "\u6269\u5c55\u4e49\u52a1\u903b\u8f91\u56fe (OLG)\uff0c\u5f15\u5165\u66f4\u4e30\u5bcc\u7684\u8282\u70b9\u548c\u8fb9\u7c7b\u578b\uff0c\u5305\u62ec\u7a7a\u95f4\u3001\u65f6\u95f4\u3001\u5f53\u4e8b\u65b9\u7fa4\u4f53\u3001\u53ef\u53cd\u9a73\u6027\u548c\u903b\u8f91\u5206\u7ec4\u7ed3\u6784\u3002", "result": "OLG++ \u6bd4 LegalRuleML \u66f4\u597d\uff0c\u652f\u6301 subClassOf\u3001\u7a7a\u95f4\u7ea6\u675f\u548c\u91cd\u8a00\u4f8b\u5916\u7ed3\u6784\uff0c\u5728\u98df\u7269\u4f01\u4e1a\u6cd5\u89c4\u793a\u4f8b\u4e2d\u5c55\u793a\u4e86\u5176\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u652f\u6301\u4f7f\u7528\u5c5e\u6027\u56fe\u67e5\u8be2\u8fdb\u884c\u6cd5\u5f8b\u95ee\u9898\u89e3\u7b54\u3002", "conclusion": "OLG++ \u6269\u5c55\u4e86\u4e49\u52a1\u903b\u8f91\u56fe (OLG)\uff0c\u7528\u4e8e\u6a21\u62df\u5e02\u653f\u548c\u8de8\u8f96\u533a\u73af\u5883\u4e2d\u7684\u89c4\u7ae0\u5236\u5ea6\uff0c\u5e76\u901a\u8fc7\u98df\u7269\u4f01\u4e1a\u6cd5\u89c4\u793a\u4f8b\u5c55\u793a\u4e86\u5176\u8868\u8fbe\u80fd\u529b\uff0c\u652f\u6301\u4f7f\u7528\u5c5e\u6027\u56fe\u67e5\u8be2\u8fdb\u884c\u6cd5\u5f8b\u95ee\u9898\u89e3\u7b54\uff0c\u5e76\u4e14\u6bd4\u4e4b\u524d\u7684\u57fa\u4e8e\u56fe\u7684\u6cd5\u5f8b\u77e5\u8bc6\u8868\u793a\u6a21\u578b\u66f4\u5177\u8868\u8fbe\u529b\u3002"}}
{"id": "2507.05495", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05495", "abs": "https://arxiv.org/abs/2507.05495", "authors": ["Prahaladh Chandrahasan", "Jiahe Jin", "Zhihan Zhang", "Tevin Wang", "Andy Tang", "Lucy Mo", "Morteza Ziyadi", "Leonardo F. R. Ribeiro", "Zimeng Qiu", "Markus Dreyer", "Akari Asai", "Chenyan Xiong"], "title": "Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents", "comment": null, "summary": "Effectively evaluating deep research agents that autonomously search the web,\nanalyze information, and generate reports remains a major challenge,\nparticularly when it comes to assessing long reports and giving detailed\nfeedback on their intermediate steps. To address these gaps, we introduce Deep\nResearch Comparator, a platform that offers a holistic framework for deep\nresearch agent hosting, side-by-side comparison, fine-grained human feedback\ncollection, and ranking calculation. Given a user query, our platform displays\nthe final reports from two different agents along with their intermediate steps\nduring generation. Annotators can evaluate the overall quality of final reports\nbased on side-by-side comparison, and also provide detailed feedback separately\nby assessing intermediate steps or specific text spans within the final report.\nFurthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This\nscaffold serves as a baseline that facilitates the easy integration of various\nlarge language models to transform them into deep research agents for\nevaluation. To demonstrate the platform's utility for deep research agent\ndevelopment, we have collected real user preference data from 17 annotators on\nthree deep research agents. A demo video of our platform can be found at\nhttps://www.youtube.com/watch?v=g4d2dnbdseg.", "AI": {"tldr": "Deep Research Comparator\u5e73\u53f0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6846\u67b6\u6765\u8bc4\u4f30\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u540d\u4e3aSimple Deepresearch\u7684\u7aef\u5230\u7aef\u4ee3\u7406\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u8bc4\u4f30\u81ea\u4e3b\u641c\u7d22\u7f51\u7edc\u3001\u5206\u6790\u4fe1\u606f\u548c\u751f\u6210\u62a5\u544a\u7684\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff0c\u5c24\u5176\u662f\u5728\u8bc4\u4f30\u957f\u62a5\u544a\u548c\u63d0\u4f9b\u4e2d\u95f4\u6b65\u9aa4\u7684\u8be6\u7ec6\u53cd\u9988\u65b9\u9762\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u5e73\u53f0\uff0c\u8be5\u5e73\u53f0\u5141\u8bb8\u5e76\u6392\u6bd4\u8f83\u4e0d\u540c\u4ee3\u7406\u7684\u6700\u7ec8\u62a5\u544a\u53ca\u5176\u4e2d\u95f4\u6b65\u9aa4\uff0c\u5e76\u6536\u96c6\u7ec6\u7c92\u5ea6\u7684\u7528\u6237\u53cd\u9988\u3002\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aSimple Deepresearch\u7684\u7aef\u5230\u7aef\u4ee3\u7406\u6846\u67b6\uff0c\u65b9\u4fbf\u5c06\u5404\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u3002", "result": "\u6536\u96c6\u4e8617\u4f4d\u6807\u6ce8\u8005\u5bf9\u4e09\u4e2a\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u771f\u5b9e\u7528\u6237\u504f\u597d\u6570\u636e\uff0c\u8bc1\u660e\u4e86\u8be5\u5e73\u53f0\u5bf9\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u5f00\u53d1\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u5e73\u53f0Deep Research Comparator\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u4ee3\u7406\u6846\u67b6Simple Deepresearch\u3002"}}
{"id": "2507.05515", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.05515", "abs": "https://arxiv.org/abs/2507.05515", "authors": ["Haochen Huang", "Jiahuan Pei", "Mohammad Aliannejadi", "Xin Sun", "Moonisa Ahsan", "Pablo Cesar", "Chuang Yu", "Zhaochun Ren", "Junxiao Wang"], "title": "Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality", "comment": "20 pages", "summary": "Vision-language models (VLMs) are essential for enabling AI-powered smart\nassistants to interpret and reason in multimodal environments. However, their\napplication in augmented reality (AR) training remains largely unexplored. In\nthis work, we introduce a comprehensive dataset tailored for AR training,\nfeaturing systematized vision-language tasks, and evaluate nine\nstate-of-the-art VLMs on it. Our results reveal that even advanced models,\nincluding GPT-4o, struggle with fine-grained assembly tasks, achieving a\nmaximum F1 score of just 40.54% on state detection. These findings highlight\nthe demand for enhanced datasets, benchmarks, and further research to improve\nfine-grained vision-language alignment. Beyond technical contributions, our\nwork has broader social implications, particularly in empowering blind and\nvisually impaired users with equitable access to AI-driven learning\nopportunities. We provide all related resources, including the dataset, source\ncode, and evaluation results, to support the research community.", "AI": {"tldr": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u589e\u5f3a\u73b0\u5b9e\u8bad\u7ec3\uff0c\u7279\u522b\u662f\u7ec6\u7c92\u5ea6\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u589e\u5f3a\u73b0\u5b9e\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u9488\u5bf9\u7ec6\u7c92\u5ea6\u88c5\u914d\u4efb\u52a1\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u7528\u4e8eAR\u8bad\u7ec3\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e5d\u4e2a\u6700\u5148\u8fdb\u7684VLMs\u3002", "result": "\u5373\u4f7f\u662f\u5148\u8fdb\u7684VLMs\u6a21\u578b\uff0c\u5728\u7ec6\u7c92\u5ea6\u88c5\u914d\u4efb\u52a1\u4e0a\u7684F1\u5206\u6570\u4e5f\u53ea\u670940.54%\uff0c\u8fd9\u8868\u660e\u9700\u8981\u6539\u8fdb\u6570\u636e\u96c6\u3001\u57fa\u51c6\u548c\u6a21\u578b\u3002", "conclusion": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u589e\u5f3a\u73b0\u5b9e(AR)\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\u4ecd\u6709\u5f85\u63a2\u7d22\uff0c\u5373\u4f7f\u662fGPT-4o\u7b49\u5148\u8fdb\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u88c5\u914d\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e5f\u4e0d\u7406\u60f3\uff0c\u8fd9\u51f8\u663e\u4e86\u6539\u8fdb\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u7684\u9700\u6c42\u3002"}}
{"id": "2507.05519", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.05519", "abs": "https://arxiv.org/abs/2507.05519", "authors": ["Gopal Gupta", "Abhiramon Rajasekharan", "Alexis R. Tudor", "Elmer Salazar", "Joaqu\u00edn Arias"], "title": "Modeling (Deontic) Modal Operators With the s(CASP) Goal-directed Predicated Answer Set Programming System", "comment": null, "summary": "We consider the problem of implementing deontic modal logic. We show how\n(deontic) modal operators can be expressed elegantly using default negation\n(negation-as-failure) and strong negation present in answer set programming\n(ASP). We propose using global constraints of ASP to represent obligations and\nimpermissibilities of deontic modal logic. We show that our proposed\nrepresentation results in the various paradoxes of deontic modal logic being\nelegantly resolved.", "AI": {"tldr": "\u7528ASP\u4f18\u96c5\u5730\u5b9e\u73b0\u4e86\u5fb7\u6602\u63d0\u514b\u6a21\u6001\u903b\u8f91\uff0c\u5e76\u89e3\u51b3\u4e86\u5176\u6096\u8bba\u3002", "motivation": "\u5b9e\u73b0\u5fb7\u6602\u63d0\u514b\u6a21\u6001\u903b\u8f91\u3002", "method": "\u4f7f\u7528Answer Set Programming (ASP)\u4e2d\u7684\u9ed8\u8ba4\u5426\u5b9a\u548c\u5f3a\u5426\u5b9a\u4ee5\u53ca\u5168\u5c40\u7ea6\u675f\u6765\u8868\u8fbe\u5fb7\u6602\u63d0\u514b\u6a21\u6001\u903b\u8f91\u4e2d\u7684\u4e49\u52a1\u548c\u7981\u6b62\u3002", "result": "\u4f18\u96c5\u5730\u89e3\u51b3\u4e86\u5fb7\u6602\u63d0\u514b\u6a21\u6001\u903b\u8f91\u7684\u5404\u79cd\u6096\u8bba\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528Answer Set Programming (ASP)\u4e2d\u7684\u9ed8\u8ba4\u5426\u5b9a\u548c\u5f3a\u5426\u5b9a\u4f18\u96c5\u5730\u8868\u8fbe(\u5fb7\u6602\u63d0\u514b)\u6a21\u6001\u7b97\u5b50\uff0c\u5e76\u63d0\u51fa\u4f7f\u7528ASP\u7684\u5168\u5c40\u7ea6\u675f\u6765\u8868\u793a\u5fb7\u6602\u63d0\u514b\u6a21\u6001\u903b\u8f91\u4e2d\u7684\u4e49\u52a1\u548c\u7981\u6b62\uff0c\u4ece\u800c\u4f18\u96c5\u5730\u89e3\u51b3\u4e86\u5fb7\u6602\u63d0\u514b\u6a21\u6001\u903b\u8f91\u7684\u5404\u79cd\u6096\u8bba\u3002"}}
{"id": "2507.05520", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05520", "abs": "https://arxiv.org/abs/2507.05520", "authors": ["Karishma Thakrar", "Shreyas Basavatia", "Akshay Daftardar"], "title": "Cultivating Multimodal Intelligence: Interpretive Reasoning and Agentic RAG Approaches to Dermatological Diagnosis", "comment": "2025 ImageCLEF MEDIQA-MAGIC Challenge", "summary": "The second edition of the 2025 ImageCLEF MEDIQA-MAGIC challenge, co-organized\nby researchers from Microsoft, Stanford University, and the Hospital Clinic of\nBarcelona, focuses on multimodal dermatology question answering and\nsegmentation, using real-world patient queries and images. This work addresses\nthe Closed Visual Question Answering (CVQA) task, where the goal is to select\nthe correct answer to multiple-choice clinical questions based on both\nuser-submitted images and accompanying symptom descriptions. The proposed\napproach combines three core components: (1) fine-tuning open-source multimodal\nmodels from the Qwen, Gemma, and LLaMA families on the competition dataset, (2)\nintroducing a structured reasoning layer that reconciles and adjudicates\nbetween candidate model outputs, and (3) incorporating agentic\nretrieval-augmented generation (agentic RAG), which adds relevant information\nfrom the American Academy of Dermatology's symptom and condition database to\nfill in gaps in patient context. The team achieved second place with a\nsubmission that scored sixth, demonstrating competitive performance and high\naccuracy. Beyond competitive benchmarks, this research addresses a practical\nchallenge in telemedicine: diagnostic decisions must often be made\nasynchronously, with limited input and with high accuracy and interpretability.\nBy emulating the systematic reasoning patterns employed by dermatologists when\nevaluating skin conditions, this architecture provided a pathway toward more\nreliable automated diagnostic support systems.", "AI": {"tldr": "\u5229\u7528\u591a\u6a21\u6001\u6a21\u578b\u3001\u7ed3\u6784\u5316\u63a8\u7406\u548c\u77e5\u8bc6\u589e\u5f3a\uff0c\u6784\u5efa\u76ae\u80a4\u75c5\u95ee\u7b54\u7cfb\u7edf\uff0c\u5728\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u4f73\u7ee9\uff0c\u5e76\u4e3a\u8fdc\u7a0b\u533b\u7597\u4e2d\u7684\u5f02\u6b65\u8bca\u65ad\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u8fdc\u7a0b\u533b\u7597\u4e2d\u5f02\u6b65\u8bca\u65ad\u573a\u666f\u4e0b\uff0c\u57fa\u4e8e\u6709\u9650\u8f93\u5165\u505a\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u8bca\u65ad\u7684\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5fae\u8c03\u5f00\u6e90\u591a\u6a21\u6001\u6a21\u578b\uff08Qwen, Gemma, LLaMA\uff09\u3001\u7ed3\u6784\u5316\u63a8\u7406\u5c42\u548c\u589e\u5f3a\u578b\u68c0\u7d22\u751f\u6210\uff08agentic RAG\uff09\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u3002", "result": "\u5728MEDIQA-MAGIC\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e8c\u540d\uff08\u63d0\u4ea4\u7ed3\u679c\u6392\u540d\u7b2c\u516d\uff09\uff0c\u5c55\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u548c\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5728MEDIQA-MAGIC\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e8c\u540d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u6a21\u6001\u6a21\u578b\u3001\u7ed3\u6784\u5316\u63a8\u7406\u5c42\u548c\u589e\u5f3a\u578b\u68c0\u7d22\u751f\u6210\u65b9\u6cd5\u7684\u76ae\u80a4\u75c5\u95ee\u7b54\u7cfb\u7edf\uff0c\u5728\u5f02\u6b65\u8bca\u65ad\u573a\u666f\u4e0b\u5c55\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u53ef\u9760\u7684\u81ea\u52a8\u5316\u8bca\u65ad\u652f\u6301\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9014\u5f84\u3002"}}
{"id": "2507.05528", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05528", "abs": "https://arxiv.org/abs/2507.05528", "authors": ["Jiahuan Pei", "Fanghua Ye", "Xin Sun", "Wentao Deng", "Koen Hindriks", "Junxiao Wang"], "title": "Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment", "comment": "14 pages", "summary": "Large language models (LLMs) have advanced virtual educators and learners,\nbridging NLP with AI4Education. Existing work often lacks scalability and fails\nto leverage diverse, large-scale course content, with limited frameworks for\nassessing pedagogic quality. To this end, we propose WikiHowAgent, a\nmulti-agent workflow leveraging LLMs to simulate interactive teaching-learning\nconversations. It integrates teacher and learner agents, an interaction\nmanager, and an evaluator to facilitate procedural learning and assess\npedagogic quality. We introduce a dataset of 114,296 teacher-learner\nconversations grounded in 14,287 tutorials across 17 domains and 727 topics.\nOur evaluation protocol combines computational and rubric-based metrics with\nhuman judgment alignment. Results demonstrate the workflow's effectiveness in\ndiverse setups, offering insights into LLM capabilities across domains. Our\ndatasets and implementations are fully open-sourced.", "AI": {"tldr": "WikiHowAgent:\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\uff0c\u53ef\u6269\u5c55\u7684\u4e92\u52a8\u6559\u5b66\u7cfb\u7edf\uff0c\u5728\u591a\u6837\u5316\u73af\u5883\u4e0b\u6709\u6548\u3002", "motivation": "\u73b0\u6709\u865a\u62df\u6559\u80b2\u8005\u548c\u5b66\u4e60\u8005\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\uff0c\u672a\u80fd\u5229\u7528\u591a\u6837\u5316\u7684\u5927\u89c4\u6a21\u8bfe\u7a0b\u5185\u5bb9\uff0c\u4e14\u7f3a\u4e4f\u8bc4\u4f30\u6559\u5b66\u8d28\u91cf\u7684\u6846\u67b6\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edfWikiHowAgent\uff0c\u5305\u542b\u6559\u5e08\u4ee3\u7406\u3001\u5b66\u4e60\u8005\u4ee3\u7406\u3001\u4ea4\u4e92\u7ba1\u7406\u5668\u548c\u8bc4\u4f30\u5668\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e14,287\u7bc7\u6559\u7a0b\u7684\u5927\u578b\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "WikiHowAgent\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u5747\u6709\u6548\uff0c\u7ed3\u5408\u8ba1\u7b97\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u6307\u6807\u4ee5\u53ca\u4eba\u5de5\u5224\u65ad\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002\u6570\u636e\u96c6\u548c\u5b9e\u73b0\u5747\u5df2\u5f00\u6e90\u3002", "conclusion": "WikiHowAgent\uff0c\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u4e92\u52a8\u6559\u5b66\u5b66\u4e60\u5bf9\u8bdd\u7684\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6709\u6548\u4e14\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u63d0\u4f9b\u5bf9LLM\u8de8\u9886\u57df\u80fd\u529b\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.05538", "categories": ["cs.AI", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.05538", "abs": "https://arxiv.org/abs/2507.05538", "authors": ["Subhabrata Majumdar", "Brian Pendleton", "Abhishek Gupta"], "title": "Red Teaming AI Red Teaming", "comment": null, "summary": "Red teaming has evolved from its origins in military applications to become a\nwidely adopted methodology in cybersecurity and AI. In this paper, we take a\ncritical look at the practice of AI red teaming. We argue that despite its\ncurrent popularity in AI governance, there exists a significant gap between red\nteaming's original intent as a critical thinking exercise and its narrow focus\non discovering model-level flaws in the context of generative AI. Current AI\nred teaming efforts focus predominantly on individual model vulnerabilities\nwhile overlooking the broader sociotechnical systems and emergent behaviors\nthat arise from complex interactions between models, users, and environments.\nTo address this deficiency, we propose a comprehensive framework\noperationalizing red teaming in AI systems at two levels: macro-level system\nred teaming spanning the entire AI development lifecycle, and micro-level model\nred teaming. Drawing on cybersecurity experience and systems theory, we further\npropose a set of recommendations. In these, we emphasize that effective AI red\nteaming requires multifunctional teams that examine emergent risks, systemic\nvulnerabilities, and the interplay between technical and social factors.", "AI": {"tldr": "AI\u7ea2\u961f\u5e94\u5173\u6ce8\u7cfb\u7edf\u7ea7\u98ce\u9669\uff0c\u800c\u975e\u4ec5\u5173\u6ce8\u5355\u4e2a\u6a21\u578b\u6f0f\u6d1e", "motivation": "\u5f25\u5408AI\u7ea2\u961f\u6700\u521d\u7684\u6279\u5224\u6027\u601d\u7ef4\u7ec3\u4e60\u610f\u56fe\u4e0e\u5176\u5728\u751f\u6210\u5f0fAI\u73af\u5883\u4e0b\u5bf9\u6a21\u578b\u7ea7\u7f3a\u9677\u7684\u72ed\u9698\u5173\u6ce8\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5728\u5b8f\u89c2\u548c\u5fae\u89c2\u4e24\u4e2a\u5c42\u9762\u5b9e\u65bdAI\u7cfb\u7edf\u7ea2\u961f\u7684\u7efc\u5408\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u5efa\u8bae\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u5b8f\u89c2\u7cfb\u7edf\u7ea2\u961f\u548c\u5fae\u89c2\u6a21\u578b\u7ea2\u961f\u7684\u7efc\u5408\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u5efa\u8bae\uff0c\u5f3a\u8c03\u6709\u6548AI\u7ea2\u961f\u9700\u8981\u591a\u529f\u80fd\u56e2\u961f\u6765\u68c0\u67e5\u6d8c\u73b0\u98ce\u9669\u3001\u7cfb\u7edf\u6f0f\u6d1e\u4ee5\u53ca\u6280\u672f\u548c\u793e\u4f1a\u56e0\u7d20\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "conclusion": "\u5f53\u524dAI\u7ea2\u961f\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u6a21\u578b\u6f0f\u6d1e\uff0c\u5ffd\u89c6\u4e86\u6a21\u578b\u3001\u7528\u6237\u548c\u73af\u5883\u4e4b\u95f4\u590d\u6742\u4ea4\u4e92\u4ea7\u751f\u7684\u66f4\u5e7f\u6cdb\u7684\u793e\u4f1a\u6280\u672f\u7cfb\u7edf\u548c\u6d8c\u73b0\u884c\u4e3a\u3002"}}
{"id": "2507.05541", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05541", "abs": "https://arxiv.org/abs/2507.05541", "authors": ["Shovito Barua Soumma", "Asiful Arefeen", "Stephanie M. Carpenter", "Melanie Hingle", "Hassan Ghasemzadeh"], "title": "SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation", "comment": "In review", "summary": "Counterfactual explanations (CFs) offer human-centric insights into machine\nlearning predictions by highlighting minimal changes required to alter an\noutcome. Therefore, CFs can be used as (i) interventions for abnormality\nprevention and (ii) augmented data for training robust models. In this work, we\nexplore large language models (LLMs), specifically GPT-4o-mini, for generating\nCFs in a zero-shot and three-shot setting. We evaluate our approach on two\ndatasets: the AI-Readi flagship dataset for stress prediction and a public\ndataset for heart disease detection. Compared to traditional methods such as\nDiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high\nplausibility (up to 99%), strong validity (up to 0.99), and competitive\nsparsity. Moreover, using LLM-generated CFs as augmented samples improves\ndownstream classifier performance (an average accuracy gain of 5%), especially\nin low-data regimes. This demonstrates the potential of prompt-based generative\ntechniques to enhance explainability and robustness in clinical and\nphysiological prediction tasks. Code base: github.com/anonymous/SenseCF.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u6709\u6548\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u63d0\u5347\u533b\u7597\u9884\u6d4b\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7528\u4e8e\u5f02\u5e38\u9884\u9632\u548c\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002", "method": "\u5229\u7528GPT-4o-mini\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u5e76\u5728\u538b\u529b\u9884\u6d4b\u548c\u5fc3\u810f\u75c5\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u7406\u6027\u3001\u6709\u6548\u6027\u548c\u7a00\u758f\u6027\u65b9\u9762\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5e76\u80fd\u63d0\u9ad8\u4e0b\u6e38\u5206\u7c7b\u5668\u7684\u51c6\u786e\u7387\uff08\u5e73\u5747\u63d0\u9ad85%\uff09\u3002", "conclusion": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\u5728\u533b\u7597\u9884\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u6709\u6548\u6027\uff0c\u5e76\u80fd\u63d0\u5347\u4e0b\u6e38\u5206\u7c7b\u5668\u7684\u6027\u80fd\u3002"}}
{"id": "2507.05566", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05566", "abs": "https://arxiv.org/abs/2507.05566", "authors": ["David Bensa\u00efd", "Noam Rotstein", "Roy Velich", "Daniel Bensa\u00efd", "Ron Kimmel"], "title": "SingLoRA: Low Rank Adaptation Using a Single Matrix", "comment": null, "summary": "Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient\nfine-tuning of large pretrained models. LoRA augments the pre-trained weights\nof a model by adding the product of two smaller matrices that together form a\nlow-rank matrix update. Recent research has shown that scale disparities\nbetween these two matrices often cause unstable training dynamics, leading to\nsuboptimal performance. In this paper, we propose SingLoRA, which reformulates\nlow-rank adaptation by learning the weights update as a decomposition of a\nsingle low-rank matrix multiplied by its transpose. This simple design\ninherently removes inter-matrix scale conflicts, ensuring stable optimization,\nand roughly halves the parameter count. We analyze SingLoRA within the\ninfinite-width neural network framework, showing that it guarantees stable\nfeature learning by construction. Extensive experiments on multiple tasks\nvalidate these benefits. In common sense reasoning, fine-tuning LLama 7B on\nMNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+\n(90.2%) - while using only 60% of their parameter budget. In image generation,\nfine-tuning Stable Diffusion with SingLoRA significantly improves image\nfidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to\nscores of 0.148 and 0.143 for DoRA and LoRA, respectively.", "AI": {"tldr": "SingLoRA\u901a\u8fc7\u6539\u8fdbLoRA\u7684\u6743\u91cd\u66f4\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u53c2\u6570\u66f4\u5c11\uff0c\u6027\u80fd\u66f4\u597d\u3002", "motivation": "\u89e3\u51b3LoRA\u4e2d\u77e9\u9635\u95f4\u5c3a\u5ea6\u5dee\u5f02\u5bfc\u81f4\u7684\u4e0d\u7a33\u5b9a\u8bad\u7ec3\u52a8\u6001\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5c06\u6743\u91cd\u66f4\u65b0\u5206\u89e3\u4e3a\u5355\u4e2a\u4f4e\u79e9\u77e9\u9635\u4e0e\u5176\u8f6c\u7f6e\u7684\u4e58\u79ef\u3002", "result": "\u5728MNLI\u4efb\u52a1\u4e0a\uff0cSingLoRA\u5fae\u8c03Llama 7B\u6a21\u578b\u8fbe\u523091.3%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8eLoRA(89.1%)\u548cLoRA+(90.2%)\uff1b\u5728DreamBooth\u4efb\u52a1\u4e0a\uff0cSingLoRA\u5fae\u8c03Stable Diffusion\u6a21\u578b\u7684DINO\u76f8\u4f3c\u5ea6\u5f97\u5206\u8fbe\u52300.151\uff0c\u4f18\u4e8eLoRA(0.143)\u548cDoRA(0.148)\u3002", "conclusion": "SingLoRA\u6539\u8fdb\u4e86LoRA\uff0c\u901a\u8fc7\u5b66\u4e60\u5355\u4e2a\u4f4e\u79e9\u77e9\u9635\u4e0e\u5176\u8f6c\u7f6e\u7684\u4e58\u79ef\u6765\u66f4\u65b0\u6743\u91cd\uff0c\u89e3\u51b3\u4e86LoRA\u4e2d\u77e9\u9635\u95f4\u5c3a\u5ea6\u5dee\u5f02\u5bfc\u81f4\u7684\u4e0d\u7a33\u5b9a\u8bad\u7ec3\u95ee\u9898\uff0c\u53c2\u6570\u91cf\u51cf\u5c11\u4e00\u534a\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2507.05587", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05587", "abs": "https://arxiv.org/abs/2507.05587", "authors": ["Elija Perrier"], "title": "Towards Measurement Theory for Artificial Intelligence", "comment": "Under review for Iliad Conference 2025", "summary": "We motivate and outline a programme for a formal theory of measurement of\nartificial intelligence. We argue that formalising measurement for AI will\nallow researchers, practitioners, and regulators to: (i) make comparisons\nbetween systems and the evaluation methods applied to them; (ii) connect\nfrontier AI evaluations with established quantitative risk analysis techniques\ndrawn from engineering and safety science; and (iii) foreground how what counts\nas AI capability is contingent upon the measurement operations and scales we\nelect to use. We sketch a layered measurement stack, distinguish direct from\nindirect observables, and signpost how these ingredients provide a pathway\ntoward a unified, calibratable taxonomy of AI phenomena.", "AI": {"tldr": "\u63d0\u51fa\u4eba\u5de5\u667a\u80fd\u6d4b\u91cf\u5f62\u5f0f\u7406\u8bba\uff0c\u6784\u5efa\u5206\u5c42\u6d4b\u91cf\u4f53\u7cfb\uff0c\u4ee5\u4fc3\u8fdbAI\u7cfb\u7edf\u6bd4\u8f83\u548c\u98ce\u9669\u8bc4\u4f30\u3002", "motivation": "\u5f62\u5f0f\u5316AI\u6d4b\u91cf\u80fd\u4f7f\u7814\u7a76\u8005\u3001\u5b9e\u8df5\u8005\u548c\u76d1\u7ba1\u8005\u66f4\u597d\u5730\u6bd4\u8f83\u7cfb\u7edf\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5c06\u524d\u6cbfAI\u8bc4\u4f30\u4e0e\u65e2\u6709\u5b9a\u91cf\u98ce\u9669\u5206\u6790\u6280\u672f\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u9610\u660eAI\u80fd\u529b\u5982\u4f55\u53d6\u51b3\u4e8e\u6211\u4eec\u9009\u62e9\u7684\u6d4b\u91cf\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u5206\u5c42\u6d4b\u91cf\u4f53\u7cfb\uff0c\u533a\u5206\u76f4\u63a5\u548c\u95f4\u63a5\u53ef\u89c2\u5bdf\u91cf\u3002", "result": "\u52fe\u52d2\u4e86\u4e00\u4e2a\u5206\u5c42\u7684\u6d4b\u91cf\u4f53\u7cfb\uff0c\u533a\u5206\u4e86\u76f4\u63a5\u548c\u95f4\u63a5\u53ef\u89c2\u5bdf\u91cf\uff0c\u5e76\u6307\u660e\u4e86\u8fd9\u4e9b\u8981\u7d20\u5982\u4f55\u4e3a\u6784\u5efa\u7edf\u4e00\u7684\u3001\u53ef\u6821\u51c6\u7684AI\u73b0\u8c61\u5206\u7c7b\u6cd5\u63d0\u4f9b\u9014\u5f84\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u5957\u4eba\u5de5\u667a\u80fd\u6d4b\u91cf\u5f62\u5f0f\u7406\u8bba\u7684\u7eb2\u8981\uff0c\u65e8\u5728\u4fc3\u8fdbAI\u7cfb\u7edf\u95f4\u7684\u6bd4\u8f83\u3001\u4e0e\u65e2\u6709\u98ce\u9669\u5206\u6790\u6280\u672f\u7684\u5173\u8054\uff0c\u4ee5\u53ca\u5bf9AI\u80fd\u529b\u7684\u91cd\u65b0\u5b9a\u4e49\u3002"}}
{"id": "2507.05591", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05591", "abs": "https://arxiv.org/abs/2507.05591", "authors": ["Wei Zhang", "Juan Chen", "En Zhu", "Wenhong Cheng", "YunPeng Li", "Yanbo J. Wang"], "title": "MLlm-DR: Towards Explainable Depression Recognition with MultiModal Large Language Models", "comment": null, "summary": "Automated depression diagnosis aims to analyze multimodal information from\ninterview videos to predict participants' depression scores. Previous studies\noften lack clear explanations of how these scores were determined, limiting\ntheir adoption in clinical practice. While the advent of LLMs provides a\npossible pathway for explainable depression diagnosis, current LLMs capable of\nprocessing multimodal data lack training on interview data, resulting in poor\ndiagnostic performance when used directly. In this paper, we propose a novel\nmultimodal large language model (MLlm-DR) that can understand multimodal\ninformation inputs and supports explainable depression diagnosis. MLlm-DR\nintegrates a smaller LLMs and a lightweight query module (LQ-former).\nSpecifically, the smaller LLMs is designed to generate depression scores and\ncorresponding evaluation rationales. To enhance its logical reasoning for\ndomain-specific tasks while maintaining practicality, we constructed a robust\ntraining dataset to fine-tune it. Meanwhile, the LQ-former captures\ndepression-related features from speech and visual data, aiding the model's\nability to process multimodal information, to achieve comprehensive depression\ndiagnosis. Our approach achieves state-of-the-art results on two\ninterview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its\neffectiveness and superiority.", "AI": {"tldr": "\u65b0\u578b\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578bMLlm-DR\u53ef\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u6291\u90c1\u75c7\u8bca\u65ad\uff0c\u5e76\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\u3002", "motivation": "\u4ee5\u5f80\u7684\u7814\u7a76\u7f3a\u4e4f\u6e05\u6670\u7684\u6291\u90c1\u75c7\u8bc4\u5206\u786e\u5b9a\u65b9\u6cd5\u89e3\u91ca\uff0c\u4e14\u73b0\u6709LLMs\u7f3a\u4e4f\u8bbf\u8c08\u6570\u636e\u8bad\u7ec3\uff0c\u5bfc\u81f4\u8bca\u65ad\u6027\u80fd\u5dee\u3002", "method": "\u6574\u5408\u5c0f\u578bLLMs\u548c\u8f7b\u91cf\u7ea7\u67e5\u8be2\u6a21\u5757LQ-former\uff0c\u5229\u7528\u4e00\u4e2a\u5f3a\u5927\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728CMDC\u548cE-DAIC-WOZ\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578bMLlm-DR\uff0c\u7528\u4e8e\u53ef\u89e3\u91ca\u7684\u6291\u90c1\u75c7\u8bca\u65ad\uff0c\u5e76\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}}
{"id": "2507.05613", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05613", "abs": "https://arxiv.org/abs/2507.05613", "authors": ["Lei Fan", "Fangxue Liu", "Cheng Chen"], "title": "Domain adaptation of large language models for geotechnical applications", "comment": null, "summary": "Recent developments in large language models (LLMs) are opening up new\nopportunities in geotechnical engineering and engineering geology. While\ngeneral-purpose LLMs possess broad capabilities, effective application in\ngeotechnics often requires domain-specific adaptation. Such tailored LLMs are\nincreasingly employed to streamline geotechnical workflows. This paper presents\nthe first survey of the adaptation and application of LLMs in geotechnical\nengineering. It outlines key methodologies for adaptation to geotechnical\ndomain, including prompt engineering, retrieval-augmented generation,\ndomain-adaptive pretraining, and fine-tuning. The survey examines the\nstate-of-the-art applications of geotechnical-adapted LLMs, including\ngeological interpretation, subsurface characterization, site planning, design\ncalculations, numerical modeling, safety and risk assessment, and educational\ntutoring. It also analyzes benefits and limitations of geotechnical-adapted\nLLMs, and identifies promising directions for future research in this\ninterdisciplinary discipline. The findings serve as a valuable resource for\npractitioners seeking to integrate LLMs into geotechnical practice, while also\nproviding a foundation to stimulate further investigation within the academic\ncommunity.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6b63\u9010\u6e10\u5e94\u7528\u4e8e\u5ca9\u571f\u5de5\u7a0b\uff0c\u63d0\u9ad8\u6548\u7387\u5e76\u62d3\u5c55\u5e94\u7528\u8303\u56f4\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5404\u4e2a\u9886\u57df\u5c55\u73b0\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u5ca9\u571f\u5de5\u7a0b\u4e5f\u4e0d\u4f8b\u5916\u3002\u672c\u6587\u65e8\u5728\u5bf9LLM\u5728\u5ca9\u571f\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u5168\u9762\u8c03\u7814\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u672c\u6587\u91c7\u7528\u6587\u732e\u8c03\u7814\u7684\u65b9\u6cd5\uff0c\u5bf9\u73b0\u6709\u6587\u732e\u8fdb\u884c\u7efc\u8ff0\u548c\u5206\u6790\u3002", "result": "\u672c\u6587\u6982\u8ff0\u4e86LLM\u5728\u5ca9\u571f\u5de5\u7a0b\u4e2d\u7684\u51e0\u79cd\u9002\u5e94\u65b9\u6cd5\uff0c\u5305\u62ec\u63d0\u793a\u5de5\u7a0b\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7b49\uff0c\u5e76\u603b\u7ed3\u4e86\u5176\u5728\u5ca9\u571f\u5de5\u7a0b\u5404\u4e2a\u65b9\u9762\u7684\u5e94\u7528\uff0c\u4f8b\u5982\u5730\u8d28\u89e3\u91ca\u3001\u5730\u4e0b\u8868\u5f81\u3001\u573a\u5730\u89c4\u5212\u3001\u8bbe\u8ba1\u8ba1\u7b97\u3001\u6570\u503c\u6a21\u62df\u3001\u5b89\u5168\u98ce\u9669\u8bc4\u4f30\u548c\u6559\u80b2\u8f85\u5bfc\u7b49\u3002", "conclusion": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u5ca9\u571f\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u548c\u53d1\u5c55\uff0c\u5206\u6790\u4e86\u5176\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.05624", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05624", "abs": "https://arxiv.org/abs/2507.05624", "authors": ["Wei Zhang", "Juan Chen", "Yanbo J. Wang", "En Zhu", "Xuan Yang", "Yiduo Wang"], "title": "ADMC: Attention-based Diffusion Model for Missing Modalities Feature Completion", "comment": null, "summary": "Multimodal emotion and intent recognition is essential for automated\nhuman-computer interaction, It aims to analyze users' speech, text, and visual\ninformation to predict their emotions or intent. One of the significant\nchallenges is that missing modalities due to sensor malfunctions or incomplete\ndata. Traditional methods that attempt to reconstruct missing information often\nsuffer from over-coupling and imprecise generation processes, leading to\nsuboptimal outcomes. To address these issues, we introduce an Attention-based\nDiffusion model for Missing Modalities feature Completion (ADMC). Our framework\nindependently trains feature extraction networks for each modality, preserving\ntheir unique characteristics and avoiding over-coupling. The Attention-based\nDiffusion Network (ADN) generates missing modality features that closely align\nwith authentic multimodal distribution, enhancing performance across all\nmissing-modality scenarios. Moreover, ADN's cross-modal generation offers\nimproved recognition even in full-modality contexts. Our approach achieves\nstate-of-the-art results on the IEMOCAP and MIntRec benchmarks, demonstrating\nits effectiveness in both missing and complete modality scenarios.", "AI": {"tldr": "\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6269\u6563\u6a21\u578bADMC\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u60c5\u611f\u548c\u610f\u56fe\u8bc6\u522b\u4e2d\u7f3a\u5931\u6a21\u6001\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6a21\u6001\u60c5\u611f\u548c\u610f\u56fe\u8bc6\u522b\u4e2d\u7f3a\u5931\u6a21\u6001\u95ee\u9898\u65f6\u5b58\u5728\u7684\u8fc7\u5ea6\u8026\u5408\u548c\u751f\u6210\u7cbe\u5ea6\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6269\u6563\u6a21\u578bADMC\uff0c\u8be5\u6a21\u578b\u72ec\u7acb\u8bad\u7ec3\u6bcf\u4e2a\u6a21\u6001\u7684\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\uff0c\u907f\u514d\u8fc7\u5ea6\u8026\u5408\uff0c\u5e76\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u751f\u6210\u4e0e\u771f\u5b9e\u591a\u6a21\u6001\u5206\u5e03\u9ad8\u5ea6\u4e00\u81f4\u7684\u7f3a\u5931\u6a21\u6001\u7279\u5f81\u3002", "result": "\u5728IEMOCAP\u548cMIntRec\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7f3a\u5931\u6a21\u6001\u548c\u5b8c\u6574\u6a21\u6001\u573a\u666f\u4e0b\u7684\u8bc6\u522b\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6269\u6563\u6a21\u578bADMC\u7528\u4e8e\u5904\u7406\u591a\u6a21\u6001\u60c5\u611f\u548c\u610f\u56fe\u8bc6\u522b\u4e2d\u7f3a\u5931\u6a21\u6001\u7684\u95ee\u9898\uff0c\u5e76\u5728IEMOCAP\u548cMIntRec\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}}
