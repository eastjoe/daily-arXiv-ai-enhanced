<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Renewable Energy Sources Selection Analysis with the Maximizing Deviation Method](https://arxiv.org/abs/2509.07011)
*Kirisci Murat*

Main category: cs.AI

TL;DR: 本文提出一种基于偏差最大化方法的优化模型，用于确定部分已知特征权重，并将其与区间值Fermatean模糊集相结合，应用于可再生能源的选择问题。


<details>
  <summary>Details</summary>
Motivation: 解决在不确定、复杂和冲突环境下选择可再生能源的问题。

Method: 提出一种基于偏差最大化方法的优化模型，结合区间值Fermatean模糊集处理模糊决策。

Result: 将所提出的方法应用于可再生能源的选择问题。

Conclusion: 该方法有效地处理了决策过程中的不确定性和模糊性，为可再生能源的选择提供了一种新的思路。

Abstract: Multi-criteria decision-making methods provide decision-makers with
appropriate tools to make better decisions in uncertain, complex, and
conflicting situations. Fuzzy set theory primarily deals with the uncertainty
inherent in human thoughts and perceptions and attempts to quantify this
uncertainty. Fuzzy logic and fuzzy set theory are utilized with multi-criteria
decision-making methods because they effectively handle uncertainty and
fuzziness in decision-makers' judgments, allowing for verbal judgments of the
problem. This study utilizes the Fermatean fuzzy environment, a generalization
of fuzzy sets. An optimization model based on the deviation maximization method
is proposed to determine partially known feature weights. This method is
combined with interval-valued Fermatean fuzzy sets. The proposed method was
applied to the problem of selecting renewable energy sources. The reason for
choosing renewable energy sources is that meeting energy needs from renewable
sources, balancing carbon emissions, and mitigating the effects of global
climate change are among the most critical issues of the recent period. Even
though selecting renewable energy sources is a technical issue, the managerial
and political implications of this issue are also important, and are discussed
in this study.

</details>


### [2] [From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning](https://arxiv.org/abs/2509.07017)
*Andrew Kiruluta,Priscilla Burity*

Main category: cs.AI

TL;DR: Spectral NSR框架结合图谱频谱分析进行神经符号推理，兼具可解释性和可扩展性，在推理基准测试中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号推理方法缺乏可扩展性和可解释性。

Method: 提出Spectral NSR框架，利用图信号处理和拉普拉斯特征结构，将逻辑规则嵌入为频谱模板，在图谱频谱域直接进行推理。

Result: 在ProofWriter和CLUTRR等基准测试中，Spectral NSR在准确性、推理速度、鲁棒性和可解释性方面均优于现有方法。

Conclusion: Spectral NSR为下一代推理系统提供了可扩展、可靠且具有原则性的基础。

Abstract: We introduce Spectral NSR, a fully spectral neuro-symbolic reasoning
framework that embeds logical rules as spectral templates and performs
inference directly in the graph spectral domain. By leveraging graph signal
processing (GSP) and frequency-selective filters grounded in the Laplacian
eigenstructure of knowledge graphs, the architecture unifies the
interpretability of symbolic reasoning with the scalability and adaptability of
spectral learning. Beyond the core formulation, we incorporate a comprehensive
set of extensions, including dynamic graph and basis learning, rational and
diffusion filters for sharper spectral selectivity, mixture-of-spectral-experts
for modular specialization, proof-guided training with spectral curricula, and
uncertainty quantification for calibrated confidence. Additional enhancements
such as large language model coupling, co-spectral transfer alignment,
adversarial robustness, efficient GPU kernels, generalized Laplacians, and
causal interventions further expand the versatility of the framework.
  Empirical evaluation on state-of-the-art reasoning benchmarks such as
ProofWriter and CLUTRR demonstrates that Spectral NSR achieves superior
accuracy, faster inference, improved robustness to adversarial perturbations,
and higher interpretability compared to leading baselines including
transformers, message-passing neural networks, and neuro-symbolic logic
programming systems. Spectral attribution and proof-band agreement analyses
confirm that model decisions align closely with symbolic proof structures,
while transfer experiments validate effective domain adaptation through
co-spectral alignment. These results establish Spectral NSR as a scalable and
principled foundation for the next generation of reasoning systems, offering
transparency, robustness, and generalization beyond conventional approaches.

</details>


### [3] [Statistical Methods in Generative AI](https://arxiv.org/abs/2509.07054)
*Edgar Dobriban*

Main category: cs.AI

TL;DR: 该论文综述了统计方法在提高生成式AI可靠性、质量和效率方面的应用，并探讨了其局限性和未来方向。


<details>
  <summary>Details</summary>
Motivation: 生成式AI缺乏正确性、安全性和公平性等保证，统计方法可用于提高其可靠性及评估质量。

Method: 综述现有研究工作，解释使用的统计技术及其在生成式AI中的应用。

Result: 综述了统计方法在生成式AI中的应用，并指出了其局限性。

Conclusion: 统计方法在提高生成式AI的可靠性、质量和效率方面具有巨大潜力，但仍存在一些局限性，未来需要进一步研究。

Abstract: Generative Artificial Intelligence is emerging as an important technology,
promising to be transformative in many areas. At the same time, generative AI
techniques are based on sampling from probabilistic models, and by default,
they come with no guarantees about correctness, safety, fairness, or other
properties. Statistical methods offer a promising potential approach to improve
the reliability of generative AI techniques. In addition, statistical methods
are also promising for improving the quality and efficiency of AI evaluation,
as well as for designing interventions and experiments in AI.
  In this paper, we review some of the existing work on these topics,
explaining both the general statistical techniques used, as well as their
applications to generative AI. We also discuss limitations and potential future
directions.

</details>


### [4] [Instruction Agent: Enhancing Agent with Expert Demonstration](https://arxiv.org/abs/2509.07098)
*Yinheng Li,Hailey Hultquist,Justin Wagle,Kazuhito Koishida*

Main category: cs.AI

TL;DR: Instruction Agent利用专家演示解决复杂GUI任务，成功率达60%。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理难以处理涉及新UI元素、长时序动作和个性化轨迹的复杂任务。

Method: 该代理从单一演示中提取步骤指令，严格遵循用户轨迹执行，并包含验证器和回溯模块处理意外中断。

Result: 在OSWorld数据集上，Instruction Agent成功完成了一系列顶级代理都失败的任务（成功率60%）。

Conclusion: Instruction Agent提供了一个实用且可扩展的框架，弥合了现有GUI代理与可靠的现实世界GUI任务自动化之间的差距。

Abstract: Graphical user interface (GUI) agents have advanced rapidly but still
struggle with complex tasks involving novel UI elements, long-horizon actions,
and personalized trajectories. In this work, we introduce Instruction Agent, a
GUI agent that leverages expert demonstrations to solve such tasks, enabling
completion of otherwise difficult workflows. Given a single demonstration, the
agent extracts step-by-step instructions and executes them by strictly
following the trajectory intended by the user, which avoids making mistakes
during execution. The agent leverages the verifier and backtracker modules
further to improve robustness. Both modules are critical to understand the
current outcome from each action and handle unexpected interruptions(such as
pop-up windows) during execution. Our experiments show that Instruction Agent
achieves a 60% success rate on a set of tasks in OSWorld that all top-ranked
agents failed to complete. The Instruction Agent offers a practical and
extensible framework, bridging the gap between current GUI agents and reliable
real-world GUI task automation.

</details>


### [5] [Neuro-Symbolic Frameworks: Conceptual Characterization and Empirical Comparative Analysis](https://arxiv.org/abs/2509.07122)
*Sania Sinha,Tanawan Premsri,Danial Kamali,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: 本文分析了神经符号 (NeSy) 框架的现状，指出其缺乏易用工具和统一框架，并介绍了三个代表性框架：DeepProbLog，Scallop 和 DomiKnowS。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号框架缺乏易用性，阻碍了其发展。

Method: 对现有神经符号框架的技术特性进行特征分析，并介绍三个代表性框架。

Result: 识别了每个框架在表达力和解决不同问题方面的挑战。

Conclusion: 呼吁社区重新思考神经符号建模问题，推动该领域发展。

Abstract: Neurosymbolic (NeSy) frameworks combine neural representations and learning
with symbolic representations and reasoning. Combining the reasoning
capacities, explainability, and interpretability of symbolic processing with
the flexibility and power of neural computing allows us to solve complex
problems with more reliability while being data-efficient. However, this
recently growing topic poses a challenge to developers with its learning curve,
lack of user-friendly tools, libraries, and unifying frameworks. In this paper,
we characterize the technical facets of existing NeSy frameworks, such as the
symbolic representation language, integration with neural models, and the
underlying algorithms. A majority of the NeSy research focuses on algorithms
instead of providing generic frameworks for declarative problem specification
to leverage problem solving. To highlight the key aspects of Neurosymbolic
modeling, we showcase three generic NeSy frameworks - \textit{DeepProbLog},
\textit{Scallop}, and \textit{DomiKnowS}. We identify the challenges within
each facet that lay the foundation for identifying the expressivity of each
framework in solving a variety of problems. Building on this foundation, we aim
to spark transformative action and encourage the community to rethink this
problem in novel ways.

</details>


### [6] [Autoencoder-Based Denoising of Muscle Artifacts in ECG to Preserve Skin Nerve Activity (SKNA) for Cognitive Stress Detection](https://arxiv.org/abs/2509.07146)
*Farnoush Baghestani,Jihye Moon,Youngsun Kong,Ki Chon*

Main category: cs.AI

TL;DR: 利用轻量级一维卷积自动编码器和LSTM瓶颈，提出一种新的去噪方法，有效去除肌电图伪迹，提高皮肤神经活动（SKNA）信号质量，提升交感神经活动监测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以去除肌电图伪迹对SKNA信号的影响，限制了其在自然环境下的应用。

Method: 使用一维卷积自动编码器和LSTM瓶颈的深度学习模型对ECG数据进行去噪处理，并采用留一法交叉验证。

Result: 该方法显著提高了信噪比，增加了与干净SKNA的互相关性，恢复了基于爆发的SKNA特征，并实现了高精度的交感神经刺激分类。

Conclusion: 深度学习方法可以有效去除肌电图伪迹，提高SKNA监测的鲁棒性，使其在更自然的运动环境中应用。

Abstract: The sympathetic nervous system (SNS) plays a central role in regulating the
body's responses to stress and maintaining physiological stability. Its
dysregulation is associated with a wide range of conditions, from
cardiovascular disease to anxiety disorders. Skin nerve activity (SKNA)
extracted from high-frequency electrocardiogram (ECG) recordings provides a
noninvasive window into SNS dynamics, but its measurement is highly susceptible
to electromyographic (EMG) contamination. Traditional preprocessing based on
bandpass filtering within a fixed range (e.g., 500--1000 Hz) is susceptible to
overlapping EMG and SKNA spectral components, especially during sustained
muscle activity. We present a denoising approach using a lightweight
one-dimensional convolutional autoencoder with a long short-term memory (LSTM)
bottleneck to reconstruct clean SKNA from EMG-contaminated recordings. Using
clean ECG-derived SKNA data from cognitive stress experiments and EMG noise
from chaotic muscle stimulation recordings, we simulated contamination at
realistic noise levels (--4 dB, --8 dB signal-to-noise ratio) and trained the
model in the leave-one-subject-out cross-validation framework. The method
improved signal-to-noise ratio by up to 9.65 dB, increased cross correlation
with clean SKNA from 0.40 to 0.72, and restored burst-based SKNA features to
near-clean discriminability (AUROC $\geq$ 0.96). Classification of baseline
versus sympathetic stimulation (cognitive stress) conditions reached accuracies
of 91--98\% across severe noise levels, comparable to clean data. These results
demonstrate that deep learning--based reconstruction can preserve
physiologically relevant sympathetic bursts during substantial EMG
interference, enabling more robust SKNA monitoring in naturalistic,
movement-rich environments.

</details>


### [7] [PaVeRL-SQL: Text-to-SQL via Partial-Match Rewards and Verbal Reinforcement Learning](https://arxiv.org/abs/2509.07159)
*Heng Hao,Wenjun Hu,Oxana Verkholyak,Davoud Ataee Tarzanagh,Baruch Gutow,Sima Didari,Masoud Faraki,Hankyu Moon,Seungjai Min*

Main category: cs.AI

TL;DR: PaVeRL-SQL框架结合局部匹配奖励和语言增强学习，提升了Text-to-SQL模型在处理工业级数据库和复杂问题的准确性，在Spider, Spider 2.0和BIRD基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法在处理工业规模数据库和涉及领域特定业务逻辑的复杂问题时，执行准确率低。

Method: 提出PaVeRL-SQL框架，结合局部匹配奖励和语言增强学习，包含基于大型语言模型的上下文学习框架和基于链式思维的强化学习流程两种pipeline。

Result: 在Spider, Spider 2.0和BIRD基准测试中取得最先进的结果，在Spider2.0-SQLite基准测试中，执行准确率比现有技术高出7.4% (verbal-RL pipeline) 和1.4% (CoT pipeline)。混合SQL方言训练效果显著。

Conclusion: PaVeRL-SQL在实际工业约束下提供了可靠且先进的Text-to-SQL解决方案。

Abstract: Text-to-SQL models allow users to interact with a database more easily by
generating executable SQL statements from natural-language questions. Despite
recent successes on simpler databases and questions, current Text-to-SQL
methods still suffer from low execution accuracy on industry-scale databases
and complex questions involving domain-specific business logic. We present
\emph{PaVeRL-SQL}, a framework that combines \emph{Partial-Match Rewards} and
\emph{Verbal Reinforcement Learning} to drive self-improvement in reasoning
language models (RLMs) for Text-to-SQL. To handle practical use cases, we adopt
two pipelines: (1) a newly designed in-context learning framework with group
self-evaluation (verbal-RL), using capable open- and closed-source large
language models (LLMs) as backbones; and (2) a chain-of-thought (CoT) RL
pipeline with a small backbone model (OmniSQL-7B) trained with a specially
designed reward function and two-stage RL. These pipelines achieve
state-of-the-art (SOTA) results on popular Text-to-SQL benchmarks -- Spider,
Spider 2.0, and BIRD. For the industrial-level Spider2.0-SQLite benchmark, the
verbal-RL pipeline achieves an execution accuracy 7.4\% higher than SOTA, and
the CoT pipeline is 1.4\% higher. RL training with mixed SQL dialects yields
strong, threefold gains, particularly for dialects with limited training data.
Overall, \emph{PaVeRL-SQL} delivers reliable, SOTA Text-to-SQL under realistic
industrial constraints. The code is available at
https://github.com/PaVeRL-SQL/PaVeRL-SQL.

</details>


### [8] [That's So FETCH: Fashioning Ensemble Techniques for LLM Classification in Civil Legal Intake and Referral](https://arxiv.org/abs/2509.07170)
*Quinten Steenhuis*

Main category: cs.AI

TL;DR: 本文介绍了一种名为FETCH的法律问题分类器，该分类器结合了LLM和ML方法，准确率高达97.37%，显著提高了法律援助服务的效率。


<details>
  <summary>Details</summary>
Motivation: 每年有数百万人寻求法律援助，准确识别其法律问题至关重要，错误匹配会导致严重后果。

Method: 使用419个真实世界查询数据集，结合LLM/ML集成分类方法和自动生成后续问题的方法，对法律问题进行分类。

Result: FETCH分类器的准确率达到97.37%（hits@2），超过了GPT-5模型。

Conclusion: 该方法能有效降低法律援助服务的成本，同时保证高准确率，具有显著的应用前景。

Abstract: Each year millions of people seek help for their legal problems by calling a
legal aid program hotline, walking into a legal aid office, or using a lawyer
referral service. The first step to match them to the right help is to identify
the legal problem the applicant is experiencing. Misdirection has consequences.
Applicants may miss a deadline, experience physical abuse, lose housing or lose
custody of children while waiting to connect to the right legal help. We
introduce and evaluate the FETCH classifier for legal issue classification and
describe two methods for improving accuracy: a hybrid LLM/ML ensemble
classification method, and the automatic generation of follow-up questions to
enrich the initial problem narrative. We employ a novel data set of 419
real-world queries to a nonprofit lawyer referral service. Ultimately, we show
classification accuracy (hits@2) of 97.37\% using a mix of inexpensive models,
exceeding the performance of the current state-of-the-art GPT-5 model. Our
approach shows promise in significantly reducing the cost of guiding users of
the legal system to the right resource for their problem while achieving high
accuracy.

</details>


### [9] [A Hybrid CNN-LSTM Deep Learning Model for Intrusion Detection in Smart Grid](https://arxiv.org/abs/2509.07208)
*Abdulhakim Alsaiari,Mohammad Ilyas*

Main category: cs.AI

TL;DR: 该论文提出了一种基于CNN-LSTM混合深度学习的入侵检测系统，以提高智能电网的网络安全性能，并在DNP3和IEC104数据集上取得了99.70%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 智能电网中SCADA协议的漏洞使其容易受到攻击，可能导致隐私泄露和停电。

Method: 使用CNN-LSTM混合深度学习模型进行入侵检测。

Result: 在DNP3和IEC104数据集上取得了99.70%的检测准确率，优于其他深度学习方法。

Conclusion: 该混合模型有效提高了智能电网的网络安全水平。

Abstract: The evolution of the traditional power grid into the "smart grid" has
resulted in a fundamental shift in energy management, which allows the
integration of renewable energy sources with modern communication technology.
However, this interconnection has increased smart grids' vulnerability to
attackers, which might result in privacy breaches, operational interruptions,
and massive outages. The SCADA-based smart grid protocols are critical for
real-time data collection and control, but they are vulnerable to attacks like
unauthorized access and denial of service (DoS). This research proposes a
hybrid deep learning-based Intrusion Detection System (IDS) intended to improve
the cybersecurity of smart grids. The suggested model takes advantage of
Convolutional Neural Networks' (CNN) feature extraction capabilities as well as
Long Short-Term Memory (LSTM) networks' temporal pattern recognition skills.
DNP3 and IEC104 intrusion detection datasets are employed to train and test our
CNN-LSTM model to recognize and classify the potential cyber threats. Compared
to other deep learning approaches, the results demonstrate considerable
improvements in accuracy, precision, recall, and F1-score, with a detection
accuracy of 99.70%.

</details>


### [10] [BlendedNet: A Blended Wing Body Aircraft Dataset and Surrogate Model for Aerodynamic Predictions](https://arxiv.org/abs/2509.07209)
*Nicholas Sung,Steven Spreizer,Mohamed Elrefaie,Kaira Samuel,Matthew C. Jones,Faez Ahmed*

Main category: cs.AI

TL;DR: BlendedNet数据集包含近九千个融合翼身组合体(BWB)的RANS算例，并提供一个端到端的基于点云的表面气动特性预测框架。


<details>
  <summary>Details</summary>
Motivation: 解决非传统构型气动数据稀缺问题，促进数据驱动的气动设计研究。

Method: 构建BlendedNet数据集，并基于PointNet和FiLM网络构建气动预测框架。

Result: 实现了对不同BWB构型的表面气动特性(Cp, Cfx, Cfz)的低误差预测。

Conclusion: BlendedNet数据集和预测框架可用于数据驱动的气动设计研究。

Abstract: BlendedNet is a publicly available aerodynamic dataset of 999 blended wing
body (BWB) geometries. Each geometry is simulated across about nine flight
conditions, yielding 8830 converged RANS cases with the Spalart-Allmaras model
and 9 to 14 million cells per case. The dataset is generated by sampling
geometric design parameters and flight conditions, and includes detailed
pointwise surface quantities needed to study lift and drag. We also introduce
an end-to-end surrogate framework for pointwise aerodynamic prediction. The
pipeline first uses a permutation-invariant PointNet regressor to predict
geometric parameters from sampled surface point clouds, then conditions a
Feature-wise Linear Modulation (FiLM) network on the predicted parameters and
flight conditions to predict pointwise coefficients Cp, Cfx, and Cfz.
Experiments show low errors in surface predictions across diverse BWBs.
BlendedNet addresses data scarcity for unconventional configurations and
enables research on data-driven surrogate modeling for aerodynamic design.

</details>


### [11] [OmniAcc: Personalized Accessibility Assistant Using Generative AI](https://arxiv.org/abs/2509.07220)
*Siddhant Karki,Ethan Han,Nadim Mahmud,Suman Bhunia,John Femiani,Vaskar Raychoudhury*

Main category: cs.AI

TL;DR: OmniAcc系统利用AI、卫星图像和OpenStreetMap数据，为轮椅使用者提供个性化、实时的无障碍导航服务，并在交叉路口检测中达到97.5%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决轮椅使用者在城市环境中导航的障碍，缺乏无障碍信息和工具。

Method: 利用GPT-4、卫星图像和OpenStreetMap数据，通过零样本学习和定制提示，识别、分类和绘制无障碍设施（如坡道和交叉路口）地图，提供个性化路线规划和实时免提导航。

Result: 交叉路口检测准确率达97.5%。

Conclusion: OmniAcc系统有潜力帮助城市规划者和行动不便人士，促进更包容的城市空间。

Abstract: Individuals with ambulatory disabilities often encounter significant barriers
when navigating urban environments due to the lack of accessible information
and tools. This paper presents OmniAcc, an AI-powered interactive navigation
system that utilizes GPT-4, satellite imagery, and OpenStreetMap data to
identify, classify, and map wheelchair-accessible features such as ramps and
crosswalks in the built environment. OmniAcc offers personalized route
planning, real-time hands-free navigation, and instant query responses
regarding physical accessibility. By using zero-shot learning and customized
prompts, the system ensures precise detection of accessibility features, while
supporting validation through structured workflows. This paper introduces
OmniAcc and explores its potential to assist urban planners and mobility-aid
users, demonstrated through a case study on crosswalk detection. With a
crosswalk detection accuracy of 97.5%, OmniAcc highlights the transformative
potential of AI in improving navigation and fostering more inclusive urban
spaces.

</details>


### [12] [HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring](https://arxiv.org/abs/2509.07260)
*Xin Wang,Ting Dang,Xinyu Zhang,Vassilis Kostakos,Michael J. Witbrock,Hong Jia*

Main category: cs.AI

TL;DR: 小型语言模型(SLM)在移动医疗预测中表现出色，兼顾效率和隐私，但仍需改进以应对数据不平衡和少样本场景。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型(LLM)的医疗解决方案存在隐私和效率问题，因此研究SLM在移动医疗预测中的应用。

Method: 系统评估了SLM在健康预测任务中的表现，使用了零样本、少样本和指令微调方法，并在移动设备上部署了性能最佳的微调SLM。

Result: SLM在性能上可与LLM媲美，同时在效率和隐私方面具有显著优势，但在处理数据不平衡和少样本场景方面仍面临挑战。

Conclusion: SLM是下一代隐私保护医疗监控的有前景的解决方案。

Abstract: Mobile and wearable healthcare monitoring play a vital role in facilitating
timely interventions, managing chronic health conditions, and ultimately
improving individuals' quality of life. Previous studies on large language
models (LLMs) have highlighted their impressive generalization abilities and
effectiveness in healthcare prediction tasks. However, most LLM-based
healthcare solutions are cloud-based, which raises significant privacy concerns
and results in increased memory usage and latency. To address these challenges,
there is growing interest in compact models, Small Language Models (SLMs),
which are lightweight and designed to run locally and efficiently on mobile and
wearable devices. Nevertheless, how well these models perform in healthcare
prediction remains largely unexplored. We systematically evaluated SLMs on
health prediction tasks using zero-shot, few-shot, and instruction fine-tuning
approaches, and deployed the best performing fine-tuned SLMs on mobile devices
to evaluate their real-world efficiency and predictive performance in practical
healthcare scenarios. Our results show that SLMs can achieve performance
comparable to LLMs while offering substantial gains in efficiency and privacy.
However, challenges remain, particularly in handling class imbalance and
few-shot scenarios. These findings highlight SLMs, though imperfect in their
current form, as a promising solution for next-generation, privacy-preserving
healthcare monitoring.

</details>


### [13] [Performative Thinking? The Brittle Correlation Between CoT Length and Problem Complexity](https://arxiv.org/abs/2509.07339)
*Vardhan Palod,Karthik Valmeekam,Kaya Stechly,Subbarao Kambhampati*

Main category: cs.AI

TL;DR: 模型生成的中间令牌序列长度与问题难度之间的相关性并不强，长序列并不一定代表更复杂的思考过程。


<details>
  <summary>Details</summary>
Motivation: 检验中间令牌生成长度是否反映或与问题难度相关。

Method: 从零开始训练transformer模型，使用A*搜索算法的推导轨迹作为训练数据，对不同难度的迷宫问题进行评估。

Result: 即使是最简单的任务，模型也会产生过长的推理轨迹，有时甚至无法生成解决方案；中间令牌长度和真实A*轨迹长度的相关性较弱，只有在问题接近训练分布时才出现相关性。

Conclusion: 问题实例的固有计算复杂度并非重要因素，其与训练数据的分布距离才是关键；长序列不能自动视为“思考努力”的指标。

Abstract: Intermediate token generation (ITG), where a model produces output before the
solution, has been proposed as a method to improve the performance of language
models on reasoning tasks. While these reasoning traces or Chain of Thoughts
(CoTs) are correlated with performance gains, the mechanisms underlying them
remain unclear. A prevailing assumption in the community has been to
anthropomorphize these tokens as "thinking", treating longer traces as evidence
of higher problem-adaptive computation. In this work, we critically examine
whether intermediate token sequence length reflects or correlates with problem
difficulty. To do so, we train transformer models from scratch on derivational
traces of the A* search algorithm, where the number of operations required to
solve a maze problem provides a precise and verifiable measure of problem
complexity. We first evaluate the models on trivial free-space problems,
finding that even for the simplest tasks, they often produce excessively long
reasoning traces and sometimes fail to generate a solution. We then
systematically evaluate the model on out-of-distribution problems and find that
the intermediate token length and ground truth A* trace length only loosely
correlate. We notice that the few cases where correlation appears are those
where the problems are closer to the training distribution, suggesting that the
effect arises from approximate recall rather than genuine problem-adaptive
computation. This suggests that the inherent computational complexity of the
problem instance is not a significant factor, but rather its distributional
distance from the training data. These results challenge the assumption that
intermediate trace generation is adaptive to problem difficulty and caution
against interpreting longer sequences in systems like R1 as automatically
indicative of "thinking effort".

</details>


### [14] [Autonomous Code Evolution Meets NP-Completeness](https://arxiv.org/abs/2509.07367)
*Cunxi Yu,Rongjian Liang,Chia-Tung Ho,Haoxing Ren*

Main category: cs.AI

TL;DR: LLM驱动代码进化框架SATLUTION在SAT求解器上超越人类专家


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码进化方法局限于小规模代码，SATLUTION扩展到整个代码库

Method: 利用LLM agent进化SAT求解器代码库，包含数百个文件和数万行代码，并自进化策略和规则，保证正确性

Result: 在SAT竞赛2024基准测试中，超越了2025年人工设计的冠军

Conclusion: SATLUTION证明了LLM在大型代码库进化中的潜力

Abstract: Large language models (LLMs) have recently shown strong coding abilities,
enabling not only static code generation but also iterative code self-evolving
through agentic frameworks. Recently, AlphaEvolve \cite{novikov2025alphaevolve}
demonstrated that LLM-based coding agents can autonomously improve algorithms
and surpass human experts, with scopes limited to isolated kernels spanning
hundreds of lines of code. Inspired by AlphaEvolve, we present SATLUTION, the
first framework to extend LLM-based code evolution to the full repository
scale, encompassing hundreds of files and tens of thousands of lines of C/C++
code. Targeting Boolean Satisfiability (SAT), the canonical NP-complete problem
and a cornerstone of both theory and applications. SATLUTION orchestrates LLM
agents to directly evolve solver repositories under strict correctness
guarantees and distributed runtime feedback, while simultaneously self-evolving
its own evolution policies and rules. Starting from SAT Competition 2024
codebases and benchmark, SATLUTION evolved solvers that decisively outperformed
the human-designed winners of the SAT Competition 2025, and also surpassed both
2024 and 2025 champions on the 2024 benchmarks.

</details>


### [15] [Language Self-Play For Data-Free Training](https://arxiv.org/abs/2509.07414)
*Jakub Grudzien Kuba,Mengting Gu,Qi Ma,Yuandong Tian,Vijai Mohan*

Main category: cs.AI

TL;DR: 通过语言自我博弈(LSP)方法，无需额外数据即可提升大型语言模型(LLM)性能，在指令遵循基准测试中效果优于数据驱动方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLM训练依赖海量数据的瓶颈问题。

Method: 基于博弈论框架的强化学习方法，模型通过自我博弈提升性能。

Result: 实验表明，预训练模型仅通过自我博弈即可提升其在挑战性任务上的表现，且效果优于数据驱动基线。

Conclusion: 语言自我博弈(LSP)为LLM的持续改进提供了一种无需额外数据的新途径。

Abstract: Large language models (LLMs) have advanced rapidly in recent years, driven by
scale, abundant high-quality training data, and reinforcement learning. Yet
this progress faces a fundamental bottleneck: the need for ever more data from
which models can continue to learn. In this work, we propose a reinforcement
learning approach that removes this dependency by enabling models to improve
without additional data. Our method leverages a game-theoretic framework of
self-play, where a model's capabilities are cast as performance in a
competitive game and stronger policies emerge by having the model play against
itself - a process we call Language Self-Play (LSP). Experiments with
Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained
models can not only enhance their performance on challenging tasks through
self-play alone, but can also do so more effectively than data-driven
baselines.

</details>


### [16] [SheetDesigner: MLLM-Powered Spreadsheet Layout Generation with Rule-Based and Vision-Based Reflection](https://arxiv.org/abs/2509.07473)
*Qin Chen,Yuanyi Ren,Xiaojun Ma,Mugeng Liu,Han Shi,Dongmei Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种无需训练的电子表格布局生成框架SheetDesigner，利用多模态大型语言模型结合规则和视觉反射，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动化布局模型不适用于电子表格，忽略了其离散的网格结构和语义关联。

Method: 提出SheetDesigner框架，利用多模态大型语言模型(MLLMs)结合规则和视觉反射进行组件放置和内容填充。

Result: SheetDesigner优于五个基线方法至少22.6%。视觉模态处理重叠和平衡较好，但在对齐方面存在不足，因此需要混合规则和视觉反射策略。

Conclusion: SheetDesigner为自动化电子表格布局生成提供了一种有效的方法。

Abstract: Spreadsheets are critical to data-centric tasks, with rich, structured
layouts that enable efficient information transmission. Given the time and
expertise required for manual spreadsheet layout design, there is an urgent
need for automated solutions. However, existing automated layout models are
ill-suited to spreadsheets, as they often (1) treat components as axis-aligned
rectangles with continuous coordinates, overlooking the inherently discrete,
grid-based structure of spreadsheets; and (2) neglect interrelated semantics,
such as data dependencies and contextual links, unique to spreadsheets. In this
paper, we first formalize the spreadsheet layout generation task, supported by
a seven-criterion evaluation protocol and a dataset of 3,326 spreadsheets. We
then introduce SheetDesigner, a zero-shot and training-free framework using
Multimodal Large Language Models (MLLMs) that combines rule and vision
reflection for component placement and content population. SheetDesigner
outperforms five baselines by at least 22.6\%. We further find that through
vision modality, MLLMs handle overlap and balance well but struggle with
alignment, necessitates hybrid rule and visual reflection strategies. Our codes
and data is available at Github.

</details>


### [17] [Towards explainable decision support using hybrid neural models for logistic terminal automation](https://arxiv.org/abs/2509.07577)
*Riccardo DElia,Alberto Termine,Francesco Flammini*

Main category: cs.AI

TL;DR: "将深度学习与系统动力学模型相结合，提高交通物流预测精度，同时保证模型的可解释性和因果可靠性。"


<details>
  <summary>Details</summary>
Motivation: "现有深度学习模型在交通物流预测中缺乏可解释性和因果可靠性，限制了其在关键决策系统中的应用。"

Method: "提出一种可解释神经系统动力学建模框架，融合深度学习、基于概念的可解释性、机械论可解释性和因果机器学习等技术。"

Result: "构建能够操作语义上有意义的可执行变量的神经网络模型，同时保持传统系统动力学模型的因果基础和透明度。"

Conclusion: "神经符号方法可以弥合黑盒预测模型与复杂动态环境中关键决策支持需求之间的差距。"

Abstract: The integration of Deep Learning (DL) in System Dynamics (SD) modeling for
transportation logistics offers significant advantages in scalability and
predictive accuracy. However, these gains are often offset by the loss of
explainability and causal reliability $-$ key requirements in critical
decision-making systems. This paper presents a novel framework for
interpretable-by-design neural system dynamics modeling that synergizes DL with
techniques from Concept-Based Interpretability, Mechanistic Interpretability,
and Causal Machine Learning. The proposed hybrid approach enables the
construction of neural network models that operate on semantically meaningful
and actionable variables, while retaining the causal grounding and transparency
typical of traditional SD models. The framework is conceived to be applied to
real-world case-studies from the EU-funded project AutoMoTIF, focusing on
data-driven decision support, automation, and optimization of multimodal
logistic terminals. We aim at showing how neuro-symbolic methods can bridge the
gap between black-box predictive models and the need for critical decision
support in complex dynamical environments within cyber-physical systems enabled
by the industrial Internet-of-Things.

</details>
