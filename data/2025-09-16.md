<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Situation Model of the Transport, Transport Emissions and Meteorological Conditions](https://arxiv.org/abs/2509.10541)
*V. Benes,M. Svitek,A. Michalikova,M. Melicherik*

Main category: cs.AI

TL;DR: 本文研究了交通排放与气象条件的关系，并使用模糊推理系统预测不同条件下排放量的变化，旨在为城市规划者和政策制定者提供更有效的城市交通规划和管理方法。


<details>
  <summary>Details</summary>
Motivation: 城市空气污染日益严重，交通排放是重要因素，需要系统性方法研究其与气象条件的关系。

Method: 使用模糊推理系统 (FIS) 建立交通排放预测模型，基于捷克布拉格的交通、气象和排放数据。

Result: 开发了一个基于模糊推理系统的交通排放预测模型。

Conclusion: 该模型可以帮助城市规划者和政策制定者更有效地规划和管理城市交通，从而更好地保护环境。

Abstract: Air pollution in cities and the possibilities of reducing this pollution
represents one of the most important factors that today's society has to deal
with. This paper focuses on a systemic approach to traffic emissions with their
relation to meteorological conditions, analyzing the effect of weather on the
quantity and dispersion of traffic emissions in a city. Using fuzzy inference
systems (FIS) the model for prediction of changes in emissions depending on
various conditions is developed. The proposed model is based on traffic,
meteorology and emission data measured in Prague, Czech Republic. The main
objective of the work is to provide insight into how urban planners and
policymakers can plan and manage urban transport more effectively with
environmental protection in mind.

</details>


### [2] [ZapGPT: Free-form Language Prompting for Simulated Cellular Control](https://arxiv.org/abs/2509.10660)
*Nam H. Le,Patrick Erickson,Yanbo Zhang,Michael Levin,Josh Bongard*

Main category: cs.AI

TL;DR: 该论文展示了如何仅使用自由格式的自然语言指令来引导简单智能体的集体行为，无需特定任务的调整或精心设计的评估指标。


<details>
  <summary>Details</summary>
Motivation: 弥合自然语言与人工/生物系统之间的差距，实现对复杂分散系统的更自然控制。

Method: 使用两个AI模型：一个将指令转化为对模拟细胞的干预，另一个评估指令与细胞动态的匹配程度，并通过进化改进第一个模型以提高评分。

Result: 证明该系统能够泛化到未见过的指令，无需重新训练。

Conclusion: 这项工作为未来利用语言指导计算、机器人或生物系统提供了可能性，语言将取代数学目标函数、固定规则和特定领域的编程。

Abstract: Human language is one of the most expressive tools for conveying intent, yet
most artificial or biological systems lack mechanisms to interpret or respond
meaningfully to it. Bridging this gap could enable more natural forms of
control over complex, decentralized systems. In AI and artificial life, recent
work explores how language can specify high-level goals, but most systems still
depend on engineered rewards, task-specific supervision, or rigid command sets,
limiting generalization to novel instructions. Similar constraints apply in
synthetic biology and bioengineering, where the locus of control is often
genomic rather than environmental perturbation.
  A key open question is whether artificial or biological collectives can be
guided by free-form natural language alone, without task-specific tuning or
carefully designed evaluation metrics. We provide one possible answer here by
showing, for the first time, that simple agents' collective behavior can be
guided by free-form language prompts: one AI model transforms an imperative
prompt into an intervention that is applied to simulated cells; a second AI
model scores how well the prompt describes the resulting cellular dynamics; and
the former AI model is evolved to improve the scores generated by the latter.
  Unlike previous work, our method does not require engineered fitness
functions or domain-specific prompt design. We show that the evolved system
generalizes to unseen prompts without retraining. By treating natural language
as a control layer, the system suggests a future in which spoken or written
prompts could direct computational, robotic, or biological systems to desired
behaviors. This work provides a concrete step toward this vision of AI-biology
partnerships, in which language replaces mathematical objective functions,
fixed rules, and domain-specific programming.

</details>


### [3] [Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration](https://arxiv.org/abs/2509.10704)
*Xingchen Wan,Han Zhou,Ruoxi Sun,Hootan Nakhost,Ke Jiang,Rajarishi Sinha,Sercan Ö. Arık*

Main category: cs.AI

TL;DR: Maestro系统通过迭代改进提示词，使文本转图像模型能够自主改进生成的图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有的文本转图像模型依赖人工干预，Maestro旨在解决这一问题，实现自主图像生成。

Method: Maestro系统结合了自我批评和自我进化机制，利用多模态大语言模型(MLLM)作为批评者和评判者，迭代改进提示词并选择最佳图像。

Result: 实验结果表明，Maestro显著提高了图像质量，优于初始提示词和现有自动化方法。

Conclusion: Maestro提供了一种鲁棒、可解释且有效的途径，用于改进文本转图像的生成过程。

Abstract: Text-to-image (T2I) models, while offering immense creative potential, are
highly reliant on human intervention, posing significant usability challenges
that often necessitate manual, iterative prompt engineering over often
underspecified prompts. This paper introduces Maestro, a novel self-evolving
image generation system that enables T2I models to autonomously self-improve
generated images through iterative evolution of prompts, using only an initial
prompt. Maestro incorporates two key innovations: 1) self-critique, where
specialized multimodal LLM (MLLM) agents act as 'critics' to identify
weaknesses in generated images, correct for under-specification, and provide
interpretable edit signals, which are then integrated by a 'verifier' agent
while preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge
for head-to-head comparisons between iteratively generated images, eschewing
problematic images, and evolving creative prompt candidates that align with
user intents. Extensive experiments on complex T2I tasks using black-box models
demonstrate that Maestro significantly improves image quality over initial
prompts and state-of-the-art automated methods, with effectiveness scaling with
more advanced MLLM components. This work presents a robust, interpretable, and
effective pathway towards self-improving T2I generation.

</details>


### [4] [Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions](https://arxiv.org/abs/2509.10707)
*Sajjad Abdoli,Rudi Cilibrasi,Rima Al-Shikh*

Main category: cs.AI

TL;DR: 该研究分析了不同AI模型对图像描述生成的评估策略和偏差，发现不同模型展现出独特的“评估个性”，评估能力并不与通用能力成正比


<details>
  <summary>Details</summary>
Motivation: 随着AI系统日益依赖其他AI系统进行评估，理解其评估行为至关重要，以防止偏差累积

Method: 分析NVIDIA的Describe Anything模型生成的视觉语言描述，并用三个GPT模型(GPT-4o, GPT-4o-mini, GPT-5)进行评估，并通过受控实验验证模型特性

Result: GPT-4o-mini评估一致性高，GPT-4o擅长错误检测，GPT-5评估保守且变异性高。GPT模型间相似度高，而Gemini评估策略显著不同。所有GPT模型都存在负面评估偏向

Conclusion: 评估能力并不与通用能力成正比，可靠的AI评估需要多种架构的视角

Abstract: As AI systems increasingly evaluate other AI outputs, understanding their
assessment behavior becomes crucial for preventing cascading biases. This study
analyzes vision-language descriptions generated by NVIDIA's Describe Anything
Model and evaluated by three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to
uncover distinct "evaluation personalities" the underlying assessment
strategies and biases each model demonstrates. GPT-4o-mini exhibits systematic
consistency with minimal variance, GPT-4o excels at error detection, while
GPT-5 shows extreme conservatism with high variability. Controlled experiments
using Gemini 2.5 Pro as an independent question generator validate that these
personalities are inherent model properties rather than artifacts. Cross-family
analysis through semantic similarity of generated questions reveals significant
divergence: GPT models cluster together with high similarity while Gemini
exhibits markedly different evaluation strategies. All GPT models demonstrate a
consistent 2:1 bias favoring negative assessment over positive confirmation,
though this pattern appears family-specific rather than universal across AI
architectures. These findings suggest that evaluation competence does not scale
with general capability and that robust AI assessment requires diverse
architectural perspectives.

</details>


### [5] [AI Answer Engine Citation Behavior An Empirical Analysis of the GEO16 Framework](https://arxiv.org/abs/2509.10762)
*Arlen Kumar,Leanid Palkhouski*

Main category: cs.AI

TL;DR: 该论文介绍了GEO-16框架，用于评估AI引擎引用的网页质量，并发现网页质量是AI引擎进行引用的强预测指标。


<details>
  <summary>Details</summary>
Motivation: AI引擎日益成为获取领域知识的主要途径，但其引用网页的质量参差不齐，因此需要一个评估框架。

Method: 构建了GEO-16框架，收集了三个AI引擎(Brave Summary, Google AI Overviews, and Perplexity)对70个产品意图提示的1702个引用，并审核了1100个URL，使用逻辑模型分析网页质量与被引用率的关系。

Result: 发现元数据、新鲜度、语义HTML和结构化数据等方面与被引用率高度相关；网页整体质量是引用率的强预测指标；设定简单的阈值(例如G至少0.70，且至少12个pillar hits)可以显著提高引用率。

Conclusion: 该研究为出版商提供了一个实用的指南，但同时也指出研究的局限性，例如仅关注英语B2B SaaS页面。

Abstract: AI answer engines increasingly mediate access to domain knowledge by
generating responses and citing web sources. We introduce GEO-16, a 16 pillar
auditing framework that converts on page quality signals into banded pillar
scores and a normalized GEO score G that ranges from 0 to 1. Using 70 product
intent prompts, we collected 1,702 citations across three engines (Brave
Summary, Google AI Overviews, and Perplexity) and audited 1,100 unique URLs. In
our corpus, the engines differed in the GEO quality of the pages they cited,
and pillars related to Metadata and Freshness, Semantic HTML, and Structured
Data showed the strongest associations with citation. Logistic models with
domain clustered standard errors indicate that overall page quality is a strong
predictor of citation, and simple operating points (for example, G at least
0.70 combined with at least 12 pillar hits) align with substantially higher
citation rates in our data. We report per engine contrasts, vertical effects,
threshold analysis, and diagnostics, then translate findings into a practical
playbook for publishers. The study is observational and focuses on English
language B2B SaaS pages; we discuss limitations, threats to validity, and
reproducibility considerations.

</details>


### [6] [AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise](https://arxiv.org/abs/2509.10769)
*Tara Bogavelli,Roshnee Sharma,Hari Subramani*

Main category: cs.AI

TL;DR: 研究了18种不同agent配置在复杂多agent系统中的交互作用，发现最佳模型在复杂任务上仅达到35.3%的成功率，挑战了'一刀切'的agent AI系统设计范式。


<details>
  <summary>Details</summary>
Motivation: 缺乏对复杂多agent系统中不同设计维度交互的经验性理解。

Method: 对18种基于大型语言模型的agent配置进行了基准测试，考察了编排策略、agent提示实现、内存架构和思维工具集成四个维度。

Result: 发现模型之间存在显著的架构偏好，并且agent系统在企业任务上的整体性能较弱，最高成功率仅为35.3%（复杂任务）和70.8%（简单任务）。

Conclusion: 研究结果可为未来agent系统的设计提供更具经验依据的决策，关于架构组件和模型选择。

Abstract: While individual components of agentic architectures have been studied in
isolation, there remains limited empirical understanding of how different
design dimensions interact within complex multi-agent systems. This study aims
to address these gaps by providing a comprehensive enterprise-specific
benchmark evaluating 18 distinct agentic configurations across state-of-the-art
large language models. We examine four critical agentic system dimensions:
orchestration strategy, agent prompt implementation (ReAct versus function
calling), memory architecture, and thinking tool integration. Our benchmark
reveals significant model-specific architectural preferences that challenge the
prevalent one-size-fits-all paradigm in agentic AI systems. It also reveals
significant weaknesses in overall agentic performance on enterprise tasks with
the highest scoring models achieving a maximum of only 35.3\% success on the
more complex task and 70.8\% on the simpler task. We hope these findings inform
the design of future agentic systems by enabling more empirically backed
decisions regarding architectural components and model selection.

</details>


### [7] [LLM Enhancement with Domain Expert Mental Model to Reduce LLM Hallucination with Causal Prompt Engineering](https://arxiv.org/abs/2509.10818)
*Boris Kovalerchuk,Brent D. Fegley*

Main category: cs.AI

TL;DR: 本文提出一种基于优化人机对话和单调布尔/k值函数的算法，用于构建专家决策模型，从而改进LLM在决策支持中的应用，解决LLM在处理信息缺失问题上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM及RAG方法在处理决策问题时，容易出现幻觉或信息缺失，难以应对复杂决策场景。

Method: 提出一种四步法：因素识别、分层结构化、生成通用专家模型规范和生成详细的通用专家模型。该方法基于优化人机对话和单调布尔/k值函数，旨在构建计算可处理的个人专家心智模型（EMM）。

Result: 构建了一个能够有效利用LLM进行决策支持的专家心智模型（EMM）算法。

Conclusion: 该算法通过优化人机对话和利用单调布尔/k值函数，有效地解决了LLM在处理信息缺失和复杂决策问题上的不足，为提高LLM在决策支持中的效率提供了新途径。

Abstract: Difficult decision-making problems abound in various disciplines and domains.
The proliferation of generative techniques, especially large language models
(LLMs), has excited interest in using them for decision support. However, LLMs
cannot yet resolve missingness in their training data, leading to
hallucinations. Retrieval-Augmented Generation (RAG) enhances LLMs by
incorporating external information retrieval, reducing hallucinations and
improving accuracy. Yet, RAG and related methods are only partial solutions, as
they may lack access to all necessary sources or key missing information. Even
everyday issues often challenge LLMs' abilities. Submitting longer prompts with
context and examples is one approach to address knowledge gaps, but designing
effective prompts is non-trivial and may not capture complex mental models of
domain experts. For tasks with missing critical information, LLMs are
insufficient, as are many existing systems poorly represented in available
documents. This paper explores how LLMs can make decision-making more
efficient, using a running example of evaluating whether to respond to a call
for proposals. We propose a technology based on optimized human-machine
dialogue and monotone Boolean and k-valued functions to discover a
computationally tractable personal expert mental model (EMM) of
decision-making. Our EMM algorithm for LLM prompt engineering has four steps:
(1) factor identification, (2) hierarchical structuring of factors, (3)
generating a generalized expert mental model specification, and (4) generating
a detailed generalized expert mental model from that specification.

</details>


### [8] [From Grounding to Skolemization: A Logic-Constrained Vector Symbolic Architecture for Complex Query Answering](https://arxiv.org/abs/2509.10837)
*Yuyin Lu,Hegang Chen,Yanghui Rao*

Main category: cs.AI

TL;DR: 本文提出一种名为LVSA的神经符号框架，用于解决不完整知识图谱上的复杂查询问题，该框架结合了可微的Skolem化模块和神经否定器，并在理论上保证了对所有EFO$_1$查询的普适性，同时在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在逻辑完整性和计算效率之间存在权衡，Grounding-based方法面临组合爆炸问题，而Skolemization-based方法则可能牺牲逻辑一致性。

Method: 提出Logic-constrained Vector Symbolic Architecture (LVSA)框架，结合可微Skolem化模块、神经否定器和逻辑约束优化协议。

Result: LVSA在理论上保证了对所有EFO$_1$查询的普适性，并在实验中优于现有Skolemization-based方法，且推理成本远低于Grounding-based方法。

Conclusion: LVSA框架有效地解决了不完整知识图谱上复杂查询问题的效率和逻辑一致性问题，为未来研究提供了新的方向。

Abstract: Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs),
typically formalized as reasoning with Existential First-Order predicate logic
with one free variable (EFO$_1$), faces a fundamental trade-off between logical
soundness and computational efficiency. This work establishes the
Grounding-Skolemization dichotomy for systematically analyzing CQA methods
through the lens of formal logic. While Grounding-based methods inherently
suffer from combinatorial explosion, most Skolemization-based methods neglect
to explicitly model Skolem functions and compromise logical consistency. To
address these limitations, we propose the Logic-constrained Vector Symbolic
Architecture (LVSA), a neuro-symbolic framework that unifies a differentiable
Skolemization module and a neural negator, as well as a logical
constraint-driven optimization protocol to harmonize geometric and logical
requirements. Theoretically, LVSA guarantees universality for all EFO$_1$
queries. Empirically, it outperforms state-of-the-art Skolemization-based
methods and reduces inference costs by orders of magnitude compared to
Grounding-based baselines.

</details>


### [9] [Is the `Agent' Paradigm a Limiting Framework for Next-Generation Intelligent Systems?](https://arxiv.org/abs/2509.10875)
*Jesse Gardner,Vladimir A. Baulin*

Main category: cs.AI

TL;DR: 本文批判性地评估了人工智能研究中以代理为中心的范式，认为其概念模糊和人类中心主义偏差可能具有局限性，建议转向基于系统动力学、世界建模和物质智能的框架。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能研究中普遍存在的以代理为中心的范式存在概念模糊和人类中心主义偏差等问题。

Method: 系统回顾相关文献，分析不同人工智能框架下的代理范式，并提出基于系统动力学、世界建模和物质智能的替代框架。

Result: 指出了以代理为中心的范式在定义和衡量自主性和目标导向性等属性方面的挑战，并建议转向非代理和系统框架。

Conclusion: 为了发展健壮、可扩展且可能非人类中心的一般智能形式，需要对智能的理解进行根本性反思，超越代理隐喻。

Abstract: The concept of the 'agent' has profoundly shaped Artificial Intelligence (AI)
research, guiding development from foundational theories to contemporary
applications like Large Language Model (LLM)-based systems. This paper
critically re-evaluates the necessity and optimality of this agent-centric
paradigm. We argue that its persistent conceptual ambiguities and inherent
anthropocentric biases may represent a limiting framework. We distinguish
between agentic systems (AI inspired by agency, often semi-autonomous, e.g.,
LLM-based agents), agential systems (fully autonomous, self-producing systems,
currently only biological), and non-agentic systems (tools without the
impression of agency). Our analysis, based on a systematic review of relevant
literature, deconstructs the agent paradigm across various AI frameworks,
highlighting challenges in defining and measuring properties like autonomy and
goal-directedness. We argue that the 'agentic' framing of many AI systems,
while heuristically useful, can be misleading and may obscure the underlying
computational mechanisms, particularly in Large Language Models (LLMs). As an
alternative, we propose a shift in focus towards frameworks grounded in
system-level dynamics, world modeling, and material intelligence. We conclude
that investigating non-agentic and systemic frameworks, inspired by complex
systems, biology, and unconventional computing, is essential for advancing
towards robust, scalable, and potentially non-anthropomorphic forms of general
intelligence. This requires not only new architectures but also a fundamental
reconsideration of our understanding of intelligence itself, moving beyond the
agent metaphor.

</details>


### [10] [Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding](https://arxiv.org/abs/2509.10931)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: 研究者提出了一种名为HaPLa的新型攻击技术，可绕过大型语言模型的安全机制，成功率高达95%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易被恶意利用，需要更强大的防御机制。

Method: HaPLa结合了演绎框架和符号编码两种策略，前者引导模型推断有害行为的中间步骤，后者模糊有害内容。

Result: HaPLa在GPT系列模型上成功率超过95%，其他模型也达到70%。

Conclusion: 难以安全地微调大型语言模型，使其既能抵御攻击又能保持有用性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, but their potential misuse for harmful purposes remains a
significant concern. To strengthen defenses against such vulnerabilities, it is
essential to investigate universal jailbreak attacks that exploit intrinsic
weaknesses in the architecture and learning paradigms of LLMs. In response, we
propose \textbf{H}armful \textbf{P}rompt \textbf{La}undering (HaPLa), a novel
and broadly applicable jailbreaking technique that requires only black-box
access to target models. HaPLa incorporates two primary strategies: 1)
\textit{abductive framing}, which instructs LLMs to infer plausible
intermediate steps toward harmful activities, rather than directly responding
to explicit harmful queries; and 2) \textit{symbolic encoding}, a lightweight
and flexible approach designed to obfuscate harmful content, given that current
LLMs remain sensitive primarily to explicit harmful keywords. Experimental
results show that HaPLa achieves over 95% attack success rate on GPT-series
models and 70% across all targets. Further analysis with diverse symbolic
encoding rules also reveals a fundamental challenge: it remains difficult to
safely tune LLMs without significantly diminishing their helpfulness in
responding to benign queries.

</details>


### [11] [Public Data Assisted Differentially Private In-Context Learning](https://arxiv.org/abs/2509.10932)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: 利用公共数据改进大型语言模型的私有上下文学习，在保护隐私和模型效用之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型的私有上下文学习中，差分隐私保护与模型效用之间的矛盾。

Method: 将任务相关的公共数据整合到私有上下文学习框架中，提出一种新的私有上下文学习算法。

Result: 实验表明，该方法显著提高了私有上下文学习的效用，并对成员推断攻击具有鲁棒性。

Conclusion: 该研究有效平衡了大型语言模型私有上下文学习中的隐私保护和模型效用。

Abstract: In-context learning (ICL) in Large Language Models (LLMs) has shown
remarkable performance across various tasks without requiring fine-tuning.
However, recent studies have highlighted the risk of private data leakage
through the prompt in ICL, especially when LLMs are exposed to malicious
attacks. While differential privacy (DP) provides strong privacy guarantees, it
often significantly reduces the utility of in-context learning (ICL). To
address this challenge, we incorporate task-related public data into the ICL
framework while maintaining the DP guarantee. Based on this approach, we
propose a private in-context learning algorithm that effectively balances
privacy protection and model utility. Through experiments, we demonstrate that
our approach significantly improves the utility of private ICL with the
assistance of public data. Additionally, we show that our method is robust
against membership inference attacks, demonstrating empirical privacy
protection.

</details>


### [12] [Enhancing Computational Cognitive Architectures with LLMs: A Case Study](https://arxiv.org/abs/2509.10972)
*Ron Sun*

Main category: cs.AI

TL;DR: 将大型语言模型（LLM）整合到认知架构中以提高其计算能力和心理真实性。


<details>
  <summary>Details</summary>
Motivation: 现有的认知架构计算能力有限，LLM具有更高的计算能力，因此将两者结合以应对现实世界复杂性和心理真实性。

Method: 将Clarion认知架构与LLM相结合，利用Clarion的隐式-显式二分法实现无缝集成。

Result: 结合了LLM的计算能力和Clarion的心理精确性。

Conclusion: 将LLM整合到认知架构中是提高其能力的重要途径，Clarion和LLM的结合是一个成功的案例。

Abstract: Computational cognitive architectures are broadly scoped models of the human
mind that combine different psychological functionalities (as well as often
different computational methods for these different functionalities) into one
unified framework. They structure them in a psychologically plausible and
validated way. However, such models thus far have only limited computational
capabilities, mostly limited by the computational tools and techniques that
were adopted. More recently, LLMs have proved to be more capable
computationally than any other tools. Thus, in order to deal with both
real-world complexity and psychological realism at the same time, incorporating
LLMs into cognitive architectures naturally becomes an important task. In the
present article, a synergistic combination of the Clarion cognitive
architecture and LLMs is discussed as a case study. The implicit-explicit
dichotomy that is fundamental to Clarion is leveraged for a seamless
integration of Clarion and LLMs. As a result, computational power of LLMs is
combined with psychological nicety of Clarion.

</details>


### [13] [Rethinking Human Preference Evaluation of LLM Rationales](https://arxiv.org/abs/2509.11026)
*Ziang Li,Manasi Ganti,Zixian Ma,Helena Vasconcelos,Qijia He,Ranjay Krishna*

Main category: cs.AI

TL;DR: 本文重新思考了大型语言模型(LLM)生成理由的评价方法，通过识别关键属性、自动指标、LLM判断和人工标注评估属性，并使用SHAP分析人类偏好，最终用属性特定ELO评分重新评估模型生成的理由，发现细粒度属性评估能更好地刻画理由质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成理由的评价方法存在局限性，难以深入理解不同理由的优劣。

Method: 识别关键理由属性，使用自动指标、LLM判断和人工标注进行评估，利用SHAP分析人类偏好，并使用属性特定ELO评分重新评估模型。

Result: 细粒度属性评估能更好地刻画理由质量，为未来研究提供更具解释性和可靠性的评估方法。

Conclusion: 基于属性的评估优于二元比较，为LLM生成理由的评价提供了更细致的分析和改进方向。

Abstract: Large language models (LLMs) often generate natural language rationales --
free-form explanations that help improve performance on complex reasoning tasks
and enhance interpretability for human users. However, evaluating these
rationales remains challenging. While recent work has relied on binary
preference judgments from humans or LLM judges, such evaluations are often
opaque and coarse-grained, offering limited insight into what makes one
rationale better than another. In this work, we rethink preference evaluation
for LLM-generated rationales by asking: (1) What attributes define good
rationales? (2) Can human preferences be explained by these attributes? (3) Can
attribute-based evaluation overcome the limitations of binary comparisons? We
identify a set of key rationale attributes from prior literature and assess
them using automatic metrics, LLM judgments, and human annotations. We then
analyze two standard human preference datasets MT Bench and Chatbot Arena using
SHAP to identify which attributes best explain human preference outcomes.
Finally, we re-evaluate model-generated rationales using attribute-specific ELO
scores, revealing more nuanced model comparisons and insights. Our findings
suggest that fine-grained attribute evaluations can better characterize
rationale quality and guide future research toward more interpretable and
reliable evaluation practices.

</details>


### [14] [Free-MAD: Consensus-Free Multi-Agent Debate](https://arxiv.org/abs/2509.11035)
*Yu Cui,Hang Fu,Haibin Zhang,Licheng Wang,Cong Zuo*

Main category: cs.AI

TL;DR: Free-MAD 是一种改进多智能体辩论 (MAD) 的框架，它通过基于评分的决策机制和抗从众机制，提高了大型语言模型的推理能力，并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的 MAD 方法存在多轮通信、错误传播和投票不公平等问题。

Method: Free-MAD 框架通过基于评分的决策机制评估整个辩论过程，并引入抗从众机制来减少多数意见的影响，从而实现单轮辩论。

Result: 实验结果表明，Free-MAD 在八个基准数据集上显著提高了推理性能，降低了计算成本，并提高了鲁棒性。

Conclusion: Free-MAD 是一种有效且高效的多智能体辩论框架，解决了现有方法的局限性。

Abstract: Multi-agent debate (MAD) is an emerging approach to improving the reasoning
capabilities of large language models (LLMs). Existing MAD methods rely on
multiple rounds of interaction among agents to reach consensus, and the final
output is selected by majority voting in the last round. However, this
consensus-based design faces several limitations. First, multiple rounds of
communication increases token overhead and limits scalability. Second, due to
the inherent conformity of LLMs, agents that initially produce correct
responses may be influenced by incorrect ones during the debate process,
causing error propagation. Third, majority voting introduces randomness and
unfairness in the decision-making phase, and can degrade the reasoning
performance.
  To address these issues, we propose \textsc{Free-MAD}, a novel MAD framework
that eliminates the need for consensus among agents. \textsc{Free-MAD}
introduces a novel score-based decision mechanism that evaluates the entire
debate trajectory rather than relying on the last round only. This mechanism
tracks how each agent's reasoning evolves, enabling more accurate and fair
outcomes. In addition, \textsc{Free-MAD} reconstructs the debate phase by
introducing anti-conformity, a mechanism that enables agents to mitigate
excessive influence from the majority. Experiments on eight benchmark datasets
demonstrate that \textsc{Free-MAD} significantly improves reasoning performance
while requiring only a single-round debate and thus reducing token costs. We
also show that compared to existing MAD approaches, \textsc{Free-MAD} exhibits
improved robustness in real-world attack scenarios.

</details>


### [15] [Agentic Lybic: Multi-Agent Execution System with Tiered Reasoning and Orchestration](https://arxiv.org/abs/2509.11067)
*Liangxuan Guo,Bin Zhu,Qingqian Tao,Kangning Liu,Xun Zhao,Xianzhe Qin,Jin Gao,Guangfu Hao*

Main category: cs.AI

TL;DR: Agentic Lybic，一个基于有限状态机的多Agent系统，通过动态协调和质量控制显著提高了桌面自动化任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有桌面自动化Agent难以处理复杂的多步任务，协调性差，质量控制不足。

Method: 构建了一个包含控制器、管理器、三个工作单元（技术员、操作员、分析员）和评估器的多Agent系统，系统整体运行为有限状态机，实现动态任务分配和执行策略选择。

Result: 在OSWorld基准测试中，Agentic Lybic取得了57.07%的成功率，优于现有方法。

Conclusion: 基于有限状态机的多Agent协同和持续质量控制能够显著提高复杂计算环境下桌面自动化的可靠性。

Abstract: Autonomous agents for desktop automation struggle with complex multi-step
tasks due to poor coordination and inadequate quality control. We introduce
\textsc{Agentic Lybic}, a novel multi-agent system where the entire
architecture operates as a finite-state machine (FSM). This core innovation
enables dynamic orchestration. Our system comprises four components: a
Controller, a Manager, three Workers (Technician for code-based operations,
Operator for GUI interactions, and Analyst for decision support), and an
Evaluator. The critical mechanism is the FSM-based routing between these
components, which provides flexibility and generalization by dynamically
selecting the optimal execution strategy for each subtask. This principled
orchestration, combined with robust quality gating, enables adaptive replanning
and error recovery. Evaluated officially on the OSWorld benchmark,
\textsc{Agentic Lybic} achieves a state-of-the-art 57.07\% success rate in 50
steps, substantially outperforming existing methods. Results demonstrate that
principled multi-agent orchestration with continuous quality control provides
superior reliability for generalized desktop automation in complex computing
environments.

</details>


### [16] [Tractable Asymmetric Verification for Large Language Models via Deterministic Replicability](https://arxiv.org/abs/2509.11068)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: 本文提出一种可行的框架，用于验证大型语言模型 (LLM) 输出的真实性，该框架成本低且效率高。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体LLM系统中验证输出真实性的挑战。

Method: 基于确定性可复制性原则，通过多个验证器概率审计LLM输出的小型随机片段，有效分配验证工作负载。

Result: 模拟表明，定向验证速度比完全再生快12倍以上，且可调参数用于调整检测概率。

Conclusion: 该框架为负责任的AI奠定了基础，并为未来更复杂异构多智能体系统研究奠定了基石。

Abstract: The landscape of Large Language Models (LLMs) shifts rapidly towards dynamic,
multi-agent systems. This introduces a fundamental challenge in establishing
computational trust, specifically how one agent can verify that another's
output was genuinely produced by a claimed LLM, and not falsified or generated
by a cheaper or inferior model. To address this challenge, this paper proposes
a verification framework that achieves tractable asymmetric effort, where the
cost to verify a computation is substantially lower than the cost to perform
it. Our approach is built upon the principle of deterministic replicability, a
property inherent to autoregressive models that strictly necessitates a
computationally homogeneous environment where all agents operate on identical
hardware and software stacks. Within this defined context, our framework
enables multiple validators to probabilistically audit small, random segments
of an LLM's output and it distributes the verification workload effectively.
The simulations demonstrated that targeted verification can be over 12 times
faster than full regeneration, with tunable parameters to adjust the detection
probability. By establishing a tractable mechanism for auditable LLM systems,
our work offers a foundational layer for responsible AI and serves as a
cornerstone for future research into the more complex, heterogeneous
multi-agent systems.

</details>


### [17] [Patient-Zero: A Unified Framework for Real-Record-Free Patient Agent Generation](https://arxiv.org/abs/2509.11078)
*Yunghwei Lai,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: Patient-Zero框架通过多步骤生成架构和动态更新机制，无需真实病历即可生成逼真、一致且多样的虚拟病人数据，并在MedQA数据集上取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有合成医疗数据方法存在数据隐私、准确性和多样性等问题，且缺乏真实病人交互能力。

Method: 提出Patient-Zero框架，采用医学知识注入的多步骤生成架构和动态更新机制，生成虚拟病人数据并优化其交互能力。

Result: 生成的虚拟病人数据在准确性、多样性和一致性方面表现良好，用于训练后改进现有模型在MedQA数据集上的表现。

Conclusion: Patient-Zero框架为解决医疗数据稀缺问题提供了一种有效方案，生成的虚拟病人数据可用于训练和评估医疗AI模型。

Abstract: Synthetic data generation using large language models (LLMs) has emerged as a
promising solution across various domains, particularly in medical field, to
mitigate data collection challenges. However, existing studies mainly utilize
LLMs to rewrite and complete existing medical records, where the limitations in
data privacy, accuracy, and diversity sill exist, and additionally lack the
ability to interact like real patients. To address these issues, we propose a
realistic patient generation framework, Patient-Zero, which requires no real
medical records. Patient-Zero first introduces a medically-aligned multi-step
generation architecture, which builds comprehensive patient records through
hierarchical medical knowledge injection without real medical records. Then, to
optimize the virtual patient's interaction abilities with humans, Patient-Zero
designs a dynamic updating mechanism to improve the consistency and
conversational performance. Our framework enables the generation of
contextually diverse patient records while maintaining strict medical
coherence, supported by adaptive dialogue strategies and real-time clinical
plausibility verification. Experimental results demonstrate that our model
achieves good performance in accuracy, diversity, and consistency. After
training with our generated virtual patients, existing models show significant
improvements on the MedQA dataset.

</details>
