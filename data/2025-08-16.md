<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions](https://arxiv.org/abs/2508.10047)
*Ziyang Xiao,Jingrong Xie,Lilin Xu,Shisi Guan,Jingyan Zhu,Xiongwei Han,Xiaojin Fu,WingYin Yu,Han Wu,Wei Shi,Qingcan Kang,Jiahui Duan,Tao Zhong,Mingxuan Yuan,Jia Zeng,Yuan Wang,Gang Chen,Dongxiang Zhang*

Main category: cs.AI

TL;DR: Survey on using LLMs for automated optimization modeling, highlighting dataset issues and providing solutions.


<details>
  <summary>Details</summary>
Motivation: To automate the process of mathematical modeling using LLMs, addressing the need for OR professionals and improving the quality of benchmark datasets.

Method: Survey and analysis of existing methods, dataset cleaning and creation of a new leaderboard.

Result: A comprehensive review, cleaned benchmark datasets, a new leaderboard with fair evaluation, and an online resource portal.

Conclusion: This survey reviews advancements in using LLMs for automated mathematical modeling, identifies dataset issues, and proposes solutions including a cleaned dataset and online portal.

Abstract: By virtue of its great utility in solving real-world problems, optimization
modeling has been widely employed for optimal decision-making across various
sectors, but it requires substantial expertise from operations research
professionals. With the advent of large language models (LLMs), new
opportunities have emerged to automate the procedure of mathematical modeling.
This survey presents a comprehensive and timely review of recent advancements
that cover the entire technical stack, including data synthesis and fine-tuning
for the base model, inference frameworks, benchmark datasets, and performance
evaluation. In addition, we conducted an in-depth analysis on the quality of
benchmark datasets, which was found to have a surprisingly high error rate. We
cleaned the datasets and constructed a new leaderboard with fair performance
evaluation in terms of base LLM model and datasets. We also build an online
portal that integrates resources of cleaned datasets, code and paper repository
to benefit the community. Finally, we identify limitations in current
methodologies and outline future research opportunities.

</details>


### [2] [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
*Sattvik Sahai,Prasoon Goyal,Michael Johnston,Anna Gottardi,Yao Lu,Lucy Hu,Luke Dai,Shaohua Liu,Samyuth Sagi,Hangjie Shi,Desheng Zhang,Lavina Vaz,Leslie Ball,Maureen Murray,Rahul Gupta,Shankar Ananthakrishna*

Main category: cs.AI

TL;DR: Nova AI挑战赛推动AI软件开发安全性的提升，各团队在安全对齐等方面取得突破。


<details>
  <summary>Details</summary>
Motivation: 应对AI软件开发安全挑战。

Method: 通过对抗性锦标赛评估自动化红队和安全对齐方法，并提供高质量的标注数据。

Result: 开发了先进技术，包括基于推理的安全对齐、强大的模型防护、多轮越狱和高效的大型语言模型探测。

Conclusion: 亚马逊Nova AI挑战赛促进了AI软件开发安全性的进步，大学团队和亚马逊团队在安全对齐、模型防护和大型语言模型探测等方面取得了显著进展。

Abstract: AI systems for software development are rapidly gaining prominence, yet
significant challenges remain in ensuring their safety. To address this, Amazon
launched the Trusted AI track of the Amazon Nova AI Challenge, a global
competition among 10 university teams to drive advances in secure AI. In the
challenge, five teams focus on developing automated red teaming bots, while the
other five create safe AI assistants. This challenge provides teams with a
unique platform to evaluate automated red-teaming and safety alignment methods
through head-to-head adversarial tournaments where red teams have multi-turn
conversations with the competing AI coding assistants to test their safety
alignment. Along with this, the challenge provides teams with a feed of high
quality annotated data to fuel iterative improvement. Throughout the challenge,
teams developed state-of-the-art techniques, introducing novel approaches in
reasoning-based safety alignment, robust model guardrails, multi-turn
jail-breaking, and efficient probing of large language models (LLMs). To
support these efforts, the Amazon Nova AI Challenge team made substantial
scientific and engineering investments, including building a custom baseline
coding specialist model for the challenge from scratch, developing a tournament
orchestration service, and creating an evaluation harness. This paper outlines
the advancements made by university teams and the Amazon Nova AI Challenge team
in addressing the safety challenges of AI for software development,
highlighting this collaborative effort to raise the bar for AI safety.

</details>


### [3] [MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection](https://arxiv.org/abs/2508.10143)
*Alexandru-Andrei Avram,Adrian Groza,Alexandru Lecu*

Main category: cs.AI

TL;DR: 一个多智能体系统通过关系抽取检测新闻标题和短文本片段中的虚假信息，准确率高达95.3%。


<details>
  <summary>Details</summary>
Motivation: 解决数字平台上虚假信息的大量传播对信息完整性造成的重大挑战。

Method: 该系统结合了四个智能体：机器学习智能体（逻辑回归）、维基百科知识检查智能体（依赖命名实体识别）、连贯性检测智能体（使用LLM提示工程）和网络抓取数据分析器。系统通过模型上下文协议（MCP）协调，提供共享上下文和跨组件的实时学习。

Result: 多智能体集成系统准确率达95.3%，F1分数为0.964，加权聚合方法优于算法阈值优化。

Conclusion: 多智能体系统在新闻文章中检测虚假信息的准确率达到95.3%，F1分数为0.964，显著优于单个智能体和传统方法。

Abstract: The large spread of disinformation across digital platforms creates
significant challenges to information integrity. This paper presents a
multi-agent system that uses relation extraction to detect disinformation in
news articles, focusing on titles and short text snippets. The proposed Agentic
AI system combines four agents: (i) a machine learning agent (logistic
regression), (ii) a Wikipedia knowledge check agent (which relies on named
entity recognition), (iii) a coherence detection agent (using LLM prompt
engineering), and (iv) a web-scraped data analyzer that extracts relational
triplets for fact checking. The system is orchestrated via the Model Context
Protocol (MCP), offering shared context and live learning across components.
Results demonstrate that the multi-agent ensemble achieves 95.3% accuracy with
an F1 score of 0.964, significantly outperforming individual agents and
traditional approaches. The weighted aggregation method, mathematically derived
from individual agent misclassification rates, proves superior to algorithmic
threshold optimization. The modular architecture makes the system easily
scalable, while also maintaining details of the decision processes.

</details>


### [4] [Agentic AI Frameworks: Architectures, Protocols, and Design Challenges](https://arxiv.org/abs/2508.10146)
*Hana Derouiche,Zaki Brahmi,Haithem Mazeni*

Main category: cs.AI

TL;DR: 本文综述了智能体AI框架，分析了其架构、通信和挑战，并提出了未来研究方向


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的出现带来了智能体AI的变革，本文旨在对智能体AI框架进行全面综述，并提出未来的研究方向。

Method: 本文对现有智能体AI框架进行了系统的回顾和比较分析，并对几种代理通信协议进行了深入分析。

Result: 本文建立了智能体AI系统的基础分类法，并提出了增强可扩展性、鲁棒性和互操作性的未来研究方向。

Conclusion: 本文对领先的智能体AI框架（包括CrewAI、LangGraph、AutoGen、Semantic Kernel、Agno、Google ADK和MetaGPT）进行了系统的回顾和比较分析，评估了它们的架构原则、通信机制、内存管理、安全防护和与面向服务的计算范例的一致性，并确定了该领域的关键限制、新兴趋势和开放性挑战。

Abstract: The emergence of Large Language Models (LLMs) has ushered in a transformative
paradigm in artificial intelligence, Agentic AI, where intelligent agents
exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent
coordination. This paper provides a systematic review and comparative analysis
of leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen,
Semantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural
principles, communication mechanisms, memory management, safety guardrails, and
alignment with service-oriented computing paradigms. Furthermore, we identify
key limitations, emerging trends, and open challenges in the field. To address
the issue of agent communication, we conduct an in-depth analysis of protocols
such as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network
Protocol (ANP), and Agora. Our findings not only establish a foundational
taxonomy for Agentic AI systems but also propose future research directions to
enhance scalability, robustness, and interoperability. This work serves as a
comprehensive reference for researchers and practitioners working to advance
the next generation of autonomous AI systems.

</details>


### [5] [Improving and Evaluating Open Deep Research Agents](https://arxiv.org/abs/2508.10152)
*Doaa Allabadi,Kyle Bradbury,Jordan M. Malof*

Main category: cs.AI

TL;DR: 通过改进开源深度研究代理ODR，使其在BrowseComp-Small基准测试中取得了10%的成功率。


<details>
  <summary>Details</summary>
Motivation: 评估开源DRA系统的能力，并改进其性能。

Method: 对现有的开源DRA系统ODR进行了改进，并与闭源系统进行了比较。

Result: ODR+模型在BC-Small基准测试中取得了10%的成功率，所有三个系统在测试集上的准确率均为0%。

Conclusion: ODR+模型在BC-Small基准测试中取得了10%的成功率，优于其他闭源和开源系统。

Abstract: We focus here on Deep Research Agents (DRAs), which are systems that can take
a natural language prompt from a user, and then autonomously search for, and
utilize, internet-based content to address the prompt. Recent DRAs have
demonstrated impressive capabilities on public benchmarks however, recent
research largely involves proprietary closed-source systems. At the time of
this work, we only found one open-source DRA, termed Open Deep Research (ODR).
In this work we adapt the challenging recent BrowseComp benchmark to compare
ODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small),
comprising a subset of BrowseComp, as a more computationally-tractable DRA
benchmark for academic labs. We benchmark ODR and two other proprietary systems
on BC-Small: one system from Anthropic and one system from Google. We find that
all three systems achieve 0% accuracy on the test set of 60 questions. We
introduce three strategic improvements to ODR, resulting in the ODR+ model,
which achieves a state-of-the-art 10% success rate on BC-Small among both
closed-source and open-source systems. We report ablation studies indicating
that all three of our improvements contributed to the success of ODR+.

</details>


### [6] [Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization](https://arxiv.org/abs/2508.10164)
*Bin Hong,Jiayu Liu,Zhenya Huang,Kai Zhang,Mengdi Zhang*

Main category: cs.AI

TL;DR: 通过长度控制偏好优化，有效缩短大型推理模型的输出长度，提升推理效率，且性能无明显下降。


<details>
  <summary>Details</summary>
Motivation: 现有的高效推理方法往往会影响推理质量或需要大量的资源，因此该论文旨在研究减少LRM生成长度的高效方法，平衡推理有效性和效率。

Method: 分析生成路径分布，通过难度估计过滤生成的轨迹，并基于Bradley-Terry损失框架分析不同偏好优化方法的目标收敛行为，最终提出LCPO方法直接平衡与NLL损失相关的隐式奖励。

Result: LCPO方法能够有效地学习长度偏好，并在多个基准测试中显著减少平均输出长度，同时保持推理性能。

Conclusion: 该论文提出了一种名为长度控制偏好优化（LCPO）的方法，显著减少了大型推理模型（LRM）的输出长度（超过50%），同时保持了推理性能。

Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated strong
performance on complex tasks through long Chain-of-Thought (CoT) reasoning.
However, their lengthy outputs increase computational costs and may lead to
overthinking, raising challenges in balancing reasoning effectiveness and
efficiency. Current methods for efficient reasoning often compromise reasoning
quality or require extensive resources. This paper investigates efficient
methods to reduce the generation length of LRMs. We analyze generation path
distributions and filter generated trajectories through difficulty estimation.
Subsequently, we analyze the convergence behaviors of the objectives of various
preference optimization methods under a Bradley-Terry loss based framework.
Based on the analysis, we propose Length Controlled Preference Optimization
(LCPO) that directly balances the implicit reward related to NLL loss. LCPO can
effectively learn length preference with limited data and training. Extensive
experiments demonstrate that our approach significantly reduces the average
output length by over 50\% across multiple benchmarks while maintaining the
reasoning performance. Our work highlights the potential for computationally
efficient approaches in guiding LRMs toward efficient reasoning.

</details>


### [7] [KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems](https://arxiv.org/abs/2508.10177)
*Stepan Kulibaba,Artem Dzhalilov,Roman Pakhomov,Oleg Svidchenko,Alexander Gasnikov,Aleksei Shpilman*

Main category: cs.AI

TL;DR: KompeteAI: 新的AutoML框架，速度更快，性能更好。


<details>
  <summary>Details</summary>
Motivation: 克服现有LLM-based AutoML系统在探索策略（单次方法缺乏多样性，MCTS方法无法重组强局部解）和执行瓶颈（冗长的代码验证周期）方面的限制。

Method: 动态解决方案空间探索、顶尖候选方案合并、检索增强生成（RAG）、预测评分模型和加速调试方法。

Result: KompeteAI在主要AutoML基准MLE-Bench上平均超过领先方法（例如，RD-agent，AIDE和Ml-Master）3％，并在新的Kompete-bench基准测试中也取得了最先进的结果，pipeline评估速度提升6.9倍。

Conclusion: KompeteAI，一个新型AutoML框架，通过动态解决方案空间探索、顶尖候选方案的合并和检索增强生成（RAG）技术，克服了现有LLM-based AutoML系统在探索策略和执行瓶颈方面的限制，并在MLE-Bench和Kompete-bench基准测试中取得了最先进的结果。

Abstract: Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive
capabilities but face significant limitations such as constrained exploration
strategies and a severe execution bottleneck. Exploration is hindered by
one-shot methods lacking diversity and Monte Carlo Tree Search (MCTS)
approaches that fail to recombine strong partial solutions. The execution
bottleneck arises from lengthy code validation cycles that stifle iterative
refinement. To overcome these challenges, we introduce KompeteAI, a novel
AutoML framework with dynamic solution space exploration. Unlike previous MCTS
methods that treat ideas in isolation, KompeteAI introduces a merging stage
that composes top candidates. We further expand the hypothesis space by
integrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle
notebooks and arXiv papers to incorporate real-world strategies. KompeteAI also
addresses the execution bottleneck via a predictive scoring model and an
accelerated debugging method, assessing solution potential using early stage
metrics to avoid costly full-code execution. This approach accelerates pipeline
evaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent,
AIDE, and Ml-Master) by an average of 3\% on the primary AutoML benchmark,
MLE-Bench. Additionally, we propose Kompete-bench to address limitations in
MLE-Bench, where KompeteAI also achieves state-of-the-art results

</details>


### [8] [Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence](https://arxiv.org/abs/2508.10241)
*Mark Zilberman*

Main category: cs.AI

TL;DR: 事件熵势概念能增强AI中的不确定性量化、决策和可解释性，并应用于多个领域。


<details>
  <summary>Details</summary>
Motivation: 增强人工智能中的不确定性量化、决策和可解释性。

Method: 将物理学中事件熵势的概念应用于AI，引入了一种事件中心的度量方法，并对原始定义和AI调整后的定义进行了形式化。

Result: 提出了一个理论上可靠、可解释且通用的方法来管理AI中的不确定性，该方法结合了热力学、信息论和机器学习的原理。

Conclusion: 该工作展示了事件熵势概念如何增强人工智能中的不确定性量化、决策和可解释性，并将其应用于策略评估、内在奖励设计、可解释AI和异常检测。

Abstract: This work demonstrates how the concept of the entropic potential of events --
a parameter quantifying the influence of discrete events on the expected future
entropy of a system -- can enhance uncertainty quantification, decision-making,
and interpretability in artificial intelligence (AI). Building on its original
formulation in physics, the framework is adapted for AI by introducing an
event-centric measure that captures how actions, observations, or other
discrete occurrences impact uncertainty at future time horizons. Both the
original and AI-adjusted definitions of entropic potential are formalized, with
the latter emphasizing conditional expectations to account for counterfactual
scenarios. Applications are explored in policy evaluation, intrinsic reward
design, explainable AI, and anomaly detection, highlighting the metric's
potential to unify and strengthen uncertainty modeling in intelligent systems.
Conceptual examples illustrate its use in reinforcement learning, Bayesian
inference, and anomaly detection, while practical considerations for
computation in complex AI models are discussed. The entropic potential
framework offers a theoretically grounded, interpretable, and versatile
approach to managing uncertainty in AI, bridging principles from
thermodynamics, information theory, and machine learning.

</details>


### [9] [Why Cannot Large Language Models Ever Make True Correct Reasoning?](https://arxiv.org/abs/2508.10265)
*Jingde Cheng*

Main category: cs.AI

TL;DR: LLM无法真正理解和推理。


<details>
  <summary>Details</summary>
Motivation: 纠正对LLM能力的误解，澄清其并非真正具有理解和推理能力。

Method: 分析LLM的工作原理，指出其内在局限性。

Result: 论证了LLM由于其工作原理的本质限制，无法拥有真正的正确推理能力。

Conclusion: 大型语言模型（LLM）不可能拥有真正的理解力和推理能力，其所谓的“理解能力”和“推理能力”只是人们的错觉。

Abstract: Recently, with the application progress of AIGC tools based on large language
models (LLMs), led by ChatGPT, many AI experts and more non-professionals are
trumpeting the "understanding ability" and "reasoning ability" of the LLMs. The
present author considers that the so-called "understanding ability" and
"reasoning ability" of LLMs are just illusions of those people who with vague
concepts. In fact, the LLMs can never have the true understanding ability and
true reasoning ability. This paper intents to explain that, because the
essential limitations of their working principle, the LLMs can never have the
ability of true correct reasoning.

</details>


### [10] [Promoting Efficient Reasoning with Verifiable Stepwise Reward](https://arxiv.org/abs/2508.10293)
*Chuhuai Yue,Chengqi Dong,Yinan Gao,Hang He,Jiajun Chai,Guojun Yin,Wei Lin*

Main category: cs.AI

TL;DR: 提出一种新的奖励机制VSRM有效缓解大型推理模型的过度思考问题，提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的高效推理方法通常需要准确的任务评估来预设token预算或选择推理模式，这限制了它们的灵活性和可靠性。

Method: 提出了一种基于规则的可验证逐步奖励机制（VSRM），并将其与PPO和Reinforce++结合使用，在标准数学推理基准上进行实验。

Result: 实验结果表明，该方法在保持原始推理性能的同时，实现了显著的输出长度减少，在效率和准确性之间取得了最佳平衡。

Conclusion: 这项工作提出了一种基于规则的可验证逐步奖励机制（VSRM），通过奖励推理轨迹中中间状态的性能来解决大型推理模型（LRMs）的过度思考问题，从而在保持推理性能的同时显著减少输出长度。

Abstract: Large reasoning models (LRMs) have recently achieved significant progress in
complex reasoning tasks, aided by reinforcement learning with verifiable
rewards. However, LRMs often suffer from overthinking, expending excessive
computation on simple problems and reducing efficiency. Existing efficient
reasoning methods typically require accurate task assessment to preset token
budgets or select reasoning modes, which limits their flexibility and
reliability. In this work, we revisit the essence of overthinking and identify
that encouraging effective steps while penalizing ineffective ones is key to
its solution. To this end, we propose a novel rule-based verifiable stepwise
reward mechanism (VSRM), which assigns rewards based on the performance of
intermediate states in the reasoning trajectory. This approach is intuitive and
naturally fits the step-by-step nature of reasoning tasks. We conduct extensive
experiments on standard mathematical reasoning benchmarks, including AIME24 and
AIME25, by integrating VSRM with PPO and Reinforce++. Results show that our
method achieves substantial output length reduction while maintaining original
reasoning performance, striking an optimal balance between efficiency and
accuracy. Further analysis of overthinking frequency and pass@k score before
and after training demonstrates that our approach in deed effectively
suppresses ineffective steps and encourages effective reasoning, fundamentally
alleviating the overthinking problem. All code will be released upon
acceptance.

</details>


### [11] [A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering](https://arxiv.org/abs/2508.10337)
*Chenliang Zhang,Lin Wang,Yuanyuan Lu,Yusheng Qi,Kexin Wang,Peixu Hou,Wenshi Chen*

Main category: cs.AI

TL;DR: Dianping团队的解决方案在META CRAG-MM挑战赛中取得了优异成绩，特别是任务一中以显著优势获得第一名，这归功于其基于视觉大型语言模型并结合课程学习和强化学习的有效方法。


<details>
  <summary>Details</summary>
Motivation: META CRAG-MM挑战赛要求构建一个能够进行多模态多轮问答的综合检索增强生成系统，该论文旨在解决这一挑战。

Method: 该方案基于视觉大型语言模型，并结合了监督微调、知识蒸馏和课程学习等技术，利用GPT-4.1进行知识蒸馏，并针对不同任务分别使用不同的策略，例如在任务一中使用课程学习指导强化学习，在任务二和任务三中额外利用网络搜索API。

Result: 在挑战赛的三个任务中，该方案在任务一中获得第一名（领先第二名52.38%），在任务三中获得第三名。

Conclusion: 该论文描述了Dianping-Trust-Safety团队在META CRAG-MM挑战赛中的解决方案，该方案在三个任务中取得了优异的成绩，特别是第一个任务中以显著优势获得第一名，证明了课程学习与强化学习相结合的有效性。

Abstract: This paper describes the solutions of the Dianping-Trust-Safety team for the
META CRAG-MM challenge. The challenge requires building a comprehensive
retrieval-augmented generation system capable for multi-modal multi-turn
question answering. The competition consists of three tasks: (1) answering
questions using structured data retrieved from an image-based mock knowledge
graph, (2) synthesizing information from both knowledge graphs and web search
results, and (3) handling multi-turn conversations that require context
understanding and information aggregation from multiple sources. For Task 1,
our solution is based on the vision large language model, enhanced by
supervised fine-tuning with knowledge distilled from GPT-4.1. We further
applied curriculum learning strategies to guide reinforcement learning,
resulting in improved answer accuracy and reduced hallucination. For Task 2 and
Task 3, we additionally leveraged web search APIs to incorporate external
knowledge, enabling the system to better handle complex queries and multi-turn
conversations. Our approach achieved 1st place in Task 1 with a significant
lead of 52.38\%, and 3rd place in Task 3, demonstrating the effectiveness of
the integration of curriculum learning with reinforcement learning in our
training pipeline.

</details>


### [12] [Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach](https://arxiv.org/abs/2508.10340)
*Chak Lam Shek,Guangyao Shi,Pratap Tokekar*

Main category: cs.AI

TL;DR: 两种新的 KL 阈值分配方法显著提升了异构多智能体强化学习的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决现有 HATRPO 方法中为所有智能体分配相同 KL 阈值导致训练缓慢和局部最优的问题。

Method: 提出 HATRPO-W (基于 Karush-Kuhn-Tucker 的方法) 和 HATRPO-G (贪婪算法) 两种 KL 散度阈值分配方法，用于优化多智能体强化学习中异构智能体的策略更新。

Result: HATRPO-W 和 HATRPO-G 都比原 HATRPO 方法提升了 22.5% 以上的最终奖励，HATRPO-W 具有更稳定的学习动态。

Conclusion: HATRPO-W 和 HATRPO-G 这两种方法显著提高了 HATRPO 的性能，实现了更快的收敛速度和更高的最终奖励。HATRPO-W展现出更稳定的学习动态，方差更低。

Abstract: Multi-agent reinforcement learning (MARL) requires coordinated and stable
policy updates among interacting agents. Heterogeneous-Agent Trust Region
Policy Optimization (HATRPO) enforces per-agent trust region constraints using
Kullback-Leibler (KL) divergence to stabilize training. However, assigning each
agent the same KL threshold can lead to slow and locally optimal updates,
especially in heterogeneous settings. To address this limitation, we propose
two approaches for allocating the KL divergence threshold across agents:
HATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes
threshold assignment under global KL constraints, and HATRPO-G, a greedy
algorithm that prioritizes agents based on improvement-to-divergence ratio. By
connecting sequential policy optimization with constrained threshold
scheduling, our approach enables more flexible and effective learning in
heterogeneous-agent settings. Experimental results demonstrate that our methods
significantly boost the performance of HATRPO, achieving faster convergence and
higher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and
HATRPO-G achieve comparable improvements in final performance, each exceeding
22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as
reflected by its lower variance.

</details>


### [13] [What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles](https://arxiv.org/abs/2508.10358)
*Mengtao Zhou,Sifan Wu,Huan Zhang,Qi Sima,Bang Liu*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在富有想象力的推理方面能力有限，并提出一个新的基准测试和评估框架来评估其性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试无法捕捉到这种推理过程的动态和探索性本质。

Method: 提出一个基于经典“乌龟汤”游戏的综合研究框架，包括一个大型双语交互式基准测试、一个新的代理和一个多维评估协议。

Result: 实验表明，领先的LLM存在明显的性能差距，并揭示了常见的失败模式。

Conclusion: 大型语言模型(LLM)在信息稀疏环境下的富有想象力的推理能力有限，该研究提出了一个基于“乌龟汤”游戏的综合研究框架，包括基准、代理和评估协议，以评估LLM在此环境下的表现。

Abstract: We investigate the capacity of Large Language Models (LLMs) for imaginative
reasoning--the proactive construction, testing, and revision of hypotheses in
information-sparse environments. Existing benchmarks, often static or focused
on social deduction, fail to capture the dynamic, exploratory nature of this
reasoning process. To address this gap, we introduce a comprehensive research
framework based on the classic "Turtle Soup" game, integrating a benchmark, an
agent, and an evaluation protocol. We present TurtleSoup-Bench, the first
large-scale, bilingual, interactive benchmark for imaginative reasoning,
comprising 800 turtle soup puzzles sourced from both the Internet and expert
authors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'
performance in this setting. To evaluate reasoning quality, we develop a
multi-dimensional protocol measuring logical consistency, detail completion,
and conclusion alignment. Experiments with leading LLMs reveal clear capability
limits, common failure patterns, and a significant performance gap compared to
humans. Our work offers new insights into LLMs' imaginative reasoning and
establishes a foundation for future research on exploratory agent behavior.

</details>


### [14] [LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval](https://arxiv.org/abs/2508.10391)
*Yaoze Zhang,Rong Wu,Pinlong Cai,Xiaoman Wang,Guohang Yan,Song Mao,Ding Wang,Botian Shi*

Main category: cs.AI

TL;DR: LeanRAG通过语义聚合和结构化检索，有效解决了现有RAG方法中知识图谱检索效率低下的问题，显著提升了问答系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于知识图谱的RAG方法存在两个关键挑战：高层概念摘要之间缺乏显式关系，检索过程效率低下。

Method: 提出了一种新的基于知识图谱的RAG框架LeanRAG，该框架结合了知识聚合和检索策略。首先，使用一种新颖的语义聚合算法形成实体集群并构建聚合级别摘要之间的显式关系，创建一个完全可导航的语义网络。然后，采用自下而上的结构化引导检索策略，将查询锚定到最相关的细粒度实体，并系统地遍历图的语义路径以收集简洁且上下文完整的证据集。

Result: 在四个具有挑战性的QA基准测试中，LeanRAG在响应质量方面显著优于现有方法，同时减少了46%的检索冗余。

Conclusion: LeanRAG框架通过语义聚合算法构建完整的语义网络，并采用自下而上的结构化检索策略，显著提高了RAG方法的响应质量并减少了冗余信息检索。

Abstract: Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large
Language Models by leveraging external knowledge, whereas the effectiveness is
often compromised by the retrieval of contextually flawed or incomplete
information. To address this, knowledge graph-based RAG methods have evolved
towards hierarchical structures, organizing knowledge into multi-level
summaries. However, these approaches still suffer from two critical,
unaddressed challenges: high-level conceptual summaries exist as disconnected
``semantic islands'', lacking the explicit relations needed for cross-community
reasoning; and the retrieval process itself remains structurally unaware, often
degenerating into an inefficient flat search that fails to exploit the graph's
rich topology. To overcome these limitations, we introduce LeanRAG, a framework
that features a deeply collaborative design combining knowledge aggregation and
retrieval strategies. LeanRAG first employs a novel semantic aggregation
algorithm that forms entity clusters and constructs new explicit relations
among aggregation-level summaries, creating a fully navigable semantic network.
Then, a bottom-up, structure-guided retrieval strategy anchors queries to the
most relevant fine-grained entities and then systematically traverses the
graph's semantic pathways to gather concise yet contextually comprehensive
evidence sets. The LeanRAG can mitigate the substantial overhead associated
with path retrieval on graphs and minimizes redundant information retrieval.
Extensive experiments on four challenging QA benchmarks with different domains
demonstrate that LeanRAG significantly outperforming existing methods in
response quality while reducing 46\% retrieval redundancy. Code is available
at: https://github.com/RaZzzyz/LeanRAG

</details>


### [15] [HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation](https://arxiv.org/abs/2508.10425)
*Yan Ting Chok,Soyon Park,Seungheun Baek,Hajung Kim,Junhyun Lee,Jaewoo Kang*

Main category: cs.AI

TL;DR: HiRef模型通过结合医疗本体的层次语义和改进的EHR共现模式，提高了药物推荐的鲁棒性和泛化能力，在基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有药物推荐模型在处理缺失或新颖条件下的泛化能力差的问题，该问题源于其对观察到的共现模式的依赖。

Method: HiRef框架结合了医疗本体的层次语义和来自真实世界EHR的细化共现模式。它将本体实体嵌入双曲空间，并引入先验引导的稀疏正则化方案来细化EHR共现图。

Result: HiRef模型在EHR基准测试上取得了强劲的性能，并在模拟未见代码的场景下保持了高精度。消融实验验证了模型的有效性。

Conclusion: HiRef模型在EHR基准测试（MIMIC-III和MIMIC-IV）上取得了优异的性能，并在模拟的未见代码设置下保持了较高的准确性。消融研究和深入分析证实了HiRef模型对未见医疗代码的鲁棒性。

Abstract: Medication recommendation is a crucial task for assisting physicians in
making timely decisions from longitudinal patient medical records. However,
real-world EHR data present significant challenges due to the presence of
rarely observed medical entities and incomplete records that may not fully
capture the clinical ground truth. While data-driven models trained on
longitudinal Electronic Health Records often achieve strong empirical
performance, they struggle to generalize under missing or novel conditions,
largely due to their reliance on observed co-occurrence patterns. To address
these issues, we propose Hierarchical Ontology and Network Refinement for
Robust Medication Recommendation (HiRef), a unified framework that combines two
complementary structures: (i) the hierarchical semantics encoded in curated
medical ontologies, and (ii) refined co-occurrence patterns derived from
real-world EHRs. We embed ontology entities in hyperbolic space, which
naturally captures tree-like relationships and enables knowledge transfer
through shared ancestors, thereby improving generalizability to unseen codes.
To further improve robustness, we introduce a prior-guided sparse
regularization scheme that refines the EHR co-occurrence graph by suppressing
spurious edges while preserving clinically meaningful associations. Our model
achieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and
maintains high accuracy under simulated unseen-code settings. Extensive
experiments with comprehensive ablation studies demonstrate HiRef's resilience
to unseen medical codes, supported by in-depth analyses of the learned
sparsified graph structure and medical code embeddings.

</details>


### [16] [MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance](https://arxiv.org/abs/2508.10429)
*Yi Dong,Yusuke Muraoka,Scott Shi,Yi Zhang*

Main category: cs.AI

TL;DR: 发布了一个大型多模态食品数据集MM-Food-100K，并证明其可用于改进图像营养预测。


<details>
  <summary>Details</summary>
Motivation: 创建高质量的多模态食品数据集，用于推进食品智能研究。

Method: 使用Codatta贡献模型收集数据，结合社区资源和AI辅助质量检查，并对大型视觉语言模型进行微调。

Result: 成功创建并发布了MM-Food-100K数据集，并验证了其在图像营养预测任务中的有效性。

Conclusion: 发布了MM-Food-100K，一个拥有10万样本的多模态食品智能数据集，并通过微调大型视觉语言模型验证了其在图像营养预测方面的效用。

Abstract: We present MM-Food-100K, a public 100,000-sample multimodal food intelligence
dataset with verifiable provenance. It is a curated approximately 10% open
subset of an original 1.2 million, quality-accepted corpus of food images
annotated for a wide range of information (such as dish name, region of
creation). The corpus was collected over six weeks from over 87,000
contributors using the Codatta contribution model, which combines community
sourcing with configurable AI-assisted quality checks; each submission is
linked to a wallet address in a secure off-chain ledger for traceability, with
a full on-chain protocol on the roadmap. We describe the schema, pipeline, and
QA, and validate utility by fine-tuning large vision-language models (ChatGPT
5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning
yields consistent gains over out-of-box baselines across standard metrics; we
report results primarily on the MM-Food-100K subset. We release MM-Food-100K
for publicly free access and retain approximately 90% for potential commercial
access with revenue sharing to contributors.

</details>
