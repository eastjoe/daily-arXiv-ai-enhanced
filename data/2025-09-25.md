<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Indispensable Role of User Simulation in the Pursuit of AGI](https://arxiv.org/abs/2509.19456)
*Krisztian Balog,ChengXiang Zhai*

Main category: cs.AI

TL;DR: "用户模拟是加速 AGI 发展的关键催化剂"


<details>
  <summary>Details</summary>
Motivation: "解决评估复杂交互系统和获取海量交互数据的瓶颈"

Method: "构建模拟人类与 AI 系统交互的计算代理"

Result: "可扩展评估、交互学习数据生成和增强自适应能力"

Conclusion: "用户模拟技术和智能任务代理研究需同步发展"

Abstract: Progress toward Artificial General Intelligence (AGI) faces significant
bottlenecks, particularly in rigorously evaluating complex interactive systems
and acquiring the vast interaction data needed for training adaptive agents.
This paper posits that user simulation -- creating computational agents that
mimic human interaction with AI systems -- is not merely a useful tool, but is
a critical catalyst required to overcome these bottlenecks and accelerate AGI
development. We argue that realistic simulators provide the necessary
environments for scalable evaluation, data generation for interactive learning,
and fostering the adaptive capabilities central to AGI. Therefore, research
into user simulation technology and intelligent task agents are deeply
synergistic and must advance hand-in-hand. This article elaborates on the
critical role of user simulation for AGI, explores the interdisciplinary nature
of building realistic simulators, identifies key challenges including those
posed by large language models, and proposes a future research agenda.

</details>


### [2] [Evaluation-Aware Reinforcement Learning](https://arxiv.org/abs/2509.19464)
*Shripad Vilasrao Deshmukh,Will Schwarzer,Scott Niekum*

Main category: cs.AI

TL;DR: 本文提出了一种评估感知强化学习方法EvA-RL，该方法在训练策略时同时最小化评估误差，从而提高策略评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有策略评估方法存在方差大或偏差大的问题，EvA-RL旨在解决这些问题。

Method: EvA-RL框架，以及一个评估条件状态值预测器与策略的联合学习方法。

Result: 实验证明EvA-RL能显著降低评估误差，同时保持较高的回报。

Conclusion: EvA-RL为强化学习方法提供了一种新的思路，即在训练过程中将可靠的评估作为首要原则。

Abstract: Policy evaluation is often a prerequisite for deploying safety- and
performance-critical systems. Existing evaluation approaches frequently suffer
from high variance due to limited data and long-horizon tasks, or high bias due
to unequal support or inaccurate environmental models. We posit that these
challenges arise, in part, from the standard reinforcement learning (RL)
paradigm of policy learning without explicit consideration of evaluation. As an
alternative, we propose evaluation-aware reinforcement learning (EvA-RL), in
which a policy is trained to maximize expected return while simultaneously
minimizing expected evaluation error under a given value prediction scheme --
in other words, being "easy" to evaluate. We formalize a framework for EvA-RL
and design an instantiation that enables accurate policy evaluation,
conditioned on a small number of rollouts in an assessment environment that can
be different than the deployment environment. However, our theoretical analysis
and empirical results show that there is often a tradeoff between evaluation
accuracy and policy performance when using a fixed value-prediction scheme
within EvA-RL. To mitigate this tradeoff, we extend our approach to co-learn an
assessment-conditioned state-value predictor alongside the policy. Empirical
results across diverse discrete and continuous action domains demonstrate that
EvA-RL can substantially reduce evaluation error while maintaining competitive
returns. This work lays the foundation for a broad new class of RL methods that
treat reliable evaluation as a first-class principle during training.

</details>


### [3] [Estimating the Self-Consistency of LLMs](https://arxiv.org/abs/2509.19489)
*Robert Nowak*

Main category: cs.AI

TL;DR: 本文分析了大型语言模型 (LLM) 自一致性的估计量及其在固定计算预算下的权衡。


<details>
  <summary>Details</summary>
Motivation: LLM系统通常会对大型语言模型重复相同的提示并聚合响应以提高可靠性。

Method: 分析了在固定计算预算下，从任务分布中采样的提示数量m和每个提示的重复LLM调用次数n之间的权衡。

Result: 分析结果表明，m和n大致相等时，效果最佳。

Conclusion: 在固定计算预算下，对提示数量和重复次数进行合理分配，可以有效提高LLM的自一致性。

Abstract: Systems often repeat the same prompt to large language models (LLMs) and
aggregate responses to improve reliability. This short note analyzes an
estimator of the self-consistency of LLMs and the tradeoffs it induces under a
fixed compute budget $B=mn$, where $m$ is the number of prompts sampled from
the task distribution and $n$ is the number of repeated LLM calls per prompt;
the resulting analysis favors a rough split $m,n\propto\sqrt{B}$.

</details>


### [4] [Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning](https://arxiv.org/abs/2509.19517)
*Sai Teja Reddy Adapala*

Main category: cs.AI

TL;DR: 大型语言模型(LLM)在动态环境中的表现不如静态基准测试中那么好，本文提出了一种计算认知负荷的理论，并设计了一个基准测试来评估模型在认知负荷下的性能。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在动态信息环境下性能不佳的原因，以及如何评估高级AI系统的鲁棒性和安全性。

Method: 提出了计算认知负荷的理论，并设计了Interleaved Cognitive Evaluation (ICE)基准测试，对五个指令微调模型进行了评估。

Result: 不同模型的性能差异显著，小型开源架构模型表现脆弱，而Gemini-2.0-Flash-001表现出一定的鲁棒性，但在上下文饱和度增加时性能下降。

Conclusion: 认知负荷是导致推理失败的关键因素之一，动态的认知感知压力测试对于评估高级AI系统的鲁棒性和安全性至关重要。

Abstract: The scaling of Large Language Models (LLMs) has exposed a critical gap
between their performance on static benchmarks and their fragility in dynamic,
information-rich environments. While models excel at isolated tasks, the
computational limits that govern their reasoning under cognitive load remain
poorly understood. In this work, we introduce a formal theory of computational
cognitive load, positing that extraneous, task-irrelevant information (Context
Saturation) and interference from task-switching (Attentional Residue) are key
mechanisms that degrade performance. We designed the Interleaved Cognitive
Evaluation (ICE), a deconfounded benchmark to systematically manipulate these
load factors on challenging multi-hop reasoning tasks. A comprehensive study (N
= 10 replications per item across 200 questions) revealed significant
performance variations across five instruction-tuned models. Smaller
open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2)
exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all
conditions, including clean controls, on this high-intrinsic-load task. In
contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85%
accuracy in control conditions, with a statistically significant degradation
under context saturation ($\beta = -0.003$ per % load, $p < 0.001$). These
findings provide preliminary evidence that cognitive load is a key contributor
to reasoning failures, supporting theories of hallucination-as-guessing under
uncertainty. We conclude that dynamic, cognitive-aware stress testing, as
exemplified by the ICE benchmark, is essential for evaluating the true
resilience and safety of advanced AI systems.

</details>


### [5] [Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation](https://arxiv.org/abs/2509.19524)
*Ramy ElMallah,Krish Chhajer,Chi-Guhn Lee*

Main category: cs.AI

TL;DR: 本文提出StepEval框架，用于更细致地评估机器人学习中多步骤操作任务的策略成功率，而非仅依赖单一的最终成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人学习论文通常只报告单一的二元成功率，无法体现策略在多步骤任务中各个子目标上的成功与失败情况。

Method: 提出StepEval框架，利用视觉语言模型(VLMs)自动判断子目标结果，并考虑成本因素，支持单视图或多视图输入，旨在成为一个可扩展的开源项目。

Result: 提出了StepEval框架的设计蓝图，强调了以子目标成功率向量作为主要评估指标，并考虑其他因素例如延迟和成本。

Conclusion: StepEval框架旨在促进机器人学习领域评估标准化和可重复性，鼓励社区贡献，推动以子目标为单位的评估成为标准实践。

Abstract: Robot learning papers typically report a single binary success rate (SR),
which obscures where a policy succeeds or fails along a multi-step manipulation
task. We argue that subgoal-level reporting should become routine: for each
trajectory, a vector of per-subgoal SRs that makes partial competence visible
(e.g., grasp vs. pour). We propose a blueprint for StepEval, a cost-aware
plug-in evaluation framework that utilizes vision-language models (VLMs) as
automated judges of subgoal outcomes from recorded images or videos. Rather
than proposing new benchmarks or APIs, our contribution is to outline design
principles for a scalable, community-driven open-source project. In StepEval,
the primary artifact for policy evaluation is the per-subgoal SR vector;
however, other quantities (e.g., latency or cost estimates) are also considered
for framework-optimization diagnostics to help the community tune evaluation
efficiency and accuracy when ground-truth subgoal success labels are available.
We discuss how such a framework can remain model-agnostic, support single- or
multi-view inputs, and be lightweight enough to adopt across labs. The intended
contribution is a shared direction: a minimal, extensible seed that invites
open-source contributions, so that scoring the steps, not just the final goal,
becomes a standard and reproducible practice.

</details>


### [6] [Nano Bio-Agents (NBA): Small Language Model Agents for Genomics](https://arxiv.org/abs/2509.19566)
*George Hong,Daniel Trejo Banos*

Main category: cs.AI

TL;DR: 小型语言模型结合代理框架可用于基因组学问答，在GeneTuring基准测试中准确率高达98%，且资源消耗远低于大型模型。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在基因组学问答中存在的幻觉问题和高计算成本问题。

Method: 使用Nano Bio-Agent (NBA)框架，结合任务分解、工具编排和API访问（NCBI和AlphaGenome）。

Result: 小型语言模型(3-10B参数)在GeneTuring基准测试中准确率达到85-97%，优于许多现有方法。

Conclusion: 小型语言模型结合代理框架在基因组学问答中具有高效、低成本和高性能的优势，有助于推动基因组学工具的普及。

Abstract: We investigate the application of Small Language Models (<10 billion
parameters) for genomics question answering via agentic framework to address
hallucination issues and computational cost challenges. The Nano Bio-Agent
(NBA) framework we implemented incorporates task decomposition, tool
orchestration, and API access into well-established systems such as NCBI and
AlphaGenome. Results show that SLMs combined with such agentic framework can
achieve comparable and in many cases superior performance versus existing
approaches utilising larger models, with our best model-agent combination
achieving 98% accuracy on the GeneTuring benchmark. Notably, small 3-10B
parameter models consistently achieve 85-97% accuracy while requiring much
lower computational resources than conventional approaches. This demonstrates
promising potential for efficiency gains, cost savings, and democratization of
ML-powered genomics tools while retaining highly robust and accurate
performance.

</details>


### [7] [What Does Your Benchmark Really Measure? A Framework for Robust Inference of AI Capabilities](https://arxiv.org/abs/2509.19590)
*Nathanael Jo,Ashia Wilson*

Main category: cs.AI

TL;DR: 该论文质疑了生成模型评估的可靠性，提出了一种基于能力理论的推论框架，以更可靠地评估AI能力。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型评估方法可靠性存疑，难以真实反映模型性能。

Method: 提出一个将评估作为推理的框架，从能力理论出发，推导出评估方法，并引入减少样本复杂度的自适应算法。

Result: 提出一种新的评估框架，能够更可靠地估计AI能力，并降低了对扰动的敏感性。

Conclusion: 该框架为更可靠、更值得信赖的AI能力评估奠定了基础。

Abstract: Evaluations of generative models on benchmark data are now ubiquitous, and
their outcomes critically shape public and scientific expectations of AI's
capabilities. Yet growing skepticism surrounds their reliability. How can we
know that a reported accuracy genuinely reflects a model's true performance?
Evaluations are often presented as simple measurements, but in reality they are
inferences: to treat benchmark scores as evidence of capability is already to
assume a theory of what capability is and how it manifests in a test. We make
this step explicit by proposing a principled framework for evaluation as
inference: begin from a theory of capability, and then derive methods for
estimating it. This perspective, familiar in fields such as psychometrics, has
not yet become commonplace in AI evaluation. As a proof of concept, we address
a central challenge that undermines reliability: sensitivity to perturbations.
After formulating a model of ability, we introduce methods that infer ability
while accounting for uncertainty from sensitivity and finite samples, including
an adaptive algorithm that significantly reduces sample complexity. Together,
these contributions lay the groundwork for more reliable and trustworthy
estimates of AI capabilities as measured through benchmarks.

</details>


### [8] [SteinerSQL: Graph-Guided Mathematical Reasoning for Text-to-SQL Generation](https://arxiv.org/abs/2509.19623)
*Xutao Mao,Tao Liu,Hongying Zan*

Main category: cs.AI

TL;DR: SteinerSQL框架通过将数学推理和模式导航统一到一个图优化问题中，提高了复杂Text-to-SQL查询的准确性，在LogicCat和Spider2.0-Lite基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法分别处理数学推理和模式导航，导致Text-to-SQL查询的逻辑和结构错误。

Method: SteinerSQL分三个阶段：数学分解、通过Steiner树问题构建最优推理支架和多级验证。

Result: 在LogicCat和Spider2.0-Lite基准测试中，SteinerSQL分别达到了36.10%和40.04%的执行准确率。

Conclusion: SteinerSQL提供了一种新的、统一的Text-to-SQL范式，为更健壮和有原则的复杂推理任务解决方案铺平了道路。

Abstract: Large Language Models (LLMs) struggle with complex Text-to-SQL queries that
demand both sophisticated mathematical reasoning and intricate schema
navigation. Existing methods often tackle these challenges in isolation,
creating a fractured reasoning process that compromises logical and structural
correctness. To resolve this, we introduce SteinerSQL, a framework that unifies
these dual challenges into a single, graph-centric optimization problem.
SteinerSQL operates in three stages: mathematical decomposition to identify
required tables (terminals), optimal reasoning scaffold construction via a
Steiner tree problem, and multi-level validation to ensure correctness. On the
challenging LogicCat and Spider2.0-Lite benchmarks, SteinerSQL establishes a
new state-of-the-art with 36.10% and 40.04% execution accuracy, respectively,
using Gemini-2.5-Pro. Beyond accuracy, SteinerSQL presents a new, unified
paradigm for Text-to-SQL, paving the way for more robust and principled
solutions to complex reasoning tasks.

</details>


### [9] [Calibrated Reasoning: An Explanatory Verifier for Dynamic and Efficient Problem-Solving](https://arxiv.org/abs/2509.19681)
*Anisha Garg,Engin Tekin,Yash More,David Bick,Nishit Neema,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: 该论文提出了一种基于强化学习的成对解释验证器，用于改进推理模型的测试时计算策略，提高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型的自我评估能力差，限制了测试时计算策略的有效性。

Method: 提出了一种基于强化学习 (GRPO) 的成对解释验证器，生成校准的置信度分数和自然语言推理。

Result: 该验证器提高了最佳n和自省等测试时策略的准确性和效率，尤其擅长识别标准方法（如多数投票）难以解决的挑战性失效模式。

Conclusion: 该论文提出的成对解释验证器有效提升了推理模型的测试时计算策略，为大规模推理模型的应用提供了新的方向。

Abstract: Advanced test-time computing strategies are essential for scaling reasoning
models, but their effectiveness is capped by the models' poor self-evaluation.
We propose a pairwise Explanatory Verifier, trained via reinforcement learning
(GRPO), that produces calibrated confidence scores and associated natural
language reasoning for generated solutions. Our verifier improves the accuracy
and efficiency of test-time strategies like best-of-n and self-reflection.
Crucially, it excels at identifying challenging failure modes, such as when
both candidate solutions are identically incorrect, succeeding where standard
methods like majority voting fail.

</details>


### [10] [UserRL: Training Interactive User-Centric Agent via Reinforcement Learning](https://arxiv.org/abs/2509.19736)
*Cheng Qian,Zuxin Liu,Akshara Prabhakar,Jielin Qiu,Zhiwei Liu,Haolin Chen,Shirley Kokane,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.AI

TL;DR: UserRL框架通过标准化gym环境和模拟用户，训练和评估以用户为中心的智能体能力，研究发现SFT冷启动、周全的轨迹评分和开源模拟器对训练至关重要。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体在与用户的动态多轮交互中存在挑战，需要改进以更好地辅助用户。

Method: 提出UserRL框架，结合标准化gym环境和模拟用户，系统性地改变回合奖励分配和轨迹评分计算，并使用GRPO算法进行训练和评估。

Result: 发现SFT冷启动、周全的轨迹评分和选择合适的模拟用户（即使是开源的）对训练效果至关重要。

Conclusion: 奖励塑造和用户模拟选择的设计与模型规模同等重要，UserRL为开发强大的以用户为中心的智能体模型提供了有效途径。

Abstract: Reinforcement learning (RL) has shown promise in training agentic models that
move beyond static benchmarks to engage in dynamic, multi-turn interactions.
Yet, the ultimate value of such agents lies in their ability to assist users, a
setting where diversity and dynamics of user interaction pose challenges. In
this work, we propose UserRL, a unified framework for training and evaluating
user-centric abilities through standardized gym environments paired with
simulated users. We systematically vary turn-level reward assignment and
trajectory-level score calculation to analyze how different formulations affect
learning under the GRPO algorithm. Our experiments across Qwen3 models reveal
three key findings: (i) SFT cold start is critical for unlocking initial
interaction ability and enabling sustained RL improvements; (ii) deliberate
trajectory scoring yields more efficient and effective multi-turn interactions;
and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training,
open-source simulators (e.g., Qwen3-32B) remain a cost-effective and
transferable option. Together, these results highlight that careful design of
reward shaping and user simulation choice is as crucial as model scale, and
establish UserRL as a practical pathway for developing robust user-centric
agentic models. All codes and data are public for future research.

</details>


### [11] [The Conductor and the Engine: A Path Towards Co-Designed Reasoning](https://arxiv.org/abs/2509.19762)
*Yuanxin Wang,Pawel Filipczuk,Anisha Garg,Amaan Dhada,Mohammad Hassanpour,David Bick,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: 小型开源模型通过优化的推理流程（CEPO）在性能上超越大型模型，该流程减少了冗余计算，提高了指令遵循效率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型推理依赖大量测试时计算，但模型冗余和指令遵循不佳导致计算浪费，本文旨在优化这一过程。

Method: 提出了一种优化的推理流程（CEPO），使小型开源模型能够超越更大模型。

Result: 小型开源模型在推理性能上超过更大模型。

Conclusion: 通过模型能力与协调框架的协同设计，小型到中等规模的模型也能实现强大的推理能力。

Abstract: Modern LLM reasoning relies on extensive test-time computation, driven by
internal model training and external agentic orchestration. However, this
synergy is often inefficient, as model verbosity and poor instruction following
lead to wasted compute. We analyze this capability-cost trade-off and introduce
an optimized reasoning workflow (\cepo) that empowers smaller open-source
models to outperform models multiple times their size. We will open-source this
workflow to enable further research. Our work demonstrates a clear path toward
co-designing orchestration frameworks with the underlying model capabilities to
unlock powerful reasoning in small-to-medium sized models.

</details>


### [12] [Agentic Metacognition: Designing a "Self-Aware" Low-Code Agent for Failure Prediction and Human Handoff](https://arxiv.org/abs/2509.19783)
*Jiexi Xu*

Main category: cs.AI

TL;DR: 低代码/无代码环境下的自主代理的非确定性本质会导致可靠性问题，本文提出一种元认知层架构来预测并处理代理的失败，通过主动将任务移交给人类来提高任务成功率，但这会增加计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决低代码/无代码环境下自主代理的可靠性问题，提高用户信任度。

Method: 设计并实现了一个包含元认知层的架构，该层监控主代理，预测潜在的失败并进行人工干预。

Result: 实证分析表明该方法显著提高了任务成功率，但增加了计算开销。

Conclusion: 将人工干预作为系统设计的一个核心特性，提高系统弹性，改善用户体验，并通过透明化代理的内部状态来建立信任。

Abstract: The inherent non-deterministic nature of autonomous agents, particularly
within low-code/no-code (LCNC) environments, presents significant reliability
challenges. Agents can become trapped in unforeseen loops, generate inaccurate
outputs, or encounter unrecoverable failures, leading to user frustration and a
breakdown of trust. This report proposes a novel architectural pattern to
address these issues: the integration of a secondary, "metacognitive" layer
that actively monitors the primary LCNC agent. Inspired by human introspection,
this layer is designed to predict impending task failures based on a defined
set of triggers, such as excessive latency or repetitive actions. Upon
predicting a failure, the metacognitive agent proactively initiates a human
handoff, providing the user with a clear summary of the agent's "thought
process" and a detailed explanation of why it could not proceed. An empirical
analysis of a prototype system demonstrates that this approach significantly
increases the overall task success rate. However, this performance gain comes
with a notable increase in computational overhead. The findings reframe human
handoffs not as an admission of defeat but as a core design feature that
enhances system resilience, improves user experience, and builds trust by
providing transparency into the agent's internal state. The report discusses
the practical and ethical implications of this approach and identifies key
directions for future research.

</details>


### [13] [Analysis of approximate linear programming solution to Markov decision problem with log barrier function](https://arxiv.org/abs/2509.19800)
*Donghwan Lee,Hyukjun Yang,Bum Geun Park*

Main category: cs.AI

TL;DR: 本文提出一种基于对数障碍函数的求解马尔可夫决策过程线性规划的方法，该方法将不等式约束优化问题转化为无约束优化问题，便于使用梯度下降法求解。


<details>
  <summary>Details</summary>
Motivation: 现有求解马尔可夫决策过程的方法主要有基于贝尔曼方程的动态规划和线性规划两种，线性规划方法因求解困难而较少使用，本文旨在改进线性规划方法的求解效率。

Method: 利用对数障碍函数将马尔可夫决策过程的线性规划公式转化为无约束优化问题，然后使用梯度下降法求解。

Result: 提出了一种新的基于对数障碍函数的线性规划求解方法，并对其进行了理论分析。

Conclusion: 本文为高效求解基于线性规划的马尔可夫决策过程提供了理论基础，该方法简单有效，且具有较好的理论解释。

Abstract: There are two primary approaches to solving Markov decision problems (MDPs):
dynamic programming based on the Bellman equation and linear programming (LP).
Dynamic programming methods are the most widely used and form the foundation of
both classical and modern reinforcement learning (RL). By contrast, LP-based
methods have been less commonly employed, although they have recently gained
attention in contexts such as offline RL. The relative underuse of the LP-based
methods stems from the fact that it leads to an inequality-constrained
optimization problem, which is generally more challenging to solve effectively
compared with Bellman-equation-based methods. The purpose of this paper is to
establish a theoretical foundation for solving LP-based MDPs in a more
effective and practical manner. Our key idea is to leverage the log-barrier
function, widely used in inequality-constrained optimization, to transform the
LP formulation of the MDP into an unconstrained optimization problem. This
reformulation enables approximate solutions to be obtained easily via gradient
descent. While the method may appear simple, to the best of our knowledge, a
thorough theoretical interpretation of this approach has not yet been
developed. This paper aims to bridge this gap.

</details>


### [14] [LatentGuard: Controllable Latent Steering for Robust Refusal of Attacks and Reliable Response Generation](https://arxiv.org/abs/2509.19839)
*Huizhen Shu,Xuying Li,Zhuo Li*

Main category: cs.AI

TL;DR: LATENTGUARD框架结合行为对齐和监督潜在空间控制，实现大型语言模型的安全可控性和实用性平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以兼顾大型语言模型的安全性和实用性，尤其是在表示层面的精细控制。

Method: 三阶段框架：1. 基于理性化数据集微调LLM；2. 训练结构化变分自编码器(VAE)学习解纠缠的潜在表示；3. 通过操控潜在维度实现选择性拒绝行为。

Result: 在Qwen3-8B和Mistral-7B模型上实验表明，LATENTGUARD显著提升了安全可控性和响应可解释性，且未影响实用性。

Conclusion: 结构化表示层干预为构建更安全实用的LLM系统提供了有前景的途径。

Abstract: Achieving robust safety alignment in large language models (LLMs) while
preserving their utility remains a fundamental challenge. Existing approaches
often struggle to balance comprehensive safety with fine-grained
controllability at the representation level. We introduce LATENTGUARD, a novel
three-stage framework that combines behavioral alignment with supervised latent
space control for interpretable and precise safety steering. Our approach
begins by fine-tuning an LLM on rationalized datasets containing both
reasoning-enhanced refusal responses to adversarial prompts and
reasoning-enhanced normal responses to benign queries, establishing robust
behavioral priors across both safety-critical and utility-preserving scenarios.
We then train a structured variational autoencoder (VAE) on intermediate MLP
activations, supervised by multi-label annotations including attack types,
attack methods, and benign indicators. This supervision enables the VAE to
learn disentangled latent representations that capture distinct adversarial
characteristics while maintaining semantic interpretability. Through targeted
manipulation of learned latent dimensions, LATENTGUARD achieves selective
refusal behavior, effectively blocking harmful requests while preserving
helpfulness for legitimate use cases. Experiments on Qwen3-8B demonstrate
significant improvements in both safety controllability and response
interpretability without compromising utility. Cross-architecture validation on
Mistral-7B confirms the generalizability of our latent steering approach,
showing consistent effectiveness across different model families. Our results
suggest that structured representation-level intervention offers a promising
pathway toward building safer yet practical LLM systems.

</details>


### [15] [CON-QA: Privacy-Preserving QA using cloud LLMs in Contract Domain](https://arxiv.org/abs/2509.19925)
*Ajeet Kumar Singh,Rajsabi Surya,Anurag Tripathi,Santanu Choudhury,Sudhir Bisane*

Main category: cs.AI

TL;DR: CON-QA框架结合本地和云端LLM，安全地对企业合同进行问答，在保护敏感信息的同时保证答案质量。


<details>
  <summary>Details</summary>
Motivation: 企业将LLM应用于法律文档处理时，保护敏感信息（如PII）成为关键挑战。

Method: CON-QA框架包含三个阶段：语义查询分解和查询感知文档块检索（本地LLM）；敏感实体匿名化（结构化一对多映射）；匿名化响应生成和本地答案重建（云端LLM和本地反向映射）。

Result: 在CUAD-QA数据集（8.5万个问答对）上进行评估，结果表明CON-QA有效地保护了隐私，同时保持了答案质量和语义一致性，显著降低了隐私风险。

Conclusion: CON-QA框架适用于企业级合同文档的安全问答，兼顾隐私和实用性。

Abstract: As enterprises increasingly integrate cloud-based large language models
(LLMs) such as ChatGPT and Gemini into their legal document workflows,
protecting sensitive contractual information - including Personally
Identifiable Information (PII) and commercially sensitive clauses - has emerged
as a critical challenge. In this work, we propose CON-QA, a hybrid
privacy-preserving framework designed specifically for secure question
answering over enterprise contracts, effectively combining local and
cloud-hosted LLMs. The CON-QA framework operates through three stages: (i)
semantic query decomposition and query-aware document chunk retrieval using a
locally deployed LLM analysis, (ii) anonymization of detected sensitive
entities via a structured one-to-many mapping scheme, ensuring semantic
coherence while preventing cross-session entity inference attacks, and (iii)
anonymized response generation by a cloud-based LLM, with accurate
reconstruction of the original answer locally using a session-consistent
many-to-one reverse mapping. To rigorously evaluate CON-QA, we introduce
CUAD-QA, a corpus of 85k question-answer pairs generated over 510 real-world
CUAD contract documents, encompassing simple, complex, and summarization-style
queries. Empirical evaluations, complemented by detailed human assessments,
confirm that CON-QA effectively maintains both privacy and utility, preserves
answer quality, maintains fidelity to legal clause semantics, and significantly
mitigates privacy risks, demonstrating its practical suitability for secure,
enterprise-level contract documents.

</details>


### [16] [Embodied AI: From LLMs to World Models](https://arxiv.org/abs/2509.20021)
*Tongtong Feng,Xin Wang,Yu-Gang Jiang,Wenwu Zhu*

Main category: cs.AI

TL;DR: 本文综述了具身AI的最新进展，特别是结合LLM和WM的应用。


<details>
  <summary>Details</summary>
Motivation: 具身AI是实现AGI的关键，LLM和WM的突破为其发展提供了新动力。

Method: 对现有文献进行综述，分析LLM和WM在具身AI中的作用，并展望未来研究方向。

Result: 对LLM驱动和WM驱动的具身AI进行了深入探讨，提出了结合MLLM和WM的架构，并列举了其在实际应用中的案例。

Conclusion: 具身AI具有广阔的应用前景，未来研究应关注MLLM-WM融合架构以及更复杂的物理世界交互。

Abstract: Embodied Artificial Intelligence (AI) is an intelligent system paradigm for
achieving Artificial General Intelligence (AGI), serving as the cornerstone for
various applications and driving the evolution from cyberspace to physical
systems. Recent breakthroughs in Large Language Models (LLMs) and World Models
(WMs) have drawn significant attention for embodied AI. On the one hand, LLMs
empower embodied AI via semantic reasoning and task decomposition, bringing
high-level natural language instructions and low-level natural language actions
into embodied cognition. On the other hand, WMs empower embodied AI by building
internal representations and future predictions of the external world,
facilitating physical law-compliant embodied interactions. As such, this paper
comprehensively explores the literature in embodied AI from basics to advances,
covering both LLM driven and WM driven works. In particular, we first present
the history, key technologies, key components, and hardware systems of embodied
AI, as well as discuss its development via looking from unimodal to multimodal
angle. We then scrutinize the two burgeoning fields of embodied AI, i.e.,
embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs,
meticulously delineating their indispensable roles in end-to-end embodied
cognition and physical laws-driven embodied interactions. Building upon the
above advances, we further share our insights on the necessity of the joint
MLLM-WM driven embodied AI architecture, shedding light on its profound
significance in enabling complex tasks within physical worlds. In addition, we
examine representative applications of embodied AI, demonstrating its wide
applicability in real-world scenarios. Last but not least, we point out future
research directions of embodied AI that deserve further investigation.

</details>
