<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [PG-Agent: An Agent Powered by Page Graph](https://arxiv.org/abs/2509.03536)
*Weizhi Chen,Ziwei Wang,Leyang Yang,Sheng Zhou,Xiaoxuan Tang,Jiajun Bu,Yong Li,Wei Jiang*

Main category: cs.AI

TL;DR: 本文提出了一种名为PG-Agent的GUI智能体框架，该框架利用页面图和检索增强生成技术，有效提升了GUI智能体在未见过场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体通常利用多步骤操作的顺序片段作为先验知识，难以捕捉页面间的复杂关系，限制了其对GUI环境的理解和泛化能力。

Method: 将顺序操作片段转化为页面图，利用检索增强生成技术从页面图中检索可靠的GUI感知指导，并提出一个包含任务分解策略的多智能体框架PG-Agent。

Result: 在多个基准测试中，PG-Agent展现出有效性，即使页面图构建的片段有限也能取得良好效果。

Conclusion: PG-Agent框架有效提升了GUI智能体的泛化能力，为GUI智能体研究提供了新的方向。

Abstract: Graphical User Interface (GUI) agents possess significant commercial and
social value, and GUI agents powered by advanced multimodal large language
models (MLLMs) have demonstrated remarkable potential. Currently, existing GUI
agents usually utilize sequential episodes of multi-step operations across
pages as the prior GUI knowledge, which fails to capture the complex transition
relationship between pages, making it challenging for the agents to deeply
perceive the GUI environment and generalize to new scenarios. Therefore, we
design an automated pipeline to transform the sequential episodes into page
graphs, which explicitly model the graph structure of the pages that are
naturally connected by actions. To fully utilize the page graphs, we further
introduce Retrieval-Augmented Generation (RAG) technology to effectively
retrieve reliable perception guidelines of GUI from them, and a tailored
multi-agent framework PG-Agent with task decomposition strategy is proposed to
be injected with the guidelines so that it can generalize to unseen scenarios.
Extensive experiments on various benchmarks demonstrate the effectiveness of
PG-Agent, even with limited episodes for page graph construction.

</details>


### [2] [Multilinear and Linear Programs for Partially Identifiable Queries in Quasi-Markovian Structural Causal Models](https://arxiv.org/abs/2509.03548)
*João P. Arroyo,João G. Rodrigues,Daniel Lawand,Denis D. Mauá,Junkyu Lee,Radu Marinescu,Alex Gray,Eduardo R. Laurentino,Fabio G. Cozman*

Main category: cs.AI

TL;DR: 本文研究了因果模型中部分可识别查询的问题，提出了一种新的算法来简化概率界限的计算，并在单次干预的情况下，利用列生成法计算概率界限，实验表明该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究在部分可识别因果模型中计算概率值的方法。

Method: 提出了一种新的算法，利用输入概率简化多线性规划的构建，并在单次干预的情况下应用列生成法计算概率界限。

Result: 提出了一种新的算法，实验表明该方法优于现有方法，并证明了在单次干预的情况下，可以使用多项式基数表示外生变量。

Conclusion: 该研究提供了一种有效的计算部分可识别因果模型中概率界限的方法，为因果推理研究提供了新的思路。

Abstract: We investigate partially identifiable queries in a class of causal models. We
focus on acyclic Structural Causal Models that are quasi-Markovian (that is,
each endogenous variable is connected with at most one exogenous confounder).
We look into scenarios where endogenous variables are observed (and a
distribution over them is known), while exogenous variables are not fully
specified. This leads to a representation that is in essence a Bayesian network
where the distribution of root variables is not uniquely determined. In such
circumstances, it may not be possible to precisely compute a probability value
of interest. We thus study the computation of tight probability bounds, a
problem that has been solved by multilinear programming in general, and by
linear programming when a single confounded component is intervened upon. We
present a new algorithm to simplify the construction of such programs by
exploiting input probabilities over endogenous variables. For scenarios with a
single intervention, we apply column generation to compute a probability bound
through a sequence of auxiliary linear integer programs, thus showing that a
representation with polynomial cardinality for exogenous variables is possible.
Experiments show column generation techniques to be superior to existing
methods.

</details>


### [3] [Diffusion-RL Based Air Traffic Conflict Detection and Resolution Method](https://arxiv.org/abs/2509.03550)
*Tonghe Li,Jixin Liu,Weili Zeng,Hao Jiang*

Main category: cs.AI

TL;DR: 本文提出了一种名为Diffusion-AC的自主冲突解决框架，该框架利用扩散概率模型克服了现有深度强化学习方法在空中交通冲突检测与解决中的单峰偏差问题，显著提高了安全性及效率。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法在处理复杂的动态约束时缺乏决策灵活性，容易出现决策僵局。

Method: 该框架将扩散概率模型与密度递进式安全课程相结合，生成多峰动作分布，并通过反向去噪过程指导策略学习。

Result: 仿真实验表明，Diffusion-AC在高密度场景下成功率达94.1%，并将近距离空中碰撞发生率降低了约59%，显著优于现有方法。

Conclusion: Diffusion-AC的多峰决策能力使其能够灵活切换有效机动，从而显著提高了空中交通管制的安全性和效率。

Abstract: In the context of continuously rising global air traffic, efficient and safe
Conflict Detection and Resolution (CD&R) is paramount for air traffic
management. Although Deep Reinforcement Learning (DRL) offers a promising
pathway for CD&R automation, existing approaches commonly suffer from a
"unimodal bias" in their policies. This leads to a critical lack of
decision-making flexibility when confronted with complex and dynamic
constraints, often resulting in "decision deadlocks." To overcome this
limitation, this paper pioneers the integration of diffusion probabilistic
models into the safety-critical task of CD&R, proposing a novel autonomous
conflict resolution framework named Diffusion-AC. Diverging from conventional
methods that converge to a single optimal solution, our framework models its
policy as a reverse denoising process guided by a value function, enabling it
to generate a rich, high-quality, and multimodal action distribution. This core
architecture is complemented by a Density-Progressive Safety Curriculum (DPSC),
a training mechanism that ensures stable and efficient learning as the agent
progresses from sparse to high-density traffic environments. Extensive
simulation experiments demonstrate that the proposed method significantly
outperforms a suite of state-of-the-art DRL benchmarks. Most critically, in the
most challenging high-density scenarios, Diffusion-AC not only maintains a high
success rate of 94.1% but also reduces the incidence of Near Mid-Air Collisions
(NMACs) by approximately 59% compared to the next-best-performing baseline,
significantly enhancing the system's safety margin. This performance leap stems
from its unique multimodal decision-making capability, which allows the agent
to flexibly switch to effective alternative maneuvers.

</details>


### [4] [Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents](https://arxiv.org/abs/2509.03581)
*Davide Paglieri,Bartłomiej Cupiał,Jonathan Cook,Ulyana Piterbarg,Jens Tuyls,Edward Grefenstette,Jakob Nicolaus Foerster,Jack Parker-Holder,Tim Rocktäschel*

Main category: cs.AI

TL;DR: 本文提出了一种动态规划框架，使大型语言模型 (LLM) 能够灵活地决定何时分配测试时间计算资源进行规划，从而提高了其在长序列决策任务中的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，LLM 代理在每次行动前都进行规划，计算成本高且在长序列任务中性能下降。

Method: 提出一个两阶段训练流程：1. 使用多样化合成数据进行监督微调，使模型能够进行动态规划；2. 使用强化学习在长序列环境中优化动态规划能力。

Result: 实验结果表明，该方法训练出的动态规划代理更样本高效，能够完成更复杂的目标，并且能够有效地遵循人工制定的计划。

Conclusion: 本文首次探索了在序列决策任务中训练LLM代理进行动态测试时间计算分配，为构建更高效、更自适应、更可控的代理系统铺平了道路。

Abstract: Training large language models (LLMs) to reason via reinforcement learning
(RL) significantly improves their problem-solving capabilities. In agentic
settings, existing methods like ReAct prompt LLMs to explicitly plan before
every action; however, we demonstrate that always planning is computationally
expensive and degrades performance on long-horizon tasks, while never planning
further limits performance. To address this, we introduce a conceptual
framework formalizing dynamic planning for LLM agents, enabling them to
flexibly decide when to allocate test-time compute for planning. We propose a
simple two-stage training pipeline: (1) supervised fine-tuning on diverse
synthetic data to prime models for dynamic planning, and (2) RL to refine this
capability in long-horizon environments. Experiments on the Crafter environment
show that dynamic planning agents trained with this approach are more
sample-efficient and consistently achieve more complex objectives.
Additionally, we demonstrate that these agents can be effectively steered by
human-written plans, surpassing their independent capabilities. To our
knowledge, this work is the first to explore training LLM agents for dynamic
test-time compute allocation in sequential decision-making tasks, paving the
way for more efficient, adaptive, and controllable agentic systems.

</details>


### [5] [Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE](https://arxiv.org/abs/2509.03626)
*Zahra Zehtabi Sabeti Moghaddam,Zeinab Dehghani,Maneeha Rani,Koorosh Aslansefat,Bhupesh Kumar Mishra,Rameez Raja Kureshi,Dhavalkumar Thakker*

Main category: cs.AI

TL;DR: 该论文提出一种名为KG-SMILE的框架，用于提高基于知识图谱的检索增强生成模型（RAG）的可解释性，从而提升其在医疗等领域的可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式AI模型存在幻觉和不可靠性问题，尤其在医疗等对精度要求高的领域。RAG虽然提高了准确性，但其内部机制缺乏透明度。

Method: 提出了一种与方法无关的、基于扰动的框架KG-SMILE，通过控制扰动、计算相似性和训练加权线性代理模型，识别对生成输出影响最大的图实体和关系，从而实现RAG的可解释性。

Result: 评估结果表明，KG-SMILE生成的解释稳定且与人类判断一致，平衡了模型有效性和可解释性。

Conclusion: KG-SMILE框架能够提高RAG模型的透明度和可信度，促进机器学习技术的信任。

Abstract: Generative AI, such as Large Language Models (LLMs), has achieved impressive
progress but still produces hallucinations and unverifiable claims, limiting
reliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves
accuracy by grounding outputs in external knowledge, especially in domains like
healthcare, where precision is vital. However, RAG remains opaque and
essentially a black box, heavily dependent on data quality. We developed a
method-agnostic, perturbation-based framework that provides token and
component-level interoperability for Graph RAG using SMILE and named it as
Knowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing
similarities, and training weighted linear surrogates, KG-SMILE identifies the
graph entities and relations most influential to generated outputs, thereby
making RAG more transparent. We evaluate KG-SMILE using comprehensive
attribution metrics, including fidelity, faithfulness, consistency, stability,
and accuracy. Our findings show that KG-SMILE produces stable, human-aligned
explanations, demonstrating its capacity to balance model effectiveness with
interpretability and thereby fostering greater transparency and trust in
machine learning technologies.

</details>


### [6] [CausalARC: Abstract Reasoning with Causal World Models](https://arxiv.org/abs/2509.03636)
*Jacqueline Maasch,John Kalantari,Kia Khezeli*

Main category: cs.AI

TL;DR: CausalARC:一个用于评估AI在低数据和分布外环境下推理能力的实验平台，基于ARC构建，并使用结构因果模型进行数据增强。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型难以适应新颖问题设置和数据限制。

Method: 基于结构因果模型构建CausalARC平台，提供观察性、干预性和反事实性数据增强。

Result: 在四个语言模型评估场景中（抽象推理、反事实推理、程序合成和因果发现）验证了CausalARC的有效性。

Conclusion: CausalARC为评估AI推理能力提供了一个新的基准，有助于推动AI在低数据和分布外环境下的发展。

Abstract: Reasoning requires adaptation to novel problem settings under limited data
and distribution shift. This work introduces CausalARC: an experimental testbed
for AI reasoning in low-data and out-of-distribution regimes, modeled after the
Abstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is
sampled from a fully specified causal world model, formally expressed as a
structural causal model. Principled data augmentations provide observational,
interventional, and counterfactual feedback about the world model in the form
of few-shot, in-context learning demonstrations. As a proof-of-concept, we
illustrate the use of CausalARC for four language model evaluation settings:
(1) abstract reasoning with test-time training, (2) counterfactual reasoning
with in-context learning, (3) program synthesis, and (4) causal discovery with
logical reasoning.

</details>


### [7] [Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations](https://arxiv.org/abs/2509.03644)
*François Olivier,Zied Bouraoui*

Main category: cs.AI

TL;DR: 该论文提出了一种名为Embodied-LM的神经符号系统，通过将图像模式融入大型语言模型（LLM），增强其逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在逻辑推理方面容易出错，缺乏人类认知中稳健的心理表征。

Method: 该系统利用基于图像模式的示意图表示，并使用Answer Set Programming进行声明式空间推理，将LLM与具身认知结构相结合。

Result: 实验结果表明，该方法能够引导LLM通过具身认知结构解释场景，并支持有效的逻辑推理，同时增强了可解释性。

Conclusion: Embodied-LM为结合更复杂动态表示奠定了计算基础，未来可进一步扩展。

Abstract: Despite significant progress in natural language understanding, Large
Language Models (LLMs) remain error-prone when performing logical reasoning,
often lacking the robust mental representations that enable human-like
comprehension. We introduce a prototype neurosymbolic system, Embodied-LM, that
grounds understanding and logical reasoning in schematic representations based
on image schemas-recurring patterns derived from sensorimotor experience that
structure human cognition. Our system operationalizes the spatial foundations
of these cognitive structures using declarative spatial reasoning within Answer
Set Programming. Through evaluation on logical deduction problems, we
demonstrate that LLMs can be guided to interpret scenarios through embodied
cognitive structures, that these structures can be formalized as executable
programs, and that the resulting representations support effective logical
reasoning with enhanced interpretability. While our current implementation
focuses on spatial primitives, it establishes the computational foundation for
incorporating more complex and dynamic representations.

</details>


### [8] [Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning](https://arxiv.org/abs/2509.03646)
*Haozhe Wang,Qixin Xu,Che Liu,Junhong Wu,Fangzhen Lin,Wenhu Chen*

Main category: cs.AI

TL;DR: 强化学习增强大型语言模型复杂推理能力，其机制在于分层推理：先提升低级技能，后改进高级策略规划。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法未能有效解决大型语言模型推理能力提升中的瓶颈问题。

Method: 分析了大型语言模型强化学习训练中的“aha moments”、长度缩放和熵动力学现象，提出了层次感知信用分配算法(HICRA)，该算法集中优化对高级规划token的影响。

Result: HICRA算法显著优于基线算法，验证了关注策略瓶颈对提升高级推理能力的关键作用，并证实语义熵是衡量策略探索的更优指标。

Conclusion: 通过关注高级策略规划的学习瓶颈，可以显著提升大型语言模型的推理能力，HICRA算法为此提供了一种有效的解决方案。

Abstract: Reinforcement Learning (RL) has proven highly effective at enhancing the
complex reasoning abilities of Large Language Models (LLMs), yet underlying
mechanisms driving this success remain largely opaque. Our analysis reveals
that puzzling phenomena like ``aha moments", ``length-scaling'' and entropy
dynamics are not disparate occurrences but hallmarks of an emergent reasoning
hierarchy, akin to the separation of high-level strategic planning from
low-level procedural execution in human cognition. We uncover a compelling
two-phase dynamic: initially, a model is constrained by procedural correctness
and must improve its low-level skills. The learning bottleneck then decisively
shifts, with performance gains being driven by the exploration and mastery of
high-level strategic planning. This insight exposes a core inefficiency in
prevailing RL algorithms like GRPO, which apply optimization pressure
agnostically and dilute the learning signal across all tokens. To address this,
we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that
concentrates optimization efforts on high-impact planning tokens. HICRA
significantly outperforms strong baselines, demonstrating that focusing on this
strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we
validate semantic entropy as a superior compass for measuring strategic
exploration over misleading metrics such as token-level entropy.

</details>


### [9] [An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series Classification](https://arxiv.org/abs/2509.03649)
*Davide Italo Serramazza,Nikos Papadeas,Zahraa Abdallah,Georgiana Ifrim*

Main category: cs.AI

TL;DR: "本文研究了时间序列分类模型的可解释性，发现等长分割在降低SHAP方法计算复杂度同时保持较好解释质量方面表现最佳，并提出了一种新的归一化技术来进一步提高解释质量"


<details>
  <summary>Details</summary>
Motivation: "现有SHAP方法计算复杂度高，限制了其在长时序数据上的应用，本文旨在研究最优分割策略以提升效率和解释质量"

Method: "研究了八种不同的时间序列分割算法，并使用InterpretTime和AUC Difference两种方法评估其对解释质量的影响，同时提出了一种新的属性归一化技术"

Result: "等长分割优于其他自定义时间序列分割算法，且提出的归一化技术能有效提升属性质量，分割数量对解释质量的影响大于具体分割方法"

Conclusion: "等长分割结合提出的归一化技术为长时序数据的可解释AI提供了有效的解决方案"

Abstract: Explainable AI (XAI) has become an increasingly important topic for
understanding and attributing the predictions made by complex Time Series
Classification (TSC) models. Among attribution methods, SHapley Additive
exPlanations (SHAP) is widely regarded as an excellent attribution method; but
its computational complexity, which scales exponentially with the number of
features, limits its practicality for long time series. To address this, recent
studies have shown that aggregating features via segmentation, to compute a
single attribution value for a group of consecutive time points, drastically
reduces SHAP running time. However, the choice of the optimal segmentation
strategy remains an open question. In this work, we investigated eight
different Time Series Segmentation algorithms to understand how segment
compositions affect the explanation quality. We evaluate these approaches using
two established XAI evaluation methodologies: InterpretTime and AUC Difference.
Through experiments on both Multivariate (MTS) and Univariate Time Series
(UTS), we find that the number of segments has a greater impact on explanation
quality than the specific segmentation method. Notably, equal-length
segmentation consistently outperforms most of the custom time series
segmentation algorithms. Furthermore, we introduce a novel attribution
normalisation technique that weights segments by their length and we show that
it consistently improves attribution quality.

</details>


### [10] [PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming](https://arxiv.org/abs/2509.03728)
*Wesley Hanwen Deng,Sunnie S. Y. Kim,Akshita Jha,Ken Holstein,Motahhare Eslami,Lauren Wilcox,Leon A Gatys*

Main category: cs.AI

TL;DR: PersonaTeaming方法通过引入角色扮演提升了AI模型对抗样本生成的成功率，并提出新的指标衡量对抗样本的多样性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化红队方法忽略了身份因素对风险发现的影响。

Method: 提出PersonaTeaming方法，通过角色扮演（专家或普通用户）变异提示词，并开发动态角色生成算法和新的度量指标。

Result: 实验表明，PersonaTeaming方法在保持提示词多样性的同时，攻击成功率最高提升了144.1%。

Conclusion: PersonaTeaming方法有效提升了自动化红队测试的效果，为自动化和人工红队方法的结合提供了新的思路。

Abstract: Recent developments in AI governance and safety research have called for
red-teaming methods that can effectively surface potential risks posed by AI
models. Many of these calls have emphasized how the identities and backgrounds
of red-teamers can shape their red-teaming strategies, and thus the kinds of
risks they are likely to uncover. While automated red-teaming approaches
promise to complement human red-teaming by enabling larger-scale exploration of
model behavior, current approaches do not consider the role of identity. As an
initial step towards incorporating people's background and identities in
automated red-teaming, we develop and evaluate a novel method, PersonaTeaming,
that introduces personas in the adversarial prompt generation process to
explore a wider spectrum of adversarial strategies. In particular, we first
introduce a methodology for mutating prompts based on either "red-teaming
expert" personas or "regular AI user" personas. We then develop a dynamic
persona-generating algorithm that automatically generates various persona types
adaptive to different seed prompts. In addition, we develop a set of new
metrics to explicitly measure the "mutation distance" to complement existing
diversity measurements of adversarial prompts. Our experiments show promising
improvements (up to 144.1%) in the attack success rates of adversarial prompts
through persona mutation, while maintaining prompt diversity, compared to
RainbowPlus, a state-of-the-art automated red-teaming method. We discuss the
strengths and limitations of different persona types and mutation methods,
shedding light on future opportunities to explore complementarities between
automated and human red-teaming approaches.

</details>


### [11] [The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs](https://arxiv.org/abs/2509.03730)
*Pengrui Han,Rafal Kocielnik,Peiyang Song,Ramit Debnath,Dean Mobbs,Anima Anandkumar,R. Michael Alvarez*

Main category: cs.AI

TL;DR: 大型语言模型(LLM)展现出类似人类性格特征的行为倾向，但自我报告的性格特征并不能可靠地预测行为。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在训练过程中性格特征的动态变化、自我报告特征的预测效度以及针对性干预(例如角色注入)的影响。

Method: 系统性地刻画LLM的性格特征，包括训练阶段特征的动态变化、自我报告特征在行为任务中的预测效度以及定向干预的影响。

Result: 指令对齐(例如RLHF)显著稳定了LLM的性格表达并增强了特征关联性，但自我报告的特征并不能可靠预测行为，角色注入对行为的影响很小或不一致。

Conclusion: LLM的性格表达与行为一致性之间存在差异，需要更深入地评估LLM的对齐性和可解释性。

Abstract: Personality traits have long been studied as predictors of human
behavior.Recent advances in Large Language Models (LLMs) suggest similar
patterns may emerge in artificial systems, with advanced LLMs displaying
consistent behavioral tendencies resembling human traits like agreeableness and
self-regulation. Understanding these patterns is crucial, yet prior work
primarily relied on simplified self-reports and heuristic prompting, with
little behavioral validation. In this study, we systematically characterize LLM
personality across three dimensions: (1) the dynamic emergence and evolution of
trait profiles throughout training stages; (2) the predictive validity of
self-reported traits in behavioral tasks; and (3) the impact of targeted
interventions, such as persona injection, on both self-reports and behavior.
Our findings reveal that instructional alignment (e.g., RLHF, instruction
tuning) significantly stabilizes trait expression and strengthens trait
correlations in ways that mirror human data. However, these self-reported
traits do not reliably predict behavior, and observed associations often
diverge from human patterns. While persona injection successfully steers
self-reports in the intended direction, it exerts little or inconsistent effect
on actual behavior. By distinguishing surface-level trait expression from
behavioral consistency, our findings challenge assumptions about LLM
personality and underscore the need for deeper evaluation in alignment and
interpretability.

</details>


### [12] [Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation](https://arxiv.org/abs/2509.03736)
*James Mooney,Josef Woldense,Zheng Robert Jia,Shirley Anugrah Hayati,My Ha Nguyen,Vipul Raheja,Dongyeop Kang*

Main category: cs.AI

TL;DR: 大型语言模型(LLM)生成的代理无法在不同实验环境下保持一致的行为，不能替代人类参与者进行研究。


<details>
  <summary>Details</summary>
Motivation: 评估LLM代理是否可以替代人类参与者进行社会科学研究。

Method: 设计实验，揭示代理的内部状态，并在基本对话环境中检查代理行为，验证行为假设。

Result: 发现不同模型族和不同规模的LLM在内部存在显著的不一致性，虽然代理生成的回应可能与人类一致，但它们缺乏内部一致性。

Conclusion: LLM代理无法准确替代人类参与者，因为它们缺乏内部一致性。

Abstract: The impressive capabilities of Large Language Models (LLMs) have fueled the
notion that synthetic agents can serve as substitutes for real participants in
human-subject research. In an effort to evaluate the merits of this claim,
social science researchers have largely focused on whether LLM-generated survey
data corresponds to that of a human counterpart whom the LLM is prompted to
represent. In contrast, we address a more fundamental question: Do agents
maintain internal consistency, retaining similar behaviors when examined under
different experimental settings? To this end, we develop a study designed to
(a) reveal the agent's internal state and (b) examine agent behavior in a basic
dialogue setting. This design enables us to explore a set of behavioral
hypotheses to assess whether an agent's conversation behavior is consistent
with what we would expect from their revealed internal state. Our findings on
these hypotheses show significant internal inconsistencies in LLMs across model
families and at differing model sizes. Most importantly, we find that, although
agents may generate responses matching those of their human counterparts, they
fail to be internally consistent, representing a critical gap in their
capabilities to accurately substitute for real participants in human-subject
research. Our simulation code and data are publicly accessible.

</details>


### [13] [RAGuard: A Novel Approach for in-context Safe Retrieval Augmented Generation for LLMs](https://arxiv.org/abs/2509.03768)
*Connor Walker,Koorosh Aslansefat,Mohammad Naveed Akram,Yiannis Papadopoulos*

Main category: cs.AI

TL;DR: RAGuard框架通过集成安全关键文档和技术手册，提高了LLM在离岸风电维护中的准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在专业或意外场景下常失效，需要提高其在离岸风电维护中的安全性和准确性。

Method: 提出RAGuard增强型RAG框架，并开发SafetyClamp扩展，通过双索引查询和安全预算分配，保证技术深度和安全覆盖。

Result: RAGuard和SafetyClamp显著提高了安全召回率，同时保持了技术召回率。

Conclusion: RAGuard和SafetyClamp为在关键维护环境中将安全保证集成到LLM驱动的决策支持系统中，树立了新的标准。

Abstract: Accuracy and safety are paramount in Offshore Wind (OSW) maintenance, yet
conventional Large Language Models (LLMs) often fail when confronted with
highly specialised or unexpected scenarios. We introduce RAGuard, an enhanced
Retrieval-Augmented Generation (RAG) framework that explicitly integrates
safety-critical documents alongside technical manuals.By issuing parallel
queries to two indices and allocating separate retrieval budgets for knowledge
and safety, RAGuard guarantees both technical depth and safety coverage. We
further develop a SafetyClamp extension that fetches a larger candidate pool,
"hard-clamping" exact slot guarantees to safety. We evaluate across sparse
(BM25), dense (Dense Passage Retrieval) and hybrid retrieval paradigms,
measuring Technical Recall@K and Safety Recall@K. Both proposed extensions of
RAG show an increase in Safety Recall@K from almost 0\% in RAG to more than
50\% in RAGuard, while maintaining Technical Recall above 60\%. These results
demonstrate that RAGuard and SafetyClamp have the potential to establish a new
standard for integrating safety assurance into LLM-powered decision support in
critical maintenance contexts.

</details>


### [14] [Leveraging LLM-Based Agents for Intelligent Supply Chain Planning](https://arxiv.org/abs/2509.03811)
*Yongzhi Qi,Jiaheng Yin,Jianshen Zhang,Dongyang Geng,Zhengyu Chen,Hao Hu,Wei Qi,Zuo-Jun Max Shen*

Main category: cs.AI

TL;DR: 本文提出了一种供应链计划代理（SCPA）框架，利用大型语言模型（LLM）改进京东供应链规划，有效降低人工成本并提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决电商平台供应链规划中数据收集、长期规划和动态调整的难题。

Method: 构建SCPA框架，使其能够理解领域知识、理解操作员需求、分解任务、利用或创建新工具并返回基于证据的规划报告。在京东真实场景中部署该框架。

Result: 有效降低人工成本，提高准确性、库存可用性和其他关键指标。

Conclusion: LLM代理应用在供应链规划中是可行的，具有显著的实际应用价值。

Abstract: In supply chain management, planning is a critical concept. The movement of
physical products across different categories, from suppliers to warehouse
management, to sales, and logistics transporting them to customers, entails the
involvement of many entities. It covers various aspects such as demand
forecasting, inventory management, sales operations, and replenishment. How to
collect relevant data from an e-commerce platform's perspective, formulate
long-term plans, and dynamically adjust them based on environmental changes,
while ensuring interpretability, efficiency, and reliability, is a practical
and challenging problem. In recent years, the development of AI technologies,
especially the rapid progress of large language models, has provided new tools
to address real-world issues. In this work, we construct a Supply Chain
Planning Agent (SCPA) framework that can understand domain knowledge,
comprehend the operator's needs, decompose tasks, leverage or create new tools,
and return evidence-based planning reports. We deploy this framework in
JD.com's real-world scenario, demonstrating the feasibility of LLM-agent
applications in the supply chain. It effectively reduced labor and improved
accuracy, stock availability, and other key metrics.

</details>


### [15] [Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning](https://arxiv.org/abs/2509.03817)
*Wei Yang,Jesse Thomason*

Main category: cs.AI

TL;DR: 一种新的元策略协商框架(MPDF)结合SoftRankPO算法，显著提高了大型语言模型多智能体系统的推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型多智能体系统协作协议固定，忽略了智能体的内在认知能力。

Method: 提出元策略协商框架(MPDF)，智能体学习在“坚持、改进、让步”等元认知动作上的分散策略，并使用SoftRankPO强化学习算法稳定训练过程。

Result: 在五个基准测试中，平均准确率比现有六种算法提高了4-5%。

Conclusion: MPDF为多智能体大型语言模型系统学习适应性元认知策略提供了一种范例，从设计固定协议转向学习动态的、深思熟虑的策略。

Abstract: Multi-agent systems of large language models (LLMs) show promise for complex
reasoning, but their effectiveness is often limited by fixed collaboration
protocols. These frameworks typically focus on macro-level orchestration while
overlooking agents' internal deliberative capabilities. This critical
meta-cognitive blindspot treats agents as passive executors unable to adapt
their strategy based on internal cognitive states like uncertainty or
confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where
agents learn a decentralized policy over a set of high-level meta-cognitive
actions: Persist, Refine, and Concede. To overcome the instability of
traditional policy gradients in this setting, we develop SoftRankPO, a novel
reinforcement learning algorithm. SoftRankPO stabilizes training by shaping
advantages based on the rank of rewards mapped through smooth normal quantiles,
making the learning process robust to reward variance. Experiments show that
MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across
five mathematical and general reasoning benchmarks compared to six
state-of-the-art heuristic and learning-based multi-agent reasoning algorithms.
Our work presents a paradigm for learning adaptive, meta-cognitive policies for
multi-agent LLM systems, shifting the focus from designing fixed protocols to
learning dynamic, deliberative strategies.

</details>


### [16] [What Would an LLM Do? Evaluating Policymaking Capabilities of Large Language Models](https://arxiv.org/abs/2509.03827)
*Pierre Le Coz,Jia An Liu,Debarun Bhattacharjya,Georgina Curto,Serge Stinckwich*

Main category: cs.AI

TL;DR: 大型语言模型(LLM)在评估社会政策(以解决无家可归问题为例)方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在解决全球性社会问题(如无家可归)中的应用潜力。

Method: 构建涵盖四个地区的决策场景基准，结合基于代理的模型模拟政策的社会影响。

Result: 结果显示LLM在辅助制定社会政策方面具有潜力，尤其是在加入专家意见和情境校准后。

Conclusion: 负责任的监管和情境校准能够提升LLM在社会政策制定中的价值。

Abstract: Large language models (LLMs) are increasingly being adopted in high-stakes
domains. Their capacity to process vast amounts of unstructured data, explore
flexible scenarios, and handle a diversity of contextual factors can make them
uniquely suited to provide new insights for the complexity of social
policymaking. This article evaluates whether LLMs' are aligned with domain
experts (and among themselves) to inform social policymaking on the subject of
homelessness alleviation - a challenge affecting over 150 million people
worldwide. We develop a novel benchmark comprised of decision scenarios with
policy choices across four geographies (South Bend, USA; Barcelona, Spain;
Johannesburg, South Africa; Macau SAR, China). The policies in scope are
grounded in the conceptual framework of the Capability Approach for human
development. We also present an automated pipeline that connects the
benchmarked policies to an agent-based model, and we explore the social impact
of the recommended policies through simulated social scenarios. The paper
results reveal promising potential to leverage LLMs for social policy making.
If responsible guardrails and contextual calibrations are introduced in
collaboration with local domain experts, LLMs can provide humans with valuable
insights, in the form of alternative policies at scale.

</details>


### [17] [An Agentic Model Context Protocol Framework for Medical Concept Standardization](https://arxiv.org/abs/2509.03828)
*Jaerong Ahn,Andrew Wen,Nan Wang,Heling Jia,Zhiyi Yue,Sunyang Fu,Hongfang Liu*

Main category: cs.AI

TL;DR: 该论文提出了一种基于模型上下文协议(MCP)的零训练、防幻觉医学术语映射系统，用于将医疗术语映射到OMOP标准概念，提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: OMOP CDM数据标准化中的术语映射耗费资源且易出错，而大型语言模型(LLM)存在幻觉问题，该系统旨在解决这些问题。

Method: 开发基于模型上下文协议(MCP)的零训练、防幻觉映射系统，利用外部资源和工具进行可解释的映射。

Result: 该系统提高了映射效率和准确性，提供实时词汇查找和结构化推理输出，适用于探索性和生产环境。

Conclusion: 该系统为OMOP CDM数据标准化提供了一种高效、准确且可靠的解决方案。

Abstract: The Observational Medical Outcomes Partnership (OMOP) common data model (CDM)
provides a standardized representation of heterogeneous health data to support
large-scale, multi-institutional research. One critical step in data
standardization using OMOP CDM is the mapping of source medical terms to OMOP
standard concepts, a procedure that is resource-intensive and error-prone.
While large language models (LLMs) have the potential to facilitate this
process, their tendency toward hallucination makes them unsuitable for clinical
deployment without training and expert validation. Here, we developed a
zero-training, hallucination-preventive mapping system based on the Model
Context Protocol (MCP), a standardized and secure framework allowing LLMs to
interact with external resources and tools. The system enables explainable
mapping and significantly improves efficiency and accuracy with minimal effort.
It provides real-time vocabulary lookups and structured reasoning outputs
suitable for immediate use in both exploratory and production environments.

</details>
