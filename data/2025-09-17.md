<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 15]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [V-Math: An Agentic Approach to the Vietnamese National High School Graduation Mathematics Exams](https://arxiv.org/abs/2509.12251)
*Duong Q. Nguyen,Quy P. Nguyen,Nguyen Van Nhon,Quang-Thinh Bui,H. Nguyen-Xuan*

Main category: cs.AI

TL;DR: V-Math系统帮助越南高中生备考数学，集题生成、解题、个性化辅导于一体，教师端可生成符合标准的试题。


<details>
  <summary>Details</summary>
Motivation: 帮助越南高中生备考全国高中毕业数学考试，减轻教师负担，提高教学资源质量。

Method: 开发了一个名为V-Math的自主智能代理框架，包含三个AI智能体：试题生成器、解题/解释器和个性化辅导器。

Result: V-Math能生成符合要求的试卷，提供准确的解答和清晰的解释，并丰富练习题的多样性。

Conclusion: V-Math系统具有促进大规模、公平的数学学习和辅助教师命题的潜力。

Abstract: This paper develops an autonomous agentic framework called V-Math that aims
to assist Vietnamese high school students in preparing for the National High
School Graduation Mathematics Exams (NHSGMEs). The salient framework integrates
three specialized AI agents: a specification-matrix-conditioned question
generator, a solver/explainer for detailed step-by-step reasoning, and a
personalized tutor that adapts to student performance. Beyond enabling
self-paced student practice, V-Math supports teachers by generating innovative,
compliant exam questions and building diverse, high-quality question banks.
This reduces manual workload and enriches instructional resources. We describe
the system architecture, focusing on practice modes for learners and
teacher-oriented features for question generation. Preliminary evaluations
demonstrate that V-Math produces matrix-aligned exams with high solution
accuracy, delivers coherent explanations, and enhances the variety of practice
materials. These results highlight its potential to support scalable, equitable
mathematics preparation aligned with national standards while also empowering
teachers through AI-assisted exam creation.

</details>


### [2] [DISPLIB: a library of train dispatching problems](https://arxiv.org/abs/2509.12254)
*Oddvar Kloster,Bjørnar Luteberget,Carlo Mannino,Giorgio Sartor*

Main category: cs.AI

TL;DR: 该论文介绍了一个名为DISPLIB的公共问题定义和文件格式，用于火车重新调度问题，并提供了一些真实的案例数据和参考求解器实现，方便研究人员进行算法比较和复现。


<details>
  <summary>Details</summary>
Motivation: 现有的优化算法研究通常与具体的工业案例紧密相连，代码和数据很少公开，阻碍了可重复性和算法性能的比较。

Method: 提出一个公共问题定义和文件格式DISPLIB，收集多个真实案例的数据，并提供一个参考求解器实现。

Result: 创建了DISPLIB，方便研究人员无需工业连接即可进行火车调度问题的研究，并能够进行算法的实证比较。

Conclusion: DISPLIB的创建促进了火车调度问题研究的开放性和可重复性，有利于算法的改进和发展。

Abstract: Optimization-based decision support systems have a significant potential to
reduce delays, and thus improve efficiency on the railways, by automatically
re-routing and re-scheduling trains after delays have occurred. The operations
research community has dedicated a lot of effort to developing optimization
algorithms for this problem, but each study is typically tightly connected with
a specific industrial use case. Code and data are seldom shared publicly. This
fact hinders reproducibility, and has led to a proliferation of papers
describing algorithms for more or less compatible problem definitions, without
any real opportunity for readers to assess their relative performance. Inspired
by the successful communities around MILP, SAT, TSP, VRP, etc., we introduce a
common problem definition and file format, DISPLIB, which captures all the main
features of train re-routing and re-scheduling. We have gathered problem
instances from multiple real-world use cases and made them openly available. In
this paper, we describe the problem definition, the industrial instances, and a
reference solver implementation. This allows any researcher or developer to
work on the train dispatching problem without an industrial connection, and
enables the research community to perform empirical comparisons between
solvers. All materials are available online at https://displib.github.io.

</details>


### [3] [InPhyRe Discovers: Large Multimodal Models Struggle in Inductive Physical Reasoning](https://arxiv.org/abs/2509.12263)
*Gautam Sreekumar,Vishnu Naresh Boddeti*

Main category: cs.AI

TL;DR: 大型多模态模型(LMMs)难以进行归纳物理推理，尤其在违反已知物理定律的情况下。InPhyRe基准测试评估了13个LMMs的归纳物理推理能力，结果表明LMMs存在局限性，例如对语言偏差敏感且忽略视觉输入。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基准测试无法评估LMMs的归纳物理推理能力，而这是安全关键应用中不可或缺的。

Method: 提出InPhyRe基准测试，通过算法生成的合成碰撞视频评估LMMs预测碰撞结果的能力。

Result: LMMs难以将有限的物理定律知识应用于推理；在演示样本违反物理定律时，归纳物理推理能力较弱；存在语言偏差且忽略视觉输入。

Conclusion: InPhyRe揭示了LMMs在归纳物理推理方面的弱点，对提升LMMs的可靠性具有重要意义。

Abstract: Large multimodal models (LMMs) encode universal physical laws observed during
training, such as momentum conservation, as parametric knowledge. It allows
LMMs to answer physical reasoning queries, such as the outcome of a potential
collision event from visual input. However, since parametric knowledge includes
only the physical laws seen during training, it is insufficient for reasoning
when the inference scenario violates these physical laws. In contrast, humans
possess the skill to adapt their physical reasoning to unseen physical
environments from a few visual examples. This ability, which we refer to as
inductive physical reasoning, is indispensable for LMMs if they are to replace
human agents in safety-critical applications. Despite its importance, existing
visual benchmarks evaluate only the parametric knowledge in LMMs, and not
inductive physical reasoning. To this end, we propose InPhyRe, the first visual
question answering benchmark to measure inductive physical reasoning in LMMs.
InPhyRe evaluates LMMs on their ability to predict the outcome of collision
events in algorithmically generated synthetic collision videos. By inspecting
13 LMMs, InPhyRe informs us that (1) LMMs struggle to apply their limited
parametric knowledge about universal physical laws to reasoning, (2) inductive
physical reasoning in LMMs is weak when demonstration samples violate universal
physical laws, and (3) inductive physical reasoning in LMMs suffers from
language bias and largely ignores the visual inputs, questioning the
trustworthiness of LMMs regarding visual inputs.

</details>


### [4] [LLMAP: LLM-Assisted Multi-Objective Route Planning with User Preferences](https://arxiv.org/abs/2509.12273)
*Liangqi Yuan,Dong-Jun Han,Christopher G. Brinton,Sabine Brunswicker*

Main category: cs.AI

TL;DR: 本文提出了一种新型的LLM辅助路线规划系统LLMAP，该系统结合了LLM作为解析器和多步图构建迭代搜索算法，以应对现有方法在处理海量地图数据和理解自然语言偏好方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM路线规划方法存在处理地图数据和理解自然语言偏好的局限性，本文旨在提出一种更鲁棒的解决方案。

Method: LLMAP系统使用LLM作为解析器理解自然语言，提取用户偏好和任务依赖关系，并结合多步图构建迭代搜索算法进行最优路线查找。该系统采用多目标优化方法，在考虑用户时间限制、POI开放时间和任务依赖关系等约束条件下，最大化POI质量和任务完成率，同时最小化路线距离。

Result: 在14个国家27个城市的1000个不同复杂度的路线规划请求上进行了实验，结果表明该方法在多种约束条件下取得了优越的性能。

Conclusion: LLMAP系统有效地结合了LLM的自然语言处理能力和图搜索算法的效率，为自然语言驱动的路线规划提供了一种新的解决方案。

Abstract: The rise of large language models (LLMs) has made natural language-driven
route planning an emerging research area that encompasses rich user objectives.
Current research exhibits two distinct approaches: direct route planning using
LLM-as-Agent and graph-based searching strategies. However, LLMs in the former
approach struggle to handle extensive map data, while the latter shows limited
capability in understanding natural language preferences. Additionally, a more
critical challenge arises from the highly heterogeneous and unpredictable
spatio-temporal distribution of users across the globe. In this paper, we
introduce a novel LLM-Assisted route Planning (LLMAP) system that employs an
LLM-as-Parser to comprehend natural language, identify tasks, and extract user
preferences and recognize task dependencies, coupled with a Multi-Step Graph
construction with iterative Search (MSGS) algorithm as the underlying solver
for optimal route finding. Our multi-objective optimization approach adaptively
tunes objective weights to maximize points of interest (POI) quality and task
completion rate while minimizing route distance, subject to three key
constraints: user time limits, POI opening hours, and task dependencies. We
conduct extensive experiments using 1,000 routing prompts sampled with varying
complexity across 14 countries and 27 cities worldwide. The results demonstrate
that our approach achieves superior performance with guarantees across multiple
constraints.

</details>


### [5] [Developing an aeroponic smart experimental greenhouse for controlling irrigation and plant disease detection using deep learning and IoT](https://arxiv.org/abs/2509.12274)
*Mohammadreza Narimani,Ali Hajiahmad,Ali Moghimi,Reza Alimardani,Shahin Rafiee,Amir Hossein Mirzabe*

Main category: cs.AI

TL;DR: 该研究开发了一个基于物联网和人工智能的智能气雾栽培温室系统，用于监测植物状态和环境条件，并实现疾病检测。


<details>
  <summary>Details</summary>
Motivation: 现有温室管理依赖人工监测，效率低。本研究旨在开发智能系统提高效率，促进作物生产。

Method: 开发了基于物联网的平台控制环境条件，并使用VGG-19、InceptionResNetV2和InceptionV3算法构建AI疾病检测框架。

Result: 物联网系统能够实时监测温湿度、水流等，AI框架中VGG-19算法对干旱胁迫和锈病的识别准确率最高，达92%。

Conclusion: 该智能温室系统有效提高了温室环境监控和疾病检测的效率和准确性，为精准农业管理提供了新的技术手段。

Abstract: Controlling environmental conditions and monitoring plant status in
greenhouses is critical to promptly making appropriate management decisions
aimed at promoting crop production. The primary objective of this research
study was to develop and test a smart aeroponic greenhouse on an experimental
scale where the status of Geranium plant and environmental conditions are
continuously monitored through the integration of the internet of things (IoT)
and artificial intelligence (AI). An IoT-based platform was developed to
control the environmental conditions of plants more efficiently and provide
insights to users to make informed management decisions. In addition, we
developed an AI-based disease detection framework using VGG-19,
InceptionResNetV2, and InceptionV3 algorithms to analyze the images captured
periodically after an intentional inoculation. The performance of the AI
framework was compared with an expert's evaluation of disease status.
Preliminary results showed that the IoT system implemented in the greenhouse
environment is able to publish data such as temperature, humidity, water flow,
and volume of charge tanks online continuously to users and adjust the
controlled parameters to provide an optimal growth environment for the plants.
Furthermore, the results of the AI framework demonstrate that the VGG-19
algorithm was able to identify drought stress and rust leaves from healthy
leaves with the highest accuracy, 92% among the other algorithms.

</details>


### [6] [AIssistant: An Agentic Approach for Human--AI Collaborative Scientific Work on Reviews and Perspectives in Machine Learning](https://arxiv.org/abs/2509.12282)
*Sasi Kiran Gaddipati,Farhana Keya,Gollam Rabby,Sören Auer*

Main category: cs.AI

TL;DR: AIssistant，一个用于辅助科学研究的开源框架，提高了论文撰写效率和主题一致性，但仍需人工监督以保证准确性和合规性。


<details>
  <summary>Details</summary>
Motivation: 现有AI辅助研究工具分散且缺乏以人为本的工作流程，AIssistant旨在简化端到端科学工作流程的创建。

Method: 整合模块化工具和代理，涵盖文献综述、分段实验、文献管理和LaTeX论文文本自动生成等，并进行多层评估（人工评审、LLM评审和程序主席监督）。

Result: AIssistant提高了论文撰写效率和主题一致性，但存在虚构引用、难以适应动态论文结构和多模态内容整合不完整等局限性。

Conclusion: AIssistant展现了AI辅助科学研究的潜力，但人机协作对于保证准确性、方法论的合理性和伦理合规性仍然至关重要。

Abstract: Advances in AI-assisted research have introduced powerful tools for
literature retrieval, hypothesis generation, experimentation, and manuscript
preparation. However, systems remain fragmented and lack human-centred
workflows. To address these gaps, we introduce AIssistant, an agentic,
open-source Human-AI collaborative framework designed to simplify the
end-to-end creation of scientific workflows. Since our development is still in
an early stage, we present here the first experiments with AIssistant for
perspective and review research papers in machine learning. Our system
integrates modular tools and agents for literature synthesis, section-wise
experimentation, citation management, and automatic LaTeX paper text
generation, while maintaining human oversight at every stage to ensure
accuracy, coherence, and scholarly rigour. We conducted a comprehensive
evaluation across three layers: (1) Independent Human Review, following NeurIPS
double-blind standards; (2) Automated LLM Review, using GPT-5 as a scalable
human review proxy; and (3) Program Chair Oversight, where the chair monitors
the entire review process and makes final validation and acceptance decisions.
The results demonstrate that AIssistant improves drafting efficiency and
thematic consistency. Nonetheless, Human-AI collaboration remains essential for
maintaining factual correctness, methodological soundness, and ethical
compliance. Despite its effectiveness, we identify key limitations, including
hallucinated citations, difficulty adapting to dynamic paper structures, and
incomplete integration of multimodal content.

</details>


### [7] [Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition](https://arxiv.org/abs/2509.12423)
*Danielle Cohen,Yoni Halpern,Noam Kahlon,Joel Oren,Omri Berkovitch,Sapir Caduri,Ido Dagan,Anatoly Efros*

Main category: cs.AI

TL;DR: 本文提出一种新的分解方法，用于从用户交互轨迹中理解用户意图，该方法在资源受限的模型中提高了意图理解能力，甚至超过大型MLLMs的基准性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型多模态语言模型(MLLMs)在处理用户交互轨迹时面临的隐私、成本和延迟问题。

Method: 首先进行结构化交互总结，捕捉每个用户操作的关键信息；然后使用微调模型对聚合的摘要进行意图提取。

Result: 在资源受限的模型中提高了意图理解能力，甚至超过大型MLLMs的基准性能。

Conclusion: 该分解方法为在资源受限的设备上进行准确的意图推断提供了一种有效的方法。

Abstract: Understanding user intents from UI interaction trajectories remains a
challenging, yet crucial, frontier in intelligent agent development. While
massive, datacenter-based, multi-modal large language models (MLLMs) possess
greater capacity to handle the complexities of such sequences, smaller models
which can run on-device to provide a privacy-preserving, low-cost, and
low-latency user experience, struggle with accurate intent inference. We
address these limitations by introducing a novel decomposed approach: first, we
perform structured interaction summarization, capturing key information from
each user action. Second, we perform intent extraction using a fine-tuned model
operating on the aggregated summaries. This method improves intent
understanding in resource-constrained models, even surpassing the base
performance of large MLLMs.

</details>


### [8] [Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization](https://arxiv.org/abs/2509.12434)
*Jiahao Yu,Zelei Cheng,Xian Wu,Xinyu Xing*

Main category: cs.AI

TL;DR: 本文介绍了一种名为\sys的熵增强框架，该框架改进了现有的偏好优化算法，以提高大型语言模型在软件工程任务中的性能，并在SWE-bench排行榜上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型难以解决软件工程中的复杂问题，测试时缩放(TTS)的有效性依赖于模型输出的多样性，而标准的对齐方法会降低多样性，限制TTS的有效性。

Method: 提出了一种熵增强框架\sys，该框架增强了偏好目标以明确保留策略熵，并将学习推广到多轮交互的优化。结合学习验证模型和无模型方法的混合最佳轨迹选择方案。

Result: 在SWE-bench排行榜的开放权重模型中取得了最先进的结果，一个300亿参数的模型在\lite上排名第一，在\verified上排名第四，优于大多数参数规模更大的模型。

Conclusion: \sys框架有效地提高了大型语言模型在软件工程任务中的性能，为未来研究提供了新的方向。

Abstract: Software engineering presents complex, multi-step challenges for Large
Language Models (LLMs), requiring reasoning over large codebases and
coordinated tool use. The difficulty of these tasks is exemplified by
benchmarks like SWE-bench, where current LLMs still struggle to resolve
real-world issues.
  A promising approach to enhance performance is test-time scaling (TTS), but
its gains are heavily dependent on the diversity of model outputs.
  While standard alignment methods such as Direct Preference Optimization (DPO)
and Kahneman-Tversky Optimization (KTO) are effective at aligning model outputs
with human preferences, this process can come at the cost of reduced diversity,
limiting the effectiveness of TTS.
  Additionally, existing preference optimization algorithms are typically
designed for single-turn tasks and do not fully address the complexities of
multi-turn reasoning and tool integration required for interactive coding
agents.
  To bridge this gap, we introduce \sys, an entropy-enhanced framework that
adapts existing preference optimization algorithms to the multi-turn,
tool-assisted setting.
  \sys augments the preference objective to explicitly preserve policy entropy
and generalizes learning to optimize over multi-turn interactions rather than
single-turn responses.
  We validate \sys by fine-tuning a diverse suite of models from different
families and sizes (up to 106B parameters).
  To maximize performance gains from TTS, we further propose a hybrid
best-trajectory selection scheme combining a learned verifier model with model
free approaches.
  On the \swebench leaderboard, our approach establishes new state-of-the-art
results among open-weight models. A 30B parameter model trained with \sys ranks
1st on \lite and 4th on \verified on the open-weight leaderboard, surpassed
only by models with over 10x more parameters(\eg$>$350B).

</details>


### [9] [Enhancing Physical Consistency in Lightweight World Models](https://arxiv.org/abs/2509.12437)
*Dingrui Wang,Zhexiao Sun,Zhouheng Li,Cheng Wang,Youlun Peng,Hongyuan Ye,Baha Zarrouki,Wei Li,Mattia Piccinini,Lei Xie,Johannes Betz*

Main category: cs.AI

TL;DR: PIWM模型在保证性能的同时显著减小了世界模型的规模，并在多个指标上超越基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决世界模型在规模和性能之间的权衡问题，特别是在边缘设备上的部署。

Method: 提出了一种紧凑的物理信息鸟瞰图世界模型(PIWM)，使用Soft Mask改进动态目标建模和未来预测，并引入Warm Start技术提高推理质量。

Result: 在相同参数规模下，PIWM超越基线60.6%；即使与最大的基线模型相比，最小的PIWM模型(130M Soft Mask)也取得了7.4%的性能提升和28%的推理速度提升。

Conclusion: PIWM模型有效地解决了世界模型的规模与性能问题，为边缘设备上的部署提供了可行方案。

Abstract: A major challenge in deploying world models is the trade-off between size and
performance. Large world models can capture rich physical dynamics but require
massive computing resources, making them impractical for edge devices. Small
world models are easier to deploy but often struggle to learn accurate physics,
leading to poor predictions. We propose the Physics-Informed BEV World Model
(PIWM), a compact model designed to efficiently capture physical interactions
in bird's-eye-view (BEV) representations. PIWM uses Soft Mask during training
to improve dynamic object modeling and future prediction. We also introduce a
simple yet effective technique, Warm Start, for inference to enhance prediction
quality with a zero-shot model. Experiments show that at the same parameter
scale (400M), PIWM surpasses the baseline by 60.6% in weighted overall score.
Moreover, even when compared with the largest baseline model (400M), the
smallest PIWM (130M Soft Mask) achieves a 7.4% higher weighted overall score
with a 28% faster inference speed.

</details>


### [10] [Reasoning Models Can be Accurately Pruned Via Chain-of-Thought Reconstruction](https://arxiv.org/abs/2509.12464)
*Ryan Lucas,Kayhan Behdin,Zhipeng Wang,Qingquan Song,Shao Tang,Rahul Mazumder*

Main category: cs.AI

TL;DR: 针对推理语言模型推理时间长的问题，论文提出了一种推理感知压缩方法RAC，通过联合重建输入和模型的链式思考轨迹来提高模型性能，并在SparseGPT等现有剪枝方法上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 推理语言模型的推理时间长，部署成本高。

Method: 提出了一种推理感知压缩(RAC)方法，联合重建输入和模型链式思考轨迹进行模型剪枝。

Result: RAC方法显著提升了现有剪枝方法(如SparseGPT)的性能。

Conclusion: RAC方法有效解决了推理语言模型推理时间长的问题，为大规模部署推理语言模型提供了新的方向。

Abstract: Reasoning language models such as DeepSeek-R1 produce long chain-of-thought
traces during inference time which make them costly to deploy at scale. We show
that using compression techniques such as neural network pruning produces
greater performance loss than in typical language modeling tasks, and in some
cases can make the model slower since they cause the model to produce more
thinking tokens but with worse performance. We show that this is partly due to
the fact that standard LLM pruning methods often focus on input reconstruction,
whereas reasoning is a decode-dominated task. We introduce a simple, drop-in
fix: during pruning we jointly reconstruct activations from the input and the
model's on-policy chain-of-thought traces. This "Reasoning-Aware Compression"
(RAC) integrates seamlessly into existing pruning workflows such as SparseGPT,
and boosts their performance significantly. Code reproducing the results in the
paper can be found at: https://github.com/RyanLucas3/RAC

</details>


### [11] [Empowering Clinical Trial Design through AI: A Randomized Evaluation of PowerGPT](https://arxiv.org/abs/2509.12471)
*Yiwen Lu,Lu Li,Dazheng Zhang,Xinyao Jian,Tingyin Wang,Siqi Chen,Yuqing Lei,Jiayi Tong,Zhaohan Xi,Haitao Chu,Chongliang Luo,Alexis Ogdie,Brian Athey,Alparslan Turan,Michael Abramoff,Joseph C Cappelleri,Hua Xu,Yun Lu,Jesse Berlin,Daniel I. Sessler,David A. Asch,Xiaoqian Jiang,Yong Chen*

Main category: cs.AI

TL;DR: PowerGPT是一个AI系统，能自动化临床试验设计中的检验选择和样本量估计，显著提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的样本量计算方法复杂且依赖统计专业知识，阻碍了许多研究人员。

Method: 将大型语言模型与统计引擎集成，自动化测试选择和样本量估计。

Result: PowerGPT显著提高了任务完成率和准确性，并减少了平均完成时间，对统计学家和非统计学家都有益。

Conclusion: PowerGPT是一种可扩展的AI驱动方法，提高了临床研究中统计功效分析的可及性、效率和准确性。

Abstract: Sample size calculations for power analysis are critical for clinical
research and trial design, yet their complexity and reliance on statistical
expertise create barriers for many researchers. We introduce PowerGPT, an
AI-powered system integrating large language models (LLMs) with statistical
engines to automate test selection and sample size estimation in trial design.
In a randomized trial to evaluate its effectiveness, PowerGPT significantly
improved task completion rates (99.3% vs. 88.9% for test selection, 99.3% vs.
77.8% for sample size calculation) and accuracy (94.1% vs. 55.4% in sample size
estimation, p < 0.001), while reducing average completion time (4.0 vs. 9.3
minutes, p < 0.001). These gains were consistent across various statistical
tests and benefited both statisticians and non-statisticians as well as
bridging expertise gaps. Already under deployment across multiple institutions,
PowerGPT represents a scalable AI-driven approach that enhances accessibility,
efficiency, and accuracy in statistical power analysis for clinical research.

</details>


### [12] [Physical Complexity of a Cognitive Artifact](https://arxiv.org/abs/2509.12495)
*Gülce Kardeş,David Krakauer,Joshua Grochow*

Main category: cs.AI

TL;DR: 本文量化评估了七巧板的难度，并研究了不同策略如何改变其复杂性。


<details>
  <summary>Details</summary>
Motivation: 认知科学和理论计算机科学都试图对任务的难度进行分类和解释。

Method: 通过分析七巧板搜索树的度来量化评估任务难度，并逐步改进试错搜索，加入预处理、值排序、变量排序和剪枝等策略。

Result: 不同策略的运用降低了七巧板的有效时间复杂度。

Conclusion: 提出了一种智能模型，该模型是算法库，它利用了思维和物质的能力。

Abstract: Cognitive science and theoretical computer science both seek to classify and
explain the difficulty of tasks. Mechanisms of intelligence are those that
reduce task difficulty. Here we map concepts from the computational complexity
of a physical puzzle, the Soma Cube, onto cognitive problem-solving strategies
through a ``Principle of Materiality''. By analyzing the puzzle's branching
factor, measured through search tree outdegree, we quantitatively assess task
difficulty and systematically examine how different strategies modify
complexity. We incrementally refine a trial-and-error search by layering
preprocessing (cognitive chunking), value ordering (cognitive free-sorting),
variable ordering (cognitive scaffolding), and pruning (cognitive inference).
We discuss how the competent use of artifacts reduces effective time complexity
by exploiting physical constraints and propose a model of intelligence as a
library of algorithms that recruit the capabilities of both mind and matter.

</details>


### [13] [A Dimensionality-Reduced XAI Framework for Roundabout Crash Severity Insights](https://arxiv.org/abs/2509.12524)
*Rohit Chakraborty,Subasish Das*

Main category: cs.AI

TL;DR: 该研究分析了俄亥俄州2017-2021年环岛事故，发现黑暗、湿滑路面和高限速与固定物或角度碰撞事件同时发生时，事故严重程度更高；在清晰、低速环境下，事故严重程度较低。


<details>
  <summary>Details</summary>
Motivation: 环岛虽然减少了严重事故，但风险模式随条件变化而异。

Method: 采用两步可解释工作流程：1. 运用聚类对应分析(CCA)识别共现因素，得到四种事故模式；2. 使用基于树的严重程度模型和SHAP方法量化模式内外的损伤驱动因素。

Result: 发现黑暗、湿滑路面和高限速与固定物或角度碰撞事件同时发生时，事故严重程度更高；在清晰、低速环境下，事故严重程度较低。不同模式下，事故原因分别与入口处的让行失败、多车道环岛内的不当操作以及减速过程中的追尾有关。

Conclusion: 该工作流程将模式发现与案例级解释相结合，可用于场地筛选、对策选择和审计报告。为公共安全分析中的可用XAI提供了一个实用模板。

Abstract: Roundabouts reduce severe crashes, yet risk patterns vary by conditions. This
study analyzes 2017-2021 Ohio roundabout crashes using a two-step, explainable
workflow. Cluster Correspondence Analysis (CCA) identifies co-occurring factors
and yields four crash patterns. A tree-based severity model is then interpreted
with SHAP to quantify drivers of injury within and across patterns. Results
show higher severity when darkness, wet surfaces, and higher posted speeds
coincide with fixed-object or angle events, and lower severity in clear,
low-speed settings. Pattern-specific explanations highlight mechanisms at
entries (fail-to-yield, gap acceptance), within multi-lane circulation
(improper maneuvers), and during slow-downs (rear-end). The workflow links
pattern discovery with case-level explanations, supporting site screening,
countermeasure selection, and audit-ready reporting. The contribution to
Information Systems is a practical template for usable XAI in public safety
analytics.

</details>


### [14] [zELO: ELO-inspired Training Method for Rerankers and Embedding Models](https://arxiv.org/abs/2509.12541)
*Nicholas Pipitone,Ghita Houir Alami,Advaith Avadhanam,Anton Kaminskyi,Ashley Khoo*

Main category: cs.AI

TL;DR: zELO方法优化检索性能，训练出zerank-1和zerank-1-small模型，在多个领域超越闭源模型。


<details>
  <summary>Details</summary>
Motivation: 提升检索性能，尤其是在多个领域的0-shot性能。

Method: 基于Thurstone模型的zELO训练方法，使用无监督数据训练。

Result: 在金融、法律、代码和STEM领域，zerank模型在NDCG@10和Recall上超越闭源reranker，并在跨领域和私有数据集上保持0-shot性能。

Conclusion: zELO方法有效，训练出的模型性能优异且具有通用性。

Abstract: We introduce a novel training methodology named zELO, which optimizes
retrieval performance via the analysis that ranking tasks are statically
equivalent to a Thurstone model. Based on the zELO method, we use unsupervised
data in order train a suite of state-of-the-art open-weight reranker models:
zerank-1 and zerank-1-small. These models achieve the highest retrieval scores
in multiple domains, including finance, legal, code, and STEM, outperforming
closed-source proprietary rerankers on both NDCG@10 and Recall. These models
also demonstrate great versatility, maintaining their 0-shot performance on
out-of-domain and private customer datasets. The training data included 112,000
queries and 100 documents per query, and was trained end-to-end from
unannotated queries and documents in less than 10,000 H100-hours.

</details>


### [15] [Human + AI for Accelerating Ad Localization Evaluation](https://arxiv.org/abs/2509.12543)
*Harshit Rajgarhia,Shivali Dalmia,Mengyang Zhao,Mukherji Abhishek,Kiran Ganesh*

Main category: cs.AI

TL;DR: 该论文提出一种结合自动化组件和人工监督的广告本地化框架，通过场景文本检测、修复、机器翻译和文本重排，提高广告本地化效率。


<details>
  <summary>Details</summary>
Motivation: 现有广告多语言适配方法难以兼顾视觉一致性和语言准确性。

Method: 该框架集成场景文本检测、图像修复、机器翻译和文本重排技术，并结合人工审核。

Result: 实验证明该方法能生成语义准确、视觉一致的本地化广告。

Conclusion: 该框架有效提高了广告本地化效率，适用于实际应用。

Abstract: Adapting advertisements for multilingual audiences requires more than simple
text translation; it demands preservation of visual consistency, spatial
alignment, and stylistic integrity across diverse languages and formats. We
introduce a structured framework that combines automated components with human
oversight to address the complexities of advertisement localization. To the
best of our knowledge, this is the first work to integrate scene text
detection, inpainting, machine translation (MT), and text reimposition
specifically for accelerating ad localization evaluation workflows. Qualitative
results across six locales demonstrate that our approach produces semantically
accurate and visually coherent localized advertisements, suitable for
deployment in real-world workflows.

</details>
