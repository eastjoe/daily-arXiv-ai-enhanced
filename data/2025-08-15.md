<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions](https://arxiv.org/abs/2508.10047)
*Ziyang Xiao,Jingrong Xie,Lilin Xu,Shisi Guan,Jingyan Zhu,Xiongwei Han,Xiaojin Fu,WingYin Yu,Han Wu,Wei Shi,Qingcan Kang,Jiahui Duan,Tao Zhong,Mingxuan Yuan,Jia Zeng,Yuan Wang,Gang Chen,Dongxiang Zhang*

Main category: cs.AI

TL;DR: 该综述文章探讨了大型语言模型在优化建模中的应用，改进基准数据集并构建在线资源，为该领域研究提供全面概述。


<details>
  <summary>Details</summary>
Motivation: 优化建模在解决实际问题中具有重要作用，但需要专业知识。LLM为自动化建模提供了新的机会。

Method: 对现有文献进行综述，构建新的基准数据集，并开发在线资源门户。

Result: 对LLM在优化建模中的应用进行了全面的综述，发现了现有基准数据集存在高错误率的问题，构建了新的基准数据集和在线资源门户，并指出了未来的研究方向。

Conclusion: 这篇综述文章对大型语言模型(LLM)在优化建模中的应用进行了全面的回顾，分析了现有方法的局限性，并指出了未来的研究方向。文章还构建了一个新的基准数据集和在线资源门户。

Abstract: By virtue of its great utility in solving real-world problems, optimization
modeling has been widely employed for optimal decision-making across various
sectors, but it requires substantial expertise from operations research
professionals. With the advent of large language models (LLMs), new
opportunities have emerged to automate the procedure of mathematical modeling.
This survey presents a comprehensive and timely review of recent advancements
that cover the entire technical stack, including data synthesis and fine-tuning
for the base model, inference frameworks, benchmark datasets, and performance
evaluation. In addition, we conducted an in-depth analysis on the quality of
benchmark datasets, which was found to have a surprisingly high error rate. We
cleaned the datasets and constructed a new leaderboard with fair performance
evaluation in terms of base LLM model and datasets. We also build an online
portal that integrates resources of cleaned datasets, code and paper repository
to benefit the community. Finally, we identify limitations in current
methodologies and outline future research opportunities.

</details>


### [2] [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
*Sattvik Sahai,Prasoon Goyal,Michael Johnston,Anna Gottardi,Yao Lu,Lucy Hu,Luke Dai,Shaohua Liu,Samyuth Sagi,Hangjie Shi,Desheng Zhang,Lavina Vaz,Leslie Ball,Maureen Murray,Rahul Gupta,Shankar Ananthakrishna*

Main category: cs.AI

TL;DR: Nova AI挑战赛推动AI软件开发安全性的提升，各团队在安全对齐等方面取得突破。


<details>
  <summary>Details</summary>
Motivation: 应对AI软件开发安全挑战。

Method: 通过对抗性锦标赛，红队与AI编码助手进行多轮对话，评估自动化红队和安全对齐方法。

Result: 开发了基于推理的安全对齐、鲁棒的模型防护、多轮越狱和高效的LLM探测等先进技术。

Conclusion: 亚马逊Nova AI挑战赛促进了AI软件开发安全性的进步，大学团队和亚马逊团队在安全对齐、模型防护和LLM探测等方面取得了显著进展。

Abstract: AI systems for software development are rapidly gaining prominence, yet
significant challenges remain in ensuring their safety. To address this, Amazon
launched the Trusted AI track of the Amazon Nova AI Challenge, a global
competition among 10 university teams to drive advances in secure AI. In the
challenge, five teams focus on developing automated red teaming bots, while the
other five create safe AI assistants. This challenge provides teams with a
unique platform to evaluate automated red-teaming and safety alignment methods
through head-to-head adversarial tournaments where red teams have multi-turn
conversations with the competing AI coding assistants to test their safety
alignment. Along with this, the challenge provides teams with a feed of high
quality annotated data to fuel iterative improvement. Throughout the challenge,
teams developed state-of-the-art techniques, introducing novel approaches in
reasoning-based safety alignment, robust model guardrails, multi-turn
jail-breaking, and efficient probing of large language models (LLMs). To
support these efforts, the Amazon Nova AI Challenge team made substantial
scientific and engineering investments, including building a custom baseline
coding specialist model for the challenge from scratch, developing a tournament
orchestration service, and creating an evaluation harness. This paper outlines
the advancements made by university teams and the Amazon Nova AI Challenge team
in addressing the safety challenges of AI for software development,
highlighting this collaborative effort to raise the bar for AI safety.

</details>


### [3] [MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection](https://arxiv.org/abs/2508.10143)
*Alexandru-Andrei Avram,Adrian Groza,Alexandru Lecu*

Main category: cs.AI

TL;DR: 多智能体系统利用关系抽取准确检测新闻中的不实信息，取得显著成果。


<details>
  <summary>Details</summary>
Motivation: 解决数字平台上虚假信息传播的问题。

Method: 该系统结合机器学习、维基百科知识检查、连贯性检测和网络数据分析四个智能体，通过模型上下文协议进行协调。

Result: 多智能体集成系统达到95.3%的准确率和0.964的F1分数，优于个体智能体和传统方法。加权聚合方法优于算法阈值优化。

Conclusion: 多智能体系统在新闻文章中检测不实信息方面取得95.3%的准确率和0.964的F1分数，优于单个智能体和传统方法。

Abstract: The large spread of disinformation across digital platforms creates
significant challenges to information integrity. This paper presents a
multi-agent system that uses relation extraction to detect disinformation in
news articles, focusing on titles and short text snippets. The proposed Agentic
AI system combines four agents: (i) a machine learning agent (logistic
regression), (ii) a Wikipedia knowledge check agent (which relies on named
entity recognition), (iii) a coherence detection agent (using LLM prompt
engineering), and (iv) a web-scraped data analyzer that extracts relational
triplets for fact checking. The system is orchestrated via the Model Context
Protocol (MCP), offering shared context and live learning across components.
Results demonstrate that the multi-agent ensemble achieves 95.3% accuracy with
an F1 score of 0.964, significantly outperforming individual agents and
traditional approaches. The weighted aggregation method, mathematically derived
from individual agent misclassification rates, proves superior to algorithmic
threshold optimization. The modular architecture makes the system easily
scalable, while also maintaining details of the decision processes.

</details>


### [4] [Agentic AI Frameworks: Architectures, Protocols, and Design Challenges](https://arxiv.org/abs/2508.10146)
*Hana Derouiche,Zaki Brahmi,Haithem Mazeni*

Main category: cs.AI

TL;DR: 本文综述了代理人工智能框架，分析了其架构、通信机制等，并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的出现带来了代理人工智能的变革性范式，本文旨在对代理人工智能框架进行全面综述，并提出未来的研究方向。

Method: 本文对代理人工智能框架进行了系统的回顾和比较分析，并对几种代理通信协议进行了深入分析。

Result: 本文建立了代理人工智能系统的基础分类法，并提出了提高可扩展性、鲁棒性和互操作性的未来研究方向。

Conclusion: 本文对领先的代理人工智能框架（包括CrewAI、LangGraph、AutoGen、Semantic Kernel、Agno、Google ADK和MetaGPT）进行了系统的回顾和比较分析，评估了它们的体系结构原则、通信机制、内存管理、安全防护和与面向服务的计算范例的一致性，并确定了该领域的关键限制、新兴趋势和开放性挑战。

Abstract: The emergence of Large Language Models (LLMs) has ushered in a transformative
paradigm in artificial intelligence, Agentic AI, where intelligent agents
exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent
coordination. This paper provides a systematic review and comparative analysis
of leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen,
Semantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural
principles, communication mechanisms, memory management, safety guardrails, and
alignment with service-oriented computing paradigms. Furthermore, we identify
key limitations, emerging trends, and open challenges in the field. To address
the issue of agent communication, we conduct an in-depth analysis of protocols
such as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network
Protocol (ANP), and Agora. Our findings not only establish a foundational
taxonomy for Agentic AI systems but also propose future research directions to
enhance scalability, robustness, and interoperability. This work serves as a
comprehensive reference for researchers and practitioners working to advance
the next generation of autonomous AI systems.

</details>


### [5] [Improving and Evaluating Open Deep Research Agents](https://arxiv.org/abs/2508.10152)
*Doaa Allabadi,Kyle Bradbury,Jordan M. Malof*

Main category: cs.AI

TL;DR: 改进的开放源码深度研究代理ODR+在基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 评估现有开放源码深度研究代理的能力，并改进其性能。

Method: 对现有的开放源码深度研究代理ODR进行了改进，并在BrowseComp-Small基准测试上进行了评估。

Result: 改进后的ODR+模型在BrowseComp-Small基准测试中取得了10%的成功率，超过了其他闭源和开源系统。

Conclusion: 这项工作改进了开放源码深度研究代理ODR，使其在BrowseComp-Small基准测试中取得了最先进的10%的成功率，优于其他闭源系统。

Abstract: We focus here on Deep Research Agents (DRAs), which are systems that can take
a natural language prompt from a user, and then autonomously search for, and
utilize, internet-based content to address the prompt. Recent DRAs have
demonstrated impressive capabilities on public benchmarks however, recent
research largely involves proprietary closed-source systems. At the time of
this work, we only found one open-source DRA, termed Open Deep Research (ODR).
In this work we adapt the challenging recent BrowseComp benchmark to compare
ODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small),
comprising a subset of BrowseComp, as a more computationally-tractable DRA
benchmark for academic labs. We benchmark ODR and two other proprietary systems
on BC-Small: one system from Anthropic and one system from Google. We find that
all three systems achieve 0% accuracy on the test set of 60 questions. We
introduce three strategic improvements to ODR, resulting in the ODR+ model,
which achieves a state-of-the-art 10% success rate on BC-Small among both
closed-source and open-source systems. We report ablation studies indicating
that all three of our improvements contributed to the success of ODR+.

</details>


### [6] [Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization](https://arxiv.org/abs/2508.10164)
*Bin Hong,Jiayu Liu,Zhenya Huang,Kai Zhang,Mengdi Zhang*

Main category: cs.AI

TL;DR: 通过长度控制偏好优化，有效缩短大型推理模型的输出长度，提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的高效推理方法常常以牺牲推理质量或需要大量资源为代价。

Method: 分析生成路径分布，通过难度估计过滤生成的轨迹，并基于Bradley-Terry损失框架分析不同偏好优化方法的目标收敛行为。

Result: LCPO方法将平均输出长度减少了50%以上，同时保持了推理性能。

Conclusion: 该论文提出了一种名为长度控制偏好优化（LCPO）的方法，有效减少大型推理模型（LRM）的输出长度，同时保持推理性能。

Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated strong
performance on complex tasks through long Chain-of-Thought (CoT) reasoning.
However, their lengthy outputs increase computational costs and may lead to
overthinking, raising challenges in balancing reasoning effectiveness and
efficiency. Current methods for efficient reasoning often compromise reasoning
quality or require extensive resources. This paper investigates efficient
methods to reduce the generation length of LRMs. We analyze generation path
distributions and filter generated trajectories through difficulty estimation.
Subsequently, we analyze the convergence behaviors of the objectives of various
preference optimization methods under a Bradley-Terry loss based framework.
Based on the analysis, we propose Length Controlled Preference Optimization
(LCPO) that directly balances the implicit reward related to NLL loss. LCPO can
effectively learn length preference with limited data and training. Extensive
experiments demonstrate that our approach significantly reduces the average
output length by over 50\% across multiple benchmarks while maintaining the
reasoning performance. Our work highlights the potential for computationally
efficient approaches in guiding LRMs toward efficient reasoning.

</details>


### [7] [KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems](https://arxiv.org/abs/2508.10177)
*Stepan Kulibaba,Artem Dzhalilov,Roman Pakhomov,Oleg Svidchenko,Alexander Gasnikov,Aleksei Shpilman*

Main category: cs.AI

TL;DR: KompeteAI是一个新的AutoML框架，通过改进的探索策略和高效的执行方法，显著提高了AutoML的性能。


<details>
  <summary>Details</summary>
Motivation: 克服现有LLM-AutoML系统中探索策略受限（单次方法缺乏多样性，MCTS方法无法重组强部分解决方案）和执行瓶颈（冗长的代码验证周期）的问题。

Method: 动态解决方案空间探索、解决方案合并、检索增强生成（RAG）、预测评分模型和加速调试方法。

Result: KompeteAI在主要的AutoML基准MLE-Bench上平均超过领先方法（例如，RD-agent，AIDE和Ml-Master）3％，并在新的Kompete-bench基准测试上也取得了最先进的结果。其管道评估速度提高了6.9倍。

Conclusion: KompeteAI，一个新的AutoML框架，通过动态解决方案空间探索、解决方案合并和检索增强生成（RAG）等方法，克服了现有LLM-AutoML系统探索策略受限和执行瓶颈的问题，并在MLE-Bench和Kompete-bench基准测试中取得了最先进的结果。

Abstract: Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive
capabilities but face significant limitations such as constrained exploration
strategies and a severe execution bottleneck. Exploration is hindered by
one-shot methods lacking diversity and Monte Carlo Tree Search (MCTS)
approaches that fail to recombine strong partial solutions. The execution
bottleneck arises from lengthy code validation cycles that stifle iterative
refinement. To overcome these challenges, we introduce KompeteAI, a novel
AutoML framework with dynamic solution space exploration. Unlike previous MCTS
methods that treat ideas in isolation, KompeteAI introduces a merging stage
that composes top candidates. We further expand the hypothesis space by
integrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle
notebooks and arXiv papers to incorporate real-world strategies. KompeteAI also
addresses the execution bottleneck via a predictive scoring model and an
accelerated debugging method, assessing solution potential using early stage
metrics to avoid costly full-code execution. This approach accelerates pipeline
evaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent,
AIDE, and Ml-Master) by an average of 3\% on the primary AutoML benchmark,
MLE-Bench. Additionally, we propose Kompete-bench to address limitations in
MLE-Bench, where KompeteAI also achieves state-of-the-art results

</details>


### [8] [Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence](https://arxiv.org/abs/2508.10241)
*Mark Zilberman*

Main category: cs.AI

TL;DR: 利用事件熵势概念增强AI中的不确定性量化、决策和可解释性，应用于多个领域。


<details>
  <summary>Details</summary>
Motivation: 提升人工智能中的不确定性量化、决策和可解释性。

Method: 将物理学中事件熵势的概念应用于AI，引入了一种事件中心度量，并对原始定义和AI调整后的定义进行了形式化，后者强调条件期望以解释反事实场景。

Result: 该框架为人工智能中的不确定性管理提供了一种理论扎实、可解释且通用的方法，弥合了热力学、信息论和机器学习之间的原理。

Conclusion: 该工作展示了事件熵势概念如何增强人工智能中的不确定性量化、决策和可解释性，并将其应用于策略评估、内在奖励设计、可解释AI和异常检测。

Abstract: This work demonstrates how the concept of the entropic potential of events --
a parameter quantifying the influence of discrete events on the expected future
entropy of a system -- can enhance uncertainty quantification, decision-making,
and interpretability in artificial intelligence (AI). Building on its original
formulation in physics, the framework is adapted for AI by introducing an
event-centric measure that captures how actions, observations, or other
discrete occurrences impact uncertainty at future time horizons. Both the
original and AI-adjusted definitions of entropic potential are formalized, with
the latter emphasizing conditional expectations to account for counterfactual
scenarios. Applications are explored in policy evaluation, intrinsic reward
design, explainable AI, and anomaly detection, highlighting the metric's
potential to unify and strengthen uncertainty modeling in intelligent systems.
Conceptual examples illustrate its use in reinforcement learning, Bayesian
inference, and anomaly detection, while practical considerations for
computation in complex AI models are discussed. The entropic potential
framework offers a theoretically grounded, interpretable, and versatile
approach to managing uncertainty in AI, bridging principles from
thermodynamics, information theory, and machine learning.

</details>


### [9] [Why Cannot Large Language Models Ever Make True Correct Reasoning?](https://arxiv.org/abs/2508.10265)
*Jingde Cheng*

Main category: cs.AI

TL;DR: LLM的理解和推理能力是错觉，其本质限制决定了其无法进行真正正确的推理。


<details>
  <summary>Details</summary>
Motivation: 纠正对LLM“理解能力”和“推理能力”的误解。

Method: 分析LLM的工作原理，论证其无法实现真正正确的推理。

Result: 证明LLM无法具备真正的理解和推理能力。

Conclusion: 大型语言模型(LLM)不具备真正的理解和推理能力，其所谓的“理解”和“推理”能力是基于其工作原理的本质限制而产生的错觉。

Abstract: Recently, with the application progress of AIGC tools based on large language
models (LLMs), led by ChatGPT, many AI experts and more non-professionals are
trumpeting the "understanding ability" and "reasoning ability" of the LLMs. The
present author considers that the so-called "understanding ability" and
"reasoning ability" of LLMs are just illusions of those people who with vague
concepts. In fact, the LLMs can never have the true understanding ability and
true reasoning ability. This paper intents to explain that, because the
essential limitations of their working principle, the LLMs can never have the
ability of true correct reasoning.

</details>


### [10] [Promoting Efficient Reasoning with Verifiable Stepwise Reward](https://arxiv.org/abs/2508.10293)
*Chuhuai Yue,Chengqi Dong,Yinan Gao,Hang He,Jiajun Chai,Guojun Yin,Wei Lin*

Main category: cs.AI

TL;DR: 通过改进奖励机制，有效解决了大型推理模型的过度思考问题，提高了推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的LRMs在复杂推理任务中取得了显著进展，但常常过度思考，影响效率。现有高效推理方法依赖于准确的任务评估，限制了其灵活性和可靠性。

Method: 提出了一种基于规则的可验证逐步奖励机制（VSRM），并将其与PPO和Reinforce++算法结合使用。

Result: 实验证明，该方法在AIME24和AIME25等数学推理基准测试中显著减少了输出长度，同时保持了原始推理性能，有效地抑制了无效步骤，促进了有效推理。

Conclusion: 该论文提出了一种基于规则的可验证逐步奖励机制（VSRM），通过奖励有效的推理步骤并惩罚无效步骤来解决大型推理模型（LRMs）的过度思考问题，从而在保持推理性能的同时显著减少输出长度。

Abstract: Large reasoning models (LRMs) have recently achieved significant progress in
complex reasoning tasks, aided by reinforcement learning with verifiable
rewards. However, LRMs often suffer from overthinking, expending excessive
computation on simple problems and reducing efficiency. Existing efficient
reasoning methods typically require accurate task assessment to preset token
budgets or select reasoning modes, which limits their flexibility and
reliability. In this work, we revisit the essence of overthinking and identify
that encouraging effective steps while penalizing ineffective ones is key to
its solution. To this end, we propose a novel rule-based verifiable stepwise
reward mechanism (VSRM), which assigns rewards based on the performance of
intermediate states in the reasoning trajectory. This approach is intuitive and
naturally fits the step-by-step nature of reasoning tasks. We conduct extensive
experiments on standard mathematical reasoning benchmarks, including AIME24 and
AIME25, by integrating VSRM with PPO and Reinforce++. Results show that our
method achieves substantial output length reduction while maintaining original
reasoning performance, striking an optimal balance between efficiency and
accuracy. Further analysis of overthinking frequency and pass@k score before
and after training demonstrates that our approach in deed effectively
suppresses ineffective steps and encourages effective reasoning, fundamentally
alleviating the overthinking problem. All code will be released upon
acceptance.

</details>


### [11] [A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering](https://arxiv.org/abs/2508.10337)
*Chenliang Zhang,Lin Wang,Yuanyuan Lu,Yusheng Qi,Kexin Wang,Peixu Hou,Wenshi Chen*

Main category: cs.AI

TL;DR: Dianping-Trust-Safety团队通过结合多种技术，在META CRAG-MM挑战赛中取得优异成绩，特别是任务一中大幅领先。


<details>
  <summary>Details</summary>
Motivation: 为了应对META CRAG-MM挑战赛中多模态、多轮问答的需求，该团队构建了一个综合的检索增强生成系统。

Method: 该方案结合了视觉大型语言模型、知识蒸馏、强化学习和课程学习等技术，并利用网络搜索API来整合外部知识。

Result: 在挑战赛的任务一中获得第一名（领先52.38%），在任务三中获得第三名。

Conclusion: 该论文描述了Dianping-Trust-Safety团队在META CRAG-MM挑战赛中的解决方案，并在多个任务中取得了领先的成绩，特别是任务一中以显著优势获得第一名。

Abstract: This paper describes the solutions of the Dianping-Trust-Safety team for the
META CRAG-MM challenge. The challenge requires building a comprehensive
retrieval-augmented generation system capable for multi-modal multi-turn
question answering. The competition consists of three tasks: (1) answering
questions using structured data retrieved from an image-based mock knowledge
graph, (2) synthesizing information from both knowledge graphs and web search
results, and (3) handling multi-turn conversations that require context
understanding and information aggregation from multiple sources. For Task 1,
our solution is based on the vision large language model, enhanced by
supervised fine-tuning with knowledge distilled from GPT-4.1. We further
applied curriculum learning strategies to guide reinforcement learning,
resulting in improved answer accuracy and reduced hallucination. For Task 2 and
Task 3, we additionally leveraged web search APIs to incorporate external
knowledge, enabling the system to better handle complex queries and multi-turn
conversations. Our approach achieved 1st place in Task 1 with a significant
lead of 52.38\%, and 3rd place in Task 3, demonstrating the effectiveness of
the integration of curriculum learning with reinforcement learning in our
training pipeline.

</details>


### [12] [Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach](https://arxiv.org/abs/2508.10340)
*Chak Lam Shek,Guangyao Shi,Pratap Tokekar*

Main category: cs.AI

TL;DR: 改进HATRPO算法，通过优化KL阈值分配策略提升多智能体强化学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有HATRPO方法中为所有智能体分配相同KL阈值导致训练缓慢和局部最优的问题，尤其是在异构环境中。

Method: 提出两种分配KL散度阈值的方法：基于KKT的HATRPO-W方法和基于贪婪算法的HATRPO-G方法。

Result: HATRPO-W和HATRPO-G方法都将HATRPO的最终性能提高了22.5%以上，HATRPO-W学习动态更稳定。

Conclusion: 两种改进的异构智能体信任区域策略优化方法（HATRPO-W和HATRPO-G）显著提高了HATRPO的性能，实现了更快的收敛速度和更高的最终奖励。HATRPO-W在学习动态方面更加稳定，方差更低。

Abstract: Multi-agent reinforcement learning (MARL) requires coordinated and stable
policy updates among interacting agents. Heterogeneous-Agent Trust Region
Policy Optimization (HATRPO) enforces per-agent trust region constraints using
Kullback-Leibler (KL) divergence to stabilize training. However, assigning each
agent the same KL threshold can lead to slow and locally optimal updates,
especially in heterogeneous settings. To address this limitation, we propose
two approaches for allocating the KL divergence threshold across agents:
HATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes
threshold assignment under global KL constraints, and HATRPO-G, a greedy
algorithm that prioritizes agents based on improvement-to-divergence ratio. By
connecting sequential policy optimization with constrained threshold
scheduling, our approach enables more flexible and effective learning in
heterogeneous-agent settings. Experimental results demonstrate that our methods
significantly boost the performance of HATRPO, achieving faster convergence and
higher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and
HATRPO-G achieve comparable improvements in final performance, each exceeding
22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as
reflected by its lower variance.

</details>


### [13] [What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles](https://arxiv.org/abs/2508.10358)
*Mengtao Zhou,Sifan Wu,Huan Zhang,Qi Sima,Bang Liu*

Main category: cs.AI

TL;DR: 研究发现LLM在富有想象力的推理方面能力有限，与人类存在差距，并提出一个新的基准和评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法捕捉富有想象力的推理过程的动态和探索性本质。

Method: 提出了一种基于经典“海龟汤”游戏的综合研究框架，包括基准、代理和评估协议。构建了TurtleSoup-Bench，一个大型双语交互式基准，包含800个难题。并提出了Mosaic-Agent，用于评估LLM在这种环境下的性能。

Result: 实验揭示了LLM的推理能力局限、常见的失败模式以及与人类相比的显著性能差距。

Conclusion: 大型语言模型(LLM)在信息稀疏环境下的富有想象力的推理能力存在局限性，与人类相比差距显著。

Abstract: We investigate the capacity of Large Language Models (LLMs) for imaginative
reasoning--the proactive construction, testing, and revision of hypotheses in
information-sparse environments. Existing benchmarks, often static or focused
on social deduction, fail to capture the dynamic, exploratory nature of this
reasoning process. To address this gap, we introduce a comprehensive research
framework based on the classic "Turtle Soup" game, integrating a benchmark, an
agent, and an evaluation protocol. We present TurtleSoup-Bench, the first
large-scale, bilingual, interactive benchmark for imaginative reasoning,
comprising 800 turtle soup puzzles sourced from both the Internet and expert
authors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'
performance in this setting. To evaluate reasoning quality, we develop a
multi-dimensional protocol measuring logical consistency, detail completion,
and conclusion alignment. Experiments with leading LLMs reveal clear capability
limits, common failure patterns, and a significant performance gap compared to
humans. Our work offers new insights into LLMs' imaginative reasoning and
establishes a foundation for future research on exploratory agent behavior.

</details>


### [14] [LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval](https://arxiv.org/abs/2508.10391)
*Yaoze Zhang,Rong Wu,Pinlong Cai,Xiaoman Wang,Guohang Yan,Song Mao,Ding Wang,Botian Shi*

Main category: cs.AI

TL;DR: LeanRAG是一个新的RAG框架，通过语义聚合和结构引导检索，解决了现有知识图谱RAG方法的效率和准确性问题，并在多个基准测试中取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的基于知识图谱的RAG方法存在两个关键挑战：高层次概念性摘要之间缺乏显式关系，以及检索过程效率低下。

Method: LeanRAG框架采用了一种新颖的语义聚合算法，构建了一个完全可导航的语义网络，并结合自下而上的结构引导检索策略，从而提高检索效率和准确性。

Result: LeanRAG在四个具有挑战性的QA基准测试中显著优于现有方法，并减少了46%的检索冗余。

Conclusion: LeanRAG框架通过结合知识聚合和检索策略，显著提高了基于知识图谱的RAG方法的性能，在四个具有挑战性的QA基准测试中取得了优于现有方法的结果，并减少了46%的检索冗余。

Abstract: Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large
Language Models by leveraging external knowledge, whereas the effectiveness is
often compromised by the retrieval of contextually flawed or incomplete
information. To address this, knowledge graph-based RAG methods have evolved
towards hierarchical structures, organizing knowledge into multi-level
summaries. However, these approaches still suffer from two critical,
unaddressed challenges: high-level conceptual summaries exist as disconnected
``semantic islands'', lacking the explicit relations needed for cross-community
reasoning; and the retrieval process itself remains structurally unaware, often
degenerating into an inefficient flat search that fails to exploit the graph's
rich topology. To overcome these limitations, we introduce LeanRAG, a framework
that features a deeply collaborative design combining knowledge aggregation and
retrieval strategies. LeanRAG first employs a novel semantic aggregation
algorithm that forms entity clusters and constructs new explicit relations
among aggregation-level summaries, creating a fully navigable semantic network.
Then, a bottom-up, structure-guided retrieval strategy anchors queries to the
most relevant fine-grained entities and then systematically traverses the
graph's semantic pathways to gather concise yet contextually comprehensive
evidence sets. The LeanRAG can mitigate the substantial overhead associated
with path retrieval on graphs and minimizes redundant information retrieval.
Extensive experiments on four challenging QA benchmarks with different domains
demonstrate that LeanRAG significantly outperforming existing methods in
response quality while reducing 46\% retrieval redundancy. Code is available
at: https://github.com/RaZzzyz/LeanRAG

</details>


### [15] [HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation](https://arxiv.org/abs/2508.10425)
*Yan Ting Chok,Soyon Park,Seungheun Baek,Hajung Kim,Junhyun Lee,Jaewoo Kang*

Main category: cs.AI

TL;DR: HiRef模型通过结合医学本体和改进的EHR共现模式，提高了药物推荐的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有药物推荐模型在处理真实世界EHR数据中的稀有医学实体和不完整记录时泛化能力差的问题。

Method: HiRef框架结合了医学本体的层次语义和来自真实世界EHR的细化共现模式，利用双曲空间嵌入本体实体，并引入先验引导的稀疏正则化方案来细化EHR共现图。

Result: HiRef模型在EHR基准测试中取得了优异的性能，并对未见的医学代码具有较强的鲁棒性。

Conclusion: HiRef模型在EHR基准测试（MIMIC-III和MIMIC-IV）上取得了优异的性能，并在模拟的未见代码设置下保持较高的准确性。消融实验和深入分析支持了HiRef模型的有效性。

Abstract: Medication recommendation is a crucial task for assisting physicians in
making timely decisions from longitudinal patient medical records. However,
real-world EHR data present significant challenges due to the presence of
rarely observed medical entities and incomplete records that may not fully
capture the clinical ground truth. While data-driven models trained on
longitudinal Electronic Health Records often achieve strong empirical
performance, they struggle to generalize under missing or novel conditions,
largely due to their reliance on observed co-occurrence patterns. To address
these issues, we propose Hierarchical Ontology and Network Refinement for
Robust Medication Recommendation (HiRef), a unified framework that combines two
complementary structures: (i) the hierarchical semantics encoded in curated
medical ontologies, and (ii) refined co-occurrence patterns derived from
real-world EHRs. We embed ontology entities in hyperbolic space, which
naturally captures tree-like relationships and enables knowledge transfer
through shared ancestors, thereby improving generalizability to unseen codes.
To further improve robustness, we introduce a prior-guided sparse
regularization scheme that refines the EHR co-occurrence graph by suppressing
spurious edges while preserving clinically meaningful associations. Our model
achieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and
maintains high accuracy under simulated unseen-code settings. Extensive
experiments with comprehensive ablation studies demonstrate HiRef's resilience
to unseen medical codes, supported by in-depth analyses of the learned
sparsified graph structure and medical code embeddings.

</details>


### [16] [MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance](https://arxiv.org/abs/2508.10429)
*Yi Dong,Yusuke Muraoka,Scott Shi,Yi Zhang*

Main category: cs.AI

TL;DR: 发布了一个包含10万张食品图片的多模态数据集MM-Food-100K，并证明其可用于改进图像营养预测。


<details>
  <summary>Details</summary>
Motivation: 缺乏高质量的食品多模态数据集，限制了食品智能领域的发展。

Method: 收集了120万张经过质量审核的食品图片，并使用Codatta贡献模型进行数据标注，最终发布了10万样本的公开子集MM-Food-100K。

Result: 构建了MM-Food-100K数据集，并验证了其在图像营养预测任务中的有效性。

Conclusion: 构建了一个名为MM-Food-100K的公开食品多模态数据集，并通过微调大型视觉语言模型验证了其在图像营养预测方面的效用，取得了比基线更好的结果。

Abstract: We present MM-Food-100K, a public 100,000-sample multimodal food intelligence
dataset with verifiable provenance. It is a curated approximately 10% open
subset of an original 1.2 million, quality-accepted corpus of food images
annotated for a wide range of information (such as dish name, region of
creation). The corpus was collected over six weeks from over 87,000
contributors using the Codatta contribution model, which combines community
sourcing with configurable AI-assisted quality checks; each submission is
linked to a wallet address in a secure off-chain ledger for traceability, with
a full on-chain protocol on the roadmap. We describe the schema, pipeline, and
QA, and validate utility by fine-tuning large vision-language models (ChatGPT
5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning
yields consistent gains over out-of-box baselines across standard metrics; we
report results primarily on the MM-Food-100K subset. We release MM-Food-100K
for publicly free access and retain approximately 90% for potential commercial
access with revenue sharing to contributors.

</details>


### [17] [We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning](https://arxiv.org/abs/2508.10433)
*Runqi Qiao,Qiuna Tan,Peiqing Yang,Yanzi Wang,Xiaowan Wang,Enhui Wan,Sitong Zhou,Guanting Dong,Yuchen Zeng,Yida Xu,Jie Wang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.AI

TL;DR: We-Math 2.0 improves MLLM mathematical reasoning via a structured knowledge system, enhanced datasets, and a novel reinforcement learning framework.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with complex mathematical reasoning, neglecting knowledge-driven design and model-centric data space modeling.

Method: Constructing a five-level hierarchical knowledge system (MathBook), developing two datasets (MathBook-Standard & Pro), and proposing a two-stage RL framework (MathBook-RL).

Result: We-Math 2.0 shows competitive performance on existing benchmarks and strong results on the newly introduced MathBookEval benchmark.

Conclusion: We-Math 2.0, a unified system integrating structured mathematical knowledge, model-centric data space modeling, and reinforcement learning, significantly enhances MLLMs' mathematical reasoning abilities.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across various tasks, but still struggle with complex mathematical
reasoning. Existing research primarily focuses on dataset construction and
method optimization, often overlooking two critical aspects: comprehensive
knowledge-driven design and model-centric data space modeling. In this paper,
we introduce We-Math 2.0, a unified system that integrates a structured
mathematical knowledge system, model-centric data space modeling, and a
reinforcement learning (RL)-based training paradigm to comprehensively enhance
the mathematical reasoning abilities of MLLMs. The key contributions of We-Math
2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level
hierarchical system encompassing 491 knowledge points and 1,819 fundamental
principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a
dataset that ensures broad conceptual coverage and flexibility through dual
expansion. Additionally, we define a three-dimensional difficulty space and
generate 7 progressive variants per problem to build MathBook-Pro, a
challenging dataset for robust training. (3) MathBook-RL: We propose a
two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the
model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive
Alignment RL, leveraging average-reward learning and dynamic data scheduling to
achieve progressive alignment across difficulty levels. (4) MathBookEval: We
introduce a comprehensive benchmark covering all 491 knowledge points with
diverse reasoning step distributions. Experimental results show that
MathBook-RL performs competitively with existing baselines on four widely-used
benchmarks and achieves strong results on MathBookEval, suggesting promising
generalization in mathematical reasoning.

</details>
