{"id": "2508.10047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10047", "abs": "https://arxiv.org/abs/2508.10047", "authors": ["Ziyang Xiao", "Jingrong Xie", "Lilin Xu", "Shisi Guan", "Jingyan Zhu", "Xiongwei Han", "Xiaojin Fu", "WingYin Yu", "Han Wu", "Wei Shi", "Qingcan Kang", "Jiahui Duan", "Tao Zhong", "Mingxuan Yuan", "Jia Zeng", "Yuan Wang", "Gang Chen", "Dongxiang Zhang"], "title": "A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions", "comment": null, "summary": "By virtue of its great utility in solving real-world problems, optimization\nmodeling has been widely employed for optimal decision-making across various\nsectors, but it requires substantial expertise from operations research\nprofessionals. With the advent of large language models (LLMs), new\nopportunities have emerged to automate the procedure of mathematical modeling.\nThis survey presents a comprehensive and timely review of recent advancements\nthat cover the entire technical stack, including data synthesis and fine-tuning\nfor the base model, inference frameworks, benchmark datasets, and performance\nevaluation. In addition, we conducted an in-depth analysis on the quality of\nbenchmark datasets, which was found to have a surprisingly high error rate. We\ncleaned the datasets and constructed a new leaderboard with fair performance\nevaluation in terms of base LLM model and datasets. We also build an online\nportal that integrates resources of cleaned datasets, code and paper repository\nto benefit the community. Finally, we identify limitations in current\nmethodologies and outline future research opportunities.", "AI": {"tldr": "\u4f7f\u7528LLM\u81ea\u52a8\u5316\u6570\u5b66\u5efa\u6a21\u7684\u7efc\u8ff0\uff0c\u53d1\u73b0\u5e76\u4fee\u6b63\u4e86\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u7684\u9519\u8bef\uff0c\u521b\u5efa\u4e86\u65b0\u7684\u6392\u884c\u699c\u548c\u8d44\u6e90\u95e8\u6237\u3002", "motivation": "\u4f18\u5316\u5efa\u6a21\u5728\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u3002LLM \u7684\u51fa\u73b0\u4e3a\u81ea\u52a8\u5316\u5efa\u6a21\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\u3002", "method": "\u5bf9\u73b0\u6709\u6587\u732e\u8fdb\u884c\u7efc\u8ff0\uff0c\u5206\u6790\u57fa\u51c6\u6570\u636e\u96c6\u7684\u8d28\u91cf\uff0c\u6784\u5efa\u65b0\u7684\u6392\u884c\u699c\u548c\u5728\u7ebf\u8d44\u6e90\u95e8\u6237\u3002", "result": "\u5bf9 LLM \u81ea\u52a8\u5316\u6570\u5b66\u5efa\u6a21\u6280\u672f\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u53d1\u73b0\u4e86\u57fa\u51c6\u6570\u636e\u96c6\u7684\u9ad8\u9519\u8bef\u7387\uff0c\u5e76\u6784\u5efa\u4e86\u65b0\u7684\u3001\u66f4\u51c6\u786e\u7684\u6392\u884c\u699c\u548c\u5728\u7ebf\u8d44\u6e90\u95e8\u6237\u3002", "conclusion": "\u8fd9\u7bc7\u7efc\u8ff0\u6587\u7ae0\u56de\u987e\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u81ea\u52a8\u5316\u6570\u5b66\u5efa\u6a21\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6307\u51fa\u4e86\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u5b58\u5728\u7684\u9ad8\u9519\u8bef\u7387\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u6392\u884c\u699c\u548c\u5728\u7ebf\u8d44\u6e90\u95e8\u6237\u3002"}}
{"id": "2508.10108", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.6; E.0"], "pdf": "https://arxiv.org/pdf/2508.10108", "abs": "https://arxiv.org/abs/2508.10108", "authors": ["Sattvik Sahai", "Prasoon Goyal", "Michael Johnston", "Anna Gottardi", "Yao Lu", "Lucy Hu", "Luke Dai", "Shaohua Liu", "Samyuth Sagi", "Hangjie Shi", "Desheng Zhang", "Lavina Vaz", "Leslie Ball", "Maureen Murray", "Rahul Gupta", "Shankar Ananthakrishna"], "title": "Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development", "comment": "18 pages, 1st Proceedings of Amazon Nova AI Challenge (Trusted AI\n  2025)", "summary": "AI systems for software development are rapidly gaining prominence, yet\nsignificant challenges remain in ensuring their safety. To address this, Amazon\nlaunched the Trusted AI track of the Amazon Nova AI Challenge, a global\ncompetition among 10 university teams to drive advances in secure AI. In the\nchallenge, five teams focus on developing automated red teaming bots, while the\nother five create safe AI assistants. This challenge provides teams with a\nunique platform to evaluate automated red-teaming and safety alignment methods\nthrough head-to-head adversarial tournaments where red teams have multi-turn\nconversations with the competing AI coding assistants to test their safety\nalignment. Along with this, the challenge provides teams with a feed of high\nquality annotated data to fuel iterative improvement. Throughout the challenge,\nteams developed state-of-the-art techniques, introducing novel approaches in\nreasoning-based safety alignment, robust model guardrails, multi-turn\njail-breaking, and efficient probing of large language models (LLMs). To\nsupport these efforts, the Amazon Nova AI Challenge team made substantial\nscientific and engineering investments, including building a custom baseline\ncoding specialist model for the challenge from scratch, developing a tournament\norchestration service, and creating an evaluation harness. This paper outlines\nthe advancements made by university teams and the Amazon Nova AI Challenge team\nin addressing the safety challenges of AI for software development,\nhighlighting this collaborative effort to raise the bar for AI safety.", "AI": {"tldr": "Nova AI\u6311\u6218\u8d5b\u63a8\u52a8AI\u8f6f\u4ef6\u5f00\u53d1\u5b89\u5168\u6280\u672f\u8fdb\u6b65", "motivation": "\u89e3\u51b3AI\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5b89\u5168\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5bf9\u6297\u8d5b\u7684\u5f62\u5f0f\uff0c\u7ea2\u961f\u4e0eAI\u7f16\u7801\u52a9\u624b\u8fdb\u884c\u591a\u8f6e\u5bf9\u8bdd\uff0c\u6d4b\u8bd5\u5176\u5b89\u5168\u6027\u3002", "result": "\u53c2\u8d5b\u961f\u4f0d\u5f00\u53d1\u51fa\u5148\u8fdb\u6280\u672f\uff0c\u63d0\u9ad8\u4e86AI\u5b89\u5168\u6c34\u5e73\u3002", "conclusion": "\u4e9a\u9a6c\u900aNova AI\u6311\u6218\u8d5b\u4fc3\u8fdb\u4e86AI\u8f6f\u4ef6\u5f00\u53d1\u5b89\u5168\u6027\u7684\u7814\u7a76\uff0c\u53c2\u8d5b\u961f\u4f0d\u5f00\u53d1\u4e86\u5148\u8fdb\u6280\u672f\uff0c\u5305\u62ec\u57fa\u4e8e\u63a8\u7406\u7684\u5b89\u5168\u5bf9\u9f50\u3001\u9c81\u68d2\u6a21\u578b\u9632\u62a4\u3001\u591a\u8f6e\u8d8a\u72f1\u548c\u9ad8\u6548\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a2\u6d4b\u7b49\u3002"}}
{"id": "2508.10143", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10143", "abs": "https://arxiv.org/abs/2508.10143", "authors": ["Alexandru-Andrei Avram", "Adrian Groza", "Alexandru Lecu"], "title": "MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection", "comment": "8 pages + 1 page references, 5 figures, 4 tables, Registered for the\n  27th International Symposium on Symbolic and Numeric Algorithms for\n  Scientific Computing, 2025, Timisoara", "summary": "The large spread of disinformation across digital platforms creates\nsignificant challenges to information integrity. This paper presents a\nmulti-agent system that uses relation extraction to detect disinformation in\nnews articles, focusing on titles and short text snippets. The proposed Agentic\nAI system combines four agents: (i) a machine learning agent (logistic\nregression), (ii) a Wikipedia knowledge check agent (which relies on named\nentity recognition), (iii) a coherence detection agent (using LLM prompt\nengineering), and (iv) a web-scraped data analyzer that extracts relational\ntriplets for fact checking. The system is orchestrated via the Model Context\nProtocol (MCP), offering shared context and live learning across components.\nResults demonstrate that the multi-agent ensemble achieves 95.3% accuracy with\nan F1 score of 0.964, significantly outperforming individual agents and\ntraditional approaches. The weighted aggregation method, mathematically derived\nfrom individual agent misclassification rates, proves superior to algorithmic\nthreshold optimization. The modular architecture makes the system easily\nscalable, while also maintaining details of the decision processes.", "AI": {"tldr": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5229\u7528\u5173\u7cfb\u62bd\u53d6\uff0c\u51c6\u786e\u9ad8\u6548\u5730\u68c0\u6d4b\u65b0\u95fb\u4e2d\u7684\u865a\u5047\u4fe1\u606f\u3002", "motivation": "\u89e3\u51b3\u6570\u5b57\u5e73\u53f0\u4e0a\u865a\u5047\u4fe1\u606f\u5927\u91cf\u4f20\u64ad\u5bf9\u4fe1\u606f\u5b8c\u6574\u6027\u9020\u6210\u7684\u91cd\u5927\u6311\u6218\u3002", "method": "\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u673a\u5668\u5b66\u4e60\u3001\u7ef4\u57fa\u767e\u79d1\u77e5\u8bc6\u68c0\u67e5\u3001\u8fde\u8d2f\u6027\u68c0\u6d4b\u548c\u7f51\u7edc\u6570\u636e\u5206\u6790\u56db\u4e2a\u667a\u80fd\u4f53\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae (MCP) \u8fdb\u884c\u534f\u8c03\u3002", "result": "\u591a\u667a\u80fd\u4f53\u96c6\u6210\u7cfb\u7edf\u51c6\u786e\u7387\u8fbe95.3%\uff0cF1\u5206\u6570\u4e3a0.964\uff0c\u52a0\u6743\u805a\u5408\u65b9\u6cd5\u4f18\u4e8e\u7b97\u6cd5\u9608\u503c\u4f18\u5316\u3002", "conclusion": "\u8be5\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u65b0\u95fb\u6807\u9898\u548c\u7b80\u77ed\u6587\u672c\u7247\u6bb5\u4e2d\u68c0\u6d4b\u865a\u5047\u4fe1\u606f\u7684\u51c6\u786e\u7387\u8fbe\u523095.3%\uff0cF1\u5206\u6570\u4e3a0.964\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u4e2a\u667a\u80fd\u4f53\u548c\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2508.10146", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10146", "abs": "https://arxiv.org/abs/2508.10146", "authors": ["Hana Derouiche", "Zaki Brahmi", "Haithem Mazeni"], "title": "Agentic AI Frameworks: Architectures, Protocols, and Design Challenges", "comment": null, "summary": "The emergence of Large Language Models (LLMs) has ushered in a transformative\nparadigm in artificial intelligence, Agentic AI, where intelligent agents\nexhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent\ncoordination. This paper provides a systematic review and comparative analysis\nof leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen,\nSemantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural\nprinciples, communication mechanisms, memory management, safety guardrails, and\nalignment with service-oriented computing paradigms. Furthermore, we identify\nkey limitations, emerging trends, and open challenges in the field. To address\nthe issue of agent communication, we conduct an in-depth analysis of protocols\nsuch as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network\nProtocol (ANP), and Agora. Our findings not only establish a foundational\ntaxonomy for Agentic AI systems but also propose future research directions to\nenhance scalability, robustness, and interoperability. This work serves as a\ncomprehensive reference for researchers and practitioners working to advance\nthe next generation of autonomous AI systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u81ea\u4e3b\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff0c\u5206\u6790\u4e86\u5176\u67b6\u6784\u3001\u901a\u4fe1\u673a\u5236\u7b49\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\u63a8\u52a8\u4e86\u81ea\u4e3b\u667a\u80fd\u4ee3\u7406\u9886\u57df\u7684\u53d1\u5c55\uff0c\u8be5\u9886\u57df\u4e2d\u7684\u667a\u80fd\u4ee3\u7406\u8868\u73b0\u51fa\u76ee\u6807\u5bfc\u5411\u7684\u81ea\u4e3b\u6027\u3001\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u52a8\u6001\u591a\u4ee3\u7406\u534f\u8c03\u3002", "method": "\u672c\u6587\u5bf9\u81ea\u4e3b\u667a\u80fd\u4ee3\u7406\u6846\u67b6\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u56de\u987e\u548c\u6bd4\u8f83\u5206\u6790\uff0c\u5e76\u5bf9\u4ee3\u7406\u901a\u4fe1\u534f\u8bae\uff08\u5982CNP\u3001A2A\u3001ANP\u548cAgora\uff09\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\u3002", "result": "\u672c\u6587\u5efa\u7acb\u4e86\u81ea\u4e3b\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u7684\u57fa\u7840\u5206\u7c7b\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u589e\u5f3a\u53ef\u6269\u5c55\u6027\u3001\u9c81\u68d2\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u5bf9\u9886\u5148\u7684\u81ea\u4e3b\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff08\u5305\u62ecCrewAI\u3001LangGraph\u3001AutoGen\u3001Semantic Kernel\u3001Agno\u3001Google ADK\u548cMetaGPT\uff09\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u56de\u987e\u548c\u6bd4\u8f83\u5206\u6790\uff0c\u8bc4\u4f30\u4e86\u5b83\u4eec\u7684\u67b6\u6784\u539f\u5219\u3001\u901a\u4fe1\u673a\u5236\u3001\u5185\u5b58\u7ba1\u7406\u3001\u5b89\u5168\u9632\u62a4\u548c\u4e0e\u9762\u5411\u670d\u52a1\u7684\u8ba1\u7b97\u8303\u4f8b\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u786e\u5b9a\u4e86\u8be5\u9886\u57df\u7684\u5173\u952e\u9650\u5236\u3001\u65b0\u5174\u8d8b\u52bf\u548c\u5f00\u653e\u6027\u6311\u6218\u3002"}}
{"id": "2508.10152", "categories": ["cs.AI", "I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.10152", "abs": "https://arxiv.org/abs/2508.10152", "authors": ["Doaa Allabadi", "Kyle Bradbury", "Jordan M. Malof"], "title": "Improving and Evaluating Open Deep Research Agents", "comment": "8 pages, 2 figures, 2 tables", "summary": "We focus here on Deep Research Agents (DRAs), which are systems that can take\na natural language prompt from a user, and then autonomously search for, and\nutilize, internet-based content to address the prompt. Recent DRAs have\ndemonstrated impressive capabilities on public benchmarks however, recent\nresearch largely involves proprietary closed-source systems. At the time of\nthis work, we only found one open-source DRA, termed Open Deep Research (ODR).\nIn this work we adapt the challenging recent BrowseComp benchmark to compare\nODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small),\ncomprising a subset of BrowseComp, as a more computationally-tractable DRA\nbenchmark for academic labs. We benchmark ODR and two other proprietary systems\non BC-Small: one system from Anthropic and one system from Google. We find that\nall three systems achieve 0% accuracy on the test set of 60 questions. We\nintroduce three strategic improvements to ODR, resulting in the ODR+ model,\nwhich achieves a state-of-the-art 10% success rate on BC-Small among both\nclosed-source and open-source systems. We report ablation studies indicating\nthat all three of our improvements contributed to the success of ODR+.", "AI": {"tldr": "\u5f00\u6e90\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406ODR\u6539\u8fdb\u540e\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u95ed\u6e90\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u5927\u591a\u4e3a\u95ed\u6e90\u7cfb\u7edf\uff0c\u7f3a\u4e4f\u53ef\u4f9b\u5b66\u672f\u754c\u7814\u7a76\u548c\u6539\u8fdb\u7684\u5f00\u6e90\u7cfb\u7edf\u3002", "method": "\u5bf9\u73b0\u6709\u5f00\u6e90\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406ODR\u8fdb\u884c\u6539\u8fdb\uff0c\u5e76\u5728BrowseComp-Small\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6539\u8fdb\u540e\u7684ODR+\u6a21\u578b\u5728BrowseComp-Small\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e8610%\u7684\u6210\u529f\u7387\uff0c\u8d85\u8fc7\u4e86\u5176\u4ed6\u95ed\u6e90\u7cfb\u7edf\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u6539\u8fdb\u5f00\u6e90\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406ODR\uff0c\u4f7f\u5176\u5728BrowseComp-Small\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e8610%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u5176\u4ed6\u95ed\u6e90\u7cfb\u7edf\u3002"}}
{"id": "2508.10164", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10164", "abs": "https://arxiv.org/abs/2508.10164", "authors": ["Bin Hong", "Jiayu Liu", "Zhenya Huang", "Kai Zhang", "Mengdi Zhang"], "title": "Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization", "comment": "19 pages, 5 figures", "summary": "Recent advances in Large Reasoning Models (LRMs) have demonstrated strong\nperformance on complex tasks through long Chain-of-Thought (CoT) reasoning.\nHowever, their lengthy outputs increase computational costs and may lead to\noverthinking, raising challenges in balancing reasoning effectiveness and\nefficiency. Current methods for efficient reasoning often compromise reasoning\nquality or require extensive resources. This paper investigates efficient\nmethods to reduce the generation length of LRMs. We analyze generation path\ndistributions and filter generated trajectories through difficulty estimation.\nSubsequently, we analyze the convergence behaviors of the objectives of various\npreference optimization methods under a Bradley-Terry loss based framework.\nBased on the analysis, we propose Length Controlled Preference Optimization\n(LCPO) that directly balances the implicit reward related to NLL loss. LCPO can\neffectively learn length preference with limited data and training. Extensive\nexperiments demonstrate that our approach significantly reduces the average\noutput length by over 50\\% across multiple benchmarks while maintaining the\nreasoning performance. Our work highlights the potential for computationally\nefficient approaches in guiding LRMs toward efficient reasoning.", "AI": {"tldr": "\u63d0\u51fa\u957f\u5ea6\u63a7\u5236\u504f\u597d\u4f18\u5316 (LCPO) \u65b9\u6cd5\uff0c\u6709\u6548\u7f29\u77ed\u5927\u578b\u63a8\u7406\u6a21\u578b\u8f93\u51fa\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b (LRM) \u7684\u5197\u957f\u8f93\u51fa\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u5e76\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u601d\u8003\uff0c\u672c\u6587\u65e8\u5728\u7814\u7a76\u51cf\u5c11 LRM \u751f\u6210\u957f\u5ea6\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4ee5\u5e73\u8861\u63a8\u7406\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "method": "\u5206\u6790\u751f\u6210\u8def\u5f84\u5206\u5e03\u548c\u96be\u5ea6\u4f30\u8ba1\uff0c\u8fc7\u6ee4\u751f\u6210\u7684\u8f68\u8ff9\uff0c\u5e76\u57fa\u4e8e Bradley-Terry \u635f\u5931\u6846\u67b6\u5206\u6790\u4e0d\u540c\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u7684\u76ee\u6807\u6536\u655b\u884c\u4e3a\uff0c\u6700\u7ec8\u63d0\u51fa\u957f\u5ea6\u63a7\u5236\u504f\u597d\u4f18\u5316 (LCPO) \u65b9\u6cd5\u3002", "result": "LCPO \u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u5e73\u5747\u8f93\u51fa\u957f\u5ea6 (\u8d85\u8fc7 50%)\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u957f\u5ea6\u63a7\u5236\u504f\u597d\u4f18\u5316 (LCPO) \u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u6790\u751f\u6210\u8def\u5f84\u5206\u5e03\u548c\u96be\u5ea6\u4f30\u8ba1\u6765\u8fc7\u6ee4\u751f\u6210\u7684\u8f68\u8ff9\uff0c\u6709\u6548\u5e73\u8861\u4e86 NLL \u635f\u5931\u76f8\u5173\u7684\u9690\u5f0f\u5956\u52b1\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c06\u5e73\u5747\u8f93\u51fa\u957f\u5ea6\u51cf\u5c11\u4e86 50% \u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2508.10177", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10177", "abs": "https://arxiv.org/abs/2508.10177", "authors": ["Stepan Kulibaba", "Artem Dzhalilov", "Roman Pakhomov", "Oleg Svidchenko", "Alexander Gasnikov", "Aleksei Shpilman"], "title": "KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems", "comment": null, "summary": "Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive\ncapabilities but face significant limitations such as constrained exploration\nstrategies and a severe execution bottleneck. Exploration is hindered by\none-shot methods lacking diversity and Monte Carlo Tree Search (MCTS)\napproaches that fail to recombine strong partial solutions. The execution\nbottleneck arises from lengthy code validation cycles that stifle iterative\nrefinement. To overcome these challenges, we introduce KompeteAI, a novel\nAutoML framework with dynamic solution space exploration. Unlike previous MCTS\nmethods that treat ideas in isolation, KompeteAI introduces a merging stage\nthat composes top candidates. We further expand the hypothesis space by\nintegrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle\nnotebooks and arXiv papers to incorporate real-world strategies. KompeteAI also\naddresses the execution bottleneck via a predictive scoring model and an\naccelerated debugging method, assessing solution potential using early stage\nmetrics to avoid costly full-code execution. This approach accelerates pipeline\nevaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent,\nAIDE, and Ml-Master) by an average of 3\\% on the primary AutoML benchmark,\nMLE-Bench. Additionally, we propose Kompete-bench to address limitations in\nMLE-Bench, where KompeteAI also achieves state-of-the-art results", "AI": {"tldr": "KompeteAI\u901a\u8fc7\u52a8\u6001\u63a2\u7d22\u3001\u89e3\u51b3\u65b9\u6848\u5408\u5e76\u548cRAG\uff0c\u89e3\u51b3\u4e86LLM-based AutoML\u7cfb\u7edf\u7684\u74f6\u9888\u95ee\u9898\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684LLM-based AutoML\u7cfb\u7edf\u5b58\u5728\u63a2\u7d22\u7b56\u7565\u53d7\u9650\uff08\u5355\u6b21\u65b9\u6cd5\u7f3a\u4e4f\u591a\u6837\u6027\uff0cMCTS\u65b9\u6cd5\u65e0\u6cd5\u91cd\u7ec4\u5f3a\u90e8\u5206\u89e3\u51b3\u65b9\u6848\uff09\u548c\u6267\u884c\u74f6\u9888\uff08\u4ee3\u7801\u9a8c\u8bc1\u5468\u671f\u957f\uff09\u7684\u95ee\u9898\u3002", "method": "KompeteAI\u5f15\u5165\u4e86\u4e00\u4e2a\u5408\u5e76\u9636\u6bb5\u6765\u7ec4\u5408\u9876\u7ea7\u5019\u9009\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u6574\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6765\u6269\u5c55\u5047\u8bbe\u7a7a\u95f4\uff0c\u5229\u7528Kaggle\u7b14\u8bb0\u672c\u548carXiv\u8bba\u6587\u4e2d\u7684\u771f\u5b9e\u7b56\u7565\u3002\u6b64\u5916\uff0c\u5b83\u91c7\u7528\u9884\u6d4b\u8bc4\u5206\u6a21\u578b\u548c\u52a0\u901f\u8c03\u8bd5\u65b9\u6cd5\u6765\u52a0\u5feb\u8bc4\u4f30\u901f\u5ea6\u3002", "result": "KompeteAI\u5728\u4e3b\u8981\u7684AutoML\u57fa\u51c6MLE-Bench\u4e0a\u5e73\u5747\u8d85\u8d8a\u9886\u5148\u65b9\u6cd53%\uff0c\u5728\u63d0\u51fa\u7684Kompete-bench\u4e0a\u4e5f\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0cpipeline\u8bc4\u4f30\u901f\u5ea6\u63d0\u5347\u4e866.9\u500d\u3002", "conclusion": "KompeteAI\uff0c\u4e00\u4e2a\u65b0\u578bAutoML\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u63a2\u7d22\u514b\u670d\u4e86\u57fa\u4e8eLLM\u7684AutoML\u7cfb\u7edf\u4e2d\u63a2\u7d22\u7b56\u7565\u53d7\u9650\u548c\u6267\u884c\u74f6\u9888\u7684\u95ee\u9898\uff0c\u5728MLE-Bench\u548c\u63d0\u51fa\u7684Kompete-bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}}
{"id": "2508.10241", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10241", "abs": "https://arxiv.org/abs/2508.10241", "authors": ["Mark Zilberman"], "title": "Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence", "comment": "10 pages", "summary": "This work demonstrates how the concept of the entropic potential of events --\na parameter quantifying the influence of discrete events on the expected future\nentropy of a system -- can enhance uncertainty quantification, decision-making,\nand interpretability in artificial intelligence (AI). Building on its original\nformulation in physics, the framework is adapted for AI by introducing an\nevent-centric measure that captures how actions, observations, or other\ndiscrete occurrences impact uncertainty at future time horizons. Both the\noriginal and AI-adjusted definitions of entropic potential are formalized, with\nthe latter emphasizing conditional expectations to account for counterfactual\nscenarios. Applications are explored in policy evaluation, intrinsic reward\ndesign, explainable AI, and anomaly detection, highlighting the metric's\npotential to unify and strengthen uncertainty modeling in intelligent systems.\nConceptual examples illustrate its use in reinforcement learning, Bayesian\ninference, and anomaly detection, while practical considerations for\ncomputation in complex AI models are discussed. The entropic potential\nframework offers a theoretically grounded, interpretable, and versatile\napproach to managing uncertainty in AI, bridging principles from\nthermodynamics, information theory, and machine learning.", "AI": {"tldr": "\u4e8b\u4ef6\u71b5\u52bf\u6982\u5ff5\u53ef\u589e\u5f3aAI\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u51b3\u7b56\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u63d0\u5347AI\u7cfb\u7edf\u4e2d\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5c06\u7269\u7406\u5b66\u4e2d\u4e8b\u4ef6\u71b5\u52bf\u7684\u6982\u5ff5\u5e94\u7528\u4e8eAI\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u4e8b\u4ef6\u4e2d\u5fc3\u7684\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u6355\u83b7\u79bb\u6563\u4e8b\u4ef6\u5bf9\u672a\u6765\u4e0d\u786e\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "result": "\u5f62\u5f0f\u5316\u4e86\u4e8b\u4ef6\u71b5\u52bf\u7684\u5b9a\u4e49\uff0c\u5e76\u5728\u5f3a\u5316\u5b66\u4e60\u3001\u8d1d\u53f6\u65af\u63a8\u7406\u548c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8fdb\u884c\u4e86\u5e94\u7528\uff0c\u8ba8\u8bba\u4e86\u5176\u5728\u590d\u6742AI\u6a21\u578b\u4e2d\u7684\u8ba1\u7b97\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u4e8b\u4ef6\u71b5\u52bf\u6982\u5ff5\u5982\u4f55\u589e\u5f3aAI\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u51b3\u7b56\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u7b56\u7565\u8bc4\u4f30\u3001\u5185\u5728\u5956\u52b1\u8bbe\u8ba1\u3001\u53ef\u89e3\u91caAI\u548c\u5f02\u5e38\u68c0\u6d4b\u3002"}}
{"id": "2508.10265", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.10265", "abs": "https://arxiv.org/abs/2508.10265", "authors": ["Jingde Cheng"], "title": "Why Cannot Large Language Models Ever Make True Correct Reasoning?", "comment": "8 pages. arXiv admin note: substantial text overlap with\n  arXiv:2412.12408", "summary": "Recently, with the application progress of AIGC tools based on large language\nmodels (LLMs), led by ChatGPT, many AI experts and more non-professionals are\ntrumpeting the \"understanding ability\" and \"reasoning ability\" of the LLMs. The\npresent author considers that the so-called \"understanding ability\" and\n\"reasoning ability\" of LLMs are just illusions of those people who with vague\nconcepts. In fact, the LLMs can never have the true understanding ability and\ntrue reasoning ability. This paper intents to explain that, because the\nessential limitations of their working principle, the LLMs can never have the\nability of true correct reasoning.", "AI": {"tldr": "LLM\u65e0\u6cd5\u771f\u6b63\u7406\u89e3\u548c\u63a8\u7406\u3002", "motivation": "\u7ea0\u6b63\u5bf9LLM\u80fd\u529b\u7684\u8bef\u89e3\uff0c\u6f84\u6e05\u5176\u672c\u8d28\u5c40\u9650\u6027\u3002", "method": "\u5206\u6790LLM\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u8bba\u8bc1\u5176\u65e0\u6cd5\u8fdb\u884c\u771f\u6b63\u6b63\u786e\u7684\u63a8\u7406\u3002", "result": "\u8bc1\u660eLLM\u7531\u4e8e\u5176\u5de5\u4f5c\u539f\u7406\u7684\u672c\u8d28\u9650\u5236\uff0c\u4e0d\u53ef\u80fd\u62e5\u6709\u771f\u6b63\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0d\u5177\u5907\u771f\u6b63\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5176\u6240\u8c13\u7684\u201c\u7406\u89e3\u201d\u548c\u201c\u63a8\u7406\u201d\u80fd\u529b\u53ea\u662f\u4eba\u4eec\u7684\u9519\u89c9\u3002"}}
{"id": "2508.10293", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10293", "abs": "https://arxiv.org/abs/2508.10293", "authors": ["Chuhuai Yue", "Chengqi Dong", "Yinan Gao", "Hang He", "Jiajun Chai", "Guojun Yin", "Wei Lin"], "title": "Promoting Efficient Reasoning with Verifiable Stepwise Reward", "comment": null, "summary": "Large reasoning models (LRMs) have recently achieved significant progress in\ncomplex reasoning tasks, aided by reinforcement learning with verifiable\nrewards. However, LRMs often suffer from overthinking, expending excessive\ncomputation on simple problems and reducing efficiency. Existing efficient\nreasoning methods typically require accurate task assessment to preset token\nbudgets or select reasoning modes, which limits their flexibility and\nreliability. In this work, we revisit the essence of overthinking and identify\nthat encouraging effective steps while penalizing ineffective ones is key to\nits solution. To this end, we propose a novel rule-based verifiable stepwise\nreward mechanism (VSRM), which assigns rewards based on the performance of\nintermediate states in the reasoning trajectory. This approach is intuitive and\nnaturally fits the step-by-step nature of reasoning tasks. We conduct extensive\nexperiments on standard mathematical reasoning benchmarks, including AIME24 and\nAIME25, by integrating VSRM with PPO and Reinforce++. Results show that our\nmethod achieves substantial output length reduction while maintaining original\nreasoning performance, striking an optimal balance between efficiency and\naccuracy. Further analysis of overthinking frequency and pass@k score before\nand after training demonstrates that our approach in deed effectively\nsuppresses ineffective steps and encourages effective reasoning, fundamentally\nalleviating the overthinking problem. All code will be released upon\nacceptance.", "AI": {"tldr": "\u901a\u8fc7\u65b0\u7684\u5956\u52b1\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684LRM\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5bb9\u6613\u8fc7\u5ea6\u601d\u8003\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u9ad8\u6548\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u51c6\u786e\u7684\u4efb\u52a1\u8bc4\u4f30\uff0c\u9650\u5236\u4e86\u5176\u7075\u6d3b\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u53ef\u9a8c\u8bc1\u9010\u6b65\u5956\u52b1\u673a\u5236\uff08VSRM\uff09\uff0c\u5e76\u5c06\u5176\u4e0ePPO\u548cReinforce++\u7b97\u6cd5\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728AIME24\u548cAIME25\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u8f93\u51fa\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u539f\u59cb\u63a8\u7406\u6027\u80fd\uff0c\u6709\u6548\u5730\u6291\u5236\u4e86\u65e0\u6548\u6b65\u9aa4\uff0c\u4fc3\u8fdb\u4e86\u6709\u6548\u63a8\u7406\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u53ef\u9a8c\u8bc1\u9010\u6b65\u5956\u52b1\u673a\u5236\uff08VSRM\uff09\uff0c\u901a\u8fc7\u5956\u52b1\u6709\u6548\u7684\u63a8\u7406\u6b65\u9aa4\u5e76\u60e9\u7f5a\u65e0\u6548\u6b65\u9aa4\u6765\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRM\uff09\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8f93\u51fa\u957f\u5ea6\u3002"}}
{"id": "2508.10337", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10337", "abs": "https://arxiv.org/abs/2508.10337", "authors": ["Chenliang Zhang", "Lin Wang", "Yuanyuan Lu", "Yusheng Qi", "Kexin Wang", "Peixu Hou", "Wenshi Chen"], "title": "A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering", "comment": null, "summary": "This paper describes the solutions of the Dianping-Trust-Safety team for the\nMETA CRAG-MM challenge. The challenge requires building a comprehensive\nretrieval-augmented generation system capable for multi-modal multi-turn\nquestion answering. The competition consists of three tasks: (1) answering\nquestions using structured data retrieved from an image-based mock knowledge\ngraph, (2) synthesizing information from both knowledge graphs and web search\nresults, and (3) handling multi-turn conversations that require context\nunderstanding and information aggregation from multiple sources. For Task 1,\nour solution is based on the vision large language model, enhanced by\nsupervised fine-tuning with knowledge distilled from GPT-4.1. We further\napplied curriculum learning strategies to guide reinforcement learning,\nresulting in improved answer accuracy and reduced hallucination. For Task 2 and\nTask 3, we additionally leveraged web search APIs to incorporate external\nknowledge, enabling the system to better handle complex queries and multi-turn\nconversations. Our approach achieved 1st place in Task 1 with a significant\nlead of 52.38\\%, and 3rd place in Task 3, demonstrating the effectiveness of\nthe integration of curriculum learning with reinforcement learning in our\ntraining pipeline.", "AI": {"tldr": "Dianping\u56e2\u961f\u7684\u65b9\u6848\u5728META CRAG-MM\u6311\u6218\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u6280\u672f\uff0c\u53d6\u5f97\u4e86\u9886\u5148\u6210\u7ee9\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9META CRAG-MM\u6311\u6218\u8d5b\u4e2d\u591a\u6a21\u6001\u591a\u8f6e\u95ee\u7b54\u7684\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6848\u7ed3\u5408\u89c6\u89c9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u77e5\u8bc6\u84b8\u998f\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u5229\u7528\u7f51\u7edc\u641c\u7d22API\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u3002", "result": "\u5728\u7b2c\u4e00\u9636\u6bb5\u53d6\u5f97\u4e8652.38%\u7684\u9886\u5148\u4f18\u52bf\uff0c\u5728\u7b2c\u4e09\u9636\u6bb5\u83b7\u5f97\u7b2c\u4e09\u540d\u3002", "conclusion": "\u8be5\u8bba\u6587\u63cf\u8ff0\u4e86Dianping-Trust-Safety\u56e2\u961f\u5728META CRAG-MM\u6311\u6218\u8d5b\u4e2d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u7b2c\u4e00\u9636\u6bb5\u53d6\u5f97\u4e86\u663e\u8457\u9886\u5148\u4f18\u52bf\uff0c\u5728\u7b2c\u4e09\u9636\u6bb5\u53d6\u5f97\u4e86\u7b2c\u4e09\u540d\u3002"}}
{"id": "2508.10340", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10340", "abs": "https://arxiv.org/abs/2508.10340", "authors": ["Chak Lam Shek", "Guangyao Shi", "Pratap Tokekar"], "title": "Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach", "comment": null, "summary": "Multi-agent reinforcement learning (MARL) requires coordinated and stable\npolicy updates among interacting agents. Heterogeneous-Agent Trust Region\nPolicy Optimization (HATRPO) enforces per-agent trust region constraints using\nKullback-Leibler (KL) divergence to stabilize training. However, assigning each\nagent the same KL threshold can lead to slow and locally optimal updates,\nespecially in heterogeneous settings. To address this limitation, we propose\ntwo approaches for allocating the KL divergence threshold across agents:\nHATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes\nthreshold assignment under global KL constraints, and HATRPO-G, a greedy\nalgorithm that prioritizes agents based on improvement-to-divergence ratio. By\nconnecting sequential policy optimization with constrained threshold\nscheduling, our approach enables more flexible and effective learning in\nheterogeneous-agent settings. Experimental results demonstrate that our methods\nsignificantly boost the performance of HATRPO, achieving faster convergence and\nhigher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and\nHATRPO-G achieve comparable improvements in final performance, each exceeding\n22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as\nreflected by its lower variance.", "AI": {"tldr": "\u6539\u8fdbHATRPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316KL\u9608\u503c\u5206\u914d\uff0c\u63d0\u5347\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709HATRPO\u7b97\u6cd5\u4e2d\u4e3a\u6240\u6709\u667a\u80fd\u4f53\u5206\u914d\u76f8\u540cKL\u9608\u503c\u5bfc\u81f4\u8bad\u7ec3\u7f13\u6162\u548c\u5c40\u90e8\u6700\u4f18\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5f02\u6784\u73af\u5883\u4e2d\u3002", "method": "\u63d0\u51fa\u4e24\u79cdKL\u6563\u5ea6\u9608\u503c\u5206\u914d\u65b9\u6cd5\uff1a\u57fa\u4e8eKKT\u7684HATRPO-W\u548c\u8d2a\u5a6a\u7b97\u6cd5HATRPO-G\uff0c\u7ed3\u5408\u987a\u5e8f\u7b56\u7565\u4f18\u5316\u548c\u7ea6\u675f\u9608\u503c\u8c03\u5ea6\u3002", "result": "HATRPO-W\u548cHATRPO-G\u90fd\u5b9e\u73b0\u4e86\u8d85\u8fc722.5%\u7684\u6027\u80fd\u63d0\u5347\uff0cHATRPO-W\u7684\u5b66\u4e60\u52a8\u6001\u66f4\u7a33\u5b9a\u3002", "conclusion": "\u4e24\u79cd\u6539\u8fdb\u7684HATRPO\u7b97\u6cd5(HATRPO-W\u548cHATRPO-G)\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u6700\u7ec8\u5956\u52b1\u3002HATRPO-W\u5728\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002"}}
{"id": "2508.10358", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10358", "abs": "https://arxiv.org/abs/2508.10358", "authors": ["Mengtao Zhou", "Sifan Wu", "Huan Zhang", "Qi Sima", "Bang Liu"], "title": "What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles", "comment": null, "summary": "We investigate the capacity of Large Language Models (LLMs) for imaginative\nreasoning--the proactive construction, testing, and revision of hypotheses in\ninformation-sparse environments. Existing benchmarks, often static or focused\non social deduction, fail to capture the dynamic, exploratory nature of this\nreasoning process. To address this gap, we introduce a comprehensive research\nframework based on the classic \"Turtle Soup\" game, integrating a benchmark, an\nagent, and an evaluation protocol. We present TurtleSoup-Bench, the first\nlarge-scale, bilingual, interactive benchmark for imaginative reasoning,\ncomprising 800 turtle soup puzzles sourced from both the Internet and expert\nauthors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'\nperformance in this setting. To evaluate reasoning quality, we develop a\nmulti-dimensional protocol measuring logical consistency, detail completion,\nand conclusion alignment. Experiments with leading LLMs reveal clear capability\nlimits, common failure patterns, and a significant performance gap compared to\nhumans. Our work offers new insights into LLMs' imaginative reasoning and\nestablishes a foundation for future research on exploratory agent behavior.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5bcc\u6709\u60f3\u8c61\u529b\u7684\u63a8\u7406\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u6846\u67b6\u7528\u4e8e\u672a\u6765\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u6355\u6349\u5bcc\u6709\u60f3\u8c61\u529b\u7684\u63a8\u7406\u8fc7\u7a0b\u7684\u52a8\u6001\u548c\u63a2\u7d22\u6027\u672c\u8d28\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ecf\u5178\u201c\u9f9f\u6c64\u201d\u6e38\u620f\u7684\u7efc\u5408\u7814\u7a76\u6846\u67b6\uff0c\u5305\u62ec\u4e00\u4e2a\u5927\u578b\u53cc\u8bed\u4ea4\u4e92\u5f0f\u57fa\u51c6\u6d4b\u8bd5TurtleSoup-Bench\uff0c\u4e00\u4e2a\u540d\u4e3aMosaic-Agent\u7684\u65b0\u578b\u4ee3\u7406\uff0c\u4ee5\u53ca\u4e00\u4e2a\u591a\u7ef4\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u4e86LLM\u5728\u5bcc\u6709\u60f3\u8c61\u529b\u7684\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u9650\u5236\u3001\u5e38\u89c1\u9519\u8bef\u6a21\u5f0f\u4ee5\u53ca\u4e0e\u4eba\u7c7b\u76f8\u6bd4\u7684\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u4fe1\u606f\u7a00\u758f\u73af\u5883\u4e0b\u8fdb\u884c\u5bcc\u6709\u60f3\u8c61\u529b\u7684\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u201c\u9f9f\u6c64\u201d\u6e38\u620f\u7684\u7efc\u5408\u7814\u7a76\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u51c6\u3001\u4ee3\u7406\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u4ee5\u8bc4\u4f30LLM\u7684\u6027\u80fd\u3002"}}
{"id": "2508.10391", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10391", "abs": "https://arxiv.org/abs/2508.10391", "authors": ["Yaoze Zhang", "Rong Wu", "Pinlong Cai", "Xiaoman Wang", "Guohang Yan", "Song Mao", "Ding Wang", "Botian Shi"], "title": "LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large\nLanguage Models by leveraging external knowledge, whereas the effectiveness is\noften compromised by the retrieval of contextually flawed or incomplete\ninformation. To address this, knowledge graph-based RAG methods have evolved\ntowards hierarchical structures, organizing knowledge into multi-level\nsummaries. However, these approaches still suffer from two critical,\nunaddressed challenges: high-level conceptual summaries exist as disconnected\n``semantic islands'', lacking the explicit relations needed for cross-community\nreasoning; and the retrieval process itself remains structurally unaware, often\ndegenerating into an inefficient flat search that fails to exploit the graph's\nrich topology. To overcome these limitations, we introduce LeanRAG, a framework\nthat features a deeply collaborative design combining knowledge aggregation and\nretrieval strategies. LeanRAG first employs a novel semantic aggregation\nalgorithm that forms entity clusters and constructs new explicit relations\namong aggregation-level summaries, creating a fully navigable semantic network.\nThen, a bottom-up, structure-guided retrieval strategy anchors queries to the\nmost relevant fine-grained entities and then systematically traverses the\ngraph's semantic pathways to gather concise yet contextually comprehensive\nevidence sets. The LeanRAG can mitigate the substantial overhead associated\nwith path retrieval on graphs and minimizes redundant information retrieval.\nExtensive experiments on four challenging QA benchmarks with different domains\ndemonstrate that LeanRAG significantly outperforming existing methods in\nresponse quality while reducing 46\\% retrieval redundancy. Code is available\nat: https://github.com/RaZzzyz/LeanRAG", "AI": {"tldr": "LeanRAG\u6846\u67b6\u901a\u8fc7\u8bed\u4e49\u805a\u5408\u548c\u7ed3\u6784\u5f15\u5bfc\u68c0\u7d22\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684RAG\u65b9\u6cd5\u7684\u8bed\u4e49\u5b64\u5c9b\u548c\u4f4e\u6548\u68c0\u7d22\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684RAG\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a\u9ad8\u5c42\u6b21\u6982\u5ff5\u6027\u6458\u8981\u4e4b\u95f4\u7f3a\u4e4f\u663e\u5f0f\u5173\u7cfb\uff0c\u68c0\u7d22\u8fc7\u7a0b\u6548\u7387\u4f4e\u4e0b\u3002", "method": "LeanRAG\u6846\u67b6\u9996\u5148\u4f7f\u7528\u4e00\u79cd\u65b0\u7684\u8bed\u4e49\u805a\u5408\u7b97\u6cd5\u5f62\u6210\u5b9e\u4f53\u96c6\u7fa4\u5e76\u6784\u5efa\u805a\u5408\u7ea7\u6458\u8981\u4e4b\u95f4\u7684\u663e\u5f0f\u5173\u7cfb\uff0c\u521b\u5efa\u4e00\u4e2a\u5b8c\u5168\u53ef\u5bfc\u822a\u7684\u8bed\u4e49\u7f51\u7edc\u3002\u7136\u540e\uff0c\u91c7\u7528\u81ea\u4e0b\u800c\u4e0a\u7684\u7ed3\u6784\u5f15\u5bfc\u68c0\u7d22\u7b56\u7565\uff0c\u5c06\u67e5\u8be2\u951a\u5b9a\u5230\u6700\u76f8\u5173\u7684\u7ec6\u7c92\u5ea6\u5b9e\u4f53\uff0c\u5e76\u7cfb\u7edf\u5730\u904d\u5386\u56fe\u7684\u8bed\u4e49\u8def\u5f84\uff0c\u4ee5\u6536\u96c6\u7b80\u6d01\u4e14\u4e0a\u4e0b\u6587\u5b8c\u6574\u7684\u8bc1\u636e\u96c6\u3002", "result": "\u5728\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLeanRAG\u5728\u54cd\u5e94\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u51cf\u5c11\u4e8646%\u7684\u68c0\u7d22\u5197\u4f59\u3002", "conclusion": "LeanRAG\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u77e5\u8bc6\u805a\u5408\u548c\u68c0\u7d22\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684RAG\u65b9\u6cd5\u4e2d\u8bed\u4e49\u5b64\u5c9b\u548c\u4f4e\u6548\u68c0\u7d22\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u95ee\u7b54\u4efb\u52a1\u7684\u54cd\u5e94\u8d28\u91cf\u5e76\u51cf\u5c11\u4e86\u68c0\u7d22\u5197\u4f59\u3002"}}
{"id": "2508.10425", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10425", "abs": "https://arxiv.org/abs/2508.10425", "authors": ["Yan Ting Chok", "Soyon Park", "Seungheun Baek", "Hajung Kim", "Junhyun Lee", "Jaewoo Kang"], "title": "HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation", "comment": null, "summary": "Medication recommendation is a crucial task for assisting physicians in\nmaking timely decisions from longitudinal patient medical records. However,\nreal-world EHR data present significant challenges due to the presence of\nrarely observed medical entities and incomplete records that may not fully\ncapture the clinical ground truth. While data-driven models trained on\nlongitudinal Electronic Health Records often achieve strong empirical\nperformance, they struggle to generalize under missing or novel conditions,\nlargely due to their reliance on observed co-occurrence patterns. To address\nthese issues, we propose Hierarchical Ontology and Network Refinement for\nRobust Medication Recommendation (HiRef), a unified framework that combines two\ncomplementary structures: (i) the hierarchical semantics encoded in curated\nmedical ontologies, and (ii) refined co-occurrence patterns derived from\nreal-world EHRs. We embed ontology entities in hyperbolic space, which\nnaturally captures tree-like relationships and enables knowledge transfer\nthrough shared ancestors, thereby improving generalizability to unseen codes.\nTo further improve robustness, we introduce a prior-guided sparse\nregularization scheme that refines the EHR co-occurrence graph by suppressing\nspurious edges while preserving clinically meaningful associations. Our model\nachieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and\nmaintains high accuracy under simulated unseen-code settings. Extensive\nexperiments with comprehensive ablation studies demonstrate HiRef's resilience\nto unseen medical codes, supported by in-depth analyses of the learned\nsparsified graph structure and medical code embeddings.", "AI": {"tldr": "HiRef\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u533b\u5b66\u672c\u4f53\u7684\u5c42\u6b21\u8bed\u4e49\u548c\u7ec6\u5316\u7684EHR\u5171\u73b0\u6a21\u5f0f\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u836f\u7269\u63a8\u8350\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754cEHR\u6570\u636e\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u9a71\u52a8\u6a21\u578b\u96be\u4ee5\u5904\u7406\u771f\u5b9e\u4e16\u754cEHR\u6570\u636e\u4e2d\u5b58\u5728\u7684\u7a00\u6709\u533b\u7597\u5b9e\u4f53\u548c\u4e0d\u5b8c\u6574\u8bb0\u5f55\u7b49\u95ee\u9898\uff0c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "HiRef\u6846\u67b6\u7ed3\u5408\u4e86\u4e24\u79cd\u4e92\u8865\u7ed3\u6784\uff1a\uff08i\uff09\u6765\u81ea\u533b\u5b66\u672c\u4f53\u7684\u5c42\u6b21\u8bed\u4e49\uff1b\uff08ii\uff09\u6765\u81ea\u771f\u5b9e\u4e16\u754cEHR\u7684\u7ec6\u5316\u5171\u73b0\u6a21\u5f0f\u3002\u5229\u7528\u53cc\u66f2\u7a7a\u95f4\u5d4c\u5165\u672c\u4f53\u5b9e\u4f53\uff0c\u5e76\u5f15\u5165\u5148\u9a8c\u5f15\u5bfc\u7684\u7a00\u758f\u6b63\u5219\u5316\u65b9\u6848\u6765\u7ec6\u5316EHR\u5171\u73b0\u56fe\u3002", "result": "HiRef\u6a21\u578b\u5728EHR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u672a\u89c1\u533b\u7597\u4ee3\u7801\u7684\u5f3a\u5927\u9c81\u68d2\u6027\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "HiRef\u6a21\u578b\u5728EHR\u57fa\u51c6\u6d4b\u8bd5\uff08MIMIC-III\u548cMIMIC-IV\uff09\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u5e76\u5728\u6a21\u62df\u7684\u672a\u89c1\u4ee3\u7801\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u4e14\u5bf9\u672a\u89c1\u533b\u7597\u4ee3\u7801\u5177\u6709\u5f88\u5f3a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.10429", "categories": ["cs.AI", "cs.CR", "cs.CV", "I.2.10; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.10429", "abs": "https://arxiv.org/abs/2508.10429", "authors": ["Yi Dong", "Yusuke Muraoka", "Scott Shi", "Yi Zhang"], "title": "MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance", "comment": "10 pages, 5 figures, 6 tables. The dataset is available at\n  https://huggingface.co/datasets/Codatta/MM-Food-100K", "summary": "We present MM-Food-100K, a public 100,000-sample multimodal food intelligence\ndataset with verifiable provenance. It is a curated approximately 10% open\nsubset of an original 1.2 million, quality-accepted corpus of food images\nannotated for a wide range of information (such as dish name, region of\ncreation). The corpus was collected over six weeks from over 87,000\ncontributors using the Codatta contribution model, which combines community\nsourcing with configurable AI-assisted quality checks; each submission is\nlinked to a wallet address in a secure off-chain ledger for traceability, with\na full on-chain protocol on the roadmap. We describe the schema, pipeline, and\nQA, and validate utility by fine-tuning large vision-language models (ChatGPT\n5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning\nyields consistent gains over out-of-box baselines across standard metrics; we\nreport results primarily on the MM-Food-100K subset. We release MM-Food-100K\nfor publicly free access and retain approximately 90% for potential commercial\naccess with revenue sharing to contributors.", "AI": {"tldr": "\u53d1\u5e03\u4e86\u4e00\u4e2a\u5927\u578b\u591a\u6a21\u6001\u98df\u54c1\u6570\u636e\u96c6MM-Food-100K\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u8425\u517b\u9884\u6d4b\u65b9\u9762\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u53ef\u9a8c\u8bc1\u6765\u6e90\u7684\u591a\u6a21\u6001\u98df\u54c1\u6570\u636e\u96c6\u9650\u5236\u4e86\u98df\u54c1\u667a\u80fd\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u6536\u96c6\u4e86\u6765\u81ea87000\u591a\u540d\u8d21\u732e\u8005\u7684120\u4e07\u5f20\u9ad8\u8d28\u91cf\u98df\u54c1\u56fe\u7247\uff0c\u5e76\u4f7f\u7528Codatta\u8d21\u732e\u6a21\u578b\u7ed3\u5408\u793e\u533a\u8d44\u6e90\u548cAI\u8f85\u52a9\u8d28\u91cf\u68c0\u67e5\u8fdb\u884c\u6574\u7406\uff0c\u6700\u7ec8\u53d1\u5e03\u4e8610\u4e07\u5f20\u56fe\u7247\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5269\u4f59\u90e8\u5206\u7528\u4e8e\u5546\u4e1a\u7528\u9014\u3002", "result": "\u6784\u5efa\u4e86MM-Food-100K\u6570\u636e\u96c6\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u56fe\u50cf\u8425\u517b\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b10\u4e07\u5f20\u56fe\u7247\u7684MM-Food-100K\u591a\u6a21\u6001\u98df\u54c1\u667a\u80fd\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a8c\u8bc1\u4e86\u5176\u5728\u56fe\u50cf\u8425\u517b\u9884\u6d4b\u65b9\u9762\u7684\u6548\u7528\uff0c\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\u7684\u7ed3\u679c\u3002"}}
{"id": "2508.10433", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10433", "abs": "https://arxiv.org/abs/2508.10433", "authors": ["Runqi Qiao", "Qiuna Tan", "Peiqing Yang", "Yanzi Wang", "Xiaowan Wang", "Enhui Wan", "Sitong Zhou", "Guanting Dong", "Yuchen Zeng", "Yida Xu", "Jie Wang", "Chong Sun", "Chen Li", "Honggang Zhang"], "title": "We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning", "comment": "Working in progress", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across various tasks, but still struggle with complex mathematical\nreasoning. Existing research primarily focuses on dataset construction and\nmethod optimization, often overlooking two critical aspects: comprehensive\nknowledge-driven design and model-centric data space modeling. In this paper,\nwe introduce We-Math 2.0, a unified system that integrates a structured\nmathematical knowledge system, model-centric data space modeling, and a\nreinforcement learning (RL)-based training paradigm to comprehensively enhance\nthe mathematical reasoning abilities of MLLMs. The key contributions of We-Math\n2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level\nhierarchical system encompassing 491 knowledge points and 1,819 fundamental\nprinciples. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a\ndataset that ensures broad conceptual coverage and flexibility through dual\nexpansion. Additionally, we define a three-dimensional difficulty space and\ngenerate 7 progressive variants per problem to build MathBook-Pro, a\nchallenging dataset for robust training. (3) MathBook-RL: We propose a\ntwo-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the\nmodel with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive\nAlignment RL, leveraging average-reward learning and dynamic data scheduling to\nachieve progressive alignment across difficulty levels. (4) MathBookEval: We\nintroduce a comprehensive benchmark covering all 491 knowledge points with\ndiverse reasoning step distributions. Experimental results show that\nMathBook-RL performs competitively with existing baselines on four widely-used\nbenchmarks and achieves strong results on MathBookEval, suggesting promising\ngeneralization in mathematical reasoning.", "AI": {"tldr": "We-Math 2.0 improves MLLM mathematical reasoning through a unified system incorporating structured knowledge, model-centric data modeling, and reinforcement learning, showing strong performance on benchmarks.", "motivation": "Existing MLLMs struggle with complex mathematical reasoning due to lack of comprehensive knowledge-driven design and model-centric data space modeling.", "method": "Developed We-Math 2.0, including a five-level hierarchical knowledge system (MathBook Knowledge System), two datasets (MathBook-Standard & Pro), and a two-stage RL framework (MathBook-RL).  Used a comprehensive benchmark (MathBookEval) for evaluation.", "result": "We-Math 2.0 shows competitive performance on existing benchmarks and strong results on MathBookEval, demonstrating promising generalization in mathematical reasoning.", "conclusion": "We-Math 2.0, a unified system integrating structured mathematical knowledge, model-centric data space modeling, and reinforcement learning, significantly enhances MLLMs' mathematical reasoning abilities."}}
