{"id": "2508.00081", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00081", "abs": "https://arxiv.org/abs/2508.00081", "authors": ["Fred Mutisya", "Shikoh Gitau", "Nasubo Ongoma", "Keith Mbae", "Elizabeth Wamicha"], "title": "Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench", "comment": null, "summary": "HealthBench, a benchmark designed to measure the capabilities of AI systems\nfor health better (Arora et al., 2025), has advanced medical language model\nevaluation through physician-crafted dialogues and transparent rubrics.\nHowever, its reliance on expert opinion, rather than high-tier clinical\nevidence, risks codifying regional biases and individual clinician\nidiosyncrasies, further compounded by potential biases in automated grading\nsystems. These limitations are particularly magnified in low- and middle-income\nsettings, where issues like sparse neglected tropical disease coverage and\nregion-specific guideline mismatches are prevalent.\n  The unique challenges of the African context, including data scarcity,\ninadequate infrastructure, and nascent regulatory frameworks, underscore the\nurgent need for more globally relevant and equitable benchmarks. To address\nthese shortcomings, we propose anchoring reward functions in version-controlled\nClinical Practice Guidelines (CPGs) that incorporate systematic reviews and\nGRADE evidence ratings.\n  Our roadmap outlines \"evidence-robust\" reinforcement learning via\nrubric-to-guideline linkage, evidence-weighted scoring, and contextual override\nlogic, complemented by a focus on ethical considerations and the integration of\ndelayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,\nwhile preserving HealthBench's transparency and physician engagement, we aim to\nfoster medical language models that are not only linguistically polished but\nalso clinically trustworthy, ethically sound, and globally relevant.", "AI": {"tldr": "\u6539\u8fdbHealthBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u5176\u66f4\u5ba2\u89c2\u3001\u66f4\u7b26\u5408\u5168\u7403\u6807\u51c6\u3002", "motivation": "HealthBench\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u5730\u533a\u504f\u89c1\u548c\u4e2a\u4f53\u4e34\u5e8a\u533b\u751f\u7684\u7279\u6b8a\u6027\u7b49\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u4e2d\u4f4e\u6536\u5165\u5730\u533a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7248\u672c\u63a7\u5236\u7684\u4e34\u5e8a\u5b9e\u8df5\u6307\u5357\uff08CPG\uff09\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u5177\u6709\u5168\u7403\u76f8\u5173\u6027\u548c\u516c\u5e73\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u3002", "conclusion": "\u672c\u8bba\u6587\u8ba4\u4e3aHealthBench\u57fa\u51c6\u6d4b\u8bd5\u4f9d\u8d56\u4e13\u5bb6\u610f\u89c1\u800c\u975e\u9ad8\u7ea7\u4e34\u5e8a\u8bc1\u636e\uff0c\u53ef\u80fd\u5bfc\u81f4\u5730\u533a\u504f\u89c1\u548c\u4e2a\u4f53\u4e34\u5e8a\u533b\u751f\u7684\u7279\u6b8a\u6027\uff0c\u5c24\u5176\u662f\u5728\u4e2d\u4f4e\u6536\u5165\u5730\u533a\u95ee\u9898\u66f4\u4e3a\u4e25\u91cd\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7248\u672c\u63a7\u5236\u7684\u4e34\u5e8a\u5b9e\u8df5\u6307\u5357\uff08CPG\uff09\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4ee5\u589e\u5f3a\u533b\u7597\u8bed\u8a00\u6a21\u578b\u7684\u4e34\u5e8a\u53ef\u4fe1\u5ea6\u3001\u4f26\u7406\u6027\u548c\u5168\u7403\u76f8\u5173\u6027\u3002"}}
{"id": "2508.00106", "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.00106", "abs": "https://arxiv.org/abs/2508.00106", "authors": ["Ernest Bonnah", "Luan Viet Nguyen", "Khaza Anuarul Hoque"], "title": "Hyperproperty-Constrained Secure Reinforcement Learning", "comment": "Accepted in IEEE/ACM MEMOCODE 2025", "summary": "Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a\ndomain-specific formal specification language known for its effectiveness in\ncompactly representing security, opacity, and concurrency properties for\nrobotics applications. This paper focuses on HyperTWTL-constrained secure\nreinforcement learning (SecRL). Although temporal logic-constrained safe\nreinforcement learning (SRL) is an evolving research problem with several\nexisting literature, there is a significant research gap in exploring\nsecurity-aware reinforcement learning (RL) using hyperproperties. Given the\ndynamics of an agent as a Markov Decision Process (MDP) and opacity/security\nconstraints formalized as HyperTWTL, we propose an approach for learning\nsecurity-aware optimal policies using dynamic Boltzmann softmax RL while\nsatisfying the HyperTWTL constraints. The effectiveness and scalability of our\nproposed approach are demonstrated using a pick-up and delivery robotic mission\ncase study. We also compare our results with two other baseline RL algorithms,\nshowing that our proposed method outperforms them.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86 HyperTWTL \u7ea6\u675f\u4e0b\u7684\u5b89\u5168\u7b56\u7565\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u8f83\u5c11\u5173\u6ce8\u5229\u7528\u8d85\u5c5e\u6027\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001 Boltzmann softmax \u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u6ee1\u8db3 HyperTWTL \u7ea6\u675f\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728 pick-up and delivery \u673a\u5668\u4eba\u4efb\u52a1\u6848\u4f8b\u7814\u7a76\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u5176\u4ed6\u4e24\u79cd\u57fa\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001 Boltzmann softmax \u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u6ee1\u8db3 HyperTWTL \u7ea6\u675f\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5728 pick-up and delivery \u673a\u5668\u4eba\u4efb\u52a1\u6848\u4f8b\u7814\u7a76\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4e0e\u5176\u4ed6\u4e24\u79cd\u57fa\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6027\u80fd\u66f4\u4f18\u3002"}}
{"id": "2508.00116", "categories": ["cs.AI", "H.4.1; I.2.1"], "pdf": "https://arxiv.org/pdf/2508.00116", "abs": "https://arxiv.org/abs/2508.00116", "authors": ["Wil M. P. van der Aalst"], "title": "No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence", "comment": "10 pages, 4 figures, preprint keynote paper of the seventh\n  International Conference on Intelligent and Fuzzy Systems (INFUS 2025)", "summary": "The uptake of Artificial Intelligence (AI) impacts the way we work, interact,\ndo business, and conduct research. However, organizations struggle to apply AI\nsuccessfully in industrial settings where the focus is on end-to-end\noperational processes. Here, we consider generative, predictive, and\nprescriptive AI and elaborate on the challenges of diagnosing and improving\nsuch processes. We show that AI needs to be grounded using Object-Centric\nProcess Mining (OCPM). Process-related data are structured and\norganization-specific and, unlike text, processes are often highly dynamic.\nOCPM is the missing link connecting data and processes and enables different\nforms of AI. We use the term Process Intelligence (PI) to refer to the\namalgamation of process-centric data-driven techniques able to deal with a\nvariety of object and event types, enabling AI in an organizational context.\nThis paper explains why AI requires PI to improve operational processes and\nhighlights opportunities for successfully combining OCPM and generative,\npredictive, and prescriptive AI.", "AI": {"tldr": "\u4eba\u5de5\u667a\u80fd\u9700\u8981\u6d41\u7a0b\u667a\u80fd\u6765\u6539\u8fdb\u8fd0\u8425\u6d41\u7a0b\u3002", "motivation": "\u7ec4\u7ec7\u673a\u6784\u96be\u4ee5\u5728\u6ce8\u91cd\u7aef\u5230\u7aef\u8fd0\u8425\u6d41\u7a0b\u7684\u5de5\u4e1a\u73af\u5883\u4e2d\u6210\u529f\u5e94\u7528\u4eba\u5de5\u667a\u80fd\u3002", "method": "\u672c\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0f\u3001\u9884\u6d4b\u5f0f\u548c\u89c4\u8303\u6027\u4eba\u5de5\u667a\u80fd\uff0c\u5e76\u9610\u8ff0\u4e86\u8bca\u65ad\u548c\u6539\u8fdb\u6b64\u7c7b\u6d41\u7a0b\u7684\u6311\u6218\uff0c\u63d0\u51fa\u5c06\u9762\u5411\u5bf9\u8c61\u6d41\u7a0b\u6316\u6398\u4f5c\u4e3a\u8fde\u63a5\u6570\u636e\u548c\u6d41\u7a0b\u7684\u6865\u6881\u3002", "result": "\u9762\u5411\u5bf9\u8c61\u6d41\u7a0b\u6316\u6398\u662f\u8fde\u63a5\u6570\u636e\u548c\u6d41\u7a0b\u7684\u7f3a\u5931\u73af\u8282\uff0c\u5b83\u80fd\u591f\u652f\u6301\u4e0d\u540c\u5f62\u5f0f\u7684\u4eba\u5de5\u667a\u80fd\uff0c\u5e76\u901a\u8fc7\u6d41\u7a0b\u667a\u80fd\u5c06\u591a\u79cd\u6570\u636e\u9a71\u52a8\u6280\u672f\u7ed3\u5408\uff0c\u4ece\u800c\u5728\u7ec4\u7ec7\u73af\u5883\u4e2d\u5b9e\u73b0\u4eba\u5de5\u667a\u80fd\u3002", "conclusion": "\u672c\u6587\u8bba\u8ff0\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u6539\u8fdb\u8fd0\u8425\u6d41\u7a0b\u4e2d\u9700\u8981\u6d41\u7a0b\u667a\u80fd\u7684\u539f\u56e0\uff0c\u5e76\u5f3a\u8c03\u4e86\u6210\u529f\u7ed3\u5408\u9762\u5411\u5bf9\u8c61\u6d41\u7a0b\u6316\u6398\u548c\u751f\u6210\u5f0f\u3001\u9884\u6d4b\u5f0f\u4ee5\u53ca\u89c4\u8303\u6027\u4eba\u5de5\u667a\u80fd\u7684\u673a\u4f1a\u3002"}}
{"id": "2508.00129", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.00129", "abs": "https://arxiv.org/abs/2508.00129", "authors": ["Agust\u00edn Borda", "Juan Bautista Cabral", "Gonzalo Giarda", "Diego Nicol\u00e1s Gimenez Irusta", "Paula Pacheco", "Alvaro Roy Schachner"], "title": "Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis", "comment": null, "summary": "In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem\nthat can greatly affect the results of a Multi-Criteria Decision Method against\na particular set of alternatives. It is therefore useful to have a mechanism\nthat allows one to measure the performance of a method on a set of\nalternatives. This idea could be taken further to build a global ranking of the\neffectiveness of different methods to solve a problem. In this paper, we\npresent three tests that detect the presence of Rank Reversals, along with\ntheir implementation in the Scikit-Criteria library. We also address the\ncomplications that arise when implementing these tests for general scenarios\nand the design considerations we made to handle them. We close with a\ndiscussion about how these additions could play a major role in the judgment of\nmulti-criteria decision methods for problem solving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e09\u79cd\u68c0\u6d4b\u591a\u51c6\u5219\u51b3\u7b56\u65b9\u6cd5\u4e2d\u6392\u5e8f\u9006\u8f6c\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5e76\u5728Scikit-Criteria\u5e93\u4e2d\u5b9e\u73b0\uff0c\u4ee5\u6539\u8fdb\u591a\u51c6\u5219\u51b3\u7b56\u65b9\u6cd5\u7684\u8bc4\u5224\u3002", "motivation": "\u591a\u51c6\u5219\u51b3\u7b56\u5206\u6790\u4e2d\u6392\u5e8f\u9006\u8f6c\u95ee\u9898\u4f1a\u4e25\u91cd\u5f71\u54cd\u7ed3\u679c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u8861\u91cf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u68c0\u6d4b\u6392\u5e8f\u9006\u8f6c\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5e76\u5728Scikit-Criteria\u5e93\u4e2d\u5b9e\u73b0\u3002", "result": "\u5b9e\u73b0\u4e86\u4e09\u79cd\u68c0\u6d4b\u6392\u5e8f\u9006\u8f6c\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5b9e\u73b0\u590d\u6742\u6027\u548c\u8bbe\u8ba1\u8003\u8651\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e09\u79cd\u68c0\u6d4b\u6392\u5e8f\u9006\u8f6c\u7684\u6d4b\u8bd5\u65b9\u6cd5\u53ca\u5176\u5728Scikit-Criteria\u5e93\u4e2d\u7684\u5b9e\u73b0\uff0c\u5e76\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u6d4b\u8bd5\u5728\u4e00\u822c\u573a\u666f\u4e2d\u7684\u5b9e\u73b0\u590d\u6742\u6027\u4ee5\u53ca\u8bbe\u8ba1\u8003\u8651\uff0c\u6700\u540e\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u65b0\u589e\u529f\u80fd\u5982\u4f55\u5bf9\u591a\u51c6\u5219\u51b3\u7b56\u65b9\u6cd5\u7684\u8bc4\u5224\u4ea7\u751f\u91cd\u5927\u5f71\u54cd\u3002"}}
{"id": "2508.00137", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00137", "abs": "https://arxiv.org/abs/2508.00137", "authors": ["Shqiponja Ahmetaj", "George Konstantinidis", "Magdalena Ortiz", "Paolo Pareti", "Mantas Simkus"], "title": "SHACL Validation under Graph Updates (Extended Paper)", "comment": "Accepted at the International Semantic Web Conference (ISWC 2025)", "summary": "SHACL (SHApe Constraint Language) is a W3C standardized constraint language\nfor RDF graphs. In this paper, we study SHACL validation in RDF graphs under\nupdates. We present a SHACL-based update language that can capture intuitive\nand realistic modifications on RDF graphs and study the problem of static\nvalidation under such updates. This problem asks to verify whether every graph\nthat validates a SHACL specification will still do so after applying a given\nupdate sequence. More importantly, it provides a basis for further services for\nreasoning about evolving RDF graphs. Using a regression technique that embeds\nthe update actions into SHACL constraints, we show that static validation under\nupdates can be reduced to (un)satisfiability of constraints in (a minor\nextension of) SHACL. We analyze the computational complexity of the static\nvalidation problem for SHACL and some key fragments. Finally, we present a\nprototype implementation that performs static validation and other static\nanalysis tasks on SHACL constraints and demonstrate its behavior through\npreliminary experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cdSHACL\u66f4\u65b0\u8bed\u8a00\uff0c\u901a\u8fc7\u56de\u5f52\u6280\u672f\u5c06\u66f4\u65b0\u4e0b\u7684\u9759\u6001\u9a8c\u8bc1\u7b80\u5316\u4e3aSHACL\u7ea6\u675f\u7684\uff08\u4e0d\uff09\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u539f\u578b\u7cfb\u7edf\u3002", "motivation": "\u7814\u7a76RDF\u56fe\u5728\u66f4\u65b0\u4e0b\u7684SHACL\u9a8c\u8bc1\u95ee\u9898\uff0c\u4e3a\u63a8\u7406\u6f14\u53d8\u7684RDF\u56fe\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSHACL\u7684\u66f4\u65b0\u8bed\u8a00\uff0c\u4f7f\u7528\u5c06\u66f4\u65b0\u64cd\u4f5c\u5d4c\u5165SHACL\u7ea6\u675f\u7684\u56de\u5f52\u6280\u672f\u3002", "result": "\u5c06\u66f4\u65b0\u4e0b\u7684\u9759\u6001\u9a8c\u8bc1\u7b80\u5316\u4e3aSHACL\u7ea6\u675f\u7684\uff08\u4e0d\uff09\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\uff0c\u5e76\u5206\u6790\u4e86\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5b9e\u73b0\u4e86\u4e00\u4e2a\u539f\u578b\u7cfb\u7edf\u3002", "conclusion": "\u7814\u7a76\u4e86RDF\u56fe\u5728\u66f4\u65b0\u4e0b\u7684SHACL\u9a8c\u8bc1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSHACL\u7684\u66f4\u65b0\u8bed\u8a00\uff0c\u5e76\u901a\u8fc7\u5c06\u66f4\u65b0\u64cd\u4f5c\u5d4c\u5165SHACL\u7ea6\u675f\u7684\u56de\u5f52\u6280\u672f\uff0c\u5c06\u66f4\u65b0\u4e0b\u7684\u9759\u6001\u9a8c\u8bc1\u7b80\u5316\u4e3aSHACL\u7ea6\u675f\u7684\uff08\u4e0d\uff09\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\uff0c\u5206\u6790\u4e86SHACL\u53ca\u5176\u5173\u952e\u7247\u6bb5\u7684\u9759\u6001\u9a8c\u8bc1\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u6700\u540e\u5b9e\u73b0\u4e86\u4e00\u4e2a\u539f\u578b\u7cfb\u7edf\u3002"}}
{"id": "2508.00138", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00138", "abs": "https://arxiv.org/abs/2508.00138", "authors": ["Rashid Mushkani", "Hugo Berard", "Toumadher Ammar", "Cassandre Chatonnier", "Shin Koseki"], "title": "Co-Producing AI: Toward an Augmented, Participatory Lifecycle", "comment": "Eighth AAAI/ACM Conference on AI, Ethics, and Society 2025", "summary": "Despite efforts to mitigate the inherent risks and biases of artificial\nintelligence (AI) algorithms, these algorithms can disproportionately impact\nculturally marginalized groups. A range of approaches has been proposed to\naddress or reduce these risks, including the development of ethical guidelines\nand principles for responsible AI, as well as technical solutions that promote\nalgorithmic fairness. Drawing on design justice, expansive learning theory, and\nrecent empirical work on participatory AI, we argue that mitigating these harms\nrequires a fundamental re-architecture of the AI production pipeline. This\nre-design should center co-production, diversity, equity, inclusion (DEI), and\nmultidisciplinary collaboration. We introduce an augmented AI lifecycle\nconsisting of five interconnected phases: co-framing, co-design,\nco-implementation, co-deployment, and co-maintenance. The lifecycle is informed\nby four multidisciplinary workshops and grounded in themes of distributed\nauthority and iterative knowledge exchange. Finally, we relate the proposed\nlifecycle to several leading ethical frameworks and outline key research\nquestions that remain for scaling participatory governance.", "AI": {"tldr": "\u91cd\u65b0\u8bbe\u8ba1AI\u751f\u4ea7\u6d41\u7a0b\uff0c\u4ee5\u5408\u4f5c\u751f\u4ea7\u548c\u591a\u5b66\u79d1\u534f\u4f5c\u4e3a\u6838\u5fc3\uff0c\u51cf\u8f7bAI\u7b97\u6cd5\u5bf9\u5f31\u52bf\u7fa4\u4f53\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "motivation": "AI\u7b97\u6cd5\u56fa\u6709\u7684\u98ce\u9669\u548c\u504f\u5dee\u53ef\u80fd\u5bf9\u5f31\u52bf\u7fa4\u4f53\u4ea7\u751f\u4e0d\u6210\u6bd4\u4f8b\u7684\u5f71\u54cd\u3002", "method": "\u57fa\u4e8e\u8bbe\u8ba1\u6b63\u4e49\u3001\u6269\u5c55\u5b66\u4e60\u7406\u8bba\u548c\u53c2\u4e0e\u5f0fAI\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684AI\u751f\u547d\u5468\u671f\uff0c\u5305\u542b\u4e94\u4e2a\u9636\u6bb5\uff1a\u5408\u4f5c\u6784\u5efa\u3001\u5408\u4f5c\u8bbe\u8ba1\u3001\u5408\u4f5c\u5b9e\u65bd\u3001\u5408\u4f5c\u90e8\u7f72\u548c\u5408\u4f5c\u7ef4\u62a4\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u4e94\u4e2a\u9636\u6bb5\u7684\u589e\u5f3a\u578bAI\u751f\u547d\u5468\u671f\u6a21\u578b\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u516c\u5e73\u3001\u66f4\u516c\u6b63\u7684AI\u7cfb\u7edf\u5f00\u53d1\u3002", "conclusion": "\u672c\u6587\u8ba4\u4e3a\uff0c\u51cf\u8f7bAI\u7b97\u6cd5\u5bf9\u5f31\u52bf\u7fa4\u4f53\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u9700\u8981\u91cd\u65b0\u6784\u5efaAI\u751f\u4ea7\u6d41\u7a0b\uff0c\u63d0\u5021\u5408\u4f5c\u751f\u4ea7\u3001\u591a\u6837\u6027\u3001\u516c\u5e73\u3001\u5305\u5bb9\u548c\u591a\u5b66\u79d1\u534f\u4f5c\u3002"}}
{"id": "2508.00143", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.00143", "abs": "https://arxiv.org/abs/2508.00143", "authors": ["Danielle R. Thomas", "Conrad Borchers", "Kenneth R. Koedinger"], "title": "Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation", "comment": "Accepted for presentation at NCME AIME-Con 2025", "summary": "Humans can be notoriously imperfect evaluators. They are often biased,\nunreliable, and unfit to define \"ground truth.\" Yet, given the surging need to\nproduce large amounts of training data in educational applications using AI,\ntraditional inter-rater reliability (IRR) metrics like Cohen's kappa remain\ncentral to validating labeled data. IRR remains a cornerstone of many machine\nlearning pipelines for educational data. Take, for example, the classification\nof tutors' moves in dialogues or labeling open responses in machine-graded\nassessments. This position paper argues that overreliance on human IRR as a\ngatekeeper for annotation quality hampers progress in classifying data in ways\nthat are valid and predictive in relation to improving learning. To address\nthis issue, we highlight five examples of complementary evaluation methods,\nsuch as multi-label annotation schemes, expert-based approaches, and\nclose-the-loop validity. We argue that these approaches are in a better\nposition to produce training data and subsequent models that produce improved\nstudent learning and more actionable insights than IRR approaches alone. We\nalso emphasize the importance of external validity, for example, by\nestablishing a procedure of validating tutor moves and demonstrating that it\nworks across many categories of tutor actions (e.g., providing hints). We call\non the field to rethink annotation quality and ground truth--prioritizing\nvalidity and educational impact over consensus alone.", "AI": {"tldr": "\u8fc7\u5ea6\u4f9d\u8d56\u4eba\u7c7bIRR\u9a8c\u8bc1\u6570\u636e\u8d28\u91cf\u4f1a\u963b\u788d\u8fdb\u6b65\uff0c\u5efa\u8bae\u7ed3\u5408\u591a\u6807\u7b7e\u6807\u6ce8\u3001\u4e13\u5bb6\u65b9\u6cd5\u548c\u95ed\u73af\u6548\u5ea6\u7b49\u65b9\u6cd5\uff0c\u4f18\u5148\u8003\u8651\u6548\u5ea6\u548c\u6559\u80b2\u5f71\u54cd\u3002", "motivation": "\u6539\u8fdb\u6559\u80b2\u6570\u636e\u6807\u6ce8\u548c\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u5b66\u751f\u5b66\u4e60\u6548\u679c\u548c\u4ea7\u751f\u66f4\u6709\u6548\u7684\u89c1\u89e3\u3002", "method": "\u5206\u6790\u4e86\u73b0\u6709\u57fa\u4e8eIRR\u7684\u6807\u6ce8\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e94\u79cd\u8865\u5145\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e94\u79cd\u8865\u5145\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u5f3a\u8c03\u5916\u90e8\u6548\u5ea6\u7684\u91cd\u8981\u6027\uff0c\u547c\u5401\u91cd\u65b0\u601d\u8003\u6807\u6ce8\u8d28\u91cf\u548cground truth\u7684\u6982\u5ff5\u3002", "conclusion": "\u8fc7\u5ea6\u4f9d\u8d56\u4eba\u7c7b\u8bc4\u5224\u8005\u95f4\u4fe1\u5ea6 (IRR) \u6765\u9a8c\u8bc1\u6807\u6ce8\u8d28\u91cf\u963b\u788d\u4e86\u6559\u80b2\u6570\u636e\u5206\u7c7b\u7684\u8fdb\u5c55\uff0c\u8be5\u8bba\u6587\u5efa\u8bae\u91c7\u7528\u591a\u6807\u7b7e\u6807\u6ce8\u65b9\u6848\u3001\u4e13\u5bb6\u65b9\u6cd5\u548c\u95ed\u73af\u6548\u5ea6\u7b49\u8865\u5145\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6570\u636e\u7684\u8d28\u91cf\u548c\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u6700\u7ec8\u63d0\u5347\u5b66\u751f\u5b66\u4e60\u6548\u679c\u548c\u4ea7\u751f\u66f4\u6709\u6548\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.00159", "categories": ["cs.AI", "cs.CY", "cs.LG", "econ.TH", "math.OC", "68Txx", "I.2"], "pdf": "https://arxiv.org/pdf/2508.00159", "abs": "https://arxiv.org/abs/2508.00159", "authors": ["Jobst Heitzig", "Ram Potham"], "title": "Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power", "comment": null, "summary": "Power is a key concept in AI safety: power-seeking as an instrumental goal,\nsudden or gradual disempowerment of humans, power balance in human-AI\ninteraction and international AI governance. At the same time, power as the\nability to pursue diverse goals is essential for wellbeing.\n  This paper explores the idea of promoting both safety and wellbeing by\nforcing AI agents explicitly to empower humans and to manage the power balance\nbetween humans and AI agents in a desirable way. Using a principled, partially\naxiomatic approach, we design a parametrizable and decomposable objective\nfunction that represents an inequality- and risk-averse long-term aggregate of\nhuman power. It takes into account humans' bounded rationality and social\nnorms, and, crucially, considers a wide variety of possible human goals.\n  We derive algorithms for computing that metric by backward induction or\napproximating it via a form of multi-agent reinforcement learning from a given\nworld model. We exemplify the consequences of (softly) maximizing this metric\nin a variety of paradigmatic situations and describe what instrumental\nsub-goals it will likely imply. Our cautious assessment is that softly\nmaximizing suitable aggregate metrics of human power might constitute a\nbeneficial objective for agentic AI systems that is safer than direct\nutility-based objectives.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u8f6f\u5316\u6700\u5927\u5316\u4eba\u7c7b\u80fd\u529b\u4f5c\u4e3a\u4eba\u5de5\u667a\u80fd\u5b89\u5168\u7684\u65b0\u76ee\u6807\uff0c\u8ba4\u4e3a\u5176\u6bd4\u76f4\u63a5\u57fa\u4e8e\u6548\u7528\u7684\u76ee\u6807\u66f4\u5b89\u5168\u3002", "motivation": "\u63a2\u7d22\u901a\u8fc7\u5f3a\u5236\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u660e\u786e\u8d4b\u80fd\u4eba\u7c7b\u5e76\u4ee5\u7406\u60f3\u65b9\u5f0f\u7ba1\u7406\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u4e4b\u95f4\u7684\u529b\u91cf\u5e73\u8861\u6765\u4fc3\u8fdb\u5b89\u5168\u548c\u798f\u7949\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u539f\u7406\u7684\u3001\u90e8\u5206\u516c\u7406\u5316\u7684\u65b9\u6cd5\u8bbe\u8ba1\u53c2\u6570\u5316\u548c\u53ef\u5206\u89e3\u7684\u76ee\u6807\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u4ee3\u8868\u4eba\u7c7b\u80fd\u529b\u7684\u98ce\u9669\u89c4\u907f\u578b\u957f\u671f\u603b\u91cf\uff0c\u5e76\u901a\u8fc7\u9006\u5411\u5f52\u7eb3\u6216\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8fd1\u4f3c\u8ba1\u7b97\u8be5\u6307\u6807\u3002", "result": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53c2\u6570\u5316\u548c\u53ef\u5206\u89e3\u7684\u76ee\u6807\u51fd\u6570\uff0c\u4ee3\u8868\u4eba\u7c7b\u80fd\u529b\u7684\u4e0d\u5e73\u7b49\u548c\u98ce\u9669\u89c4\u907f\u7684\u957f\u671f\u603b\u548c\uff0c\u5e76\u5bfc\u51fa\u4e86\u8ba1\u7b97\u8be5\u6307\u6807\u7684\u7b97\u6cd5\u3002", "conclusion": "\u8f6f\u5316\u6700\u5927\u5316\u4eba\u7c7b\u80fd\u529b\u7684\u805a\u5408\u6307\u6807\u53ef\u80fd\u6784\u6210\u8d4b\u80fd\u578b\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u6709\u76ca\u76ee\u6807\uff0c\u6bd4\u76f4\u63a5\u57fa\u4e8e\u6548\u7528\u7684\u76ee\u6807\u66f4\u5b89\u5168\u3002"}}
{"id": "2508.00222", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00222", "abs": "https://arxiv.org/abs/2508.00222", "authors": ["Yihong Dong", "Xue Jiang", "Yongding Tao", "Huanyu Liu", "Kechi Zhang", "Lili Mou", "Rongyu Cao", "Yingwei Ma", "Jue Chen", "Binhua Li", "Zhi Jin", "Fei Huang", "Yongbin Li", "Ge Li"], "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization", "comment": null, "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its inherently on-policy strategy with LLM's immense\naction space and sparse reward. Further, RLVR can lead to the capability\nboundary collapse, narrowing the LLM's problem-solving scope. To address this\nproblem, we propose RL-PLUS, a novel approach that synergizes internal\nexploitation (i.e., Thinking) with external data (i.e., Learning) to achieve\nstronger reasoning capabilities and surpass the boundaries of base models.\nRL-PLUS integrates two core components: Multiple Importance Sampling to address\nfor distributional mismatch from external data, and an Exploration-Based\nAdvantage Function to guide the model towards high-value, unexplored reasoning\npaths. We provide both theoretical analysis and extensive experiments to\ndemonstrate the superiority and generalizability of our approach. The results\nshow that RL-PLUS achieves state-of-the-art performance compared with existing\nRLVR methods on six math reasoning benchmarks and exhibits superior performance\non six out-of-distribution reasoning tasks. It also achieves consistent and\nsignificant gains across diverse model families, with average relative\nimprovements ranging from 21.1\\% to 69.2\\%. Moreover, Pass@k curves across\nmultiple benchmarks indicate that RL-PLUS effectively resolves the capability\nboundary collapse problem.", "AI": {"tldr": "RL-PLUS\u901a\u8fc7\u7ed3\u5408\u5185\u90e8\u601d\u8003\u548c\u5916\u90e8\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u80fd\u529b\u8fb9\u754c\u5d29\u6e83\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684RLVR\u65b9\u6cd5\u7531\u4e8e\u5176\u56fa\u6709\u7684\u7b56\u7565\u4ee5\u53caLLM\u5de8\u5927\u7684\u52a8\u4f5c\u7a7a\u95f4\u548c\u7a00\u758f\u5956\u52b1\uff0c\u96be\u4ee5\u7a81\u7834\u57fa\u7840LLM\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u751a\u81f3\u53ef\u80fd\u5bfc\u81f4\u80fd\u529b\u8fb9\u754c\u5d29\u6e83\u3002", "method": "RL-PLUS\u6574\u5408\u4e86\u591a\u91cd\u8981\u6027\u91c7\u6837\u548c\u57fa\u4e8e\u63a2\u7d22\u7684\u4f18\u52bf\u51fd\u6570\uff0c\u4ee5\u89e3\u51b3\u5916\u90e8\u6570\u636e\u5206\u5e03\u9519\u914d\u95ee\u9898\uff0c\u5e76\u5f15\u5bfc\u6a21\u578b\u63a2\u7d22\u9ad8\u4ef7\u503c\u7684\u63a8\u7406\u8def\u5f84\u3002", "result": "RL-PLUS\u5728\u516d\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548c\u516d\u4e2a\u5206\u5e03\u5916\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e0e\u73b0\u6709RLVR\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5e73\u5747\u76f8\u5bf9\u6539\u8fdb\u7387\u8fbe21.1%\u523069.2%\u3002Pass@k\u66f2\u7ebf\u8868\u660e\uff0cRL-PLUS\u6709\u6548\u89e3\u51b3\u4e86\u80fd\u529b\u8fb9\u754c\u5d29\u6e83\u95ee\u9898\u3002", "conclusion": "RL-PLUS\uff0c\u4e00\u79cd\u7ed3\u5408\u5185\u90e8\u5229\u7528\uff08\u601d\u8003\uff09\u548c\u5916\u90e8\u6570\u636e\uff08\u5b66\u4e60\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709RLVR\u65b9\u6cd5\u80fd\u529b\u8fb9\u754c\u5d29\u6e83\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.00271", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.00271", "abs": "https://arxiv.org/abs/2508.00271", "authors": ["Hongjin Qian", "Zheng Liu"], "title": "MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning", "comment": "Technical Report, 14 pages", "summary": "In this work, we propose MetaAgent, an agentic paradigm inspired by the\nprinciple of learning-by-doing, where expertise is developed through hands-on\npractice and continual self-improvement. MetaAgent starts with a minimal\nworkflow, equipped only with basic reasoning and adaptive help-seeking\nabilities. When a knowledge gap is encountered, MetaAgent generates natural\nlanguage help requests, which are routed to the most suitable external tool by\na dedicated tool router. As MetaAgent solves tasks, it continually conducts\nself-reflection and answer verification, distilling actionable experience into\nconcise texts that are dynamically incorporated into future task contexts.\nBesides, MetaAgent autonomously builds in-house tools and a persistent\nknowledge base by organizing its tool-use history, further enhancing its\nability to retrieve and integrate relevant information We term this continual,\ndata-driven process as \\textit{meta tool learning}, through which MetaAgent\nincrementally refines its reasoning and tool-use strategies, without changing\nmodel parameters or requiring further post-training. Evaluated on challenging\nknowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,\nMetaAgent consistently outperforms workflow-based baselines and matches or\nexceeds end-to-end trained agents, demonstrating the promise of self-evolving\nagentic systems for robust, general-purpose knowledge discovery. We provide our\nsource codes in https://github.com/qhjqhj00/MetaAgent.", "AI": {"tldr": "MetaAgent:\u4e00\u4e2a\u901a\u8fc7\u81ea\u6211\u6539\u8fdb\u548c\u5143\u5de5\u5177\u5b66\u4e60\u6301\u7eed\u63d0\u5347\u80fd\u529b\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u5728\u77e5\u8bc6\u53d1\u73b0\u65b9\u9762\u8868\u73b0\u4f18\u5f02", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u901a\u8fc7\u5b9e\u8df5\u548c\u81ea\u6211\u6539\u8fdb\u6301\u7eed\u63d0\u5347\u4e13\u4e1a\u6280\u80fd\u7684\u4ee3\u7406\u7cfb\u7edf\u3002", "method": "MetaAgent\u662f\u4e00\u79cd\u57fa\u4e8e\u201c\u5b66\u4e60\u901a\u8fc7\u5b9e\u8df5\u201d\u539f\u5219\u7684\u4ee3\u7406\u8303\u5f0f\uff0c\u5b83\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5bfb\u6c42\u5e2e\u52a9\uff0c\u5e76\u901a\u8fc7\u5143\u5de5\u5177\u5b66\u4e60\u6301\u7eed\u6539\u8fdb\u5176\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u7b56\u7565\uff0c\u65e0\u9700\u66f4\u6539\u6a21\u578b\u53c2\u6570\u6216\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u3002", "result": "MetaAgent\u5728GAIA\u3001WebWalkerQA\u548cBrowseCamp\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u77e5\u8bc6\u53d1\u73b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6301\u7eed\u4f18\u4e8e\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7684\u57fa\u7ebf\uff0c\u5e76\u5339\u914d\u6216\u8d85\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u3002", "conclusion": "MetaAgent\u5728\u77e5\u8bc6\u53d1\u73b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7684\u57fa\u7ebf\uff0c\u5e76\u5339\u914d\u6216\u8d85\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\uff0c\u5c55\u793a\u4e86\u81ea\u6211\u8fdb\u5316\u4ee3\u7406\u7cfb\u7edf\u5728\u5065\u58ee\u3001\u901a\u7528\u77e5\u8bc6\u53d1\u73b0\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00282", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00282", "abs": "https://arxiv.org/abs/2508.00282", "authors": ["Yi-Long Lu", "Jiajun Song", "Chunhui Zhang", "Wei Wang"], "title": "Mind the Gap: The Divergence Between Human and LLM-Generated Tasks", "comment": null, "summary": "Humans constantly generate a diverse range of tasks guided by internal\nmotivations. While generative agents powered by large language models (LLMs)\naim to simulate this complex behavior, it remains uncertain whether they\noperate on similar cognitive principles. To address this, we conducted a\ntask-generation experiment comparing human responses with those of an LLM agent\n(GPT-4o). We find that human task generation is consistently influenced by\npsychological drivers, including personal values (e.g., Openness to Change) and\ncognitive style. Even when these psychological drivers are explicitly provided\nto the LLM, it fails to reflect the corresponding behavioral patterns. They\nproduce tasks that are markedly less social, less physical, and thematically\nbiased toward abstraction. Interestingly, while the LLM's tasks were perceived\nas more fun and novel, this highlights a disconnect between its linguistic\nproficiency and its capacity to generate human-like, embodied goals.We conclude\nthat there is a core gap between the value-driven, embodied nature of human\ncognition and the statistical patterns of LLMs, highlighting the necessity of\nincorporating intrinsic motivation and physical grounding into the design of\nmore human-aligned agents.", "AI": {"tldr": "LLM\u751f\u6210\u7684\u4efb\u52d9\u8207\u4eba\u985e\u751f\u6210\u7684\u4efb\u52d9\u5b58\u5728\u6839\u672c\u5dee\u7570\uff0c\u7f3a\u4e4f\u4eba\u985e\u7684\u50f9\u503c\u89c0\u548c\u9ad4\u73fe\u6027\u3002", "motivation": "\u63a2\u7a76LLM\u4ee3\u7406\u662f\u5426\u57fa\u65bc\u8207\u4eba\u985e\u76f8\u4f3c\u7684\u8a8d\u77e5\u539f\u5247\u751f\u6210\u4efb\u52d9\u3002", "method": "\u6bd4\u8f03\u4eba\u985e\u548cLLM\u4ee3\u7406(GPT-4o)\u7684\u4efb\u52d9\u751f\u6210\u7d50\u679c\u3002", "result": "\u4eba\u985e\u4efb\u52d9\u751f\u6210\u53d7\u5fc3\u7406\u56e0\u7d20(\u4f8b\u5982\uff0c\u50f9\u503c\u89c0\u548c\u8a8d\u77e5\u98a8\u683c)\u5f71\u97ff\uff0c\u800cLLM\u5373\u4f7f\u88ab\u63d0\u4f9b\u9019\u4e9b\u56e0\u7d20\uff0c\u4e5f\u7121\u6cd5\u9ad4\u73fe\u76f8\u61c9\u7684\u884c\u70ba\u6a21\u5f0f\uff0c\u751f\u6210\u7684\u4efb\u52d9\u7f3a\u4e4f\u793e\u6703\u6027\u3001\u9ad4\u73fe\u6027\u548c\u50f9\u503c\u9a45\u52d5\u6027\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u751f\u6210\u7684\u4efb\u52d9\u8207\u4eba\u985e\u751f\u6210\u7684\u4efb\u52d9\u5b58\u5728\u6838\u5fc3\u5dee\u7570\uff0cLLM\u751f\u6210\u7684\u4efb\u52d9\u7f3a\u4e4f\u793e\u6703\u6027\u3001\u9ad4\u73fe\u6027\u548c\u50f9\u503c\u9a45\u52d5\u6027\uff0c\u7a81\u986f\u4e86\u5728\u8a2d\u8a08\u66f4\u4eba\u6027\u5316\u667a\u80fd\u9ad4\u6642\u7d0d\u5165\u5167\u5728\u52d5\u6a5f\u548c\u7269\u7406\u57fa\u790e\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2508.00323", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00323", "abs": "https://arxiv.org/abs/2508.00323", "authors": ["Jianyi Zhang", "Xu Ji", "Ziyin Zhou", "Yuchen Zhou", "Shubo Shi", "Haoyu Wu", "Zhen Li", "Shizhao Liu"], "title": "Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning", "comment": null, "summary": "Evaluating the performance of visual language models (VLMs) in graphic\nreasoning tasks has become an important research topic. However, VLMs still\nshow obvious deficiencies in simulating human-level graphic reasoning\ncapabilities, especially in complex graphic reasoning and abstract problem\nsolving, which are less studied and existing studies only focus on simple\ngraphics. To evaluate the performance of VLMs in complex graphic reasoning, we\npropose ReasonBench, the first evaluation benchmark focused on structured\ngraphic reasoning tasks, which includes 1,613 questions from real-world\nintelligence tests. ReasonBench covers reasoning dimensions related to\nlocation, attribute, quantity, and multi-element tasks, providing a\ncomprehensive evaluation of the performance of VLMs in spatial, relational, and\nabstract reasoning capabilities. We benchmark 11 mainstream VLMs (including\nclosed-source and open-source models) and reveal significant limitations of\ncurrent models. Based on these findings, we propose a dual optimization\nstrategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability\nof reasoning by decomposing layers, and ReasonTune enhances the task\nadaptability of model reasoning through training, all of which improves VLM\nperformance by 33.5\\%. All experimental data and code are in the repository:\nhttps://huggingface.co/datasets/cistine/ReasonBench.", "AI": {"tldr": "ReasonBench:\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30VLMs\u590d\u6742\u56fe\u5f62\u63a8\u7406\u80fd\u529b\u7684\u65b0\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u91cd\u4f18\u5316\u7b56\u7565\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u8bc4\u4f30VLMs\u5728\u590d\u6742\u56fe\u5f62\u63a8\u7406\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u9488\u5bf9\u73b0\u6709\u7814\u7a76\u8f83\u5c11\u5173\u6ce8\u7684\u590d\u6742\u56fe\u5f62\u63a8\u7406\u548c\u62bd\u8c61\u95ee\u9898\u89e3\u51b3\u3002", "method": "\u6784\u5efaReasonBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1613\u4e2a\u6765\u81ea\u73b0\u5b9e\u4e16\u754c\u667a\u529b\u6d4b\u8bd5\u7684\u95ee\u9898\uff1b\u5bf911\u4e2a\u4e3b\u6d41VLMs\u8fdb\u884c\u8bc4\u4f30\uff1b\u63d0\u51faDiaCoT\u548cReasonTune\u4e24\u79cd\u4f18\u5316\u7b56\u7565\u3002", "result": "ReasonBench\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u5f53\u524dVLMs\u7684\u5c40\u9650\u6027\uff1bDiaCoT\u548cReasonTune\u7b56\u7565\u63d0\u9ad8\u4e86VLM\u6027\u80fd33.5%\u3002", "conclusion": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u590d\u6742\u56fe\u5f62\u63a8\u7406\u548c\u62bd\u8c61\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff0c\u672c\u6587\u63d0\u51fa\u7684ReasonBench\u57fa\u51c6\u6d4b\u8bd5\u6db5\u76d6\u4e86\u4f4d\u7f6e\u3001\u5c5e\u6027\u3001\u6570\u91cf\u548c\u591a\u5143\u7d20\u4efb\u52a1\u7b49\u63a8\u7406\u7ef4\u5ea6\uff0c\u5bf911\u4e2a\u4e3b\u6d41VLMs\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u91cd\u4f18\u5316\u7b56\u7565(DiaCoT\u548cReasonTune)\u63d0\u9ad8\u4e86VLM\u6027\u80fd33.5%\u3002"}}
{"id": "2508.00324", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00324", "abs": "https://arxiv.org/abs/2508.00324", "authors": ["Yeonjun In", "Wonjoong Kim", "Sangwu Park", "Chanyoung Park"], "title": "R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge", "comment": "under review", "summary": "Although large reasoning models (LRMs) have demonstrated impressive\ncapabilities on complex tasks, recent studies reveal that these models\nfrequently fulfill harmful user instructions, raising significant safety\nconcerns. In this paper, we investigate the underlying cause of LRM safety\nrisks and find that models already possess sufficient safety knowledge but fail\nto activate it during reasoning. Based on this insight, we propose R1-Act, a\nsimple and efficient post-training method that explicitly triggers safety\nknowledge through a structured reasoning process. R1-Act achieves strong safety\nimprovements while preserving reasoning performance, outperforming prior\nalignment methods. Notably, it requires only 1,000 training examples and 90\nminutes of training on a single RTX A6000 GPU. Extensive experiments across\nmultiple LRM backbones and sizes demonstrate the robustness, scalability, and\npractical efficiency of our approach.", "AI": {"tldr": "R1-Act\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u89e6\u53d1\u6a21\u578b\u7684\u5b89\u5168\u77e5\u8bc6\uff0c\u6709\u6548\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u5b89\u5168\u6027\uff0c\u8bad\u7ec3\u6548\u7387\u9ad8\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b(LRM)\u7ecf\u5e38\u6267\u884c\u6709\u5bb3\u7684\u7528\u6237\u6307\u4ee4\uff0c\u5b58\u5728\u5b89\u5168\u98ce\u9669\u3002\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5df2\u5177\u5907\u8db3\u591f\u7684\u5b89\u5168\u6027\u77e5\u8bc6\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u4e2d\u672a\u80fd\u6fc0\u6d3b\u3002", "method": "\u63d0\u51faR1-Act\uff0c\u4e00\u79cd\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\u663e\u5f0f\u89e6\u53d1\u5b89\u5168\u77e5\u8bc6\u7684\u7b80\u5355\u9ad8\u6548\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "R1-Act\u5728\u591a\u4e2aLRM\u4e3b\u5e72\u548c\u89c4\u6a21\u4e0a\u90fd\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u4ec5\u97001000\u4e2a\u8bad\u7ec3\u6837\u672c\u548c90\u5206\u949f\u7684\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "R1-Act\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u4e14\u8bad\u7ec3\u6548\u7387\u9ad8\u3002"}}
{"id": "2508.00378", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00378", "abs": "https://arxiv.org/abs/2508.00378", "authors": ["Shixin Yi", "Lin Shang"], "title": "CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding", "comment": "Preparing for AAAI 2026, Multimodal Reasoning", "summary": "Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in\nvision-language models (VLMs), but it often produces explanations that are\nlinguistically fluent yet lack grounding in visual content. We observe that\nsuch hallucinations arise in part from the absence of an explicit verification\nmechanism during multi-step reasoning. To address this, we propose\n\\textbf{CoRGI}(\\textbf{C}hain \\textbf{o}f \\textbf{R}easoning with\n\\textbf{G}rounded \\textbf{I}nsights), a modular framework that introduces\nvisual verification into the reasoning process. CoRGI follows a three-stage\npipeline: it first generates a textual reasoning chain, then extracts\nsupporting visual evidence for each reasoning step via a dedicated module\n(VEVM), and finally synthesizes the textual rationale with visual evidence to\ngenerate a grounded, verified answer. The framework can be integrated with\nexisting VLMs without end-to-end retraining. We evaluate CoRGI on the VCR\nbenchmark and find that it improves reasoning performance on two representative\nopen-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm\nthe contribution of each step in the verification module, and human evaluations\nsuggest that CoRGI leads to more factual and helpful explanations. We also\nexamine alternative designs for the visual verification step and discuss\npotential limitations of post-hoc verification frameworks. These findings\nhighlight the importance of grounding intermediate reasoning steps in visual\nevidence to enhance the robustness of multimodal reasoning.", "AI": {"tldr": "CoRGI\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u9a8c\u8bc1\u6539\u8fdbVLM\u63a8\u7406\uff0c\u51cf\u5c11\u5e7b\u89c9\uff0c\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684CoT prompting\u65b9\u6cd5\u751f\u6210\u7684\u89e3\u91ca\u867d\u7136\u6d41\u7545\uff0c\u4f46\u7f3a\u4e4f\u89c6\u89c9\u5185\u5bb9\u7684\u652f\u6491\uff0c\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u3002", "method": "CoRGI\u662f\u4e00\u4e2a\u4e09\u9636\u6bb5\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5305\u62ec\u6587\u672c\u63a8\u7406\u94fe\u751f\u6210\u3001\u89c6\u89c9\u8bc1\u636e\u63d0\u53d6\u548c\u6587\u672c\u4e0e\u89c6\u89c9\u8bc1\u636e\u6574\u5408\u3002\u5b83\u53ef\u4ee5\u4e0e\u73b0\u6709\u7684VLM\u96c6\u6210\uff0c\u65e0\u9700\u7aef\u5230\u7aef\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728VCR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoRGI\u63d0\u9ad8\u4e86\u4e24\u4e2a\u5f00\u6e90VLM\uff08Qwen-2.5VL\u548cLLaVA-1.6\uff09\u7684\u63a8\u7406\u6027\u80fd\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u660e\u4e86\u9a8c\u8bc1\u6a21\u5757\u4e2d\u6bcf\u4e2a\u6b65\u9aa4\u7684\u8d21\u732e\uff0c\u4eba\u5de5\u8bc4\u4f30\u8868\u660eCoRGI\u751f\u6210\u7684\u89e3\u91ca\u66f4\u51c6\u786e\u3001\u66f4\u6709\u5e2e\u52a9\u3002", "conclusion": "CoRGI\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u9a8c\u8bc1\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u591a\u6b65\u63a8\u7406\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u5e7b\u89c9\uff0c\u5e76\u751f\u6210\u66f4\u51c6\u786e\u3001\u66f4\u6709\u5e2e\u52a9\u7684\u89e3\u91ca\u3002"}}
{"id": "2508.00401", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.00401", "abs": "https://arxiv.org/abs/2508.00401", "authors": ["Riddhi J. Pitliya", "Ozan Catal", "Toon Van de Maele", "Corrado Pezzato", "Tim Verbelen"], "title": "Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation", "comment": null, "summary": "We present a novel approach to multi-agent cooperation by implementing theory\nof mind (ToM) within active inference. ToM - the ability to understand that\nothers can have differing knowledge and goals - enables agents to reason about\nothers' beliefs while planning their own actions. Unlike previous active\ninference approaches to multi-agent cooperation, our method neither relies on\ntask-specific shared generative models nor requires explicit communication,\nwhile being generalisable. In our framework, the ToM-equipped agent maintains\ndistinct representations of its own and others' beliefs and goals. We extend\nthe sophisticated inference tree-based planning algorithm to systematically\nexplore joint policy spaces through recursive reasoning. Our approach is\nevaluated through collision avoidance and foraging task simulations. Results\ndemonstrate that ToM-equipped agents cooperate better compared to non-ToM\ncounterparts by being able to avoid collisions and reduce redundant efforts.\nCrucially, ToM agents accomplish this by inferring others' beliefs solely from\nobservable behaviour. This work advances practical applications in artificial\nintelligence while providing computational insights into ToM.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u548c\u5fc3\u667a\u7406\u8bba\u7684\u591a\u667a\u80fd\u4f53\u5408\u4f5c\u65b9\u6cd5\uff0c\u65e0\u9700\u5171\u4eab\u6a21\u578b\u548c\u663e\u5f0f\u901a\u4fe1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728\u78b0\u649e\u907f\u514d\u548c\u89c5\u98df\u4efb\u52a1\u4e2d\u4f18\u4e8e\u65e0ToM\u667a\u80fd\u4f53\u3002", "motivation": "\u73b0\u6709\u4e3b\u52a8\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u5171\u4eab\u751f\u6210\u6a21\u578b\u6216\u663e\u5f0f\u901a\u4fe1\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002\u8be5\u7814\u7a76\u65e8\u5728\u6784\u5efa\u4e00\u79cd\u65e0\u9700\u5171\u4eab\u6a21\u578b\u548c\u663e\u5f0f\u901a\u4fe1\u7684\u901a\u7528\u591a\u667a\u80fd\u4f53\u5408\u4f5c\u65b9\u6cd5\u3002", "method": "\u5728\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\u4e0b\uff0c\u901a\u8fc7\u6784\u5efa\u81ea\u8eab\u548c\u4ed6\u4eba\u4fe1\u5ff5\u4e0e\u76ee\u6807\u7684\u72ec\u7acb\u8868\u5f81\uff0c\u5e76\u6269\u5c55\u590d\u6742\u7684\u57fa\u4e8e\u63a8\u7406\u6811\u7684\u89c4\u5212\u7b97\u6cd5\uff0c\u7cfb\u7edf\u5730\u63a2\u7d22\u8054\u5408\u7b56\u7565\u7a7a\u95f4\u3002", "result": "ToM\u667a\u80fd\u4f53\u80fd\u591f\u66f4\u597d\u5730\u5408\u4f5c\uff0c\u907f\u514d\u78b0\u649e\u5e76\u51cf\u5c11\u5197\u4f59\u5de5\u4f5c\uff0c\u8fd9\u901a\u8fc7\u4ec5\u4ec5\u6839\u636e\u53ef\u89c2\u5bdf\u884c\u4e3a\u63a8\u65ad\u4ed6\u4eba\u7684\u4fe1\u5ff5\u6765\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u5408\u4f5c\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4e3b\u52a8\u63a8\u7406\u4e2d\u5b9e\u73b0\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u6765\u5b9e\u73b0\u66f4\u597d\u7684\u5408\u4f5c\uff0c\u5e76\u5728\u78b0\u649e\u907f\u514d\u548c\u89c5\u98df\u4efb\u52a1\u6a21\u62df\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2508.00414", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00414", "abs": "https://arxiv.org/abs/2508.00414", "authors": ["Tianqing Fang", "Zhisong Zhang", "Xiaoyang Wang", "Rui Wang", "Can Qin", "Yuxuan Wan", "Jun-Yu Ma", "Ce Zhang", "Jiaqi Chen", "Xiyun Li", "Hongming Zhang", "Haitao Mi", "Dong Yu"], "title": "Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training", "comment": "16 pages", "summary": "General AI Agents are increasingly recognized as foundational frameworks for\nthe next generation of artificial intelligence, enabling complex reasoning, web\ninteraction, coding, and autonomous research capabilities. However, current\nagent systems are either closed-source or heavily reliant on a variety of paid\nAPIs and proprietary tools, limiting accessibility and reproducibility for the\nresearch community. In this work, we present \\textbf{Cognitive Kernel-Pro}, a\nfully open-source and (to the maximum extent) free multi-module agent framework\ndesigned to democratize the development and evaluation of advanced AI agents.\nWithin Cognitive Kernel-Pro, we systematically investigate the curation of\nhigh-quality training data for Agent Foundation Models, focusing on the\nconstruction of queries, trajectories, and verifiable answers across four key\ndomains: web, file, code, and general reasoning. Furthermore, we explore novel\nstrategies for agent test-time reflection and voting to enhance agent\nrobustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving\nstate-of-the-art results among open-source and free agents. Notably, our\n8B-parameter open-source model surpasses previous leading systems such as\nWebDancer and WebSailor, establishing a new performance standard for\naccessible, high-capability AI agents. Code is available at\nhttps://github.com/Tencent/CognitiveKernel-Pro", "AI": {"tldr": "\u5f00\u6e90\u591a\u6a21\u5757\u667a\u80fd\u4f53\u6846\u67b6Cognitive Kernel-Pro\uff0c\u5728GAIA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u7ed3\u679c\uff0c\u63d0\u5347\u4e86AI\u667a\u80fd\u4f53\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u8981\u4e48\u662f\u95ed\u6e90\u7684\uff0c\u8981\u4e48\u4e25\u91cd\u4f9d\u8d56\u5404\u79cd\u4ed8\u8d39API\u548c\u4e13\u6709\u5de5\u5177\uff0c\u9650\u5236\u4e86\u7814\u7a76\u754c\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "method": "\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u9ad8\u7ea7AI\u667a\u80fd\u4f53\u7684\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u5173\u6ce8\u56db\u4e2a\u5173\u952e\u9886\u57df\uff08\u7f51\u7edc\u3001\u6587\u4ef6\u3001\u4ee3\u7801\u548c\u4e00\u822c\u63a8\u7406\uff09\u4e2d\u67e5\u8be2\u3001\u8f68\u8ff9\u548c\u53ef\u9a8c\u8bc1\u7b54\u6848\u7684\u6784\u5efa\uff0c\u5e76\u63a2\u7d22\u4e86\u667a\u80fd\u4f53\u6d4b\u8bd5\u65f6\u53cd\u5c04\u548c\u6295\u7968\u7684\u65b0\u7b56\u7565\u4ee5\u589e\u5f3a\u667a\u80fd\u4f53\u7684\u7a33\u5065\u6027\u548c\u6027\u80fd\u3002", "result": "Cognitive Kernel-Pro \u5728GAIA\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u51768B\u53c2\u6570\u7684\u5f00\u6e90\u6a21\u578b\u8d85\u8fc7\u4e86WebDancer\u548cWebSailor\u7b49\u4e4b\u524d\u7684\u9886\u5148\u7cfb\u7edf\u3002", "conclusion": "Cognitive Kernel-Pro\uff0c\u4e00\u4e2a\u5b8c\u5168\u5f00\u6e90\u4e14\u514d\u8d39\u7684\u591a\u6a21\u5757\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5728GAIA\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u51768B\u53c2\u6570\u7684\u5f00\u6e90\u6a21\u578b\u8d85\u8fc7\u4e86WebDancer\u548cWebSailor\u7b49\u4e4b\u524d\u7684\u9886\u5148\u7cfb\u7edf\uff0c\u4e3a\u6613\u4e8e\u8bbf\u95ee\u3001\u9ad8\u80fd\u529b\u7684AI\u667a\u80fd\u4f53\u5efa\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u6807\u51c6\u3002"}}
