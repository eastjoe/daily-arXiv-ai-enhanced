<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Unifying Post-hoc Explanations of Knowledge Graph Completions](https://arxiv.org/abs/2507.22951)
*Alessandro Lonardi,Samy Badreddine,Tarek R. Besold,Pablo Sanchez Martin*

Main category: cs.AI

TL;DR: 本文提出了一种统一的KGC事后可解释性框架，改进了评估协议，并强调了解释的可解释性，以提高可重复性和影响力。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱补全(KGC)的事后可解释性缺乏形式化和一致的评估，阻碍了可重复性和跨研究比较。

Method: 提出了一种通用的多目标优化框架来表征KGC中的事后解释，平衡其有效性和简洁性，并改进了使用MRR和Hits@$k$等流行指标的评估协议。

Result: 统一了现有的KGC中的事后可解释性算法及其产生的解释，并强调了解释的可解释性，即解释解决对最终用户有意义的查询的能力。

Conclusion: 这项工作旨在通过统一方法和改进评估标准，使知识图谱补全的可解释性研究更具可重复性和影响力。

Abstract: Post-hoc explainability for Knowledge Graph Completion (KGC) lacks
formalization and consistent evaluations, hindering reproducibility and
cross-study comparisons. This paper argues for a unified approach to post-hoc
explainability in KGC. First, we propose a general framework to characterize
post-hoc explanations via multi-objective optimization, balancing their
effectiveness and conciseness. This unifies existing post-hoc explainability
algorithms in KGC and the explanations they produce. Next, we suggest and
empirically support improved evaluation protocols using popular metrics like
Mean Reciprocal Rank and Hits@$k$. Finally, we stress the importance of
interpretability as the ability of explanations to address queries meaningful
to end-users. By unifying methods and refining evaluation standards, this work
aims to make research in KGC explainability more reproducible and impactful.

</details>


### [2] [Data Readiness for Scientific AI at Scale](https://arxiv.org/abs/2507.23018)
*Wesley Brewer,Patrick Widener,Valentine Anantharaj,Feiyi Wang,Tom Beck,Arjun Shankar,Sarp Oral*

Main category: cs.AI

TL;DR: This paper presents a new framework for evaluating the readiness of large scientific datasets for AI training, focusing on HPC environments and transformer models.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of using leadership-scale scientific datasets to train foundation models by applying Data Readiness for AI (DRAI) principles.

Method: The paper analyzes archetypal workflows across four scientific domains (climate, nuclear fusion, bio/health, and materials) to identify common preprocessing patterns and domain-specific constraints.  A two-dimensional framework (Data Readiness Levels and Data Processing Stages) tailored to HPC environments is introduced.

Result: A two-dimensional readiness framework and a conceptual maturity matrix for characterizing scientific data readiness are developed, guiding infrastructure development for standardized, cross-domain support for scalable and reproducible AI for science.

Conclusion: This paper proposes a two-dimensional framework for assessing the readiness of scientific datasets for AI training, addressing challenges in transforming scientific data for scalable AI, particularly for transformer-based generative models.

Abstract: This paper examines how Data Readiness for AI (DRAI) principles apply to
leadership-scale scientific datasets used to train foundation models. We
analyze archetypal workflows across four representative domains - climate,
nuclear fusion, bio/health, and materials - to identify common preprocessing
patterns and domain-specific constraints. We introduce a two-dimensional
readiness framework composed of Data Readiness Levels (raw to AI-ready) and
Data Processing Stages (ingest to shard), both tailored to high performance
computing (HPC) environments. This framework outlines key challenges in
transforming scientific data for scalable AI training, emphasizing
transformer-based generative models. Together, these dimensions form a
conceptual maturity matrix that characterizes scientific data readiness and
guides infrastructure development toward standardized, cross-domain support for
scalable and reproducible AI for science.

</details>


### [3] [FairReason: Balancing Reasoning and Social Bias in MLLMs](https://arxiv.org/abs/2507.23067)
*Zhenyu Pan,Yutong Zhang,Jianshu Zhang,Haoran Lu,Haozheng Luo,Yuwei Han,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: 在多模态大型语言模型中，通过强化学习和 1:4 的去偏见样本与推理样本比例，可以在减少偏见的同时保持较高的推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的方法虽然提高了逻辑准确性，但常常导致模型输出带有明显的社会偏见。

Method: 基准测试三种偏见缓解策略（监督微调、知识蒸馏和基于规则的强化学习），并改变每个范例中去偏见样本和推理样本的比例，绘制推理与偏见的权衡图。

Result: 发现强化学习在 1:4 的样本比例下取得了推理能力和公平性的最佳平衡。

Conclusion: 这项研究调查了多模态大型语言模型 (MLLM) 中推理能力提升与偏见缓解之间的权衡。研究发现，使用强化学习训练的模型，在去偏见样本与推理样本比例为 1:4 时，能够在减少 10% 的刻板印象分数的同时，保持 88% 的原始推理准确率。

Abstract: Multimodal Large Language Models (MLLMs) already achieve state-of-the-art
results across a wide range of tasks and modalities. To push their reasoning
ability further, recent studies explore advanced prompting schemes and
post-training fine-tuning. Although these techniques improve logical accuracy,
they frequently leave the models' outputs burdened with pronounced social
biases. Clarifying how reasoning gains interact with bias mitigation-and
whether the two objectives inherently trade off-therefore remains an open and
pressing research problem. Our study begins by benchmarking three
bias-mitigation strategies-supervised fine-uning (SFT), knowledge distillation
(KD), and rule-based reinforcement learning (RL)-under identical conditions,
establishing their baseline strengths and weaknesses. Building on these
results, we vary the proportion of debias-focused and reasoning-centric samples
within each paradigm to chart the reasoning-versus-bias trade-off. Our sweeps
reveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement
learning cuts stereotype scores by 10% while retaining 88% of the model's
original reasoning accuracy, offering concrete guidance for balancing fairness
and capability in MLLMs.

</details>


### [4] [Moravec's Paradox: Towards an Auditory Turing Test](https://arxiv.org/abs/2507.23091)
*David Noever,Forrest McKee*

Main category: cs.AI

TL;DR: AI在听觉任务上远不如人类，需改进选择性注意、物理声学理解和上下文感知等能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在听觉任务上的失败，以及对类人听觉场景分析机制的缺乏。

Method: 构建了一个包含917个挑战的听觉图灵测试，涵盖七个类别，并对包括GPT-4和Whisper在内的多个先进音频模型进行了评估。

Result: 最先进的音频模型的失败率超过93%，与人类表现存在巨大差距，这突显了AI系统在选择性注意、噪声鲁棒性和上下文适应性方面的不足。

Conclusion: 当前AI系统在人类轻松完成的听觉任务上存在巨大缺陷，该研究通过构建听觉图灵测试评估了最先进的音频模型，结果显示其失败率超过93%，与人类表现存在巨大差距，这揭示了AI系统在处理复杂听觉场景时，尤其是在选择性注意、噪声鲁棒性和上下文适应性方面的不足。

Abstract: This research work demonstrates that current AI systems fail catastrophically
on auditory tasks that humans perform effortlessly. Drawing inspiration from
Moravec's paradox (i.e., tasks simple for humans often prove difficult for
machines, and vice versa), we introduce an auditory Turing test comprising 917
challenges across seven categories: overlapping speech, speech in noise,
temporal distortion, spatial audio, coffee-shop noise, phone distortion, and
perceptual illusions. Our evaluation of state-of-the-art audio models including
GPT-4's audio capabilities and OpenAI's Whisper reveals a striking failure rate
exceeding 93%, with even the best-performing model achieving only 6.9% accuracy
on tasks that humans solved at 7.5 times higher success (52%). These results
expose focusing failures in how AI systems process complex auditory scenes,
particularly in selective attention, noise robustness, and contextual
adaptation. Our benchmark not only quantifies the human-machine auditory gap
but also provides insights into why these failures occur, suggesting that
current architectures lack fundamental mechanisms for human-like auditory scene
analysis. The traditional design of audio CAPTCHAs highlights common filters
that humans evolved but machines fail to select in multimodal language models.
This work establishes a diagnostic framework for measuring progress toward
human-level machine listening and highlights the need for novel approaches
integrating selective attention, physics-based audio understanding, and
context-aware perception into multimodal AI systems.

</details>


### [5] [Argumentatively Coherent Judgmental Forecasting](https://arxiv.org/abs/2507.23163)
*Deniz Gorur,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: Argumentative coherence improves forecasting accuracy but requires explicit filtering because users don't naturally apply it.


<details>
  <summary>Details</summary>
Motivation: To formally define and evaluate the impact of argumentative coherence in judgmental forecasting, aiming to improve accuracy and address the disconnect between intuitive coherence and user practice.

Method: Evaluated the impact of argumentative coherence on human and LLM forecasters through three experiments; assessed user alignment with coherence via crowd-sourced experiments.

Result: Filtering incoherent predictions improves forecasting accuracy; users generally do not align with the coherence property.

Conclusion: Enforcing argumentative coherence in judgmental forecasting improves accuracy for both human and LLM forecasters, but users don't inherently align with this property, highlighting the need for filtering mechanisms.

Abstract: Judgmental forecasting employs human opinions to make predictions about
future events, rather than exclusively historical data as in quantitative
forecasting. When these opinions form an argumentative structure around
forecasts, it is useful to study the properties of the forecasts from an
argumentative perspective. In this paper, we advocate and formally define a
property of argumentative coherence, which, in essence, requires that a
forecaster's reasoning is coherent with their forecast. We then conduct three
evaluations with our notion of coherence. First, we assess the impact of
enforcing coherence on human forecasters as well as on Large Language Model
(LLM)-based forecasters, given that they have recently shown to be competitive
with human forecasters. In both cases, we show that filtering out incoherent
predictions improves forecasting accuracy consistently, supporting the
practical value of coherence in both human and LLM-based forecasting. Then, via
crowd-sourced user experiments, we show that, despite its apparent
intuitiveness and usefulness, users do not generally align with this coherence
property. This points to the need to integrate, within argumentation-based
judgmental forecasting, mechanisms to filter out incoherent opinions before
obtaining group forecasting predictions.

</details>


### [6] [Tractable Responsibility Measures for Ontology-Mediated Query Answering](https://arxiv.org/abs/2507.23191)
*Meghyn Bienvenu,Diego Figueira,Pierre Lafourcade*

Main category: cs.AI

TL;DR: 研究本体介导查询应答中责任分数计算的复杂性，发现其复杂度与本体语言和查询结构有关，部分情况下是多项式复杂度，部分情况下是难解的。


<details>
  <summary>Details</summary>
Motivation: 量化解释查询答案的贡献。

Method: 利用数据库设置的结果，并分析了WSMS计算的组合复杂性。

Result: 对于first-order-rewritable的本体介导查询类，WSMS具有多项式数据复杂度；当本体语言可以编码可达性查询时，问题变为'shP'-hard。在本体语言支持合取的情况下，即使没有本体，原子查询的计算也是难解的。在DL-Lite方言中，识别出允许易处理WSMS计算的结构受限的查询类。

Conclusion: 研究了在本体介导查询应答环境下计算责任分数的复杂性，发现基于加权最小支持的Shapley值责任度量在某些本体介导查询类上具有多项式数据复杂度，而在其他情况下则为'shP'-hard。

Abstract: Recent work on quantitative approaches to explaining query answers employs
responsibility measures to assign scores to facts in order to quantify their
respective contributions to obtaining a given answer. In this paper, we study
the complexity of computing such responsibility scores in the setting of
ontology-mediated query answering, focusing on a very recently introduced
family of Shapley-value-based responsibility measures defined in terms of
weighted sums of minimal supports (WSMS). By exploiting results from the
database setting, we can show that such measures enjoy polynomial data
complexity for classes of ontology-mediated queries that are
first-order-rewritable, whereas the problem becomes "shP"-hard when the
ontology language can encode reachability queries (via axioms like $\exists R.
A \sqsubseteq A$). To better understand the tractability frontier, we next
explore the combined complexity of WSMS computation. We prove that
intractability applies already to atomic queries if the ontology language
supports conjunction, as well as to unions of `well-behaved' conjunctive
queries, even in the absence of an ontology. By contrast, our study yields
positive results for common DL-Lite dialects: by means of careful analysis, we
identify classes of structurally restricted conjunctive queries (which
intuitively disallow undesirable interactions between query atoms) that admit
tractable WSMS computation.

</details>


### [7] [Solution-aware vs global ReLU selection: partial MILP strikes back for DNN verification](https://arxiv.org/abs/2507.23197)
*Yuke Liao,Blaise Genest,Kuldeep Meel,Shaan Aryaman*

Main category: cs.AI

TL;DR: 通过改进的分治策略和SAS方法，构建高效精确的CNN验证器，显著减少二元变量和未确定实例数量。


<details>
  <summary>Details</summary>
Motivation: 为了处理复杂的实例，改进现有的分治方法，减少复杂BaB调用次数，提高效率和准确性。

Method: 采用分治法，结合解决方案感知ReLU评分(SAS)选择少量关键ReLU变量用二元变量处理，并改进BaB-SR和BaB-FSB分支函数作为全局ReLU评分(GS)函数。最终实现Hybrid MILP方法，先用α,β-CROWN进行快速求解，再用部分MILP求解。

Result: SAS方法将二元变量数量减少了约6倍，同时保持了相同的精度；Hybrid MILP方法将未确定实例的数量减少了高达40%，平均运行时间为46s-417s。

Conclusion: 提出了一种新的解决方案感知ReLU评分(SAS)方法，并改进了BaB-SR和BaB-FSB分支函数作为全局ReLU评分(GS)函数，有效减少了二元变量数量，提高了效率，并结合Hybrid MILP方法构建了一个精确高效的验证器。

Abstract: To handle complex instances, we revisit a divide-and-conquer approach to
break down the complexity: instead of few complex BaB calls, we rely on many
small {\em partial} MILP calls. The crucial step is to select very few but very
important ReLUs to treat using (costly) binary variables. The previous attempts
were suboptimal in that respect. To select these important ReLU variables, we
propose a novel {\em solution-aware} ReLU scoring ({\sf SAS}), as well as adapt
the BaB-SR and BaB-FSB branching functions as {\em global} ReLU scoring ({\sf
GS}) functions. We compare them theoretically as well as experimentally, and
{\sf SAS} is more efficient at selecting a set of variables to open using
binary variables. Compared with previous attempts, SAS reduces the number of
binary variables by around 6 times, while maintaining the same level of
accuracy. Implemented in {\em Hybrid MILP}, calling first $\alpha,\beta$-CROWN
with a short time-out to solve easier instances, and then partial MILP,
produces a very accurate yet efficient verifier, reducing by up to $40\%$ the
number of undecided instances to low levels ($8-15\%$), while keeping a
reasonable runtime ($46s-417s$ on average per instance), even for fairly large
CNNs with 2 million parameters.

</details>


### [8] [How Far Are AI Scientists from Changing the World?](https://arxiv.org/abs/2507.23276)
*Qiujie Xie,Yixuan Weng,Minjun Zhu,Fuchen Shen,Shulin Huang,Zhen Lin,Jiahui Zhou,Zilan Mao,Zijie Yang,Linyi Yang,Jian Wu,Yue Zhang*

Main category: cs.AI

TL;DR: AI科学家系统发展迅速，但仍存在局限性，未来目标是实现具有突破性发现能力的科学AI。


<details>
  <summary>Details</summary>
Motivation: 探讨AI科学家系统距离改变世界和重塑科学研究范式的距离。

Method: 综述性研究，分析现有AI科学家系统。

Result: 识别了AI科学家系统的瓶颈和所需的关键组件，为未来发展方向提供了见解。

Conclusion: 该综述分析了AI科学家系统的当前成就、关键瓶颈和实现突破性发现所需的关键组件，旨在促进对AI科学家系统局限性的理解，并明确科学AI的最终目标。

Abstract: The emergence of large language models (LLMs) is propelling automated
scientific discovery to the next level, with LLM-based Artificial Intelligence
(AI) Scientist systems now taking the lead in scientific research. Several
influential works have already appeared in the field of AI Scientist systems,
with AI-generated research papers having been accepted at the ICLR 2025
workshop, suggesting that a human-level AI Scientist capable of uncovering
phenomena previously unknown to humans, may soon become a reality. In this
survey, we focus on the central question: How far are AI scientists from
changing the world and reshaping the scientific research paradigm? To answer
this question, we provide a prospect-driven review that comprehensively
analyzes the current achievements of AI Scientist systems, identifying key
bottlenecks and the critical components required for the emergence of a
scientific agent capable of producing ground-breaking discoveries that solve
grand challenges. We hope this survey will contribute to a clearer
understanding of limitations of current AI Scientist systems, showing where we
are, what is missing, and what the ultimate goals for scientific AI should be.

</details>


### [9] [AI Must not be Fully Autonomous](https://arxiv.org/abs/2507.23330)
*Tosin Adewumi,Lama Alkhaled,Florent Imbert,Hui Han,Nudrat Habib,Karl Löwenmark*

Main category: cs.AI

TL;DR: Fully autonomous AI is too risky; responsible human oversight is crucial.


<details>
  <summary>Details</summary>
Motivation: To argue against fully autonomous AI due to its inherent risks.

Method: The authors identify three levels of autonomous AI, discuss theories of autonomy, AI, and agents, present 12 arguments against fully autonomous AI and 6 counterarguments with rebuttals, and provide 15 pieces of evidence in the appendix.

Result: The paper presents a strong case against fully autonomous AI, supported by arguments, counterarguments, and evidence.

Conclusion: Fully autonomous AI is risky and should not be pursued due to the potential for misaligned values and other risks, especially as ASI is approaching.

Abstract: Autonomous Artificial Intelligence (AI) has many benefits. It also has many
risks. In this work, we identify the 3 levels of autonomous AI. We are of the
position that AI must not be fully autonomous because of the many risks,
especially as artificial superintelligence (ASI) is speculated to be just
decades away. Fully autonomous AI, which can develop its own objectives, is at
level 3 and without responsible human oversight. However, responsible human
oversight is crucial for mitigating the risks. To ague for our position, we
discuss theories of autonomy, AI and agents. Then, we offer 12 distinct
arguments and 6 counterarguments with rebuttals to the counterarguments. We
also present 15 pieces of recent evidence of AI misaligned values and other
risks in the appendix.

</details>


### [10] [DSBC : Data Science task Benchmarking with Context engineering](https://arxiv.org/abs/2507.23336)
*Ram Mohan Rao Kadiyala,Siddhant Gupta,Jebish Purbey,Giulio Martini,Suman Debnath,Hamza Farooq*

Main category: cs.AI

TL;DR: 该论文提出了一个新的基准测试，评估了三种LLM在数据科学任务中的性能，结果表明模型和方法的选择对最终结果有显著影响。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对数据科学代理进行系统性评估的基准测试，本文旨在通过模拟真实世界用户交互来弥补这一差距。

Method: 对三种LLM（Claude-4.0-Sonnet、Gemini-2.5-Flash和OpenAI-o4-Mini）进行基准测试，评估其在三种方法（zero-shot with context engineering, multi-step with context engineering, and with SmolAgent）下，八个数据科学任务类别中的性能，并探究了提示问题（如数据泄露和模棱两可的指令）以及温度参数的影响。

Result: 不同模型和方法的性能差异显著，突出了影响实际部署的关键因素。

Conclusion: 本文介绍了一个评估数据科学代理有效性和局限性的综合基准测试，并对三种LLM（Claude-4.0-Sonnet、Gemini-2.5-Flash和OpenAI-o4-Mini）在不同方法下的性能进行了评估，结果揭示了模型和方法之间的性能差异，为未来更强大、更有效的数据科学代理研究奠定了基础。

Abstract: Recent advances in large language models (LLMs) have significantly impacted
data science workflows, giving rise to specialized data science agents designed
to automate analytical tasks. Despite rapid adoption, systematic benchmarks
evaluating the efficacy and limitations of these agents remain scarce. In this
paper, we introduce a comprehensive benchmark specifically crafted to reflect
real-world user interactions with data science agents by observing usage of our
commercial applications. We evaluate three LLMs: Claude-4.0-Sonnet,
Gemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with
context engineering, multi-step with context engineering, and with SmolAgent.
Our benchmark assesses performance across a diverse set of eight data science
task categories, additionally exploring the sensitivity of models to common
prompting issues, such as data leakage and slightly ambiguous instructions. We
further investigate the influence of temperature parameters on overall and
task-specific outcomes for each model and approach. Our findings reveal
distinct performance disparities among the evaluated models and methodologies,
highlighting critical factors that affect practical deployment. The benchmark
dataset and evaluation framework introduced herein aim to provide a foundation
for future research of more robust and effective data science agents.

</details>


### [11] [LLM4Rail: An LLM-Augmented Railway Service Consulting Platform](https://arxiv.org/abs/2507.23377)
*Zhuo Li,Xianghuai Deng,Chiwei Feng,Hanmeng Li,Shenjie Wang,Haichao Zhang,Teng Jia,Conlin Chen,Louis Linchun Wu,Jia Wang*

Main category: cs.AI

TL;DR: LLM4Rail平台提供个性化铁路服务，包含购票、餐饮推荐等功能，并利用QTAO框架和CRFD-25数据集提升推荐准确性


<details>
  <summary>Details</summary>
Motivation: 满足日益增长的个性化铁路服务需求

Method: 基于LLM的QTAO提示框架，零样本对话推荐系统，CRFD-25数据集构建

Result: 开发了LLM4Rail平台，并构建了CRFD-25数据集和基于LLM的零样本对话推荐系统，能够提供个性化铁路服务

Conclusion: LLM4Rail，一个基于LLM的铁路服务咨询平台，通过QTAO提示框架有效整合言语推理和面向任务的动作，提供个性化购票、餐饮推荐、天气信息和闲聊等服务，并基于CRFD-25数据集构建了零样本对话推荐系统，确保推荐结果与数据集一致。

Abstract: Large language models (LLMs) have significantly reshaped different walks of
business. To meet the increasing demands for individualized railway service, we
develop LLM4Rail - a novel LLM-augmented railway service consulting platform.
Empowered by LLM, LLM4Rail can provide custom modules for ticketing, railway
food & drink recommendations, weather information, and chitchat. In LLM4Rail,
we propose the iterative "Question-Thought-Action-Observation (QTAO)" prompting
framework. It meticulously integrates verbal reasoning with task-oriented
actions, that is, reasoning to guide action selection, to effectively retrieve
external observations relevant to railway operation and service to generate
accurate responses. To provide personalized onboard dining services, we first
construct the Chinese Railway Food and Drink (CRFD-25) - a publicly accessible
takeout dataset tailored for railway services. CRFD-25 covers a wide range of
signature dishes categorized by cities, cuisines, age groups, and spiciness
levels. We further introduce an LLM-based zero-shot conversational recommender
for railway catering. To address the unconstrained nature of open
recommendations, the feature similarity-based post-processing step is
introduced to ensure all the recommended items are aligned with CRFD-25
dataset.

</details>


### [12] [Chatting with your ERP: A Recipe](https://arxiv.org/abs/2507.23429)
*Jorge Ruiz Gómez,Lidia Andrés Susinos,Jorge Alamo Olivé,Sonia Rey Osorno,Manuel Luis Gonzalez Hernández*

Main category: cs.AI

TL;DR: LLM 代理与 ERP 系统对话，将自然语言查询转换为 SQL 语句，并采用双代理架构提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 为了能够使用自然语言查询工业生产级 ERP 系统。

Method: 该代理能够解释自然语言查询并将它们转换为可执行的 SQL 语句，利用开放权重的 LLMs。提出了一种新颖的双代理架构，结合推理和批判阶段，以提高查询生成的可靠性。

Result: 实现了能够解释自然语言查询并将它们转换为可执行的 SQL 语句的 LLM 代理，并提出了一种新颖的双代理架构，提高了查询生成的可靠性。

Conclusion: 本文设计、实现并评估了一个大型语言模型 (LLM) 代理，该代理可以与工业生产级 ERP 系统进行对话。

Abstract: This paper presents the design, implementation, and evaluation behind a Large
Language Model (LLM) agent that chats with an industrial production-grade ERP
system. The agent is capable of interpreting natural language queries and
translating them into executable SQL statements, leveraging open-weight LLMs. A
novel dual-agent architecture combining reasoning and critique stages was
proposed to improve query generation reliability.

</details>


### [13] [Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation](https://arxiv.org/abs/2507.23440)
*Mingzhe Li,Xin Lu,Yanyan Zhao*

Main category: cs.AI

TL;DR: 提出了一种新的LLM驱动指令合成方法Self-Foveate，显著提升了合成指令的多样性和难度。


<details>
  <summary>Details</summary>
Motivation: 现有的指令合成方法依赖大量人工标注，且难以保证合成指令的多样性和难度。

Method: 提出了一种名为Self-Foveate的LLM驱动指令合成方法，采用“微散射-宏观”多层次注视机制，引导LLM深入挖掘非监督文本中的细粒度信息。

Result: 实验证明Self-Foveate方法有效且优于现有方法。数据和代码已公开发布。

Conclusion: Self-Foveate方法有效提高了指令合成的数据多样性和难度，并在多个语料库和模型架构上验证了其有效性。

Abstract: Large language models (LLMs) with instruction following capabilities have
demonstrated impressive problem-solving abilities. While synthesizing
instructional data from unsupervised text has become a common approach for
training such models, conventional methods rely heavily on human effort for
data annotation. Although existing automated synthesis paradigms have
alleviated this constraint, they still exhibit significant limitations in
ensuring adequate diversity and difficulty of synthesized instructions. To
address these challenges, we propose Self-Foveate, an innovative LLM-driven
method for instruction synthesis. This approach introduces a
"Micro-Scatter-Macro" multi-level foveation methodology that effectively guides
the LLM to deeply excavate fine-grained information embedded in unsupervised
text, thereby enhancing both the diversity and difficulty of synthesized
instructions. Comprehensive experiments across multiple unsupervised corpora
and diverse model architectures validate the effectiveness and superiority of
our proposed method. We publicly release our data and codes:
https://github.com/Mubuky/Self-Foveate

</details>


### [14] [Causal Reasoning in Pieces: Modular In-Context Learning for Causal Discovery](https://arxiv.org/abs/2507.23488)
*Kacper Kadziolka,Saber Salehkaleybar*

Main category: cs.AI

TL;DR: 先进推理模型结合改进的上下文学习方法，显著提升了因果发现的性能。


<details>
  <summary>Details</summary>
Motivation: 研究最先进的推理模型在因果发现任务中的鲁棒性，以及如何改进其性能。

Method: 使用OpenAI的o系列和DeepSeek-R模型家族，结合Tree-of-Thoughts和Chain-of-Thoughts方法构建模块化上下文管道。

Result: 推理模型比传统方法取得了显著的改进，精心设计的上下文框架进一步提升了性能。

Conclusion: 先进的推理模型在因果发现方面取得了显著进展，但需要精心设计的上下文框架来最大限度地发挥其能力。

Abstract: Causal inference remains a fundamental challenge for large language models.
Recent advances in internal reasoning with large language models have sparked
interest in whether state-of-the-art reasoning models can robustly perform
causal discovery-a task where conventional models often suffer from severe
overfitting and near-random performance under data perturbations. We study
causal discovery on the Corr2Cause benchmark using the emergent OpenAI's
o-series and DeepSeek-R model families and find that these reasoning-first
architectures achieve significantly greater native gains than prior approaches.
To capitalize on these strengths, we introduce a modular in-context pipeline
inspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies, yielding
nearly three-fold improvements over conventional baselines. We further probe
the pipeline's impact by analyzing reasoning chain length, complexity, and
conducting qualitative and quantitative comparisons between conventional and
reasoning models. Our findings suggest that while advanced reasoning models
represent a substantial leap forward, carefully structured in-context
frameworks are essential to maximize their capabilities and offer a
generalizable blueprint for causal discovery across diverse domains.

</details>


### [15] [Causal Identification of Sufficient, Contrastive and Complete Feature Sets in Image Classification](https://arxiv.org/abs/2507.23497)
*David A Kelly,Hana Chockler*

Main category: cs.AI

TL;DR: 本文提出了一种新的基于因果关系的图像分类器解释方法，该方法既具有形式上的严谨性，又适用于黑盒模型，并且计算效率高。


<details>
  <summary>Details</summary>
Motivation: 现有的图像分类器输出解释算法缺乏形式上的严谨性，而基于逻辑的解释虽然形式上严格，但其可计算性依赖于对模型的严格假设，这些假设在图像分类器中并不成立。

Method: 本文证明了因果解释的形式性质，并引入了图像分类器的对比因果解释。

Result: 不同模型具有不同的充分性、对比性和完整性模式。算法计算效率高，在ResNet50模型上平均每张图像计算所有类型的解释需要6秒。算法是完全黑盒的，不需要了解模型，不需要访问模型内部，不需要访问梯度，也不需要模型的任何属性，例如单调性。

Conclusion: 本文证明了因果解释在形式上与逻辑解释一样严格，同时适用于黑盒算法，并自然适用于图像分类器。作者证明了因果解释的形式性质，并引入了图像分类器的对比因果解释。此外，作者用置信度感知增强了解释的定义，并引入了完整的因果解释。实验结果表明，不同模型具有不同的充分性、对比性和完整性模式。

Abstract: Existing algorithms for explaining the outputs of image classifiers are based
on a variety of approaches and produce explanations that lack formal rigor. On
the other hand, logic-based explanations are formally and rigorously defined
but their computability relies on strict assumptions about the model that do
not hold on image classifiers.
  In this paper, we show that causal explanations, in addition to being
formally and rigorously defined, enjoy the same formal properties as
logic-based ones, while still lending themselves to black-box algorithms and
being a natural fit for image classifiers. We prove formal properties of causal
explanations and introduce contrastive causal explanations for image
classifiers. Moreover, we augment the definition of explanation with confidence
awareness and introduce complete causal explanations: explanations that are
classified with exactly the same confidence as the original image.
  We implement our definitions, and our experimental results demonstrate that
different models have different patterns of sufficiency, contrastiveness, and
completeness. Our algorithms are efficiently computable, taking on average 6s
per image on a ResNet50 model to compute all types of explanations, and are
totally black-box, needing no knowledge of the model, no access to model
internals, no access to gradient, nor requiring any properties, such as
monotonicity, of the model.

</details>


### [16] [DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer](https://arxiv.org/abs/2507.23554)
*Ruoyu Wang,Junda Wu,Yu Xia,Tong Yu,Ryan A. Rossi,Julian McAuley,Lina Yao*

Main category: cs.AI

TL;DR: DICE框架通过因果推理，动态选择最相关的上下文示例，从而提高LLM智能体的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型的智能体，其效果很大程度上依赖于上下文学习中示例的选择，而缺乏一个普遍适用的选择标准。

Method: DICE框架通过因果视角分解示例知识，并提出一种逐步选择标准，具有改进智能体性能的理论保证。

Result: 实验结果表明，DICE框架在多个领域都提高了LLM智能体的性能，证明了其有效性和通用性。

Conclusion: 本文提出DICE框架，一种基于因果关系的动态上下文示例选择方法，用于提高大型语言模型（LLM）智能体的性能。该方法能够区分可迁移和不可迁移的知识，避免由于示例选择不当导致的性能下降。

Abstract: Large language model-based agents, empowered by in-context learning (ICL),
have demonstrated strong capabilities in complex reasoning and tool-use tasks.
However, existing works have shown that the effectiveness of ICL is highly
sensitive to the choice of demonstrations, with suboptimal examples often
leading to unstable or degraded performance. While prior work has explored
example selection, including in some agentic or multi-step settings, existing
approaches typically rely on heuristics or task-specific designs and lack a
general, theoretically grounded criterion for what constitutes an effective
demonstration across reasoning steps. Therefore, it is non-trivial to develop a
principled, general-purpose method for selecting demonstrations that
consistently benefit agent performance. In this paper, we address this
challenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a
theoretically grounded ICL framework for agentic tasks that selects the most
relevant demonstrations at each step of reasoning. Our approach decomposes
demonstration knowledge into transferable and non-transferable components
through a causal lens, showing how the latter can introduce spurious
dependencies that impair generalization. We further propose a stepwise
selection criterion with a formal guarantee of improved agent performance.
Importantly, DICE is a general, framework-agnostic solution that can be
integrated as a plug-in module into existing agentic frameworks without any
additional training cost. Extensive experiments across diverse domains
demonstrate our method's effectiveness and generality, highlighting the
importance of principled, context-aware demo selection for robust and efficient
LLM agents.

</details>


### [17] [Semantic Chain-of-Trust: Autonomous Trust Orchestration for Collaborator Selection via Hypergraph-Aided Agentic AI](https://arxiv.org/abs/2507.23565)
*Botao Zhu,Xianbin Wang,Dusit Niyato*

Main category: cs.AI

TL;DR: 提出一种基于代理AI和超图的语义信任链方法，用于高效评估分布式协作系统中的设备信任。


<details>
  <summary>Details</summary>
Motivation: 解决协作系统中任务特定信任评估的复杂性和资源消耗问题，避免不当的信任评估降低资源利用率。

Method: 采用代理AI和超图建立和维护设备间的信任关系，仅在设备空闲期间基于历史性能数据进行信任评估，并根据资源能力和任务需求进行特定任务的信任评估。

Result: 实验结果表明该方法实现了资源高效的信任评估。

Conclusion: 提出了一种基于语义信任链的自主信任协调方法，利用代理AI和超图高效地评估分布式协作系统中设备的信任，平衡了开销和信任精度，并在实验中证明了其资源效率。

Abstract: In collaborative systems, the effective completion of tasks hinges on
task-specific trust evaluations of potential devices for distributed
collaboration. However, the complexity of tasks, the spatiotemporal dynamism of
distributed device resources, and the inevitable assessment overhead
dramatically increase the complexity and resource consumption of the trust
evaluation process. As a result, ill-timed or overly frequent trust evaluations
can reduce utilization rate of constrained resources, negatively affecting
collaborative task execution. To address this challenge, this paper proposes an
autonomous trust orchestration method based on a new concept of semantic
chain-of-trust. Our technique employs agentic AI and hypergraph to establish
and maintain trust relationships among devices. By leveraging its strengths in
autonomous perception, task decomposition, and semantic reasoning, we propose
agentic AI to perceive device states and autonomously perform trust evaluations
of collaborators based on historical performance data only during device idle
periods, thereby enabling efficient utilization of distributed resources. In
addition, agentic AI performs task-specific trust evaluations on collaborator
resources by analyzing the alignment between resource capabilities and task
requirements. Moreover, by maintaining a trust hypergraph embedded with trust
semantics for each device, agentic AI enables hierarchical management of
collaborators and identifies collaborators requiring trust evaluation based on
trust semantics, thereby achieving a balance between overhead and trust
accuracy. Furthermore, local trust hypergraphs from multiple devices can be
chained together to support multi-hop collaboration, enabling efficient
coordination in large-scale systems. Experimental results demonstrate that the
proposed method achieves resource-efficient trust evaluation.

</details>
