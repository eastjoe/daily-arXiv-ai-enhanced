<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [When Truthful Representations Flip Under Deceptive Instructions?](https://arxiv.org/abs/2507.22149)
*Xianxuan Long,Yao Fu,Runchao Li,Mu Sheng,Haotian Yu,Xiaotian Han,Pan Li*

Main category: cs.AI

TL;DR: 研究发现恶意指令会显著改变LLM的内部表征，这种变化可被检测并用于改进LLM的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易受到恶意指令的影响，生成欺骗性回应，这带来了安全挑战。而对这些欺骗性指令如何改变LLM内部表征的理解仍然不足。

Method: 研究者使用线性探测和稀疏自动编码器(SAE)分析了Llama-3.1-8B-Instruct和Gemma-2-9B-Instruct模型在事实验证任务上的内部表征。

Result: 研究发现，模型的真/假输出可以通过线性探测预测；欺骗性指令会引起模型表征的显著变化，这种变化集中在早期到中期层，且在复杂数据集上也能检测到；研究者识别出对欺骗性指令高度敏感的SAE特征，并通过可视化确认了真实/欺骗表征的显著差异。

Conclusion: 该论文研究了大型语言模型(LLM)在恶意指令下生成欺骗性回应的内部表征变化，发现欺骗性指令会导致模型表征发生显著偏移，这种偏移主要集中在模型的早期到中期层，并且即使在复杂的数据集上也能被检测到。研究者利用线性探测和稀疏自动编码器(SAE)等方法，识别出对欺骗性指令高度敏感的SAE特征，并通过可视化确认了真实/欺骗表征的显著差异。

Abstract: Large language models (LLMs) tend to follow maliciously crafted instructions
to generate deceptive responses, posing safety challenges. How deceptive
instructions alter the internal representations of LLM compared to truthful
ones remains poorly understood beyond output analysis. To bridge this gap, we
investigate when and how these representations ``flip'', such as from truthful
to deceptive, under deceptive versus truthful/neutral instructions. Analyzing
the internal representations of Llama-3.1-8B-Instruct and Gemma-2-9B-Instruct
on a factual verification task, we find the model's instructed True/False
output is predictable via linear probes across all conditions based on the
internal representation. Further, we use Sparse Autoencoders (SAEs) to show
that the Deceptive instructions induce significant representational shifts
compared to Truthful/Neutral representations (which are similar), concentrated
in early-to-mid layers and detectable even on complex datasets. We also
identify specific SAE features highly sensitive to deceptive instruction and
use targeted visualizations to confirm distinct truthful/deceptive
representational subspaces. % Our analysis pinpoints layer-wise and
feature-level correlates of instructed dishonesty, offering insights for LLM
detection and control. Our findings expose feature- and layer-level signatures
of deception, offering new insights for detecting and mitigating instructed
dishonesty in LLMs.

</details>


### [2] [Explainability Through Systematicity: The Hard Systematicity Challenge for Artificial Intelligence](https://arxiv.org/abs/2507.22197)
*Matthieu Queloz*

Main category: cs.AI

TL;DR: 本文探讨了人工智能的系统性问题，认为系统性比可解释性更重要，并提出了一个动态理解系统性需求的框架。


<details>
  <summary>Details</summary>
Motivation: 澄清系统性与连接主义之间的关系，并探讨AI模型是否应遵循系统性理想。

Method: 本文提出了一个关于“思维的系统性”的概念框架，区分了该短语的四种含义，并应用五个理由来评估AI模型的系统性。

Result: 本文构建了一个关于系统性的更丰富的概念，并提出了“硬系统性挑战”，认为对系统化的需求需要由系统化的理由来规范。

Conclusion: 本文认为，可解释性只是塑造我们对人工智能期望的更广泛理想的一个方面，系统性是人工智能的更重要考量。

Abstract: This paper argues that explainability is only one facet of a broader ideal
that shapes our expectations towards artificial intelligence (AI).
Fundamentally, the issue is to what extent AI exhibits systematicity--not
merely in being sensitive to how thoughts are composed of recombinable
constituents, but in striving towards an integrated body of thought that is
consistent, coherent, comprehensive, and parsimoniously principled. This richer
conception of systematicity has been obscured by the long shadow of the
"systematicity challenge" to connectionism, according to which network
architectures are fundamentally at odds with what Fodor and colleagues termed
"the systematicity of thought." I offer a conceptual framework for thinking
about "the systematicity of thought" that distinguishes four senses of the
phrase. I use these distinctions to defuse the perceived tension between
systematicity and connectionism and show that the conception of systematicity
that historically shaped our sense of what makes thought rational,
authoritative, and scientific is more demanding than the Fodorian notion. To
determine whether we have reason to hold AI models to this ideal of
systematicity, I then argue, we must look to the rationales for systematization
and explore to what extent they transfer to AI models. I identify five such
rationales and apply them to AI. This brings into view the "hard systematicity
challenge." However, the demand for systematization itself needs to be
regulated by the rationales for systematization. This yields a dynamic
understanding of the need to systematize thought, which tells us how systematic
we need AI models to be and when.

</details>


### [3] [CoEx -- Co-evolving World-model and Exploration](https://arxiv.org/abs/2507.22281)
*Minsoo Kim,Seung-won Hwang*

Main category: cs.AI

TL;DR: CoEx 通过动态更新世界模型提升 LLM 智能体的规划能力，在多个复杂场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 智能体依赖预训练获得的静态世界模型进行规划，易与真实世界状态脱节，导致计划偏差。

Method: CoEx 使用分层状态抽象，结合神经符号信念状态（包含文本推理和代码符号记忆）持续更新世界模型，并利用 LLM 推理协调动态计划中的子目标。

Result: 实验表明，CoEx 在 ALFWorld、PDDL 和 Jericho 等复杂环境和任务中，优于现有智能体范式。

Conclusion: CoEx 是一种分层式智能体架构，通过分层状态抽象，使 LLM 规划与动态更新的世界模型共同进化，从而克服现有智能体在规划中依赖静态世界模型导致的错误。

Abstract: Planning in modern LLM agents relies on the utilization of LLM as an internal
world model, acquired during pretraining. However, existing agent designs fail
to effectively assimilate new observations into dynamic updates of the world
model. This reliance on the LLM's static internal world model is progressively
prone to misalignment with the underlying true state of the world, leading to
the generation of divergent and erroneous plans. We introduce a hierarchical
agent architecture, CoEx, in which hierarchical state abstraction allows LLM
planning to co-evolve with a dynamically updated model of the world. CoEx plans
and interacts with the world by using LLM reasoning to orchestrate dynamic
plans consisting of subgoals, and its learning mechanism continuously
incorporates these subgoal experiences into a persistent world model in the
form of a neurosymbolic belief state, comprising textual inferences and
code-based symbolic memory. We evaluate our agent across a diverse set of agent
scenarios involving rich environments and complex tasks including ALFWorld,
PDDL, and Jericho. Our experiments show that CoEx outperforms existing agent
paradigms in planning and exploration.

</details>


### [4] [An Explainable Emotion Alignment Framework for LLM-Empowered Agent in Metaverse Service Ecosystem](https://arxiv.org/abs/2507.22326)
*Qun Ma,Xiao Xue,Ming Zhang,Yifan Shen,Zihan Zhao*

Main category: cs.AI

TL;DR: 提出可解释情绪对齐框架，解决元宇宙服务生态系统中基于LLM的代理面临的挑战，并在点对点外卖场景中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的代理在元宇宙服务生态系统建模中面临挑战，例如虚拟世界服务与现实世界服务的桥接、角色数据融合、角色知识关联和伦理安全问题。

Method: 提出了一种可解释的情绪对齐框架，并在点对点外卖场景中进行了仿真实验。

Result: 该框架能够将事实因素整合到基于LLM的代理的决策循环中，从而实现更真实的社会涌现。

Conclusion: 提出了一种用于元宇宙服务生态系统中基于LLM的代理的可解释情绪对齐框架，以实现更真实的社会涌现，并在点对点外卖场景中进行了仿真实验。

Abstract: Metaverse service is a product of the convergence between Metaverse and
service systems, designed to address service-related challenges concerning
digital avatars, digital twins, and digital natives within Metaverse. With the
rise of large language models (LLMs), agents now play a pivotal role in
Metaverse service ecosystem, serving dual functions: as digital avatars
representing users in the virtual realm and as service assistants (or NPCs)
providing personalized support. However, during the modeling of Metaverse
service ecosystems, existing LLM-based agents face significant challenges in
bridging virtual-world services with real-world services, particularly
regarding issues such as character data fusion, character knowledge
association, and ethical safety concerns. This paper proposes an explainable
emotion alignment framework for LLM-based agents in Metaverse Service
Ecosystem. It aims to integrate factual factors into the decision-making loop
of LLM-based agents, systematically demonstrating how to achieve more
relational fact alignment for these agents. Finally, a simulation experiment in
the Offline-to-Offline food delivery scenario is conducted to evaluate the
effectiveness of this framework, obtaining more realistic social emergence.

</details>


### [5] [Magentic-UI: Towards Human-in-the-loop Agentic Systems](https://arxiv.org/abs/2507.22358)
*Hussein Mozannar,Gagan Bansal,Cheng Tan,Adam Fourney,Victor Dibia,Jingya Chen,Jack Gerrits,Tyler Payne,Matheus Kunzler Maldaner,Madeleine Grunde-McLaughlin,Eric Zhu,Griffin Bassman,Jacob Alber,Peter Chang,Ricky Loynd,Friederike Niedtner,Ece Kamar,Maya Murad,Rafah Hosn,Saleema Amershi*

Main category: cs.AI

TL;DR: Magentic-UI是一个用于安全高效人机协作的开源界面，结合人工监督和AI效率，降低了AI智能体的安全风险，并提高了生产力。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体在复杂任务上仍逊于人类，且存在安全风险，因此需要结合人工参与。

Method: 构建了一个灵活的多智能体架构的开源网络界面Magentic-UI，支持网页浏览、代码执行和文件操作，并通过模型上下文协议(MCP)扩展了多种工具，并引入了六种交互机制。

Result: 评估表明Magentic-UI能够促进安全高效的人机协作。

Conclusion: Magentic-UI，一个用于开发和研究人机交互的开源网络界面，通过结合人工监督和AI效率，提高了不完善系统的生产力，并降低了安全风险。

Abstract: AI agents powered by large language models are increasingly capable of
autonomously completing complex, multi-step tasks using external tools. Yet,
they still fall short of human-level performance in most domains including
computer use, software development, and research. Their growing autonomy and
ability to interact with the outside world, also introduces safety and security
risks including potentially misaligned actions and adversarial manipulation. We
argue that human-in-the-loop agentic systems offer a promising path forward,
combining human oversight and control with AI efficiency to unlock productivity
from imperfect systems. We introduce Magentic-UI, an open-source web interface
for developing and studying human-agent interaction. Built on a flexible
multi-agent architecture, Magentic-UI supports web browsing, code execution,
and file manipulation, and can be extended with diverse tools via Model Context
Protocol (MCP). Moreover, Magentic-UI presents six interaction mechanisms for
enabling effective, low-cost human involvement: co-planning, co-tasking,
multi-tasking, action guards, and long-term memory. We evaluate Magentic-UI
across four dimensions: autonomous task completion on agentic benchmarks,
simulated user testing of its interaction capabilities, qualitative studies
with real users, and targeted safety assessments. Our findings highlight
Magentic-UI's potential to advance safe and efficient human-agent
collaboration.

</details>


### [6] [LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of Large Language Models](https://arxiv.org/abs/2507.22359)
*Qianhong Guo,Wei Xie,Xiaofang Cai,Enze Wang,Shuoyoucheng Ma,Kai Chen,Xiaofeng Wang,Baosheng Wang*

Main category: cs.AI

TL;DR: 提出一种新的LLM评估方法LLM-Crowdsourced，解决了现有方法的不足，并发现了LLM的一些新特性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在数据污染、黑盒操作和主观偏好等问题，难以全面评估大型语言模型的真实能力。

Method: 提出了一种新的基于LLM的众包评估范式，该范式利用LLM生成问题、独立作答和相互评价，并整合了动态、透明、客观和专业的四个关键评估标准。

Result: 实验验证了该方法在区分LLM性能方面的优势，并发现了一些新发现，例如Gemini在原创性和专业性方面表现突出，一些LLM存在基于记忆的作答行为等。

Conclusion: 提出了一种新的无基准评估范式LLM-Crowdsourced，用于更全面地评估大型语言模型的能力，并通过实验验证了其优势，发现了现有方法难以检测到的新发现。

Abstract: Although large language models (LLMs) demonstrate remarkable capabilities
across various tasks, evaluating their capabilities remains a challenging task.
Existing evaluation methods suffer from issues such as data contamination,
black-box operation, and subjective preference. These issues make it difficult
to evaluate the LLMs' true capabilities comprehensively. To tackle these
challenges, we propose a novel benchmark-free evaluation paradigm,
LLM-Crowdsourced. It utilizes LLMs to generate questions, answer independently,
and evaluate mutually. This method integrates four key evaluation criteria:
dynamic, transparent, objective, and professional, which existing evaluation
methods cannot satisfy simultaneously. Experiments on eight mainstream LLMs
across mathematics and programming verify the advantages of our method in
distinguishing LLM performance. Furthermore, our study reveals several novel
findings that are difficult for traditional methods to detect, including but
not limited to: (1) Gemini demonstrates the highest original and professional
question-design capabilities among others; (2) Some LLMs exhibit
''memorization-based answering'' by misrecognizing questions as familiar ones
with a similar structure; (3) LLM evaluation results demonstrate high
consistency (robustness).

</details>


### [7] [Beyond Accuracy: How AI Metacognitive Sensitivity improves AI-assisted Decision Making](https://arxiv.org/abs/2507.22365)
*ZhaoBin Li,Mark Steyvers*

Main category: cs.AI

TL;DR: AI的元认知敏感性对提升人类决策至关重要，应与预测准确性一起评估和优化。


<details>
  <summary>Details</summary>
Motivation: 研究AI系统预测准确性和元认知敏感性对混合决策的影响。

Method: 理论分析和行为实验

Result: 元认知敏感性更高的AI系统能提升人类决策绩效，应同时优化AI的准确性和元认知敏感性。

Conclusion: AI系统预测准确性和元认知敏感性均会影响人类决策质量，后者甚至在预测准确性较低时也能提升人类决策准确性。

Abstract: In settings where human decision-making relies on AI input, both the
predictive accuracy of the AI system and the reliability of its confidence
estimates influence decision quality. We highlight the role of AI metacognitive
sensitivity -- its ability to assign confidence scores that accurately
distinguish correct from incorrect predictions -- and introduce a theoretical
framework for assessing the joint impact of AI's predictive accuracy and
metacognitive sensitivity in hybrid decision-making settings. Our analysis
identifies conditions under which an AI with lower predictive accuracy but
higher metacognitive sensitivity can enhance the overall accuracy of human
decision making. Finally, a behavioral experiment confirms that greater AI
metacognitive sensitivity improves human decision performance. Together, these
findings underscore the importance of evaluating AI assistance not only by
accuracy but also by metacognitive sensitivity, and of optimizing both to
achieve superior decision outcomes.

</details>


### [8] [On the Definition of Intelligence](https://arxiv.org/abs/2507.22423)
*Kei-Sing Ng*

Main category: cs.AI

TL;DR: 提出样本保真度作为衡量智能的通用标准，用于评估AGI。


<details>
  <summary>Details</summary>
Motivation: 为了设计AGI，需要一个物种无关的、能够评估多种智能行为方式的通用智能标准。

Method: 提出了一种基于样本保真度的通用智能标准，并给出了形式化框架和经验协议。

Result: 提出了ε-类别智能的概念和评估框架。

Conclusion: 本文提出了一种基于样本保真度的通用智能标准，即ε-类别智能，并以此评估AGI。

Abstract: To engineer AGI, we should first capture the essence of intelligence in a
species-agnostic form that can be evaluated, while being sufficiently general
to encompass diverse paradigms of intelligent behavior, including reinforcement
learning, generative models, classification, analogical reasoning, and
goal-directed decision-making. We propose a general criterion based on sample
fidelity: intelligence is the ability, given sample(s) from a category, to
generate sample(s) from the same category. We formalise this intuition as
{\epsilon}-category intelligence: it is {\epsilon}-intelligent with respect to
a category if no chosen admissible distinguisher can separate generated from
original samples beyond tolerance {\epsilon}. We present the formal framework,
outline empirical protocols, and discuss implications for evaluation, safety,
and generalization.

</details>


### [9] [Cross-Border Legal Adaptation of Autonomous Vehicle Design based on Logic and Non-monotonic Reasoning](https://arxiv.org/abs/2507.22432)
*Zhe Yu,Yiwei Lu,Burkhard Schafer,Zhe Lin*

Main category: cs.AI

TL;DR: 论文提出一种基于论证理论的推理系统，以帮助自动驾驶汽车设计师在跨国背景下进行合规设计。


<details>
  <summary>Details</summary>
Motivation: 关注自动驾驶汽车在跨国背景下的法律合规挑战，从设计师视角出发，为设计过程提供法律依据。

Method: 基于论证理论，结合自然数偏序集表达优先级，构建逻辑来表示基于论证的实践（规范性）推理的基本属性。

Result: 提出一个推理系统，帮助设计师更灵活地适应自动驾驶汽车跨境应用，并更容易理解其决策的法律含义。

Conclusion: 该论文通过案例分析，展示了其提出的推理系统如何帮助设计师更灵活地适应自动驾驶汽车跨境应用中的设计方案，并更容易理解其决策的法律含义。

Abstract: This paper focuses on the legal compliance challenges of autonomous vehicles
in a transnational context. We choose the perspective of designers and try to
provide supporting legal reasoning in the design process. Based on
argumentation theory, we introduce a logic to represent the basic properties of
argument-based practical (normative) reasoning, combined with partial order
sets of natural numbers to express priority. Finally, through case analysis of
legal texts, we show how the reasoning system we provide can help designers to
adapt their design solutions more flexibly in the cross-border application of
autonomous vehicles and to more easily understand the legal implications of
their decisions.

</details>


### [10] [Nearest-Better Network for Visualizing and Analyzing Combinatorial Optimization Problems: A Unified Tool](https://arxiv.org/abs/2507.22440)
*Yiya Diao,Changhe Li,Sanyou Zeng,Xinye Cai,Wenjian Luo,Shengxiang Yang,Carlos A. Coello Coello*

Main category: cs.AI

TL;DR: 高效NBN算法及其在OneMax和TSP问题中的应用，揭示了适应度景观特征和算法局限性。


<details>
  <summary>Details</summary>
Motivation: 改进NBN计算效率，并将其应用于组合优化问题分析。

Method: 提出了一种高效的NBN计算方法，时间复杂度为对数线性。

Result: 发现了OneMax问题的适应度景观特征（中性、崎岖、多峰性），以及EAX和LKH算法在处理TSP问题时的局限性（EAX在多吸引盆时效率降低，LKH在全局最优解附近存在欺骗解时失效）。

Conclusion: Nearest-Better Network (NBN) 的高效计算方法，并通过OneMax问题和旅行商问题 (TSP) 验证了其有效性，发现了OneMax问题的适应度景观特征以及两种TSP算法 (EAX和LKH) 的局限性。

Abstract: The Nearest-Better Network (NBN) is a powerful method to visualize sampled
data for continuous optimization problems while preserving multiple landscape
features. However, the calculation of NBN is very time-consuming, and the
extension of the method to combinatorial optimization problems is challenging
but very important for analyzing the algorithm's behavior. This paper provides
a straightforward theoretical derivation showing that the NBN network
essentially functions as the maximum probability transition network for
algorithms. This paper also presents an efficient NBN computation method with
logarithmic linear time complexity to address the time-consuming issue. By
applying this efficient NBN algorithm to the OneMax problem and the Traveling
Salesman Problem (TSP), we have made several remarkable discoveries for the
first time: The fitness landscape of OneMax exhibits neutrality, ruggedness,
and modality features. The primary challenges of TSP problems are ruggedness,
modality, and deception. Two state-of-the-art TSP algorithms (i.e., EAX and
LKH) have limitations when addressing challenges related to modality and
deception, respectively. LKH, based on local search operators, fails when there
are deceptive solutions near global optima. EAX, which is based on a single
population, can efficiently maintain diversity. However, when multiple
attraction basins exist, EAX retains individuals within multiple basins
simultaneously, reducing inter-basin interaction efficiency and leading to
algorithm's stagnation.

</details>


### [11] [Collaborative Medical Triage under Uncertainty: A Multi-Agent Dynamic Matching Approach](https://arxiv.org/abs/2507.22504)
*Hongyan Cheng,Chengzhang Yu,Yanshu Shi,Chiyue Wang,Cong Liu,Zhanpeng Jin*

Main category: cs.AI

TL;DR: 开发了一个多智能体医疗分诊系统，有效解决了数据缺失和医院结构差异问题，提高了分诊准确率。


<details>
  <summary>Details</summary>
Motivation: 解决疫情后医疗需求激增和护理人员短缺导致的急诊分诊系统压力，以及现有AI分诊系统在医疗专业化不足、医院结构异质性和提问效率低等方面的问题。

Method: 该系统采用三个专业代理（RecipientAgent、InquirerAgent和DepartmentAgent）协同工作，通过结构化询问机制和特定部门的指导规则，将非结构化患者症状转化为准确的科室推荐。利用大型语言模型进行数据插补，解决了真实世界数据中医疗记录不完整的问题。

Result: 在初级科室分类中准确率达到89.2%，在二级科室分类中准确率达到73.9%。

Conclusion: 提出了一种基于多智能体交互的智能医疗分诊系统，以解决现有AI分诊系统中存在的不足，并在中文医疗分诊数据集上取得了较高的准确率。

Abstract: The post-pandemic surge in healthcare demand, coupled with critical nursing
shortages, has placed unprecedented pressure on emergency department triage
systems, necessitating innovative AI-driven solutions. We present a multi-agent
interactive intelligent system for medical triage that addresses three
fundamental challenges in current AI-based triage systems: insufficient medical
specialization leading to hallucination-induced misclassifications,
heterogeneous department structures across healthcare institutions, and
inefficient detail-oriented questioning that impedes rapid triage decisions.
Our system employs three specialized agents - RecipientAgent, InquirerAgent,
and DepartmentAgent - that collaborate through structured inquiry mechanisms
and department-specific guidance rules to transform unstructured patient
symptoms into accurate department recommendations. To ensure robust evaluation,
we constructed a comprehensive Chinese medical triage dataset from a medical
website, comprising 3,360 real-world cases spanning 9 primary departments and
62 secondary departments. Through systematic data imputation using large
language models, we address the prevalent issue of incomplete medical records
in real-world data. Experimental results demonstrate that our multi-agent
system achieves 89.2% accuracy in primary department classification and 73.9%
accuracy in secondary department classification after four rounds of patient
interaction. The system's pattern-matching-based guidance mechanisms enable
efficient adaptation to diverse hospital configurations while maintaining high
triage accuracy. Our work provides a scalable framework for deploying
AI-assisted triage systems that can accommodate the organizational
heterogeneity of healthcare institutions while ensuring clinically sound
decision-making.

</details>


### [12] [MetaAgent: Automatically Constructing Multi-Agent Systems Based on Finite State Machines](https://arxiv.org/abs/2507.22606)
*Yaolun Zhang,Xiaogeng Liu,Chaowei Xiao*

Main category: cs.AI

TL;DR: MetaAgent框架自动生成高效的多智能体系统。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于少量预定义场景或存在工具集成不足、依赖外部训练数据、通信结构僵化等问题。

Method: 基于有限状态机的框架，结合优化算法。

Result: 实验结果表明，生成的系统优于其他自动设计方法，与人工设计系统性能相当。

Conclusion: MetaAgent框架自动生成多智能体系统，其性能优于其他自动设计方法，与针对特定任务进行优化的，人工设计的系统性能相当。

Abstract: Large Language Models (LLMs) have demonstrated the ability to solve a wide
range of practical tasks within multi-agent systems. However, existing
human-designed multi-agent frameworks are typically limited to a small set of
pre-defined scenarios, while current automated design methods suffer from
several limitations, such as the lack of tool integration, dependence on
external training data, and rigid communication structures. In this paper, we
propose MetaAgent, a finite state machine based framework that can
automatically generate a multi-agent system. Given a task description,
MetaAgent will design a multi-agent system and polish it through an
optimization algorithm. When the multi-agent system is deployed, the finite
state machine will control the agent's actions and the state transitions. To
evaluate our framework, we conduct experiments on both text-based tasks and
practical tasks. The results indicate that the generated multi-agent system
surpasses other auto-designed methods and can achieve a comparable performance
with the human-designed multi-agent system, which is optimized for those
specific tasks.

</details>


### [13] [Enhancing Manufacturing Knowledge Access with LLMs and Context-aware Prompting](https://arxiv.org/abs/2507.22619)
*Sebastian Monka,Irlan Grangel-González,Stefan Schmid,Lavdim Halilaj,Marc Rickart,Oliver Rudolph,Rui Dias*

Main category: cs.AI

TL;DR: 使用LLM将自然语言查询转换为SPARQL查询，通过提供KG模式上下文显著提升查询准确性，降低了非专家用户使用知识图谱的门槛。


<details>
  <summary>Details</summary>
Motivation: 为了解决非专家用户难以使用SPARQL查询知识图谱的问题，探索使用LLM将自然语言查询转换为SPARQL查询。

Method: 评估多种使用LLM作为中介从知识图谱检索信息的方法，比较了不同上下文输入方式对生成SPARQL查询的影响。

Result: 结果表明，仅提供足够的KG模式上下文，LLM就能显著提高生成正确、完整的查询的性能。上下文感知提示技术有助于LLM专注于本体的相关部分，降低幻觉风险。

Conclusion: 本文评估了多种使用LLM作为中介从知识图谱检索信息的方法，发现为LLM提供足够的KG模式上下文可以显著提高其生成正确SPARQL查询的性能。

Abstract: Knowledge graphs (KGs) have transformed data management within the
manufacturing industry, offering effective means for integrating disparate data
sources through shared and structured conceptual schemas. However, harnessing
the power of KGs can be daunting for non-experts, as it often requires
formulating complex SPARQL queries to retrieve specific information. With the
advent of Large Language Models (LLMs), there is a growing potential to
automatically translate natural language queries into the SPARQL format, thus
bridging the gap between user-friendly interfaces and the sophisticated
architecture of KGs. The challenge remains in adequately informing LLMs about
the relevant context and structure of domain-specific KGs, e.g., in
manufacturing, to improve the accuracy of generated queries. In this paper, we
evaluate multiple strategies that use LLMs as mediators to facilitate
information retrieval from KGs. We focus on the manufacturing domain,
particularly on the Bosch Line Information System KG and the I40 Core
Information Model. In our evaluation, we compare various approaches for feeding
relevant context from the KG to the LLM and analyze their proficiency in
transforming real-world questions into SPARQL queries. Our findings show that
LLMs can significantly improve their performance on generating correct and
complete queries when provided only the adequate context of the KG schema. Such
context-aware prompting techniques help LLMs to focus on the relevant parts of
the ontology and reduce the risk of hallucination. We anticipate that the
proposed techniques help LLMs to democratize access to complex data
repositories and empower informed decision-making in manufacturing settings.

</details>


### [14] [ASP-FZN: A Translation-based Constraint Answer Set Solver](https://arxiv.org/abs/2507.22774)
*Thomas Eiter,Tobias Geibinger,Tobias Kaminski,Nysret Musliu,Johannes Oetsch*

Main category: cs.AI

TL;DR: asp-fzn:一个新的CASP求解器，在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 扩展ASP以支持线性约束。

Method: 将CASP程序转换为支持多种约束编程和整数规划后端求解器的独立于求解器的FlatZinc语言。

Result: asp-fzn求解器在基准测试中表现良好，与最先进的ASP求解器相比具有竞争力，并且在一些CASP基准测试中优于clingcon。

Conclusion: asp-fzn求解器在基准测试中与最先进的ASP求解器相比具有竞争力，并且在一些CASP基准测试中甚至优于clingcon。

Abstract: We present the solver asp-fzn for Constraint Answer Set Programming (CASP),
which extends ASP with linear constraints. Our approach is based on translating
CASP programs into the solver-independent FlatZinc language that supports
several Constraint Programming and Integer Programming backend solvers. Our
solver supports a rich language of linear constraints, including some common
global constraints. As for evaluation, we show that asp-fzn is competitive with
state-of-the-art ASP solvers on benchmarks taken from past ASP competitions.
Furthermore, we evaluate it on several CASP problems from the literature and
compare its performance with clingcon, which is a prominent CASP solver that
supports most of the asp-fzn language. The performance of asp-fzn is very
promising as it is already competitive on plain ASP and even outperforms
clingcon on some CASP benchmarks.

</details>


### [15] [Enhancing Multi-Agent Collaboration with Attention-Based Actor-Critic Policies](https://arxiv.org/abs/2507.22782)
*Hugo Garrido-Lestache,Jeremy Kedziora*

Main category: cs.AI

TL;DR: TAAC算法通过注意力机制和惩罚损失函数提高了多智能体协作效率，并在模拟足球环境中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 为了增强多智能体协作环境下的协作能力，有效管理联合动作空间的指数增长。

Method: 采用集中式训练/集中式执行方案，结合Actor和Critic中的多头注意力机制，并引入惩罚损失函数以促进智能体之间多样化且互补的角色。

Result: 在模拟足球环境中，TAAC在多种指标（胜率、净胜球、Elo等级分、智能体间连接性、平衡的空间分布以及频繁的战术互动等）上均表现出优越的性能和增强的协作行为。

Conclusion: TAAC算法在多智能体协作环境中表现出优越的性能和增强的协作行为，优于基准算法。

Abstract: This paper introduces Team-Attention-Actor-Critic (TAAC), a reinforcement
learning algorithm designed to enhance multi-agent collaboration in cooperative
environments. TAAC employs a Centralized Training/Centralized Execution scheme
incorporating multi-headed attention mechanisms in both the actor and critic.
This design facilitates dynamic, inter-agent communication, allowing agents to
explicitly query teammates, thereby efficiently managing the exponential growth
of joint-action spaces while ensuring a high degree of collaboration. We
further introduce a penalized loss function which promotes diverse yet
complementary roles among agents. We evaluate TAAC in a simulated soccer
environment against benchmark algorithms representing other multi-agent
paradigms, including Proximal Policy Optimization and Multi-Agent
Actor-Attention-Critic. We find that TAAC exhibits superior performance and
enhanced collaborative behaviors across a variety of metrics (win rates, goal
differentials, Elo ratings, inter-agent connectivity, balanced spatial
distributions, and frequent tactical interactions such as ball possession
swaps).

</details>


### [16] [The Incomplete Bridge: How AI Research (Mis)Engages with Psychology](https://arxiv.org/abs/2507.22847)
*Han Jiang,Pengda Wang,Xiaoyuan Yi,Xing Xie,Ziang Xiao*

Main category: cs.AI

TL;DR: 通过分析AI论文及其引用的心理学文献，研究揭示了AI与心理学交叉领域的现状、挑战和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 探索AI和心理学之间的跨学科协同作用，特别是心理学在AI设计和理解中的价值。

Method: 分析了2023年至2025年期间发表在主要AI期刊上的1006篇与LLM相关的论文及其引用的2544篇心理学论文，识别关键的跨学科整合模式，确定最常引用的心理学领域，并揭示了仍未充分探索的领域。

Result: 识别了跨学科整合的关键模式，确定了最常引用的心理学领域，指出了应用中的误区，并为更有效的整合提供了指导。

Conclusion: 该研究提供了AI与心理学交叉领域综合性研究，促进了学科间的深度合作和AI系统发展。

Abstract: Social sciences have accumulated a rich body of theories and methodologies
for investigating the human mind and behaviors, while offering valuable
insights into the design and understanding of Artificial Intelligence (AI)
systems. Focusing on psychology as a prominent case, this study explores the
interdisciplinary synergy between AI and the field by analyzing 1,006
LLM-related papers published in premier AI venues between 2023 and 2025, along
with the 2,544 psychology publications they cite. Through our analysis, we
identify key patterns of interdisciplinary integration, locate the psychology
domains most frequently referenced, and highlight areas that remain
underexplored. We further examine how psychology theories/frameworks are
operationalized and interpreted, identify common types of misapplication, and
offer guidance for more effective incorporation. Our work provides a
comprehensive map of interdisciplinary engagement between AI and psychology,
thereby facilitating deeper collaboration and advancing AI systems.

</details>
