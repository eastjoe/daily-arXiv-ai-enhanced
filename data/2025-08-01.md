<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Unifying Post-hoc Explanations of Knowledge Graph Completions](https://arxiv.org/abs/2507.22951)
*Alessandro Lonardi,Samy Badreddine,Tarek R. Besold,Pablo Sanchez Martin*

Main category: cs.AI

TL;DR: 提出一个统一框架，改进评估协议，提升 KGC 可解释性研究的可重复性和影响力。


<details>
  <summary>Details</summary>
Motivation: KGC 的事后可解释性缺乏形式化和一致的评估，阻碍了可重复性和跨研究比较。

Method: 提出了一种通用的多目标优化框架来描述 KGC 中的事后解释，平衡了其有效性和简洁性；并建议并实证支持使用平均倒数排名和 Hits@$k$ 等常用指标改进评估协议。

Result: 统一了现有的 KGC 事后可解释性算法及其产生的解释；改进评估协议。

Conclusion: 这项工作旨在通过统一方法和改进评估标准，使知识图谱补全 (KGC) 可解释性研究更具可重复性和影响力。

Abstract: Post-hoc explainability for Knowledge Graph Completion (KGC) lacks
formalization and consistent evaluations, hindering reproducibility and
cross-study comparisons. This paper argues for a unified approach to post-hoc
explainability in KGC. First, we propose a general framework to characterize
post-hoc explanations via multi-objective optimization, balancing their
effectiveness and conciseness. This unifies existing post-hoc explainability
algorithms in KGC and the explanations they produce. Next, we suggest and
empirically support improved evaluation protocols using popular metrics like
Mean Reciprocal Rank and Hits@$k$. Finally, we stress the importance of
interpretability as the ability of explanations to address queries meaningful
to end-users. By unifying methods and refining evaluation standards, this work
aims to make research in KGC explainability more reproducible and impactful.

</details>


### [2] [Data Readiness for Scientific AI at Scale](https://arxiv.org/abs/2507.23018)
*Wesley Brewer,Patrick Widener,Valentine Anantharaj,Feiyi Wang,Tom Beck,Arjun Shankar,Sarp Oral*

Main category: cs.AI

TL;DR: 研究如何将数据准备就绪度原则应用于科学数据集以训练基础模型，提出一个二维框架评估科学数据准备情况。


<details>
  <summary>Details</summary>
Motivation: 为了解决将科学数据转化为可扩展AI训练的关键挑战，特别是基于转换器的生成模型。

Method: 分析了四个代表性领域（气候、核聚变、生物/健康和材料）的典型工作流程，确定了常见的预处理模式和特定领域的约束，并提出了一个由数据准备就绪级别和数据处理阶段组成的二维框架。

Result: 提出了一个二维准备就绪框架，对高性能计算环境下的科学数据准备情况进行了表征。

Conclusion: 本文研究了数据准备就绪度 (DRAI) 原则如何应用于用于训练基础模型的领导规模科学数据集，并引入了一个二维框架来表征科学数据的准备就绪状态，指导基础设施开发，从而为科学的AI提供标准化、跨领域的支撑。

Abstract: This paper examines how Data Readiness for AI (DRAI) principles apply to
leadership-scale scientific datasets used to train foundation models. We
analyze archetypal workflows across four representative domains - climate,
nuclear fusion, bio/health, and materials - to identify common preprocessing
patterns and domain-specific constraints. We introduce a two-dimensional
readiness framework composed of Data Readiness Levels (raw to AI-ready) and
Data Processing Stages (ingest to shard), both tailored to high performance
computing (HPC) environments. This framework outlines key challenges in
transforming scientific data for scalable AI training, emphasizing
transformer-based generative models. Together, these dimensions form a
conceptual maturity matrix that characterizes scientific data readiness and
guides infrastructure development toward standardized, cross-domain support for
scalable and reproducible AI for science.

</details>


### [3] [FairReason: Balancing Reasoning and Social Bias in MLLMs](https://arxiv.org/abs/2507.23067)
*Zhenyu Pan,Yutong Zhang,Jianshu Zhang,Haoran Lu,Haozheng Luo,Yuwei Han,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: 研究发现，强化学习结合特定比例的数据能够有效平衡MLLM的推理能力和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLM虽然在多种任务和模态上取得了最先进的结果，但在推理能力和社会偏见方面仍有改进空间。

Method: 对三种偏见缓解策略（监督微调、知识蒸馏和基于规则的强化学习）进行了基准测试，并通过改变不同类型样本的比例来研究推理能力和偏见之间的权衡。

Result: 强化学习方法在1:4的样本比例下，将刻板印象得分降低了10%，同时保留了88%的原始推理准确率。

Conclusion: 这项研究调查了多模态大型语言模型 (MLLM) 中推理能力提升与偏见缓解之间的权衡。结果表明，使用强化学习方法，并以1:4的比例混合去偏见和推理样本，可以在减少刻板印象得分的同时，保持较高的推理准确率。

Abstract: Multimodal Large Language Models (MLLMs) already achieve state-of-the-art
results across a wide range of tasks and modalities. To push their reasoning
ability further, recent studies explore advanced prompting schemes and
post-training fine-tuning. Although these techniques improve logical accuracy,
they frequently leave the models' outputs burdened with pronounced social
biases. Clarifying how reasoning gains interact with bias mitigation-and
whether the two objectives inherently trade off-therefore remains an open and
pressing research problem. Our study begins by benchmarking three
bias-mitigation strategies-supervised fine-uning (SFT), knowledge distillation
(KD), and rule-based reinforcement learning (RL)-under identical conditions,
establishing their baseline strengths and weaknesses. Building on these
results, we vary the proportion of debias-focused and reasoning-centric samples
within each paradigm to chart the reasoning-versus-bias trade-off. Our sweeps
reveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement
learning cuts stereotype scores by 10% while retaining 88% of the model's
original reasoning accuracy, offering concrete guidance for balancing fairness
and capability in MLLMs.

</details>


### [4] [Moravec's Paradox: Towards an Auditory Turing Test](https://arxiv.org/abs/2507.23091)
*David Noever,Forrest McKee*

Main category: cs.AI

TL;DR: AI听觉能力远落后于人类，需改进选择性注意、基于物理的音频理解和上下文感知等机制。


<details>
  <summary>Details</summary>
Motivation: 莫拉维克悖论启发，旨在量化人类和机器在听觉能力上的差距，并找出AI系统在处理复杂听觉场景时存在的问题。

Method: 设计了一个包含917个挑战的听觉图灵测试，评估了包括GPT-4和Whisper在内的多个最先进的音频模型。

Result: AI模型在听觉任务上的失败率超过93%，即使是表现最好的模型准确率也只有6.9%，远低于人类的52%。结果揭示了AI系统在选择性注意、噪声鲁棒性和上下文适应性方面的不足。

Conclusion: 当前AI系统在人类轻松完成的听觉任务上存在严重缺陷，该研究通过一个包含七个类别的听觉图灵测试评估了最先进的音频模型，结果显示AI模型的失败率超过93%，与人类表现存在巨大差距。

Abstract: This research work demonstrates that current AI systems fail catastrophically
on auditory tasks that humans perform effortlessly. Drawing inspiration from
Moravec's paradox (i.e., tasks simple for humans often prove difficult for
machines, and vice versa), we introduce an auditory Turing test comprising 917
challenges across seven categories: overlapping speech, speech in noise,
temporal distortion, spatial audio, coffee-shop noise, phone distortion, and
perceptual illusions. Our evaluation of state-of-the-art audio models including
GPT-4's audio capabilities and OpenAI's Whisper reveals a striking failure rate
exceeding 93%, with even the best-performing model achieving only 6.9% accuracy
on tasks that humans solved at 7.5 times higher success (52%). These results
expose focusing failures in how AI systems process complex auditory scenes,
particularly in selective attention, noise robustness, and contextual
adaptation. Our benchmark not only quantifies the human-machine auditory gap
but also provides insights into why these failures occur, suggesting that
current architectures lack fundamental mechanisms for human-like auditory scene
analysis. The traditional design of audio CAPTCHAs highlights common filters
that humans evolved but machines fail to select in multimodal language models.
This work establishes a diagnostic framework for measuring progress toward
human-level machine listening and highlights the need for novel approaches
integrating selective attention, physics-based audio understanding, and
context-aware perception into multimodal AI systems.

</details>


### [5] [Argumentatively Coherent Judgmental Forecasting](https://arxiv.org/abs/2507.23163)
*Deniz Gorur,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 预测一致性提高准确性，但用户不认同，需改进过滤机制。


<details>
  <summary>Details</summary>
Motivation: 研究论证结构中预测的一致性，以提高判断预测的准确性。

Method: 评估强制一致性对人类和大型语言模型预测准确性的影响，并通过众包用户实验评估用户对一致性的认同程度。

Result: 强制一致性提高了预测准确性，但用户并不总是认同这种一致性，这表明需要在基于论证的判断预测中加入过滤不一致意见的机制。

Conclusion: 强制预测一致性可以提高人类和大型语言模型的预测准确性，但用户通常并不认同这种一致性。

Abstract: Judgmental forecasting employs human opinions to make predictions about
future events, rather than exclusively historical data as in quantitative
forecasting. When these opinions form an argumentative structure around
forecasts, it is useful to study the properties of the forecasts from an
argumentative perspective. In this paper, we advocate and formally define a
property of argumentative coherence, which, in essence, requires that a
forecaster's reasoning is coherent with their forecast. We then conduct three
evaluations with our notion of coherence. First, we assess the impact of
enforcing coherence on human forecasters as well as on Large Language Model
(LLM)-based forecasters, given that they have recently shown to be competitive
with human forecasters. In both cases, we show that filtering out incoherent
predictions improves forecasting accuracy consistently, supporting the
practical value of coherence in both human and LLM-based forecasting. Then, via
crowd-sourced user experiments, we show that, despite its apparent
intuitiveness and usefulness, users do not generally align with this coherence
property. This points to the need to integrate, within argumentation-based
judgmental forecasting, mechanisms to filter out incoherent opinions before
obtaining group forecasting predictions.

</details>


### [6] [Tractable Responsibility Measures for Ontology-Mediated Query Answering](https://arxiv.org/abs/2507.23191)
*Meghyn Bienvenu,Diego Figueira,Pierre Lafourcade*

Main category: cs.AI

TL;DR: 本体介导查询应答中计算责任分数的复杂性研究，结果表明在某些情况下是多项式复杂度，而在另一些情况下是难处理的。


<details>
  <summary>Details</summary>
Motivation: 量化解释查询答案的定量方法，利用责任度量为事实分配分数以量化它们对获得给定答案的贡献。

Method: 利用数据库设置的结果，分析了加权最小支持计算的组合复杂度，证明了在本体语言支持合取的情况下，即使对于原子查询，计算也是难处理的。

Result: 确定了具有易处理WSMS计算的结构受限查询类，并指出了本体语言支持合取时计算的难处理性。

Conclusion: 研究了在本体介导查询应答环境下计算责任分数的复杂性，发现基于加权最小支持的Shapley值责任度量在某些本体介导查询类上具有多项式数据复杂度，而在其他情况下则为'shP'-hard。

Abstract: Recent work on quantitative approaches to explaining query answers employs
responsibility measures to assign scores to facts in order to quantify their
respective contributions to obtaining a given answer. In this paper, we study
the complexity of computing such responsibility scores in the setting of
ontology-mediated query answering, focusing on a very recently introduced
family of Shapley-value-based responsibility measures defined in terms of
weighted sums of minimal supports (WSMS). By exploiting results from the
database setting, we can show that such measures enjoy polynomial data
complexity for classes of ontology-mediated queries that are
first-order-rewritable, whereas the problem becomes "shP"-hard when the
ontology language can encode reachability queries (via axioms like $\exists R.
A \sqsubseteq A$). To better understand the tractability frontier, we next
explore the combined complexity of WSMS computation. We prove that
intractability applies already to atomic queries if the ontology language
supports conjunction, as well as to unions of `well-behaved' conjunctive
queries, even in the absence of an ontology. By contrast, our study yields
positive results for common DL-Lite dialects: by means of careful analysis, we
identify classes of structurally restricted conjunctive queries (which
intuitively disallow undesirable interactions between query atoms) that admit
tractable WSMS computation.

</details>


### [7] [Solution-aware vs global ReLU selection: partial MILP strikes back for DNN verification](https://arxiv.org/abs/2507.23197)
*Yuke Liao,Blaise Genest,Kuldeep Meel,Shaan Aryaman*

Main category: cs.AI

TL;DR: 提出一种高效的基于分治和混合MILP的深度学习模型验证方法，显著提高效率并保持高准确率


<details>
  <summary>Details</summary>
Motivation: 处理复杂实例的验证问题，提高效率和准确率。

Method: 采用分治策略，结合解决方案感知ReLU评分（SAS）以及改进的BaB-SR和BaB-FSB分支函数，使用混合MILP方法，先调用α,β-CROWN进行快速求解，再进行局部MILP求解。

Result: 将二元变量数量减少约6倍，同时保持相同精度；将未确定实例的数量减少高达40%，降低至8-15%；平均运行时间为46-417秒。

Conclusion: 提出了一种新的解决方案感知ReLU评分（SAS）方法，并改进了BaB-SR和BaB-FSB分支函数，将其作为全局ReLU评分（GS）函数，有效减少了二元变量数量，提高了验证效率。结合混合MILP方法，该方法显著降低了未确定实例的数量，并在合理的运行时间内保持较高的准确率。

Abstract: To handle complex instances, we revisit a divide-and-conquer approach to
break down the complexity: instead of few complex BaB calls, we rely on many
small {\em partial} MILP calls. The crucial step is to select very few but very
important ReLUs to treat using (costly) binary variables. The previous attempts
were suboptimal in that respect. To select these important ReLU variables, we
propose a novel {\em solution-aware} ReLU scoring ({\sf SAS}), as well as adapt
the BaB-SR and BaB-FSB branching functions as {\em global} ReLU scoring ({\sf
GS}) functions. We compare them theoretically as well as experimentally, and
{\sf SAS} is more efficient at selecting a set of variables to open using
binary variables. Compared with previous attempts, SAS reduces the number of
binary variables by around 6 times, while maintaining the same level of
accuracy. Implemented in {\em Hybrid MILP}, calling first $\alpha,\beta$-CROWN
with a short time-out to solve easier instances, and then partial MILP,
produces a very accurate yet efficient verifier, reducing by up to $40\%$ the
number of undecided instances to low levels ($8-15\%$), while keeping a
reasonable runtime ($46s-417s$ on average per instance), even for fairly large
CNNs with 2 million parameters.

</details>


### [8] [How Far Are AI Scientists from Changing the World?](https://arxiv.org/abs/2507.23276)
*Qiujie Xie,Yixuan Weng,Minjun Zhu,Fuchen Shen,Shulin Huang,Zhen Lin,Jiahui Zhou,Zilan Mao,Zijie Yang,Linyi Yang,Jian Wu,Yue Zhang*

Main category: cs.AI

TL;DR: 综述了AI科学家系统的现状、挑战和未来，旨在促进对科学AI的更清晰理解。


<details>
  <summary>Details</summary>
Motivation: 探究AI科学家系统距离改变世界和重塑科学研究范式还有多远。

Method: 对现有AI科学家系统的综述分析，包括其成就、瓶颈和关键组件。

Result: 识别了AI科学家系统当前的局限性，指出了未来发展方向和最终目标。

Conclusion: 该综述分析了AI科学家系统的当前成就、关键瓶颈和实现突破性发现所需的关键组件，旨在对AI科学家系统的能力和局限性进行更清晰的理解，并展望其未来发展方向。

Abstract: The emergence of large language models (LLMs) is propelling automated
scientific discovery to the next level, with LLM-based Artificial Intelligence
(AI) Scientist systems now taking the lead in scientific research. Several
influential works have already appeared in the field of AI Scientist systems,
with AI-generated research papers having been accepted at the ICLR 2025
workshop, suggesting that a human-level AI Scientist capable of uncovering
phenomena previously unknown to humans, may soon become a reality. In this
survey, we focus on the central question: How far are AI scientists from
changing the world and reshaping the scientific research paradigm? To answer
this question, we provide a prospect-driven review that comprehensively
analyzes the current achievements of AI Scientist systems, identifying key
bottlenecks and the critical components required for the emergence of a
scientific agent capable of producing ground-breaking discoveries that solve
grand challenges. We hope this survey will contribute to a clearer
understanding of limitations of current AI Scientist systems, showing where we
are, what is missing, and what the ultimate goals for scientific AI should be.

</details>


### [9] [AI Must not be Fully Autonomous](https://arxiv.org/abs/2507.23330)
*Tosin Adewumi,Lama Alkhaled,Florent Imbert,Hui Han,Nudrat Habib,Karl Löwenmark*

Main category: cs.AI

TL;DR: Fully autonomous AI is too risky; responsible human oversight is crucial.


<details>
  <summary>Details</summary>
Motivation: To argue against fully autonomous AI by highlighting its risks and the importance of responsible human oversight.

Method: Identifies three levels of autonomous AI, discusses theories of autonomy, AI, and agents, presents 12 arguments for and 6 counterarguments (with rebuttals) against fully autonomous AI, and provides 15 pieces of evidence of AI risks.

Result: The paper argues convincingly against fully autonomous AI, providing a structured analysis with supporting evidence.

Conclusion: Fully autonomous AI is risky and should not be pursued due to the potential for misaligned values and other risks, especially with the advent of ASI.

Abstract: Autonomous Artificial Intelligence (AI) has many benefits. It also has many
risks. In this work, we identify the 3 levels of autonomous AI. We are of the
position that AI must not be fully autonomous because of the many risks,
especially as artificial superintelligence (ASI) is speculated to be just
decades away. Fully autonomous AI, which can develop its own objectives, is at
level 3 and without responsible human oversight. However, responsible human
oversight is crucial for mitigating the risks. To ague for our position, we
discuss theories of autonomy, AI and agents. Then, we offer 12 distinct
arguments and 6 counterarguments with rebuttals to the counterarguments. We
also present 15 pieces of recent evidence of AI misaligned values and other
risks in the appendix.

</details>


### [10] [DSBC : Data Science task Benchmarking with Context engineering](https://arxiv.org/abs/2507.23336)
*Ram Mohan Rao Kadiyala,Siddhant Gupta,Jebish Purbey,Giulio Martini,Suman Debnath,Hamza Farooq*

Main category: cs.AI

TL;DR: 研究构建了一个评估数据科学代理的基准测试，结果显示不同LLM和方法的性能差异显著，为未来研究提供了基础。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对数据科学代理的系统性评估，本文旨在通过模拟真实用户交互，构建一个全面的基准测试来弥补这一不足。

Method: 对三种LLM在三种方法（zero-shot with context engineering, multi-step with context engineering, and with SmolAgent）下，针对八个数据科学任务类别进行评估，并分析了prompting issues（如数据泄露和模棱两可的指令）以及temperature参数的影响。

Result: 不同模型和方法的性能差异显著，突出了影响实际部署的关键因素。

Conclusion: 本文介绍了一个评估数据科学代理有效性和局限性的综合基准测试，并对三种LLM（Claude-4.0-Sonnet，Gemini-2.5-Flash和OpenAI-o4-Mini）在不同方法下的性能进行了评估，结果揭示了模型和方法之间明显的性能差异，为未来构建更强大有效的数据科学代理的研究奠定了基础。

Abstract: Recent advances in large language models (LLMs) have significantly impacted
data science workflows, giving rise to specialized data science agents designed
to automate analytical tasks. Despite rapid adoption, systematic benchmarks
evaluating the efficacy and limitations of these agents remain scarce. In this
paper, we introduce a comprehensive benchmark specifically crafted to reflect
real-world user interactions with data science agents by observing usage of our
commercial applications. We evaluate three LLMs: Claude-4.0-Sonnet,
Gemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with
context engineering, multi-step with context engineering, and with SmolAgent.
Our benchmark assesses performance across a diverse set of eight data science
task categories, additionally exploring the sensitivity of models to common
prompting issues, such as data leakage and slightly ambiguous instructions. We
further investigate the influence of temperature parameters on overall and
task-specific outcomes for each model and approach. Our findings reveal
distinct performance disparities among the evaluated models and methodologies,
highlighting critical factors that affect practical deployment. The benchmark
dataset and evaluation framework introduced herein aim to provide a foundation
for future research of more robust and effective data science agents.

</details>


### [11] [LLM4Rail: An LLM-Augmented Railway Service Consulting Platform](https://arxiv.org/abs/2507.23377)
*Zhuo Li,Xianghuai Deng,Chiwei Feng,Hanmeng Li,Shenjie Wang,Haichao Zhang,Teng Jia,Conlin Chen,Louis Linchun Wu,Jia Wang*

Main category: cs.AI

TL;DR: LLM4Rail平台利用LLM提供个性化铁路服务，包含订票、餐饮推荐等功能，并使用QTAO提示框架和CRFD-25数据集提升推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 满足日益增长的个性化铁路服务需求。

Method: 开发了LLM增强的铁路服务咨询平台LLM4Rail，提出QTAO提示框架，构建CRFD-25数据集，并引入基于LLM的零样本对话推荐系统。

Result: 开发了LLM4Rail平台，提供了定制化铁路服务模块，并有效提升了推荐系统的准确性和个性化程度。

Conclusion: LLM4Rail平台利用LLM提供定制化铁路服务模块，并提出迭代式QTAO提示框架，有效整合言语推理和面向任务的动作，生成准确的回复。为提供个性化餐饮服务，构建了CRFD-25数据集，并引入基于LLM的零样本对话推荐系统，结合特征相似性后处理步骤，确保推荐结果与数据集一致。

Abstract: Large language models (LLMs) have significantly reshaped different walks of
business. To meet the increasing demands for individualized railway service, we
develop LLM4Rail - a novel LLM-augmented railway service consulting platform.
Empowered by LLM, LLM4Rail can provide custom modules for ticketing, railway
food & drink recommendations, weather information, and chitchat. In LLM4Rail,
we propose the iterative "Question-Thought-Action-Observation (QTAO)" prompting
framework. It meticulously integrates verbal reasoning with task-oriented
actions, that is, reasoning to guide action selection, to effectively retrieve
external observations relevant to railway operation and service to generate
accurate responses. To provide personalized onboard dining services, we first
construct the Chinese Railway Food and Drink (CRFD-25) - a publicly accessible
takeout dataset tailored for railway services. CRFD-25 covers a wide range of
signature dishes categorized by cities, cuisines, age groups, and spiciness
levels. We further introduce an LLM-based zero-shot conversational recommender
for railway catering. To address the unconstrained nature of open
recommendations, the feature similarity-based post-processing step is
introduced to ensure all the recommended items are aligned with CRFD-25
dataset.

</details>


### [12] [Chatting with your ERP: A Recipe](https://arxiv.org/abs/2507.23429)
*Jorge Ruiz Gómez,Lidia Andrés Susinos,Jorge Alamo Olivé,Sonia Rey Osorno,Manuel Luis Gonzalez Hernández*

Main category: cs.AI

TL;DR: LLM代理通过双代理架构，将自然语言查询转换为可执行SQL语句，以提高与ERP系统的交互效率。


<details>
  <summary>Details</summary>
Motivation: 为了提高与工业生产级ERP系统的交互效率和便捷性。

Method: 该代理能够解释自然语言查询并将其转换为可执行的SQL语句，利用开放权重的LLM。提出了一种新颖的双代理架构，结合推理和批判阶段，以提高查询生成的可靠性。

Result: 设计并实现了一个能够将自然语言查询转换为可执行SQL语句的LLM代理，并通过双代理架构提高了查询生成的可靠性。

Conclusion: 本文介绍了一个大型语言模型(LLM)代理的设计、实现和评估，该代理与工业生产级ERP系统进行交互。

Abstract: This paper presents the design, implementation, and evaluation behind a Large
Language Model (LLM) agent that chats with an industrial production-grade ERP
system. The agent is capable of interpreting natural language queries and
translating them into executable SQL statements, leveraging open-weight LLMs. A
novel dual-agent architecture combining reasoning and critique stages was
proposed to improve query generation reliability.

</details>


### [13] [Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation](https://arxiv.org/abs/2507.23440)
*Mingzhe Li,Xin Lu,Yanyan Zhao*

Main category: cs.AI

TL;DR: Self-Foveate 使用多层次注意力机制，有效提升了 LLM 指令合成的多样性和难度。


<details>
  <summary>Details</summary>
Motivation: 现有的自动指令合成方法在指令的多样性和难度方面存在局限性，需要一种更有效的方法来合成高质量的指令数据。

Method: 提出了一种名为 "Micro-Scatter-Macro" 的多层次注意力机制，用于指导 LLM 深入挖掘非监督文本中的细粒度信息。

Result: 实验证明 Self-Foveate 方法在多个非监督语料库和不同模型架构上都取得了显著效果，优于现有方法。代码和数据已公开发布。

Conclusion: Self-Foveate 是一种创新的 LLM 驱动指令合成方法，通过多层次的注意力机制有效提高了合成指令的多样性和难度。

Abstract: Large language models (LLMs) with instruction following capabilities have
demonstrated impressive problem-solving abilities. While synthesizing
instructional data from unsupervised text has become a common approach for
training such models, conventional methods rely heavily on human effort for
data annotation. Although existing automated synthesis paradigms have
alleviated this constraint, they still exhibit significant limitations in
ensuring adequate diversity and difficulty of synthesized instructions. To
address these challenges, we propose Self-Foveate, an innovative LLM-driven
method for instruction synthesis. This approach introduces a
"Micro-Scatter-Macro" multi-level foveation methodology that effectively guides
the LLM to deeply excavate fine-grained information embedded in unsupervised
text, thereby enhancing both the diversity and difficulty of synthesized
instructions. Comprehensive experiments across multiple unsupervised corpora
and diverse model architectures validate the effectiveness and superiority of
our proposed method. We publicly release our data and codes:
https://github.com/Mubuky/Self-Foveate

</details>


### [14] [Causal Reasoning in Pieces: Modular In-Context Learning for Causal Discovery](https://arxiv.org/abs/2507.23488)
*Kacper Kadziolka,Saber Salehkaleybar*

Main category: cs.AI

TL;DR: 先进推理模型结合精心设计的上下文框架，显著提升了因果发现能力。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型中因果推理的根本性挑战，以及现有模型在因果发现任务中容易过拟合和性能不稳定的问题。

Method: 使用Corr2Cause基准，研究了OpenAI的o系列和DeepSeek-R模型家族，并引入了一种模块化的上下文管道。

Result: 推理优先架构比以往方法取得了更大的进步，模块化上下文管道使性能提升近三倍。

Conclusion: 先进推理模型在因果发现方面取得了显著进展，但需要精心设计的上下文框架来最大化其能力。

Abstract: Causal inference remains a fundamental challenge for large language models.
Recent advances in internal reasoning with large language models have sparked
interest in whether state-of-the-art reasoning models can robustly perform
causal discovery-a task where conventional models often suffer from severe
overfitting and near-random performance under data perturbations. We study
causal discovery on the Corr2Cause benchmark using the emergent OpenAI's
o-series and DeepSeek-R model families and find that these reasoning-first
architectures achieve significantly greater native gains than prior approaches.
To capitalize on these strengths, we introduce a modular in-context pipeline
inspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies, yielding
nearly three-fold improvements over conventional baselines. We further probe
the pipeline's impact by analyzing reasoning chain length, complexity, and
conducting qualitative and quantitative comparisons between conventional and
reasoning models. Our findings suggest that while advanced reasoning models
represent a substantial leap forward, carefully structured in-context
frameworks are essential to maximize their capabilities and offer a
generalizable blueprint for causal discovery across diverse domains.

</details>


### [15] [Causal Identification of Sufficient, Contrastive and Complete Feature Sets in Image Classification](https://arxiv.org/abs/2507.23497)
*David A Kelly,Hana Chockler*

Main category: cs.AI

TL;DR: 提出一种新的基于因果关系的图像分类器解释方法，该方法形式上严格、可计算，并且是完全黑盒的，平均每张图片解释时间为6秒。


<details>
  <summary>Details</summary>
Motivation: 现有的图像分类器解释算法缺乏形式上的严格性，而逻辑解释方法的计算依赖于对模型的严格假设，这在图像分类器中并不成立。

Method: 提出了一种基于因果关系的图像分类器解释方法，该方法具有形式上的严格性和可计算性，并且不需要了解模型内部细节。

Result: 实现了因果解释的定义，并通过实验验证了不同模型具有不同的充分性、对比性和完整性模式。算法平均每张图片计算所有类型的解释需要6秒，并且是完全黑盒的。

Conclusion: 本文证明了因果解释在形式上与逻辑解释一样严格，同时适用于黑盒算法和图像分类器，并引入了对比因果解释和完整因果解释的概念。实验结果表明不同模型具有不同的充分性、对比性和完整性模式。

Abstract: Existing algorithms for explaining the outputs of image classifiers are based
on a variety of approaches and produce explanations that lack formal rigor. On
the other hand, logic-based explanations are formally and rigorously defined
but their computability relies on strict assumptions about the model that do
not hold on image classifiers.
  In this paper, we show that causal explanations, in addition to being
formally and rigorously defined, enjoy the same formal properties as
logic-based ones, while still lending themselves to black-box algorithms and
being a natural fit for image classifiers. We prove formal properties of causal
explanations and introduce contrastive causal explanations for image
classifiers. Moreover, we augment the definition of explanation with confidence
awareness and introduce complete causal explanations: explanations that are
classified with exactly the same confidence as the original image.
  We implement our definitions, and our experimental results demonstrate that
different models have different patterns of sufficiency, contrastiveness, and
completeness. Our algorithms are efficiently computable, taking on average 6s
per image on a ResNet50 model to compute all types of explanations, and are
totally black-box, needing no knowledge of the model, no access to model
internals, no access to gradient, nor requiring any properties, such as
monotonicity, of the model.

</details>


### [16] [DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer](https://arxiv.org/abs/2507.23554)
*Ruoyu Wang,Junda Wu,Yu Xia,Tong Yu,Ryan A. Rossi,Julian McAuley,Lina Yao*

Main category: cs.AI

TL;DR: DICE框架通过因果分解演示知识，选择最相关的示例，从而提高LLM智能体性能，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有的基于ICL的LLM智能体对演示选择高度敏感，次优示例会导致性能不稳定或下降。现有方法通常依赖启发式或特定任务设计，缺乏普遍适用的有效演示标准。

Method: 提出了一种名为DICE的动态上下文示例选择方法，该方法基于因果关系将演示知识分解为可转移和不可转移部分，并提出了一种逐步选择标准，具有提升智能体性能的理论保证。

Result: 实验证明了DICE方法的有效性和通用性，强调了在鲁棒和高效的LLM智能体中，基于原则的上下文感知演示选择的重要性。

Conclusion: 本文提出了DICE，一个理论上有根据的基于因果关系的ICL框架，用于选择最相关的演示，从而提高LLM智能体的性能。DICE能够分解演示知识，识别并去除有害的依赖关系，并保证性能提升，且无需额外训练成本。

Abstract: Large language model-based agents, empowered by in-context learning (ICL),
have demonstrated strong capabilities in complex reasoning and tool-use tasks.
However, existing works have shown that the effectiveness of ICL is highly
sensitive to the choice of demonstrations, with suboptimal examples often
leading to unstable or degraded performance. While prior work has explored
example selection, including in some agentic or multi-step settings, existing
approaches typically rely on heuristics or task-specific designs and lack a
general, theoretically grounded criterion for what constitutes an effective
demonstration across reasoning steps. Therefore, it is non-trivial to develop a
principled, general-purpose method for selecting demonstrations that
consistently benefit agent performance. In this paper, we address this
challenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a
theoretically grounded ICL framework for agentic tasks that selects the most
relevant demonstrations at each step of reasoning. Our approach decomposes
demonstration knowledge into transferable and non-transferable components
through a causal lens, showing how the latter can introduce spurious
dependencies that impair generalization. We further propose a stepwise
selection criterion with a formal guarantee of improved agent performance.
Importantly, DICE is a general, framework-agnostic solution that can be
integrated as a plug-in module into existing agentic frameworks without any
additional training cost. Extensive experiments across diverse domains
demonstrate our method's effectiveness and generality, highlighting the
importance of principled, context-aware demo selection for robust and efficient
LLM agents.

</details>
