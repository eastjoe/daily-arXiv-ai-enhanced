<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: 自我进化AI智能体STELLA在生物医学基准测试中表现优异，且性能随经验提升，具有显著的应用前景。


<details>
  <summary>Details</summary>
Motivation: 生物医学数据、工具和文献的快速增长导致研究领域碎片化，AI智能体通常依赖静态工具集，限制了其适应性和扩展性。

Method: 多智能体架构，包含不断发展的模板库和动态工具库，工具创建代理自动发现和集成新的生物信息学工具。

Result: 在生物医学基准测试中取得了最先进的准确率：Humanity's Last Exam (约26%)，LAB-Bench: DBQA (54%)，LAB-Bench: LitQA (63%)，优于领先模型6个百分点，且性能随经验系统性提升。

Conclusion: STELLA，一个自我进化的AI智能体，在生物医学基准测试中取得了最先进的准确率，并随着经验的积累性能持续提升，展现了AI智能体学习和发展、动态扩展专业知识以加速生物医学发现的巨大潜力。

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [2] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: HCVR:一种新的轻量级特征选择方法，在SPAMBASE数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了消除冗余特征并保留相关特征，提出了一种新的特征选择方法。

Method: 提出了一种名为HCVR的轻量级基于规则的特征选择方法，该方法结合了参数间和参数到目标的相关性，通过多数投票的策略进行特征选择。

Result: HCVR方法在SPAMBASE数据集上的实验结果表明，其性能优于传统的非迭代方法（CFS、mRMR和MI）和迭代方法（RFE、SFS和遗传算法）。

Conclusion: HCVR方法在SPAMBASE数据集上表现出优于传统迭代和非迭代特征选择方法的性能提升。

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [3] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: 综述了大型语言模型高效推理的测试时计算策略，并指出了未来的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在推理方面效率低下，计算资源利用不合理。

Method: 对现有的大型语言模型进行基准测试，比较推理性能和token使用率。

Result: 该综述强调了测试时计算方法的实用性、适应性和可扩展性，并讨论了混合思维模型等新兴趋势。

Conclusion: 这篇论文综述了大型语言模型高效推理的测试时计算策略，并提出了一个双层分类法，区分固定计算预算和动态缩放推理的方法。

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [4] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: SciGym基准测试显示，LLM在科学实验设计和分析方面仍有很大改进空间，尤其是在处理复杂系统时。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在科学实验设计和结果分析方面的能力。

Method: 创建了一个名为SciGym的基准，用于评估LLM在开放式科学发现任务中的迭代实验设计和分析能力。SciGym使用系统生物学标记语言编码的生物系统模型进行模拟实验，避免了湿实验的高昂成本。

Result: 评估了六个LLM在137个小型系统上的表现，结果表明，虽然能力更强的模型表现更好，但所有模型的性能都随着系统复杂性的增加而显著下降。

Conclusion: 大型语言模型(LLM)在科学实验设计和结果解释方面的能力仍有待提高，尤其是在系统复杂性增加的情况下。

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [5] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: AI能否从神经科学学习以实现持续适应？本文探讨了神经科学与AI持续学习的交叉，并提出了相应的未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型训练成本高昂且缓慢，动物却能快速适应环境变化，本文旨在探索神经科学如何改进AI的持续学习能力。

Method: 整合AI的持续学习和情境学习文献以及神经科学中关于学习行为任务的文献。

Result: 提出了一份研究议程，阐述神经科学如何改进AI，以及AI如何反过来促进神经科学发展，促进NeuroAI领域发展。

Conclusion: 本文探讨了AI能否从神经科学学习，以应对现实世界中不断变化的环境和任务。

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [6] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: 审计数据显示，平衡基准比率并不能完全消除算法歧视，基于个体处理效应估计的干预方法能更好地减少歧视。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能系统中存在的偏差问题，特别是自动化决策中的偏差问题，并提高自动化招聘算法的公平性。

Method: 使用审计研究数据训练和评估自动化招聘算法，并提出基于个体处理效应估计方法的干预措施。

Result: 发现常用的公平性干预方法在传统度量下可能掩盖实际的歧视，并提出了一种新的干预方法，可以进一步减少算法歧视。

Conclusion: 本文研究了如何利用审计研究数据改进自动化招聘算法的训练和评估，发现常用的公平性干预方法（平衡基准比率）在传统度量下看似实现了平等，但在适当的度量下仍存在约10%的差距，并提出基于个体处理效应估计方法的干预措施以进一步减少算法歧视。

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [7] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: 数据多样化策略，特别是DTS方法，能有效提升LLM的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过数据多样化策略改进偏好优化，从而提升大型语言模型的数学推理能力。

Method: 评估三种数据生成方法：温度采样、思维链提示和蒙特卡洛树搜索（MCTS），并提出了一种新的结构化方法Diversified-ThinkSolve (DTS)。

Result: DTS方法在GSM8K和MATH数据集上分别取得了7.1%和4.2%的提升，计算开销仅为基线的1.03倍，而MCTS的计算开销接近五倍且收益较低。

Conclusion: 策略性多样化偏好数据能显著提高大型语言模型的数学推理性能，其中DTS方法在GSM8K和MATH数据集上分别取得了7.1%和4.2%的提升，且计算开销仅略微增加。

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [8] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: LLM在角色扮演中言行不一，需谨慎使用。


<details>
  <summary>Details</summary>
Motivation: 确保LLM作为角色扮演代理生成的合成数据与角色设定保持一致，以提高其在人类行为研究中的可靠性。

Method: 建立了一个评估框架来衡量LLM的信念与其角色扮演模拟结果之间的一致性，研究了不同因素（例如信念类型、信息呈现方式、预测时间范围）的影响，并探讨了如何调整LLM的信念以符合研究目标。

Result: 发现LLM的陈述信念与其角色扮演模拟结果之间存在系统性差异，无论在个体还是群体层面。即使模型似乎编码了合理的信念，也可能无法始终如一地应用它们。

Conclusion: LLM扮演角色生成合成数据用于人类行为研究中，其输出与角色设定的一致性至关重要。研究发现，LLM在角色扮演中表达的信念与其实际行为存在系统性差异，即使模型编码了似是而非的信念，也可能无法以一致的方式应用它们。

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [9] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: 利用多智能体Q学习算法研究空间囚徒困境中稀释和移动性的影响，发现固定更新规则与学习更新规则的游戏性质上可能等同，且多种行动下种群间会形成共生互利效应。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明，静态智能体可以通过多种机制（包括噪声注入、不同类型的学习算法和邻居收益知识）学习合作。本研究旨在使用独立多智能体Q学习算法，研究稀释和移动性对空间囚徒困境的影响，并探索该算法在模拟不同博弈论场景和基准测试方面的潜力。

Method: 使用独立多智能体Q学习算法研究了空间囚徒困境博弈中稀释和移动性的影响。

Result: 观察到一系列影响，包括固定更新规则的游戏可能与学习更新规则的游戏在性质上等同，以及在定义多种行动时，种群之间会形成共生互利效应。

Conclusion: 研究发现，具有固定更新规则的游戏在性质上可能等同于具有学习更新规则的游戏，并且在定义多种行动时，种群之间会形成共生互利效应。

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [10] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: NL2FLOW 系统自动生成规划问题并评估LLM规划能力，实验表明直接推理更高效，最好模型有效/最优计划生成成功率达86%/69%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 的规划和推理能力提升受限于可扩展、可靠数据生成和评估的瓶颈。

Method: 介绍了 NL2FLOW 系统，用于自动生成规划问题（自然语言、结构化中间表示和形式化 PDDL），并严格评估生成的计划质量。利用该系统生成了一个包含 2296 个问题的自动化工作流生成领域数据集，并评估了多个开源的指令微调 LLM。

Result: 最好的模型在生成有效计划和最优计划方面分别达到了 86% 和 69% 的成功率。直接从自然语言推理到行动可能比引入中间转换步骤更高效。

Conclusion: 该论文介绍了 NL2FLOW 系统，用于自动生成规划问题并评估大型语言模型 (LLM) 的规划能力。实验结果表明，最好的模型在生成有效计划和最优计划方面分别达到了 86% 和 69% 的成功率，并发现直接从自然语言推理到行动可能比引入中间转换步骤更高效。

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [11] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 信仰修正研究应关注机制的能力而非限制，例如达到各种信仰状态的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的信仰修正研究过于关注假设，忽略了修正机制的实际能力。

Method: 分析现有信仰修正方法的优缺点，并提出新的研究方向。

Result: 识别出信仰修正机制的多种能力（例如遗忘性、纠正性、可学习性等），并指出不同机制拥有不同的能力组合。

Conclusion: 本文批判了信仰修正领域过多关注假设而缺乏对现有方法的分析，并提出应关注修正机制的可能性而非限制性，例如是否能达到特定信仰状态（例如：全能、平等、教条式）等。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [12] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: OMS框架解决了现有LLM方法在关键词生成中的局限性，实现了无需训练数据、多目标优化和自主质量评估的关键词生成。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的关键词生成方法存在三个主要局限性：依赖大规模查询-关键词对数据、缺乏在线多目标性能监控和优化、以及关键词选择的质量控制薄弱。

Method: 提出了一种名为OMS的关键词生成框架，该框架无需训练数据、可在线监控性能并据此调整、多目标优化、并能够自主评估关键词质量。

Result: 实验结果表明，OMS框架优于现有方法。

Conclusion: OMS框架在基准测试和真实广告活动中均优于现有方法，消融实验和人工评估证实了每个组件的有效性和生成的关键词质量。

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [13] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: AI原生自主实验室实现复杂科学实验自动化，显著提高效率，并支持多种应用。


<details>
  <summary>Details</summary>
Motivation: 实现能够独立进行复杂实验并服务于非专业人员的自主科学研究，需要人工智能驱动下的一次根本性范式转变。

Method: 该系统采用模型、实验和仪器的协同设计理念，自主管理仪器、制定实验流程和优化策略，同时处理多个用户请求，支持核酸合成、转录、扩增和测序等多种功能。

Result: 该平台成功构建了一个端到端的、多用户自主实验室，能够处理生物分子工程等领域高度复杂的科学实验，并达到最先进水平。

Conclusion: 该AI原生自主实验室支持复杂的、多目标的实验，在多种仪器上运行，无需人工干预即可实现与人类科学家相同的实验性能，并显著提高仪器利用率和实验效率，为科学即服务（Science-as-a-Service）奠定了基础。

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [14] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: Uses category theory to make multiple linear regression more interpretable, resulting in a clearer understanding of parameter and residual relationships through the Gauss-Markov Adjunction.


<details>
  <summary>Details</summary>
Motivation: To improve the explainability and social implementation of AI by enhancing the intelligibility and interpretability of machine learning models.

Method: The paper uses category theory to model the relationship between parameters and data in multiple linear regression, defining categories for parameters and data and an adjoint pair of functors between them.

Result: A categorical framework for supervised learning is developed, formalizing the interplay between residuals and parameters. The Gauss-Markov Adjunction is identified as capturing the core structure, enabling a clearer understanding of information flow.

Conclusion: This paper proposes a categorical formulation of supervised learning, specifically multiple linear regression, to enhance its intelligibility and interpretability.  The Gauss-Markov Adjunction captures the essential structure, clarifying the relationship between parameters and residuals.

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [15] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: 改进任务清晰度能显著提升大型语言模型的推理能力，尤其在Coq定理证明方面。


<details>
  <summary>Details</summary>
Motivation: 研究如何改进大型语言模型的任务清晰度以增强其推理能力。

Method: 提出了一种概念层面的度量方法评估任务清晰度，并使用选择性概念展开来丰富任务描述，采用规划器-执行器架构。

Result: 在1386个Coq定理上，与现有技术相比，该方法将证明成功率提高了2.1倍，清晰度评分提高了1.85倍。

Conclusion: 提高大型语言模型任务清晰度可以增强其推理能力，尤其在Coq定理证明方面。结构化语义上下文能显著提高任务清晰度评分和证明成功率，小型模型微调后效果更佳。

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [16] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: 研究强调了在推进自动机器学习中，应同时考虑搜索策略、算子设计和评估方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 提高AI代理在MLE-bench上的性能，MLE-bench是一个具有挑战性的基准，代理在Kaggle竞赛中竞争以解决现实世界的机器学习问题。

Method: 将AI研究代理形式化为在候选方案空间中导航的搜索策略，迭代地使用算子修改它们。

Result: 最佳的搜索策略和算子集组合在MLE-bench lite上取得了最先进的结果，将获得Kaggle奖牌的成功率从39.6%提高到47.7%。

Conclusion: AI研究代理在自动设计、实现和训练机器学习模型方面显示出巨大的潜力，可以加速科学进步。研究人员通过设计和系统地改变不同的算子集和搜索策略（贪婪、MCTS、进化），证明了算子集和搜索策略的相互作用对于实现高性能至关重要。

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>
